{"\u751f\u6210\u6a21\u578b": {}, "\u591a\u6a21\u6001": {}, "Nerf": {}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.04252": "|**2024-02-06**|**EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters**|EVA-CLIP-18B\uff1a\u5c06 CLIP \u6269\u5c55\u5230 180 \u4ebf\u4e2a\u53c2\u6570|Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Xinlong Wang|Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and multimodal models. We present EVA-CLIP-18B, the largest and most powerful open-source CLIP model to date, with 18-billion parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized image classification benchmarks, outperforming its forerunner EVA-CLIP (5-billion parameters) and other open-source CLIP models by a large margin. Remarkably, we observe a consistent performance improvement with the model size scaling of EVA-CLIP, despite maintaining a constant training dataset of 2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B) employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the potential of EVA-style weak-to-strong visual model scaling. With our model weights made publicly available, we hope to facilitate future research in vision and multimodal foundation models.|\u6269\u5927\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u5bf9\u4e8e\u589e\u5f3a\u89c6\u89c9\u548c\u591a\u6a21\u6001\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63a8\u51fa\u4e86 EVA-CLIP-18B\uff0c\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u3001\u6700\u5f3a\u5927\u7684\u5f00\u6e90 CLIP \u6a21\u578b\uff0c\u62e5\u6709 180 \u4ebf\u4e2a\u53c2\u6570\u3002\u4ec5\u4f7f\u7528 60 \u4ebf\u4e2a\u8bad\u7ec3\u6837\u672c\uff0cEVA-CLIP-18B \u5728 27 \u4e2a\u5e7f\u6cdb\u8ba4\u53ef\u7684\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u4e0a\u5e73\u5747\u5b9e\u73b0\u4e86 80.7% \u7684\u51fa\u8272\u96f6\u6837\u672c top-1 \u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5176\u5148\u9a71 EVA-CLIP\uff0850 \u4ebf\u4e2a\u53c2\u6570\uff09\u548c\u5176\u4ed6\u5f00\u653e\u6a21\u578b-\u6765\u6e90 CLIP \u6a21\u578b\u6709\u5f88\u5927\u4f18\u52bf\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5c3d\u7ba1\u4fdd\u6301\u4e86\u6765\u81ea LAION-2B \u548c COYO-700M \u7684 20 \u4ebf\u56fe\u50cf\u6587\u672c\u5bf9\u7684\u6052\u5b9a\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4f46\u6211\u4eec\u89c2\u5bdf\u5230 EVA-CLIP \u7684\u6a21\u578b\u5927\u5c0f\u7f29\u653e\u5e26\u6765\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u6539\u8fdb\u3002\u8be5\u6570\u636e\u96c6\u662f\u516c\u5f00\u53ef\u7528\u7684\uff0c\u5e76\u4e14\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u7684 CLIP \u6a21\u578b\u4e2d\u4f7f\u7528\u7684\u5185\u90e8\u6570\u636e\u96c6\uff08\u4f8b\u5982 DFN-5B\u3001WebLI-10B\uff09\u5c0f\u5f97\u591a\u3002 EVA-CLIP-18B \u5c55\u793a\u4e86 EVA \u5f0f\u4ece\u5f31\u5230\u5f3a\u7684\u89c6\u89c9\u6a21\u578b\u7f29\u653e\u7684\u6f5c\u529b\u3002\u901a\u8fc7\u516c\u5f00\u6211\u4eec\u7684\u6a21\u578b\u6743\u91cd\uff0c\u6211\u4eec\u5e0c\u671b\u4fc3\u8fdb\u89c6\u89c9\u548c\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u672a\u6765\u7814\u7a76\u3002|[2402.04252v1](http://arxiv.org/pdf/2402.04252v1)|null|\n", "2402.04178": "|**2024-02-06**|**SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models**|SHIELD\uff1a\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4eba\u8138\u6b3a\u9a97\u548c\u4f2a\u9020\u68c0\u6d4b\u7684\u8bc4\u4f30\u57fa\u51c6|Yichen Shi, Yuhao Gao, Yingxin Lai, Hongyang Wang, Jun Feng, Lei He, Jun Wan, Changsheng Chen, Zitong Yu, Xiaochun Cao|Multimodal large language models (MLLMs) have demonstrated remarkable problem-solving capabilities in various vision fields (e.g., generic object recognition and grounding) based on strong visual semantic representation and language reasoning ability. However, whether MLLMs are sensitive to subtle visual spoof/forged clues and how they perform in the domain of face attack detection (e.g., face spoofing and forgery detection) is still unexplored. In this paper, we introduce a new benchmark, namely SHIELD, to evaluate the ability of MLLMs on face spoofing and forgery detection. Specifically, we design true/false and multiple-choice questions to evaluate multimodal face data in these two face security tasks. For the face anti-spoofing task, we evaluate three different modalities (i.e., RGB, infrared, depth) under four types of presentation attacks (i.e., print attack, replay attack, rigid mask, paper mask). For the face forgery detection task, we evaluate GAN-based and diffusion-based data with both visual and acoustic modalities. Each question is subjected to both zero-shot and few-shot tests under standard and chain of thought (COT) settings. The results indicate that MLLMs hold substantial potential in the face security domain, offering advantages over traditional specific models in terms of interpretability, multimodal flexible reasoning, and joint face spoof and forgery detection. Additionally, we develop a novel Multi-Attribute Chain of Thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images, which provides rich task-related knowledge for subtle spoof/forged clue mining. Extensive experiments in separate face anti-spoofing, separate face forgery detection, and joint detection tasks demonstrate the effectiveness of the proposed MA-COT. The project is available at https$:$//github.com/laiyingxin2/SHIELD|\u57fa\u4e8e\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u4e49\u8868\u793a\u548c\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5404\u79cd\u89c6\u89c9\u9886\u57df\uff08\u4f8b\u5982\u901a\u7528\u5bf9\u8c61\u8bc6\u522b\u548c\u57fa\u7840\uff09\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002\u7136\u800c\uff0cMLLM \u662f\u5426\u5bf9\u5fae\u5999\u7684\u89c6\u89c9\u6b3a\u9a97/\u4f2a\u9020\u7ebf\u7d22\u654f\u611f\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u4eba\u8138\u653b\u51fb\u68c0\u6d4b\uff08\u4f8b\u5982\uff0c\u4eba\u8138\u6b3a\u9a97\u548c\u4f2a\u9020\u68c0\u6d4b\uff09\u9886\u57df\u7684\u8868\u73b0\u5982\u4f55\uff0c\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u5373 SHIELD\uff0c\u6765\u8bc4\u4f30 MLLM \u5728\u4eba\u8138\u6b3a\u9a97\u548c\u4f2a\u9020\u68c0\u6d4b\u65b9\u9762\u7684\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u771f/\u5047\u548c\u591a\u9879\u9009\u62e9\u9898\u6765\u8bc4\u4f30\u8fd9\u4e24\u4e2a\u4eba\u8138\u5b89\u5168\u4efb\u52a1\u4e2d\u7684\u591a\u6a21\u6001\u4eba\u8138\u6570\u636e\u3002\u5bf9\u4e8e\u9762\u90e8\u53cd\u6b3a\u9a97\u4efb\u52a1\uff0c\u6211\u4eec\u5728\u56db\u79cd\u7c7b\u578b\u7684\u6f14\u793a\u653b\u51fb\uff08\u5373\u6253\u5370\u653b\u51fb\u3001\u91cd\u653e\u653b\u51fb\u3001\u521a\u6027\u63a9\u6a21\u3001\u7eb8\u63a9\u6a21\uff09\u4e0b\u8bc4\u4f30\u4e09\u79cd\u4e0d\u540c\u7684\u6a21\u5f0f\uff08\u5373 RGB\u3001\u7ea2\u5916\u3001\u6df1\u5ea6\uff09\u3002\u5bf9\u4e8e\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u4efb\u52a1\uff0c\u6211\u4eec\u4f7f\u7528\u89c6\u89c9\u548c\u58f0\u5b66\u65b9\u5f0f\u8bc4\u4f30\u57fa\u4e8e GAN \u548c\u57fa\u4e8e\u6269\u6563\u7684\u6570\u636e\u3002\u6bcf\u4e2a\u95ee\u9898\u90fd\u5728\u6807\u51c6\u548c\u601d\u7ef4\u94fe (COT) \u8bbe\u7f6e\u4e0b\u8fdb\u884c\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0cMLLM \u5728\u4eba\u8138\u5b89\u5168\u9886\u57df\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u5728\u53ef\u89e3\u91ca\u6027\u3001\u591a\u6a21\u6001\u7075\u6d3b\u63a8\u7406\u4ee5\u53ca\u8054\u5408\u4eba\u8138\u6b3a\u9a97\u548c\u4f2a\u9020\u68c0\u6d4b\u65b9\u9762\u6bd4\u4f20\u7edf\u7279\u5b9a\u6a21\u578b\u5177\u6709\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c5e\u6027\u601d\u60f3\u94fe\uff08MA-COT\uff09\u8303\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u548c\u5224\u65ad\u4eba\u8138\u56fe\u50cf\u7684\u5404\u79cd\u7279\u5b9a\u4e8e\u4efb\u52a1\u548c\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u5c5e\u6027\uff0c\u4e3a\u5fae\u5999\u7684\u6b3a\u9a97/\u4f2a\u9020\u7ebf\u7d22\u6316\u6398\u63d0\u4f9b\u4e30\u5bcc\u7684\u4efb\u52a1\u76f8\u5173\u77e5\u8bc6\u3002\u5728\u5355\u72ec\u7684\u4eba\u8138\u53cd\u6b3a\u9a97\u3001\u5355\u72ec\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u548c\u8054\u5408\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 MA-COT \u7684\u6709\u6548\u6027\u3002\u8be5\u9879\u76ee\u4f4d\u4e8ehttps$:$//github.com/laiyingxin2/SHIELD|[2402.04178v1](http://arxiv.org/pdf/2402.04178v1)|null|\n", "2402.04087": "|**2024-02-06**|**A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation**|\u65e0\u9700\u57f9\u8bad\u3001\u57fa\u4e8e CLIP \u7684\u9002\u5e94\u7684\u96be\u4ee5\u8d85\u8d8a\u7684\u57fa\u7ebf|Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, Tieniu Tan|Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization. In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches. Our code is publicly available at \\url{https://github.com/mrflogs/ICLR24}.|\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u56e0\u5176\u5353\u8d8a\u7684\u96f6\u6837\u672c\u80fd\u529b\u800c\u53d7\u5230\u6b22\u8fce\u3002\u6700\u8fd1\u7684\u7814\u7a76\u91cd\u70b9\u662f\u5f00\u53d1\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f8b\u5982\u5373\u65f6\u5b66\u4e60\u548c\u9002\u914d\u5668\uff0c\u4ee5\u589e\u5f3a CLIP \u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4ecd\u7136\u9700\u8981\u989d\u5916\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd9\u5bf9\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u8bbe\u5907\u6765\u8bf4\u662f\u4e0d\u53ef\u53d6\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u56de\u987e\u4e86\u7ecf\u5178\u7b97\u6cd5\u9ad8\u65af\u5224\u522b\u5206\u6790\uff08GDA\uff09\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e CLIP \u7684\u4e0b\u6e38\u5206\u7c7b\u3002\u901a\u5e38\uff0cGDA \u5047\u8bbe\u6bcf\u4e2a\u7c7b\u522b\u7684\u7279\u5f81\u9075\u5faa\u5177\u6709\u76f8\u540c\u534f\u65b9\u5dee\u7684\u9ad8\u65af\u5206\u5e03\u3002\u901a\u8fc7\u5229\u7528\u8d1d\u53f6\u65af\u516c\u5f0f\uff0c\u5206\u7c7b\u5668\u53ef\u4ee5\u7528\u7c7b\u5747\u503c\u548c\u534f\u65b9\u5dee\u6765\u8868\u793a\uff0c\u53ef\u4ee5\u6839\u636e\u6570\u636e\u8fdb\u884c\u4f30\u8ba1\uff0c\u800c\u65e0\u9700\u8bad\u7ec3\u3002\u4e3a\u4e86\u6574\u5408\u6765\u81ea\u89c6\u89c9\u548c\u6587\u672c\u6a21\u5f0f\u7684\u77e5\u8bc6\uff0c\u6211\u4eec\u5c06\u5176\u4e0e CLIP \u4e2d\u539f\u59cb\u7684\u96f6\u6837\u672c\u5206\u7c7b\u5668\u96c6\u6210\u3002 17 \u4e2a\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5c11\u6570\u6837\u672c\u5206\u7c7b\u3001\u4e0d\u5e73\u8861\u5b66\u4e60\u548c\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u8d85\u8d8a\u6216\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u6269\u5c55\u5230\u65b0\u7684\u6cdb\u5316\u548c\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u518d\u6b21\u8bc1\u660e\u4e86\u5176\u76f8\u5bf9\u4e8e\u7ade\u4e89\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/mrflogs/ICLR24} \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2402.04087v1](http://arxiv.org/pdf/2402.04087v1)|null|\n", "2402.04064": "|**2024-02-06**|**Multi-class Road Defect Detection and Segmentation using Spatial and Channel-wise Attention for Autonomous Road Repairing**|\u4f7f\u7528\u7a7a\u95f4\u548c\u901a\u9053\u6ce8\u610f\u8fdb\u884c\u591a\u7c7b\u9053\u8def\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u5272\u4ee5\u8fdb\u884c\u81ea\u4e3b\u9053\u8def\u4fee\u590d|Jongmin Yu, Chen Bene Chi, Sebastiano Fichera, Paolo Paoletti, Devansh Mehta, Shan Luo|Road pavement detection and segmentation are critical for developing autonomous road repair systems. However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes. We propose a novel end-to-end method for multi-class road defect detection and segmentation. The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions. Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned. To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes. The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods.|\u9053\u8def\u8def\u9762\u68c0\u6d4b\u548c\u5206\u5272\u5bf9\u4e8e\u5f00\u53d1\u81ea\u52a8\u9053\u8def\u4fee\u590d\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u9053\u8def\u8def\u9762\u56fe\u50cf\u7684\u7eb9\u7406\u7b80\u5355\u6027\u3001\u7f3a\u9677\u51e0\u4f55\u5f62\u72b6\u7684\u591a\u6837\u6027\u4ee5\u53ca\u7c7b\u4e4b\u95f4\u7684\u5f62\u6001\u6a21\u7cca\u6027\uff0c\u5f00\u53d1\u540c\u65f6\u6267\u884c\u591a\u7c7b\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u5272\u7684\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u7c7b\u9053\u8def\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u5272\u7684\u65b0\u9896\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5305\u62ec\u591a\u4e2a\u7a7a\u95f4\u548c\u901a\u9053\u65b9\u5411\u7684\u6ce8\u610f\u529b\u5757\uff0c\u53ef\u7528\u4e8e\u5b66\u4e60\u8de8\u7a7a\u95f4\u548c\u901a\u9053\u65b9\u5411\u7ef4\u5ea6\u7684\u5168\u5c40\u8868\u793a\u3002\u901a\u8fc7\u8fd9\u4e9b\u6ce8\u610f\u529b\u5757\uff0c\u53ef\u4ee5\u5b66\u4e60\u9053\u8def\u7f3a\u9677\u7684\u5f62\u6001\u4fe1\u606f\uff08\u7a7a\u95f4\u7279\u5f81\uff09\u4ee5\u53ca\u56fe\u50cf\u7684\u989c\u8272\u548c\u6df1\u5ea6\u4fe1\u606f\u7684\u66f4\u5168\u5c40\u901a\u7528\u7684\u8868\u793a\u3002\u4e3a\u4e86\u8bc1\u660e\u6211\u4eec\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5bf9\u65b0\u6536\u96c6\u7684\u5e26\u6709\u4e5d\u4e2a\u9053\u8def\u7f3a\u9677\u7c7b\u522b\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5404\u79cd\u6d88\u878d\u7814\u7a76\u5e76\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u7c7b\u9053\u8def\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u5272\u65b9\u6cd5\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002|[2402.04064v1](http://arxiv.org/pdf/2402.04064v1)|null|\n", "2402.04050": "|**2024-02-06**|**Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models**|\u8fde\u63a5\u70b9\uff1a\u9ed1\u76d2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u534f\u4f5c\u5fae\u8c03|Zhengbo Wang, Jian Liang, Ran He, Zilei Wang, Tieniu Tan|With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \\textbf{C}ollabo\\textbf{ra}tive \\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algorithm. Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT. The results show that CraFT achieves a decent gain of about 12\\% with 16-shot datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62\\% compared to the white-box method.|\u968f\u7740\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u51fa\u73b0\uff0c\u4eba\u4eec\u6295\u5165\u4e86\u5927\u91cf\u7684\u7cbe\u529b\u6765\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\u3002\u5c3d\u7ba1\u5728\u8bbe\u8ba1\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u6b64\u7c7b\u65b9\u6cd5\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u7684\u53c2\u6570\uff0c\u8fd9\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6a21\u578b\u6240\u6709\u8005\u901a\u5e38\u9009\u62e9\u5c06\u5176\u6a21\u578b\u4f5c\u4e3a\u9ed1\u5323\u5b50\u63d0\u4f9b\u4ee5\u4fdd\u62a4\u6a21\u578b\u6240\u6709\u6743\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd \\textbf{C}ollabo\\textbf{ra}tive \\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) \u65b9\u6cd5\uff0c\u7528\u4e8e\u5fae\u8c03\u9ed1\u76d2 VLM \u5230\u4e0b\u6e38\u4efb\u52a1\uff0c\u5176\u4e2d\u4eba\u4eec\u53ea\u80fd\u8bbf\u95ee\u6a21\u578b\u7684\u8f93\u5165\u63d0\u793a\u548c\u8f93\u51fa\u9884\u6d4b\u3002 CraFT \u5305\u62ec\u4e24\u4e2a\u6a21\u5757\uff0c\u4e00\u4e2a\u7528\u4e8e\u5b66\u4e60\u6587\u672c\u63d0\u793a\u7684\u63d0\u793a\u751f\u6210\u6a21\u5757\u548c\u4e00\u4e2a\u7528\u4e8e\u589e\u5f3a\u6b8b\u5dee\u6837\u5f0f\u8f93\u51fa\u9884\u6d4b\u7684\u9884\u6d4b\u7ec6\u5316\u6a21\u5757\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8f85\u52a9\u9884\u6d4b\u4e00\u81f4\u635f\u5931\u6765\u4fc3\u8fdb\u8fd9\u4e9b\u6a21\u5757\u4e4b\u95f4\u7684\u4e00\u81f4\u4f18\u5316\u3002\u8fd9\u4e9b\u6a21\u5757\u901a\u8fc7\u65b0\u9896\u7684\u534f\u4f5c\u8bad\u7ec3\u7b97\u6cd5\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5bf9\u8d85\u8fc7 15 \u4e2a\u6570\u636e\u96c6\u7684\u5c11\u91cf\u6837\u672c\u5206\u7c7b\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86 CraFT \u7684\u4f18\u8d8a\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cCraFT \u5728 16 \u4e2a\u955c\u5934\u7684\u6570\u636e\u96c6\u548c\u4ec5 8,000 \u4e2a\u67e5\u8be2\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u7ea6 12% \u7684\u4e0d\u9519\u7684\u589e\u76ca\u3002\u6b64\u5916\uff0cCraFT \u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\uff0c\u90e8\u7f72\u65f6\u4ec5\u4f7f\u7528\u7ea6 1/80 \u7684\u5185\u5b58\u5360\u7528\uff0c\u4e0e\u767d\u76d2\u65b9\u6cd5\u76f8\u6bd4\u4ec5\u727a\u7272 1.62%\u3002|[2402.04050v1](http://arxiv.org/pdf/2402.04050v1)|null|\n", "2402.04031": "|**2024-02-06**|**Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation**|Polyp-DDPM\uff1a\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u4e49\u606f\u8089\u5408\u6210\u4ee5\u589e\u5f3a\u5206\u5272|Zolnamar Dorjsembe, Hsing-Kuo Pao, Furen Xiao|This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at https://github.com/mobaidoctor/polyp-ddpm.|\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86 Polyp-DDPM\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u6234\u5728\u9762\u7f69\u4e0a\u7684\u606f\u8089\u7684\u771f\u5b9e\u56fe\u50cf\uff0c\u65e8\u5728\u589e\u5f3a\u80c3\u80a0\u9053\u606f\u8089\u7684\u5206\u5272\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u4e0e\u533b\u5b66\u56fe\u50cf\u76f8\u5173\u7684\u6570\u636e\u9650\u5236\u3001\u9ad8\u6ce8\u91ca\u6210\u672c\u548c\u9690\u79c1\u95ee\u9898\u7684\u6311\u6218\u3002\u901a\u8fc7\u5728\u5206\u5272\u63a9\u6a21\uff08\u4ee3\u8868\u5f02\u5e38\u533a\u57df\u7684\u4e8c\u8fdb\u5236\u63a9\u6a21\uff09\u4e0a\u8c03\u8282\u6269\u6563\u6a21\u578b\uff0cPolyp-DDPM \u5728\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff08\u4e0e\u4e0a\u8ff0\u5206\u6570\u76f8\u6bd4\uff0cFrechet \u8d77\u59cb\u8ddd\u79bb (FID) \u5206\u6570\u8fbe\u5230 78.47\uff09 83.79\uff09\u548c\u5206\u5272\u6027\u80fd\uff08\u5b9e\u73b0 0.7156 \u7684\u4ea4\u96c6\uff08IoU\uff09\uff0c\u800c\u6765\u81ea\u57fa\u7ebf\u6a21\u578b\u7684\u5408\u6210\u56fe\u50cf\u5c0f\u4e8e 0.6694\uff0c\u771f\u5b9e\u6570\u636e\u5c0f\u4e8e 0.7067\uff09\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\uff0c\u4ece\u800c\u589e\u5f3a\u606f\u8089\u5206\u5272\u6a21\u578b\u4ee5\u4e0e\u771f\u5b9e\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u6570\u636e\u589e\u5f3a\u529f\u80fd\u6765\u6539\u8fdb\u5206\u5272\u6a21\u578b\u3002 Polyp-DDPM \u7684\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u53ef\u5728 https://github.com/mobaidoctor/polyp-ddpm \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2402.04031v1](http://arxiv.org/pdf/2402.04031v1)|null|\n", "2402.03989": "|**2024-02-06**|**YOLOPoint Joint Keypoint and Object Detection**|YOLOPoint \u8054\u5408\u5173\u952e\u70b9\u548c\u7269\u4f53\u68c0\u6d4b|Anton Backhaus, Thorsten Luettel, Hans-Joachim Wuensche|Intelligent vehicles of the future must be capable of understanding and navigating safely through their surroundings. Camera-based vehicle systems can use keypoints as well as objects as low- and high-level landmarks for GNSS-independent SLAM and visual odometry. To this end we propose YOLOPoint, a convolutional neural network model that simultaneously detects keypoints and objects in an image by combining YOLOv5 and SuperPoint to create a single forward-pass network that is both real-time capable and accurate. By using a shared backbone and a light-weight network structure, YOLOPoint is able to perform competitively on both the HPatches and KITTI benchmarks.|\u672a\u6765\u7684\u667a\u80fd\u8f66\u8f86\u5fc5\u987b\u80fd\u591f\u7406\u89e3\u5468\u56f4\u73af\u5883\u5e76\u5b89\u5168\u5bfc\u822a\u3002\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u8f66\u8f86\u7cfb\u7edf\u53ef\u4ee5\u4f7f\u7528\u5173\u952e\u70b9\u4ee5\u53ca\u7269\u4f53\u4f5c\u4e3a\u72ec\u7acb\u4e8e GNSS \u7684 SLAM \u548c\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7684\u4f4e\u7ea7\u548c\u9ad8\u7ea7\u5730\u6807\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 YOLOPoint\uff0c\u8fd9\u662f\u4e00\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408 YOLOv5 \u548c SuperPoint \u521b\u5efa\u4e00\u4e2a\u65e2\u5b9e\u65f6\u53c8\u51c6\u786e\u7684\u5355\u4e00\u524d\u5411\u7f51\u7edc\uff0c\u540c\u65f6\u68c0\u6d4b\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u70b9\u548c\u5bf9\u8c61\u3002\u901a\u8fc7\u4f7f\u7528\u5171\u4eab\u4e3b\u5e72\u548c\u8f7b\u91cf\u7ea7\u7f51\u7edc\u7ed3\u6784\uff0cYOLOPoint \u80fd\u591f\u5728 HPatches \u548c KITTI \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002|[2402.03989v1](http://arxiv.org/pdf/2402.03989v1)|null|\n", "2402.03973": "|**2024-02-06**|**Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given Enough Time**|\u53ea\u8981\u6709\u8db3\u591f\u7684\u65f6\u95f4\uff0c\u4eba\u7c7b\u5c31\u80fd\u5728\u8bc6\u522b\u5f02\u5e38\u59ff\u52bf\u7684\u7269\u4f53\u65b9\u9762\u51fb\u8d25\u6df1\u5ea6\u7f51\u7edc|Netta Ollikka, Amro Abbas, Andrea Perin, Markku Kilpel\u00e4inen, St\u00e9phane Deny|Deep learning is closing the gap with humans on several object recognition benchmarks. Here we investigate this gap in the context of challenging images where objects are seen from unusual viewpoints. We find that humans excel at recognizing objects in unusual poses, in contrast with state-of-the-art pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are systematically brittle in this condition. Remarkably, as we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) take place when humans identify objects in unusual poses. Finally, our analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks. We conclude that more work is needed to bring computer vision systems to the level of robustness of the human visual system. Understanding the nature of the mental processes taking place during extra viewing time may be key to attain such robustness.|\u6df1\u5ea6\u5b66\u4e60\u6b63\u5728\u7f29\u5c0f\u5728\u591a\u4e2a\u7269\u4f53\u8bc6\u522b\u57fa\u51c6\u4e0a\u4e0e\u4eba\u7c7b\u7684\u5dee\u8ddd\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u56fe\u50cf\u80cc\u666f\u4e0b\u7814\u7a76\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5176\u4e2d\u7269\u4f53\u662f\u4ece\u4e0d\u5bfb\u5e38\u7684\u89c6\u89d2\u770b\u5230\u7684\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4eba\u7c7b\u64c5\u957f\u8bc6\u522b\u5f02\u5e38\u59ff\u52bf\u7684\u7269\u4f53\uff0c\u800c\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u7f51\u7edc\uff08EfficientNet\u3001SWAG\u3001ViT\u3001SWIN\u3001BEiT\u3001ConvNext\uff09\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u7cfb\u7edf\u6027\u5f88\u8106\u5f31\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u6211\u4eec\u9650\u5236\u56fe\u50cf\u66dd\u5149\u65f6\u95f4\u65f6\uff0c\u4eba\u7c7b\u7684\u8868\u73b0\u4f1a\u4e0b\u964d\u5230\u6df1\u5c42\u7f51\u7edc\u7684\u6c34\u5e73\uff0c\u8fd9\u8868\u660e\u5f53\u4eba\u7c7b\u8bc6\u522b\u4e0d\u5bfb\u5e38\u59ff\u52bf\u7684\u7269\u4f53\u65f6\uff0c\u4f1a\u53d1\u751f\u989d\u5916\u7684\u5fc3\u7406\u8fc7\u7a0b\uff08\u9700\u8981\u989d\u5916\u7684\u65f6\u95f4\uff09\u3002\u6700\u540e\uff0c\u6211\u4eec\u5bf9\u4eba\u7c7b\u4e0e\u7f51\u7edc\u7684\u9519\u8bef\u6a21\u5f0f\u7684\u5206\u6790\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6709\u65f6\u95f4\u9650\u5236\u7684\u4eba\u7c7b\u4e5f\u4e0e\u524d\u9988\u6df1\u5ea6\u7f51\u7edc\u4e0d\u540c\u3002\u6211\u4eec\u7684\u7ed3\u8bba\u662f\uff0c\u9700\u8981\u505a\u66f4\u591a\u7684\u5de5\u4f5c\u624d\u80fd\u4f7f\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u8fbe\u5230\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u6c34\u5e73\u3002\u4e86\u89e3\u989d\u5916\u89c2\u770b\u65f6\u95f4\u5185\u53d1\u751f\u7684\u5fc3\u7406\u8fc7\u7a0b\u7684\u672c\u8d28\u53ef\u80fd\u662f\u5b9e\u73b0\u8fd9\u79cd\u9c81\u68d2\u6027\u7684\u5173\u952e\u3002|[2402.03973v1](http://arxiv.org/pdf/2402.03973v1)|null|\n", "2402.03951": "|**2024-02-06**|**Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping**|\u901a\u8fc7\u53d8\u5f62\u7ea6\u675f\u7fd8\u66f2\u63d0\u9ad8\u8de8\u6a21\u578b\u5c5e\u7684\u5bf9\u6297\u6027\u53ef\u8fc1\u79fb\u6027|Qinliang Lin, Cheng Luo, Zenghao Niu, Xilin He, Weicheng Xie, Yuanbo Hou, Linlin Shen, Siyang Song|Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems. To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed. However, they show poor performances in attacking systems having different model genera from the surrogate model. In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack. Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input. To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy. Extensive experiments demonstrate that the transferable examples crafted by our DeCoWA on CNN surrogates can significantly hinder the performance of Transformers (and vice versa) on various tasks, including image classification, video action recognition, and audio recognition. Code is made available at https://github.com/LinQinLiang/DeCoWA.|\u7531\u4ee3\u7406\u6a21\u578b\u751f\u6210\u7684\u5bf9\u6297\u6027\u793a\u4f8b\u901a\u5e38\u8868\u73b0\u51fa\u5bf9\u672a\u77e5\u76ee\u6807\u7cfb\u7edf\u7684\u6709\u9650\u53ef\u8f6c\u79fb\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u8bb8\u591a\u53ef\u8fc1\u79fb\u6027\u589e\u5f3a\u65b9\u6cd5\uff08\u4f8b\u5982\u8f93\u5165\u53d8\u6362\u548c\u6a21\u578b\u589e\u5f3a\uff09\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u653b\u51fb\u5177\u6709\u4e0e\u4ee3\u7406\u6a21\u578b\u4e0d\u540c\u7684\u6a21\u578b\u5c5e\u7684\u7cfb\u7edf\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u901a\u7528\u7684\u653b\u51fb\u7b56\u7565\uff0c\u79f0\u4e3a\u53d8\u5f62\u7ea6\u675f\u7fd8\u66f2\u653b\u51fb\uff08DeCoWA\uff09\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u8de8\u6a21\u578b\u5c5e\u653b\u51fb\u3002\u5177\u4f53\u6765\u8bf4\uff0cDeCoWA \u9996\u5148\u901a\u8fc7\u5f39\u6027\u53d8\u5f62\uff0c\u5373\u53d8\u5f62\u7ea6\u675f\u626d\u66f2\uff08DeCoW\uff09\u6765\u589e\u5f3a\u8f93\u5165\u793a\u4f8b\uff0c\u4ee5\u83b7\u5f97\u589e\u5f3a\u8f93\u5165\u7684\u4e30\u5bcc\u5c40\u90e8\u7ec6\u8282\u3002\u4e3a\u4e86\u907f\u514d\u968f\u673a\u53d8\u5f62\u5bfc\u81f4\u7684\u5168\u5c40\u8bed\u4e49\u7684\u4e25\u91cd\u626d\u66f2\uff0cDeCoW \u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u63a7\u5236\u7b56\u7565\u8fdb\u4e00\u6b65\u9650\u5236\u626d\u66f2\u53d8\u6362\u7684\u5f3a\u5ea6\u548c\u65b9\u5411\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 DeCoWA \u5728 CNN \u4ee3\u7406\u4e0a\u5236\u4f5c\u7684\u53ef\u8f6c\u79fb\u793a\u4f8b\u53ef\u80fd\u4f1a\u4e25\u91cd\u963b\u788d Transformer \u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff08\u53cd\u4e4b\u4ea6\u7136\uff09\uff0c\u5305\u62ec\u56fe\u50cf\u5206\u7c7b\u3001\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u548c\u97f3\u9891\u8bc6\u522b\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/LinQinLiang/DeCoWA \u83b7\u53d6\u3002|[2402.03951v1](http://arxiv.org/pdf/2402.03951v1)|null|\n", "2402.03843": "|**2024-02-06**|**A new method for optical steel rope non-destructive damage detection**|\u4e00\u79cd\u5149\u5b66\u94a2\u4e1d\u7ef3\u65e0\u635f\u635f\u4f24\u68c0\u6d4b\u65b0\u65b9\u6cd5|Yunqing Bao, Bin Hu|This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the highest accuracy achieved by the detection model reached 0.975, and the maximum F-measure achieved by the segmentation model reached 0.948.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6d77\u62d4\u73af\u5883\uff08\u7a7a\u4e2d\u7d22\u9053\uff09\u94a2\u4e1d\u7ef3\u65e0\u635f\u635f\u4f24\u68c0\u6d4b\u7684\u65b0\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u90e8\u5206\uff1a\u9996\u5148\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3a RGBD-UNet \u7684\u5206\u5272\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u590d\u6742\u80cc\u666f\u4e2d\u51c6\u786e\u63d0\u53d6\u94a2\u4e1d\u7ef3\u3002\u8be5\u6a21\u578b\u5177\u6709\u901a\u8fc7\u6240\u63d0\u51fa\u7684 CMA \u6a21\u5757\u5904\u7406\u548c\u7ec4\u5408\u989c\u8272\u548c\u6df1\u5ea6\u4fe1\u606f\u7684\u80fd\u529b\u3002\u5176\u6b21\uff0c\u5f00\u53d1\u4e86\u540d\u4e3a VovNetV3.5 \u7684\u68c0\u6d4b\u6a21\u578b\u6765\u533a\u5206\u6b63\u5e38\u548c\u5f02\u5e38\u94a2\u4e1d\u7ef3\u3002\u5b83\u5c06 VovNet \u67b6\u6784\u4e0e DBB \u6a21\u5757\u96c6\u6210\u4ee5\u589e\u5f3a\u6027\u80fd\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u80cc\u666f\u589e\u5f3a\u65b9\u6cd5\u6765\u589e\u5f3a\u5206\u5272\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u521b\u5efa\u5305\u542b\u4e0d\u540c\u573a\u666f\u4e0b\u94a2\u4e1d\u7ef3\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5206\u5272\u548c\u68c0\u6d4b\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u5b9e\u9a8c\u8bc1\u660e\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u7740\u6539\u8fdb\u3002\u5728\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u6d4b\u6a21\u578b\u5b9e\u73b0\u7684\u6700\u9ad8\u7cbe\u5ea6\u8fbe\u52300.975\uff0c\u5206\u5272\u6a21\u578b\u5b9e\u73b0\u7684\u6700\u5927F-measure\u8fbe\u52300.948\u3002|[2402.03843v1](http://arxiv.org/pdf/2402.03843v1)|null|\n", "2402.03833": "|**2024-02-06**|**An SVD-free Approach to Nonlinear Dictionary Learning based on RVFL**|\u57fa\u4e8eRVFL\u7684\u65e0SVD\u975e\u7ebf\u6027\u5b57\u5178\u5b66\u4e60\u65b9\u6cd5|G. Madhuri, Atul Negi|This paper presents a novel nonlinear dictionary learning algorithm leveraging the theory of a feed-forward neural network called Random Vector Functional Link (RVFL). The proposed RVFL-based nonlinear Dictionary Learning (RVFLDL) learns a dictionary as a sparse-to-dense feature map from nonlinear sparse coefficients to the dense input features. Kernel-based nonlinear dictionary learning methods operate in a feature space obtained by an implicit feature map, and they are not independent of computationally expensive operations like Singular Value Decomposition (SVD). Training the RVFL-based dictionary is free from SVD computation as RVFL generates weights from the input to the output layer analytically. Sparsity-inducing Horse-shoe prior is assumed on the coefficients to generate a sparse coefficient matrix w.r.t an initial random dictionary. Higher-order dependencies between the input sparse coefficients and the dictionary atoms are incorporated into the training process by nonlinearly transforming the sparse coefficients and adding them as enhanced features. Thus the method projects sparse coefficients to a higher dimensional space while inducing nonlinearities into the dictionary. For classification using RVFL-net, a classifier matrix is learned as a transform that maps nonlinear sparse coefficients to the labels. The performance of the method illustrated in image classification and reconstruction applications is comparable to that of other nonlinear dictionary learning methods. Experiments show that RVFLDL is scalable and provides a solution better than those obtained using other nonlinear dictionary learning methods.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u7ebf\u6027\u5b57\u5178\u5b66\u4e60\u7b97\u6cd5\uff0c\u5229\u7528\u79f0\u4e3a\u968f\u673a\u5411\u91cf\u51fd\u6570\u94fe\u63a5\uff08RVFL\uff09\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\u3002\u6240\u63d0\u51fa\u7684\u57fa\u4e8e RVFL \u7684\u975e\u7ebf\u6027\u5b57\u5178\u5b66\u4e60 (RVFLDL) \u5c06\u5b57\u5178\u5b66\u4e60\u4e3a\u4ece\u975e\u7ebf\u6027\u7a00\u758f\u7cfb\u6570\u5230\u7a20\u5bc6\u8f93\u5165\u7279\u5f81\u7684\u7a00\u758f\u5230\u7a20\u5bc6\u7279\u5f81\u6620\u5c04\u3002\u57fa\u4e8e\u5185\u6838\u7684\u975e\u7ebf\u6027\u5b57\u5178\u5b66\u4e60\u65b9\u6cd5\u5728\u7531\u9690\u5f0f\u7279\u5f81\u56fe\u83b7\u5f97\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u5e76\u4e14\u5b83\u4eec\u4e0d\u72ec\u7acb\u4e8e\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u7b49\u8ba1\u7b97\u91cf\u5927\u7684\u64cd\u4f5c\u3002\u8bad\u7ec3\u57fa\u4e8e RVFL \u7684\u5b57\u5178\u65e0\u9700\u8fdb\u884c SVD \u8ba1\u7b97\uff0c\u56e0\u4e3a RVFL \u901a\u8fc7\u5206\u6790\u65b9\u5f0f\u751f\u6210\u4ece\u8f93\u5165\u5230\u8f93\u51fa\u5c42\u7684\u6743\u91cd\u3002\u5728\u7cfb\u6570\u4e0a\u5047\u8bbe\u7a00\u758f\u6027\u8bf1\u5bfc\u9a6c\u8e44\u5148\u9a8c\uff0c\u4ee5\u751f\u6210\u5173\u4e8e\u521d\u59cb\u968f\u673a\u5b57\u5178\u7684\u7a00\u758f\u7cfb\u6570\u77e9\u9635\u3002\u901a\u8fc7\u975e\u7ebf\u6027\u53d8\u6362\u7a00\u758f\u7cfb\u6570\u5e76\u5c06\u5176\u6dfb\u52a0\u4e3a\u589e\u5f3a\u7279\u5f81\uff0c\u5c06\u8f93\u5165\u7a00\u758f\u7cfb\u6570\u548c\u5b57\u5178\u539f\u5b50\u4e4b\u95f4\u7684\u9ad8\u9636\u4f9d\u8d56\u6027\u7eb3\u5165\u8bad\u7ec3\u8fc7\u7a0b\u3002\u56e0\u6b64\uff0c\u8be5\u65b9\u6cd5\u5c06\u7a00\u758f\u7cfb\u6570\u6295\u5f71\u5230\u66f4\u9ad8\u7ef4\u7a7a\u95f4\uff0c\u540c\u65f6\u5c06\u975e\u7ebf\u6027\u5f15\u5165\u5b57\u5178\u4e2d\u3002\u5bf9\u4e8e\u4f7f\u7528 RVFL-net \u8fdb\u884c\u5206\u7c7b\uff0c\u5206\u7c7b\u5668\u77e9\u9635\u88ab\u5b66\u4e60\u4e3a\u5c06\u975e\u7ebf\u6027\u7a00\u758f\u7cfb\u6570\u6620\u5c04\u5230\u6807\u7b7e\u7684\u53d8\u6362\u3002\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u91cd\u5efa\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u53ef\u4e0e\u5176\u4ed6\u975e\u7ebf\u6027\u5b57\u5178\u5b66\u4e60\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002\u5b9e\u9a8c\u8868\u660e\uff0cRVFLDL \u662f\u53ef\u6269\u5c55\u7684\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u6bd4\u4f7f\u7528\u5176\u4ed6\u975e\u7ebf\u6027\u5b57\u5178\u5b66\u4e60\u65b9\u6cd5\u83b7\u5f97\u7684\u89e3\u51b3\u65b9\u6848\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002|[2402.03833v1](http://arxiv.org/pdf/2402.03833v1)|null|\n", "2402.03796": "|**2024-02-06**|**Face Detection: Present State and Research Directions**|\u4eba\u8138\u68c0\u6d4b\uff1a\u73b0\u72b6\u548c\u7814\u7a76\u65b9\u5411|Purnendu Prabhat, Himanshu Gupta, Ajeet Kumar Vishwakarma|The majority of computer vision applications that handle images featuring humans use face detection as a core component. Face detection still has issues, despite much research on the topic. Face detection's accuracy and speed might yet be increased. This review paper shows the progress made in this area as well as the substantial issues that still need to be tackled. The paper provides research directions that can be taken up as research projects in the field of face detection.|\u5927\u591a\u6570\u5904\u7406\u4eba\u7c7b\u56fe\u50cf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u7a0b\u5e8f\u90fd\u4f7f\u7528\u4eba\u8138\u68c0\u6d4b\u4f5c\u4e3a\u6838\u5fc3\u7ec4\u4ef6\u3002\u5c3d\u7ba1\u6709\u5173\u8be5\u4e3b\u9898\u7684\u7814\u7a76\u5f88\u591a\uff0c\u4f46\u4eba\u8138\u68c0\u6d4b\u4ecd\u7136\u5b58\u5728\u95ee\u9898\u3002\u4eba\u8138\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u53ef\u80fd\u8fd8\u4f1a\u63d0\u9ad8\u3002\u8fd9\u7bc7\u7efc\u8ff0\u6587\u4ef6\u5c55\u793a\u4e86\u8be5\u9886\u57df\u53d6\u5f97\u7684\u8fdb\u5c55\u4ee5\u53ca\u4ecd\u9700\u8981\u89e3\u51b3\u7684\u5b9e\u8d28\u6027\u95ee\u9898\u3002\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u53ef\u4ee5\u4f5c\u4e3a\u4eba\u8138\u68c0\u6d4b\u9886\u57df\u7814\u7a76\u9879\u76ee\u7684\u7814\u7a76\u65b9\u5411\u3002|[2402.03796v1](http://arxiv.org/pdf/2402.03796v1)|null|\n", "2402.03795": "|**2024-02-06**|**Energy-based Domain-Adaptive Segmentation with Depth Guidance**|\u5177\u6709\u6df1\u5ea6\u5f15\u5bfc\u7684\u57fa\u4e8e\u80fd\u91cf\u7684\u57df\u81ea\u9002\u5e94\u5206\u5272|Jinjing Zhu, Zhedong Hu, Tae-Kyun Kim, Lin Wang|Recent endeavors have been made to leverage self-supervised depth estimation as guidance in unsupervised domain adaptation (UDA) for semantic segmentation. Prior arts, however, overlook the discrepancy between semantic and depth features, as well as the reliability of feature fusion, thus leading to suboptimal segmentation performance. To address this issue, we propose a novel UDA framework called SMART (croSs doMain semAntic segmentation based on eneRgy esTimation) that utilizes Energy-Based Models (EBMs) to obtain task-adaptive features and achieve reliable feature fusion for semantic segmentation with self-supervised depth estimates. Our framework incorporates two novel components: energy-based feature fusion (EB2F) and energy-based reliable fusion Assessment (RFA) modules. The EB2F module produces task-adaptive semantic and depth features by explicitly measuring and reducing their discrepancy using Hopfield energy for better feature fusion. The RFA module evaluates the reliability of the feature fusion using an energy score to improve the effectiveness of depth guidance. Extensive experiments on two datasets demonstrate that our method achieves significant performance gains over prior works, validating the effectiveness of our energy-based learning approach.|\u6700\u8fd1\u4eba\u4eec\u52aa\u529b\u5229\u7528\u81ea\u76d1\u7763\u6df1\u5ea6\u4f30\u8ba1\u4f5c\u4e3a\u8bed\u4e49\u5206\u5272\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u7684\u6307\u5bfc\u3002\u7136\u800c\uff0c\u73b0\u6709\u6280\u672f\u5ffd\u89c6\u4e86\u8bed\u4e49\u7279\u5f81\u548c\u6df1\u5ea6\u7279\u5f81\u4e4b\u95f4\u7684\u5dee\u5f02\u4ee5\u53ca\u7279\u5f81\u878d\u5408\u7684\u53ef\u9760\u6027\uff0c\u4ece\u800c\u5bfc\u81f4\u5206\u5272\u6027\u80fd\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SMART\uff08\u57fa\u4e8e\u80fd\u91cf\u4f30\u8ba1\u7684\u8de8\u57df\u8bed\u4e49\u5206\u5272\uff09\u7684\u65b0\u578b UDA \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u6a21\u578b\uff08EBM\uff09\u6765\u83b7\u53d6\u4efb\u52a1\u81ea\u9002\u5e94\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5b9e\u73b0\u8bed\u4e49\u5206\u5272\u7684\u53ef\u9760\u7279\u5f81\u878d\u5408\u6df1\u5ea6\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u65b0\u9896\u7684\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u80fd\u91cf\u7684\u7279\u5f81\u878d\u5408\uff08EB2F\uff09\u548c\u57fa\u4e8e\u80fd\u91cf\u7684\u53ef\u9760\u878d\u5408\u8bc4\u4f30\uff08RFA\uff09\u6a21\u5757\u3002 EB2F \u6a21\u5757\u901a\u8fc7\u4f7f\u7528 Hopfield \u80fd\u91cf\u663e\u5f0f\u6d4b\u91cf\u548c\u51cf\u5c11\u5176\u5dee\u5f02\u6765\u751f\u6210\u4efb\u52a1\u81ea\u9002\u5e94\u8bed\u4e49\u548c\u6df1\u5ea6\u7279\u5f81\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u7279\u5f81\u878d\u5408\u3002 RFA\u6a21\u5757\u4f7f\u7528\u80fd\u91cf\u8bc4\u5206\u6765\u8bc4\u4f30\u7279\u5f81\u878d\u5408\u7684\u53ef\u9760\u6027\uff0c\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u5f15\u5bfc\u7684\u6709\u6548\u6027\u3002\u5bf9\u4e24\u4e2a\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u4e4b\u524d\u7684\u5de5\u4f5c\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u6211\u4eec\u57fa\u4e8e\u80fd\u91cf\u7684\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2402.03795v1](http://arxiv.org/pdf/2402.03795v1)|null|\n", "2402.03783": "|**2024-02-06**|**Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning**|\u901a\u8fc7\u5f31\u76d1\u7763\u5373\u65f6\u5b66\u4e60\u63a2\u7d22\u4f4e\u8d44\u6e90\u533b\u5b66\u56fe\u50cf\u5206\u7c7b|Fudan Zheng, Jindong Cao, Weijiang Yu, Zhiguang Chen, Nong Xiao, Yutong Lu|Most advances in medical image recognition supporting clinical auxiliary diagnosis meet challenges due to the low-resource situation in the medical field, where annotations are highly expensive and professional. This low-resource problem can be alleviated by leveraging the transferable representations of large-scale pre-trained vision-language models via relevant medical text prompts. However, existing pre-trained vision-language models require domain experts to carefully design the medical prompts, which greatly increases the burden on clinicians. To address this problem, we propose a weakly supervised prompt learning method MedPrompt to automatically generate medical prompts, which includes an unsupervised pre-trained vision-language model and a weakly supervised prompt learning model. The unsupervised pre-trained vision-language model utilizes the natural correlation between medical images and corresponding medical texts for pre-training, without any manual annotations. The weakly supervised prompt learning model only utilizes the classes of images in the dataset to guide the learning of the specific class vector in the prompt, while the learning of other context vectors in the prompt requires no manual annotations for guidance. To the best of our knowledge, this is the first model to automatically generate medical prompts. With these prompts, the pre-trained vision-language model can be freed from the strong expert dependency of manual annotation and manual prompt design. Experimental results show that the model using our automatically generated prompts outperforms its full-shot learning hand-crafted prompts counterparts with only a minimal number of labeled samples for few-shot learning, and reaches superior or comparable accuracy on zero-shot image classification. The proposed prompt generator is lightweight and therefore can be embedded into any network architecture.|\u7531\u4e8e\u533b\u5b66\u9886\u57df\u8d44\u6e90\u532e\u4e4f\uff0c\u6ce8\u91ca\u6210\u672c\u9ad8\u6602\u4e14\u4e13\u4e1a\uff0c\u652f\u6301\u4e34\u5e8a\u8f85\u52a9\u8bca\u65ad\u7684\u533b\u5b66\u56fe\u50cf\u8bc6\u522b\u7684\u5927\u591a\u6570\u8fdb\u5c55\u90fd\u9762\u4e34\u7740\u6311\u6218\u3002\u901a\u8fc7\u76f8\u5173\u7684\u533b\u5b66\u6587\u672c\u63d0\u793a\uff0c\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u8f6c\u79fb\u8868\u793a\u53ef\u4ee5\u7f13\u89e3\u8d44\u6e90\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9700\u8981\u9886\u57df\u4e13\u5bb6\u7cbe\u5fc3\u8bbe\u8ba1\u533b\u7597\u63d0\u793a\uff0c\u8fd9\u5927\u5927\u589e\u52a0\u4e86\u4e34\u5e8a\u533b\u751f\u7684\u8d1f\u62c5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5MedPrompt\u6765\u81ea\u52a8\u751f\u6210\u533b\u7597\u63d0\u793a\uff0c\u5176\u4e2d\u5305\u62ec\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5f31\u76d1\u7763\u63d0\u793a\u5b66\u4e60\u6a21\u578b\u3002\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5229\u7528\u533b\u5b66\u56fe\u50cf\u548c\u76f8\u5e94\u533b\u5b66\u6587\u672c\u4e4b\u95f4\u7684\u81ea\u7136\u76f8\u5173\u6027\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u65e0\u9700\u4efb\u4f55\u624b\u52a8\u6ce8\u91ca\u3002\u5f31\u76d1\u7763\u63d0\u793a\u5b66\u4e60\u6a21\u578b\u4ec5\u5229\u7528\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u7c7b\u522b\u6765\u6307\u5bfc\u63d0\u793a\u4e2d\u7279\u5b9a\u7c7b\u522b\u5411\u91cf\u7684\u5b66\u4e60\uff0c\u800c\u63d0\u793a\u4e2d\u5176\u4ed6\u4e0a\u4e0b\u6587\u5411\u91cf\u7684\u5b66\u4e60\u4e0d\u9700\u8981\u624b\u52a8\u6ce8\u91ca\u6765\u6307\u5bfc\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u533b\u7597\u63d0\u793a\u7684\u6a21\u578b\u3002\u901a\u8fc7\u8fd9\u4e9b\u63d0\u793a\uff0c\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6446\u8131\u624b\u52a8\u6ce8\u91ca\u548c\u624b\u52a8\u63d0\u793a\u8bbe\u8ba1\u7684\u5f3a\u70c8\u4e13\u5bb6\u4f9d\u8d56\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u6211\u4eec\u81ea\u52a8\u751f\u6210\u7684\u63d0\u793a\u7684\u6a21\u578b\u4f18\u4e8e\u5176\u5168\u955c\u5934\u5b66\u4e60\u624b\u5de5\u5236\u4f5c\u7684\u63d0\u793a\uff0c\u800c\u4ec5\u4f7f\u7528\u6700\u5c11\u6570\u91cf\u7684\u6807\u8bb0\u6837\u672c\u8fdb\u884c\u5c11\u955c\u5934\u5b66\u4e60\uff0c\u5e76\u4e14\u5728\u96f6\u955c\u5934\u56fe\u50cf\u5206\u7c7b\u4e0a\u8fbe\u5230\u4e86\u4f18\u5f02\u6216\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002\u6240\u63d0\u51fa\u7684\u63d0\u793a\u751f\u6210\u5668\u662f\u8f7b\u91cf\u7ea7\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u5d4c\u5165\u5230\u4efb\u4f55\u7f51\u7edc\u67b6\u6784\u4e2d\u3002|[2402.03783v1](http://arxiv.org/pdf/2402.03783v1)|null|\n", "2402.03769": "|**2024-02-06**|**AttackNet: Enhancing Biometric Security via Tailored Convolutional Neural Network Architectures for Liveness Detection**|AttackNet\uff1a\u901a\u8fc7\u5b9a\u5236\u7684\u6d3b\u4f53\u68c0\u6d4b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u589e\u5f3a\u751f\u7269\u8bc6\u522b\u5b89\u5168\u6027|Oleksandr Kuznetsov, Dmytro Zakharov, Emanuele Frontoni, Andrea Maranesi|Biometric security is the cornerstone of modern identity verification and authentication systems, where the integrity and reliability of biometric samples is of paramount importance. This paper introduces AttackNet, a bespoke Convolutional Neural Network architecture, meticulously designed to combat spoofing threats in biometric systems. Rooted in deep learning methodologies, this model offers a layered defense mechanism, seamlessly transitioning from low-level feature extraction to high-level pattern discernment. Three distinctive architectural phases form the crux of the model, each underpinned by judiciously chosen activation functions, normalization techniques, and dropout layers to ensure robustness and resilience against adversarial attacks. Benchmarking our model across diverse datasets affirms its prowess, showcasing superior performance metrics in comparison to contemporary models. Furthermore, a detailed comparative analysis accentuates the model's efficacy, drawing parallels with prevailing state-of-the-art methodologies. Through iterative refinement and an informed architectural strategy, AttackNet underscores the potential of deep learning in safeguarding the future of biometric security.|\u751f\u7269\u8bc6\u522b\u5b89\u5168\u662f\u73b0\u4ee3\u8eab\u4efd\u9a8c\u8bc1\u548c\u8ba4\u8bc1\u7cfb\u7edf\u7684\u57fa\u77f3\uff0c\u5176\u4e2d\u751f\u7269\u8bc6\u522b\u6837\u672c\u7684\u5b8c\u6574\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u4ecb\u7ecd AttackNet\uff0c\u8fd9\u662f\u4e00\u79cd\u5b9a\u5236\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\uff0c\u65e8\u5728\u5bf9\u6297\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u6b3a\u9a97\u5a01\u80c1\u3002\u8be5\u6a21\u578b\u690d\u6839\u4e8e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u4f9b\u5206\u5c42\u9632\u5fa1\u673a\u5236\uff0c\u4ece\u4f4e\u7ea7\u7279\u5f81\u63d0\u53d6\u65e0\u7f1d\u8fc7\u6e21\u5230\u9ad8\u7ea7\u6a21\u5f0f\u8bc6\u522b\u3002\u4e09\u4e2a\u72ec\u7279\u7684\u67b6\u6784\u9636\u6bb5\u6784\u6210\u4e86\u6a21\u578b\u7684\u5173\u952e\uff0c\u6bcf\u4e2a\u9636\u6bb5\u90fd\u4ee5\u660e\u667a\u9009\u62e9\u7684\u6fc0\u6d3b\u51fd\u6570\u3001\u6807\u51c6\u5316\u6280\u672f\u548c\u4e22\u5931\u5c42\u4e3a\u57fa\u7840\uff0c\u4ee5\u786e\u4fdd\u9488\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u548c\u5f39\u6027\u3002\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u5bf9\u6211\u4eec\u7684\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u5b83\u7684\u80fd\u529b\uff0c\u4e0e\u5f53\u4ee3\u6a21\u578b\u76f8\u6bd4\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u6307\u6807\u3002\u6b64\u5916\uff0c\u8be6\u7ec6\u7684\u6bd4\u8f83\u5206\u6790\u5f3a\u8c03\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u4e0e\u6d41\u884c\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\u548c\u660e\u667a\u7684\u67b6\u6784\u7b56\u7565\uff0cAttackNet \u5f3a\u8c03\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u4fdd\u969c\u751f\u7269\u8bc6\u522b\u5b89\u5168\u7684\u672a\u6765\u65b9\u9762\u7684\u6f5c\u529b\u3002|[2402.03769v1](http://arxiv.org/pdf/2402.03769v1)|null|\n", "2402.03762": "|**2024-02-06**|**MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction**|MoD-SLAM\uff1a\u7528\u4e8e\u65e0\u754c 3D \u573a\u666f\u91cd\u5efa\u7684\u5355\u76ee\u5bc6\u96c6\u5efa\u56fe|Heng Zhou, Zhetao Guo, Shuhong Liu, Lechen Zhang, Qihao Wang, Yuxiang Ren, Mingrui Li|Neural implicit representations have recently been demonstrated in many fields including Simultaneous Localization And Mapping (SLAM). Current neural SLAM can achieve ideal results in reconstructing bounded scenes, but this relies on the input of RGB-D images. Neural-based SLAM based only on RGB images is unable to reconstruct the scale of the scene accurately, and it also suffers from scale drift due to errors accumulated during tracking. To overcome these limitations, we present MoD-SLAM, a monocular dense mapping method that allows global pose optimization and 3D reconstruction in real-time in unbounded scenes. Optimizing scene reconstruction by monocular depth estimation and using loop closure detection to update camera pose enable detailed and precise reconstruction on large scenes. Compared to previous work, our approach is more robust, scalable and versatile. Our experiments demonstrate that MoD-SLAM has more excellent mapping performance than prior neural SLAM methods, especially in large borderless scenes.|\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u6700\u8fd1\u5df2\u5728\u8bb8\u591a\u9886\u57df\u5f97\u5230\u8bc1\u660e\uff0c\u5305\u62ec\u540c\u6b65\u5b9a\u4f4d\u548c\u5efa\u56fe\uff08SLAM\uff09\u3002\u76ee\u524d\u7684\u795e\u7ecfSLAM\u5728\u91cd\u5efa\u6709\u754c\u573a\u666f\u65b9\u9762\u53ef\u4ee5\u53d6\u5f97\u7406\u60f3\u7684\u7ed3\u679c\uff0c\u4f46\u8fd9\u4f9d\u8d56\u4e8eRGB-D\u56fe\u50cf\u7684\u8f93\u5165\u3002\u4ec5\u57fa\u4e8eRGB\u56fe\u50cf\u7684\u795e\u7ecfSLAM\u65e0\u6cd5\u51c6\u786e\u91cd\u5efa\u573a\u666f\u7684\u5c3a\u5ea6\uff0c\u800c\u4e14\u8fd8\u4f1a\u56e0\u8ddf\u8e2a\u8fc7\u7a0b\u4e2d\u7d2f\u79ef\u7684\u8bef\u5dee\u800c\u51fa\u73b0\u5c3a\u5ea6\u6f02\u79fb\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 MoD-SLAM\uff0c\u8fd9\u662f\u4e00\u79cd\u5355\u76ee\u5bc6\u96c6\u5efa\u56fe\u65b9\u6cd5\uff0c\u5141\u8bb8\u5728\u65e0\u754c\u573a\u666f\u4e2d\u5b9e\u65f6\u8fdb\u884c\u5168\u5c40\u59ff\u6001\u4f18\u5316\u548c 3D \u91cd\u5efa\u3002\u901a\u8fc7\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4f18\u5316\u573a\u666f\u91cd\u5efa\u5e76\u4f7f\u7528\u95ed\u73af\u68c0\u6d4b\u6765\u66f4\u65b0\u76f8\u673a\u59ff\u6001\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u5927\u573a\u666f\u8fdb\u884c\u8be6\u7ec6\u800c\u7cbe\u786e\u7684\u91cd\u5efa\u3002\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u66f4\u52a0\u7a33\u5065\u3001\u53ef\u6269\u5c55\u4e14\u901a\u7528\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoD-SLAM \u6bd4\u73b0\u6709\u7684\u795e\u7ecf SLAM \u65b9\u6cd5\u5177\u6709\u66f4\u51fa\u8272\u7684\u5efa\u56fe\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5927\u578b\u65e0\u8fb9\u754c\u573a\u666f\u4e2d\u3002|[2402.03762v1](http://arxiv.org/pdf/2402.03762v1)|null|\n", "2402.03758": "|**2024-02-06**|**Virtual Classification: Modulating Domain-Specific Knowledge for Multidomain Crowd Counting**|\u865a\u62df\u5206\u7c7b\uff1a\u8c03\u6574\u591a\u57df\u4eba\u7fa4\u8ba1\u6570\u7684\u7279\u5b9a\u9886\u57df\u77e5\u8bc6|Mingyue Guo, Binghui Chen, Zhaoyi Yan, Yaowei Wang, Qixiang Ye|Multidomain crowd counting aims to learn a general model for multiple diverse datasets. However, deep networks prefer modeling distributions of the dominant domains instead of all domains, which is known as domain bias. In this study, we propose a simple-yet-effective Modulating Domain-specific Knowledge Network (MDKNet) to handle the domain bias issue in multidomain crowd counting. MDKNet is achieved by employing the idea of `modulating', enabling deep network balancing and modeling different distributions of diverse datasets with little bias. Specifically, we propose an Instance-specific Batch Normalization (IsBN) module, which serves as a base modulator to refine the information flow to be adaptive to domain distributions. To precisely modulating the domain-specific information, the Domain-guided Virtual Classifier (DVC) is then introduced to learn a domain-separable latent space. This space is employed as an input guidance for the IsBN modulator, such that the mixture distributions of multiple datasets can be well treated. Extensive experiments performed on popular benchmarks, including Shanghai-tech A/B, QNRF and NWPU, validate the superiority of MDKNet in tackling multidomain crowd counting and the effectiveness for multidomain learning. Code is available at \\url{https://github.com/csguomy/MDKNet}.|\u591a\u57df\u4eba\u7fa4\u8ba1\u6570\u65e8\u5728\u5b66\u4e60\u591a\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u7684\u901a\u7528\u6a21\u578b\u3002\u7136\u800c\uff0c\u6df1\u5ea6\u7f51\u7edc\u66f4\u559c\u6b22\u5bf9\u4e3b\u5bfc\u57df\u800c\u4e0d\u662f\u6240\u6709\u57df\u7684\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u8fd9\u79f0\u4e3a\u57df\u504f\u5dee\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u8c03\u5236\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7f51\u7edc\uff08MDKNet\uff09\u6765\u5904\u7406\u591a\u9886\u57df\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u9886\u57df\u504f\u5dee\u95ee\u9898\u3002 MDKNet \u662f\u901a\u8fc7\u91c7\u7528\u201c\u8c03\u5236\u201d\u7684\u601d\u60f3\u6765\u5b9e\u73b0\u7684\uff0c\u80fd\u591f\u5b9e\u73b0\u6df1\u5ea6\u7f51\u7edc\u5e73\u8861\u5e76\u4ee5\u5f88\u5c0f\u7684\u504f\u5dee\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u7684\u4e0d\u540c\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7279\u5b9a\u4e8e\u5b9e\u4f8b\u7684\u6279\u91cf\u5f52\u4e00\u5316\uff08IsBN\uff09\u6a21\u5757\uff0c\u5b83\u5145\u5f53\u57fa\u7840\u8c03\u5236\u5668\u6765\u7ec6\u5316\u4fe1\u606f\u6d41\u4ee5\u9002\u5e94\u57df\u5206\u5e03\u3002\u4e3a\u4e86\u7cbe\u786e\u8c03\u5236\u7279\u5b9a\u4e8e\u57df\u7684\u4fe1\u606f\uff0c\u5f15\u5165\u57df\u5f15\u5bfc\u865a\u62df\u5206\u7c7b\u5668\uff08DVC\uff09\u6765\u5b66\u4e60\u57df\u53ef\u5206\u79bb\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u8be5\u7a7a\u95f4\u88ab\u7528\u4f5c IsBN \u8c03\u5236\u5668\u7684\u8f93\u5165\u6307\u5bfc\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u5f88\u597d\u5730\u5904\u7406\u591a\u4e2a\u6570\u636e\u96c6\u7684\u6df7\u5408\u5206\u5e03\u3002\u5728\u4e0a\u6d77\u79d1\u6280 A/B\u3001QNRF \u548c NWPU \u7b49\u6d41\u884c\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 MDKNet \u5728\u5904\u7406\u591a\u57df\u4eba\u7fa4\u8ba1\u6570\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u4ee5\u53ca\u591a\u57df\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/csguomy/MDKNet} \u83b7\u53d6\u3002|[2402.03758v1](http://arxiv.org/pdf/2402.03758v1)|null|\n", "2402.03708": "|**2024-02-06**|**SISP: A Benchmark Dataset for Fine-grained Ship Instance Segmentation in Panchromatic Satellite Images**|SISP\uff1a\u5168\u8272\u536b\u661f\u56fe\u50cf\u4e2d\u7ec6\u7c92\u5ea6\u8239\u8236\u5b9e\u4f8b\u5206\u5272\u7684\u57fa\u51c6\u6570\u636e\u96c6|Pengming Feng, Mingjie Xie, Hongning Liu, Xuanjia Zhao, Guangjun He, Xueliang Zhang, Jian Guan|Fine-grained ship instance segmentation in satellite images holds considerable significance for monitoring maritime activities at sea. However, existing datasets often suffer from the scarcity of fine-grained information or pixel-wise localization annotations, as well as the insufficient image diversity and variations, thus limiting the research of this task. To this end, we propose a benchmark dataset for fine-grained Ship Instance Segmentation in Panchromatic satellite images, namely SISP, which contains 56,693 well-annotated ship instances with four fine-grained categories across 10,000 sliced images, and all the images are collected from SuperView-1 satellite with the resolution of 0.5m. Targets in the proposed SISP dataset have characteristics that are consistent with real satellite scenes, such as high class imbalance, various scenes, large variations in target densities and scales, and high inter-class similarity and intra-class diversity, all of which make the SISP dataset more suitable for real-world applications. In addition, we introduce a Dynamic Feature Refinement-assist Instance segmentation network, namely DFRInst, as the benchmark method for ship instance segmentation in satellite images, which can fortify the explicit representation of crucial features, thus improving the performance of ship instance segmentation. Experiments and analysis are performed on the proposed SISP dataset to evaluate the benchmark method and several state-of-the-art methods to establish baselines for facilitating future research. The proposed dataset and source codes will be available at: https://github.com/Justlovesmile/SISP.|\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8239\u8236\u5b9e\u4f8b\u5206\u5272\u5bf9\u4e8e\u76d1\u6d4b\u6d77\u4e0a\u6d3b\u52a8\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6570\u636e\u96c6\u5f80\u5f80\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u4fe1\u606f\u6216\u50cf\u7d20\u7ea7\u5b9a\u4f4d\u6ce8\u91ca\uff0c\u4ee5\u53ca\u56fe\u50cf\u591a\u6837\u6027\u548c\u53d8\u5316\u4e0d\u8db3\uff0c\u4ece\u800c\u9650\u5236\u4e86\u8be5\u4efb\u52a1\u7684\u7814\u7a76\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5168\u8272\u536b\u661f\u56fe\u50cf\u4e2d\u7ec6\u7c92\u5ea6\u8239\u8236\u5b9e\u4f8b\u5206\u5272\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5373 SISP\uff0c\u5b83\u5305\u542b 10,000 \u4e2a\u5207\u7247\u56fe\u50cf\u4e2d\u7684 56,693 \u4e2a\u7ecf\u8fc7\u826f\u597d\u6ce8\u91ca\u7684\u8239\u8236\u5b9e\u4f8b\uff0c\u5176\u4e2d\u6709\u56db\u4e2a\u7ec6\u7c92\u5ea6\u7c7b\u522b\uff0c\u5e76\u4e14\u6240\u6709\u56fe\u50cf\u90fd\u6536\u96c6\u81eaSuperView-1\u536b\u661f\uff0c\u5206\u8fa8\u73870.5m\u3002\u6240\u63d0\u51fa\u7684SISP\u6570\u636e\u96c6\u4e2d\u7684\u76ee\u6807\u5177\u6709\u4e0e\u771f\u5b9e\u536b\u661f\u573a\u666f\u4e00\u81f4\u7684\u7279\u5f81\uff0c\u4f8b\u5982\u7c7b\u4e0d\u5e73\u8861\u5ea6\u9ad8\u3001\u573a\u666f\u591a\u6837\u3001\u76ee\u6807\u5bc6\u5ea6\u548c\u5c3a\u5ea6\u53d8\u5316\u5927\u3001\u7c7b\u95f4\u76f8\u4f3c\u5ea6\u548c\u7c7b\u5185\u591a\u6837\u6027\u9ad8\uff0c\u6240\u6709\u8fd9\u4e9b\u4f7f\u5f97SISP\u6570\u636e\u96c6\u66f4\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u52a8\u6001\u7279\u5f81\u7ec6\u5316\u8f85\u52a9\u5b9e\u200b\u200b\u4f8b\u5206\u5272\u7f51\u7edc\uff0c\u5373DFRInst\uff0c\u4f5c\u4e3a\u536b\u661f\u56fe\u50cf\u4e2d\u8239\u8236\u5b9e\u4f8b\u5206\u5272\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u5f3a\u5316\u5173\u952e\u7279\u5f81\u7684\u663e\u5f0f\u8868\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u8239\u8236\u5b9e\u4f8b\u5206\u5272\u7684\u6027\u80fd\u3002\u5728\u6240\u63d0\u51fa\u7684 SISP \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u548c\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u57fa\u51c6\u65b9\u6cd5\u548c\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4ee5\u5efa\u7acb\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u7684\u57fa\u7ebf\u3002\u5efa\u8bae\u7684\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u5c06\u5728\u4ee5\u4e0b\u7f51\u5740\u63d0\u4f9b\uff1ahttps://github.com/Justlovesmile/SISP\u3002|[2402.03708v1](http://arxiv.org/pdf/2402.03708v1)|null|\n", "2402.03706": "|**2024-02-06**|**MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats**|MMAUD\uff1a\u9488\u5bf9\u73b0\u4ee3\u5fae\u578b\u65e0\u4eba\u200b\u200b\u673a\u5a01\u80c1\u7684\u7efc\u5408\u591a\u6a21\u5f0f\u53cd\u65e0\u4eba\u673a\u6570\u636e\u96c6|Shenghai Yuan, Yizhuo Yang, Thien Hoang Nguyen, Thien-Minh Nguyen, Jianfei Yang, Fen Liu, Jianping Li, Han Wang, Lihua Xie|In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools. Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds. This approach enhances the dataset's applicability, capturing the exact challenges faced during proximate vehicular operations. It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Our dataset, codes, and designs will be available in https://github.com/ntu-aris/MMAUD.|\u4e3a\u4e86\u5e94\u5bf9\u5c0f\u578b\u65e0\u4eba\u673a (UAV) \u6240\u5e26\u6765\u7684\u4e0d\u65ad\u53d8\u5316\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u65e0\u4eba\u673a\u5177\u6709\u8fd0\u8f93\u6709\u5bb3\u6709\u6548\u8f7d\u8377\u6216\u72ec\u7acb\u9020\u6210\u635f\u5bb3\u7684\u6f5c\u529b\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 MMAUD\uff1a\u7efc\u5408\u6027\u591a\u6a21\u6001\u53cd\u65e0\u4eba\u673a\u6570\u636e\u96c6\u3002 MMAUD \u901a\u8fc7\u4e13\u6ce8\u4e8e\u65e0\u4eba\u673a\u68c0\u6d4b\u3001\u65e0\u4eba\u673a\u7c7b\u578b\u5206\u7c7b\u548c\u8f68\u8ff9\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u5f53\u4ee3\u5a01\u80c1\u68c0\u6d4b\u65b9\u6cd5\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u5dee\u8ddd\u3002 MMAUD \u56e0\u7ed3\u5408\u4e86\u591a\u79cd\u611f\u5b98\u8f93\u5165\u800c\u8131\u9896\u800c\u51fa\uff0c\u5305\u62ec\u7acb\u4f53\u89c6\u89c9\u3001\u5404\u79cd\u6fc0\u5149\u96f7\u8fbe\u3001\u96f7\u8fbe\u548c\u97f3\u9891\u9635\u5217\u3002\u5b83\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u7a7a\u4e2d\u63a2\u6d4b\uff0c\u5bf9\u4e8e\u89e3\u51b3\u73b0\u5b9e\u573a\u666f\u81f3\u5173\u91cd\u8981\uff0c\u5176\u4fdd\u771f\u5ea6\u9ad8\u4e8e\u4f7f\u7528\u70ed\u6210\u50cf\u548c RGB \u5728\u7279\u5b9a\u6709\u5229\u4f4d\u7f6e\u6355\u83b7\u7684\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0cMMAUD \u63d0\u4f9b\u51c6\u786e\u7684\u5f95\u5361\u751f\u6210\u7684\u5730\u9762\u5b9e\u51b5\u6570\u636e\uff0c\u589e\u5f3a\u53ef\u4fe1\u5ea6\u5e76\u80fd\u591f\u81ea\u4fe1\u5730\u6539\u8fdb\u7b97\u6cd5\u548c\u6a21\u578b\uff0c\u8fd9\u5728\u5176\u4ed6\u6570\u636e\u96c6\u4e2d\u4ece\u672a\u89c1\u8fc7\u3002\u5927\u591a\u6570\u73b0\u6709\u4f5c\u54c1\u90fd\u6ca1\u6709\u516c\u5f00\u5176\u6570\u636e\u96c6\uff0c\u8fd9\u4f7f\u5f97 MMAUD \u6210\u4e3a\u5f00\u53d1\u51c6\u786e\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u7684\u5b9d\u8d35\u8d44\u6e90\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u5f0f\u5177\u6709\u6210\u672c\u6548\u76ca\u4e14\u9002\u5e94\u6027\u5f3a\uff0c\u5141\u8bb8\u7528\u6237\u8bd5\u9a8c\u548c\u5b9e\u65bd\u65b0\u7684\u65e0\u4eba\u673a\u5a01\u80c1\u68c0\u6d4b\u5de5\u5177\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u901a\u8fc7\u7ed3\u5408\u5468\u56f4\u7684\u91cd\u578b\u673a\u68b0\u58f0\u97f3\u6765\u5bc6\u5207\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u7684\u573a\u666f\u3002\u8fd9\u79cd\u65b9\u6cd5\u589e\u5f3a\u4e86\u6570\u636e\u96c6\u7684\u9002\u7528\u6027\uff0c\u6355\u83b7\u4e86\u90bb\u8fd1\u8f66\u8f86\u64cd\u4f5c\u671f\u95f4\u9762\u4e34\u7684\u786e\u5207\u6311\u6218\u3002\u9884\u8ba1 MMAUD \u53ef\u4ee5\u5728\u63a8\u8fdb\u65e0\u4eba\u673a\u5a01\u80c1\u68c0\u6d4b\u3001\u5206\u7c7b\u3001\u8f68\u8ff9\u4f30\u8ba1\u80fd\u529b\u7b49\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u8bbe\u8ba1\u5c06\u5728 https://github.com/ntu-aris/MMAUD \u4e2d\u63d0\u4f9b\u3002|[2402.03706v1](http://arxiv.org/pdf/2402.03706v1)|null|\n", "2402.03697": "|**2024-02-06**|**SHMC-Net: A Mask-guided Feature Fusion Network for Sperm Head Morphology Classification**|SHMC-Net\uff1a\u7528\u4e8e\u7cbe\u5b50\u5934\u90e8\u5f62\u6001\u5206\u7c7b\u7684\u63a9\u6a21\u5f15\u5bfc\u7279\u5f81\u878d\u5408\u7f51\u7edc|Nishchal Sapkota, Yejia Zhang, Sirui Li, Peixian Liang, Zhuo Zhao, Danny Z Chen|Male infertility accounts for about one-third of global infertility cases. Manual assessment of sperm abnormalities through head morphology analysis encounters issues of observer variability and diagnostic discrepancies among experts. Its alternative, Computer-Assisted Semen Analysis (CASA), suffers from low-quality sperm images, small datasets, and noisy class labels. We propose a new approach for sperm head morphology classification, called SHMC-Net, which uses segmentation masks of sperm heads to guide the morphology classification of sperm images. SHMC-Net generates reliable segmentation masks using image priors, refines object boundaries with an efficient graph-based method, and trains an image network with sperm head crops and a mask network with the corresponding masks. In the intermediate stages of the networks, image and mask features are fused with a fusion scheme to better learn morphological features. To handle noisy class labels and regularize training on small datasets, SHMC-Net applies Soft Mixup to combine mixup augmentation and a loss function. We achieve state-of-the-art results on SCIAN and HuSHeM datasets, outperforming methods that use additional pre-training or costly ensembling techniques.|\u7537\u6027\u4e0d\u80b2\u75c7\u7ea6\u5360\u5168\u7403\u4e0d\u80b2\u75c7\u75c5\u4f8b\u7684\u4e09\u5206\u4e4b\u4e00\u3002\u901a\u8fc7\u5934\u90e8\u5f62\u6001\u5206\u6790\u5bf9\u7cbe\u5b50\u5f02\u5e38\u8fdb\u884c\u624b\u52a8\u8bc4\u4f30\u4f1a\u9047\u5230\u89c2\u5bdf\u8005\u53d8\u5f02\u6027\u548c\u4e13\u5bb6\u4e4b\u95f4\u8bca\u65ad\u5dee\u5f02\u7684\u95ee\u9898\u3002\u5b83\u7684\u66ff\u4ee3\u65b9\u6848\u662f\u8ba1\u7b97\u673a\u8f85\u52a9\u7cbe\u6db2\u5206\u6790 (CASA)\uff0c\u4f46\u5b58\u5728\u7cbe\u5b50\u56fe\u50cf\u8d28\u91cf\u4f4e\u3001\u6570\u636e\u96c6\u5c0f\u548c\u7c7b\u522b\u6807\u7b7e\u5608\u6742\u7684\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u5b50\u5934\u90e8\u5f62\u6001\u5206\u7c7b\u7684\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a SHMC-Net\uff0c\u5b83\u4f7f\u7528\u7cbe\u5b50\u5934\u90e8\u7684\u5206\u5272\u63a9\u6a21\u6765\u6307\u5bfc\u7cbe\u5b50\u56fe\u50cf\u7684\u5f62\u6001\u5206\u7c7b\u3002 SHMC-Net \u4f7f\u7528\u56fe\u50cf\u5148\u9a8c\u751f\u6210\u53ef\u9760\u7684\u5206\u5272\u63a9\u6a21\uff0c\u4f7f\u7528\u9ad8\u6548\u7684\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u7ec6\u5316\u5bf9\u8c61\u8fb9\u754c\uff0c\u5e76\u4f7f\u7528\u7cbe\u5b50\u5934\u90e8\u4f5c\u7269\u8bad\u7ec3\u56fe\u50cf\u7f51\u7edc\u548c\u4f7f\u7528\u76f8\u5e94\u63a9\u6a21\u7684\u63a9\u6a21\u7f51\u7edc\u3002\u5728\u7f51\u7edc\u7684\u4e2d\u95f4\u9636\u6bb5\uff0c\u56fe\u50cf\u548c\u63a9\u6a21\u7279\u5f81\u901a\u8fc7\u878d\u5408\u65b9\u6848\u8fdb\u884c\u878d\u5408\uff0c\u4ee5\u66f4\u597d\u5730\u5b66\u4e60\u5f62\u6001\u7279\u5f81\u3002\u4e3a\u4e86\u5904\u7406\u566a\u58f0\u7c7b\u6807\u7b7e\u5e76\u89c4\u8303\u5c0f\u6570\u636e\u96c6\u7684\u8bad\u7ec3\uff0cSHMC-Net \u5e94\u7528 Soft Mixup \u5c06\u6df7\u5408\u589e\u5f3a\u548c\u635f\u5931\u51fd\u6570\u7ed3\u5408\u8d77\u6765\u3002\u6211\u4eec\u5728 SCIAN \u548c HuSHeM \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4f18\u4e8e\u4f7f\u7528\u989d\u5916\u9884\u8bad\u7ec3\u6216\u6602\u8d35\u7684\u96c6\u6210\u6280\u672f\u7684\u65b9\u6cd5\u3002|[2402.03697v1](http://arxiv.org/pdf/2402.03697v1)|null|\n", "2402.03695": "|**2024-02-06**|**ConUNETR: A Conditional Transformer Network for 3D Micro-CT Embryonic Cartilage Segmentation**|ConUNETR\uff1a\u7528\u4e8e 3D Micro-CT \u80da\u80ce\u8f6f\u9aa8\u5206\u5272\u7684\u6761\u4ef6\u53d8\u538b\u5668\u7f51\u7edc|Nishchal Sapkota, Yejia Zhang, Susan M. Motch Perrine, Yuhan Hsi, Sirui Li, Meng Wu, Greg Holmes, Abdul R. Abdulai, Ethylin W. Jabs, Joan T. Richtsmeier, et.al.|Studying the morphological development of cartilaginous and osseous structures is critical to the early detection of life-threatening skeletal dysmorphology. Embryonic cartilage undergoes rapid structural changes within hours, introducing biological variations and morphological shifts that limit the generalization of deep learning-based segmentation models that infer across multiple embryonic age groups. Obtaining individual models for each age group is expensive and less effective, while direct transfer (predicting an age unseen during training) suffers a potential performance drop due to morphological shifts. We propose a novel Transformer-based segmentation model with improved biological priors that better distills morphologically diverse information through conditional mechanisms. This enables a single model to accurately predict cartilage across multiple age groups. Experiments on the mice cartilage dataset show the superiority of our new model compared to other competitive segmentation models. Additional studies on a separate mice cartilage dataset with a distinct mutation show that our model generalizes well and effectively captures age-based cartilage morphology patterns.|\u7814\u7a76\u8f6f\u9aa8\u548c\u9aa8\u7ed3\u6784\u7684\u5f62\u6001\u53d1\u80b2\u5bf9\u4e8e\u65e9\u671f\u53d1\u73b0\u5371\u53ca\u751f\u547d\u7684\u9aa8\u9abc\u7578\u5f62\u81f3\u5173\u91cd\u8981\u3002\u80da\u80ce\u8f6f\u9aa8\u5728\u6570\u5c0f\u65f6\u5185\u7ecf\u5386\u5feb\u901f\u7684\u7ed3\u6784\u53d8\u5316\uff0c\u5f15\u5165\u751f\u7269\u53d8\u5f02\u548c\u5f62\u6001\u53d8\u5316\uff0c\u9650\u5236\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5206\u5272\u6a21\u578b\u7684\u6cdb\u5316\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u63a8\u65ad\u591a\u4e2a\u80da\u80ce\u5e74\u9f84\u7ec4\u3002\u4e3a\u6bcf\u4e2a\u5e74\u9f84\u7ec4\u83b7\u53d6\u5355\u72ec\u7684\u6a21\u578b\u6210\u672c\u9ad8\u6602\u4e14\u6548\u7387\u8f83\u4f4e\uff0c\u800c\u76f4\u63a5\u8fc1\u79fb\uff08\u9884\u6d4b\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u7684\u5e74\u9f84\uff09\u4f1a\u56e0\u5f62\u6001\u53d8\u5316\u800c\u5bfc\u81f4\u6f5c\u5728\u7684\u6027\u80fd\u4e0b\u964d\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Transformer \u7684\u65b0\u578b\u5206\u5272\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u6539\u8fdb\u7684\u751f\u7269\u5b66\u5148\u9a8c\uff0c\u53ef\u4ee5\u901a\u8fc7\u6761\u4ef6\u673a\u5236\u66f4\u597d\u5730\u63d0\u53d6\u5f62\u6001\u591a\u6837\u5316\u7684\u4fe1\u606f\u3002\u8fd9\u4f7f\u5f97\u5355\u4e00\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u591a\u4e2a\u5e74\u9f84\u7ec4\u7684\u8f6f\u9aa8\u3002\u5bf9\u5c0f\u9f20\u8f6f\u9aa8\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u7ade\u4e89\u6027\u5206\u5272\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b0\u6a21\u578b\u5177\u6709\u4f18\u8d8a\u6027\u3002\u5bf9\u5177\u6709\u660e\u663e\u7a81\u53d8\u7684\u5355\u72ec\u5c0f\u9f20\u8f6f\u9aa8\u6570\u636e\u96c6\u7684\u5176\u4ed6\u7814\u7a76\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u5f88\u597d\u5730\u6982\u62ec\u5e76\u6709\u6548\u5730\u6355\u83b7\u57fa\u4e8e\u5e74\u9f84\u7684\u8f6f\u9aa8\u5f62\u6001\u6a21\u5f0f\u3002|[2402.03695v1](http://arxiv.org/pdf/2402.03695v1)|null|\n", "2402.03634": "|**2024-02-06**|**BEAM: Beta Distribution Ray Denoising for Multi-view 3D Object Detection**|BEAM\uff1a\u7528\u4e8e\u591a\u89c6\u56fe 3D \u7269\u4f53\u68c0\u6d4b\u7684 Beta \u5206\u5e03\u5c04\u7ebf\u53bb\u566a|Feng Liu, Tengteng Huang, Qianjing Zhang, Haotian Yao, Chi Zhang, Fang Wan, Qixiang Ye, Yanzhao Zhou|Multi-view 3D object detectors struggle with duplicate predictions due to the lack of depth information, resulting in false positive detections. In this study, we introduce BEAM, a novel Beta Distribution Ray Denoising approach that can be applied to any DETR-style multi-view 3D detector to explicitly incorporate structure prior knowledge of the scene. By generating rays from cameras to objects and sampling spatial denoising queries from the Beta distribution family along these rays, BEAM enhances the model's ability to distinguish spatial hard negative samples arising from ambiguous depths. BEAM is a plug-and-play technique that adds only marginal computational costs during training, while impressively preserving the inference speed. Extensive experiments and ablation studies on the NuScenes dataset demonstrate significant improvements over strong baselines, outperforming the state-of-the-art method StreamPETR by 1.9% mAP. The code will be available at https://github.com/LiewFeng/BEAM.|\u7531\u4e8e\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\uff0c\u591a\u89c6\u56fe 3D \u7269\u4f53\u68c0\u6d4b\u5668\u96be\u4ee5\u5e94\u5bf9\u91cd\u590d\u9884\u6d4b\uff0c\u4ece\u800c\u5bfc\u81f4\u8bef\u62a5\u68c0\u6d4b\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 BEAM\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684 Beta \u5206\u5e03\u5c04\u7ebf\u53bb\u566a\u65b9\u6cd5\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55 DETR \u5f0f\u591a\u89c6\u56fe 3D \u63a2\u6d4b\u5668\uff0c\u4ee5\u660e\u786e\u5730\u7ed3\u5408\u573a\u666f\u7684\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\u3002\u901a\u8fc7\u751f\u6210\u4ece\u6444\u50cf\u673a\u5230\u7269\u4f53\u7684\u5c04\u7ebf\u5e76\u6cbf\u8fd9\u4e9b\u5c04\u7ebf\u5bf9 Beta \u5206\u5e03\u65cf\u7684\u7a7a\u95f4\u53bb\u566a\u67e5\u8be2\u8fdb\u884c\u91c7\u6837\uff0cBEAM \u589e\u5f3a\u4e86\u6a21\u578b\u533a\u5206\u7531\u6a21\u7cca\u6df1\u5ea6\u4ea7\u751f\u7684\u7a7a\u95f4\u786c\u8d1f\u6837\u672c\u7684\u80fd\u529b\u3002 BEAM \u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6280\u672f\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u4ec5\u589e\u52a0\u8fb9\u9645\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u5730\u4fdd\u6301\u4e86\u63a8\u7406\u901f\u5ea6\u3002\u5bf9 NuScenes \u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u76f8\u5bf9\u4e8e\u5f3a\u57fa\u7ebf\u6709\u663e\u7740\u6539\u8fdb\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5 StreamPETR \u9ad8 1.9% mAP\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/LiewFeng/BEAM \u4e0a\u83b7\u53d6\u3002|[2402.03634v1](http://arxiv.org/pdf/2402.03634v1)|null|\n", "2402.03631": "|**2024-02-06**|**CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model**|CAT-SAM\uff1a\u7528\u4e8e\u5206\u6bb5\u4efb\u610f\u6a21\u578b\u7684\u5c11\u6837\u672c\u81ea\u9002\u5e94\u7684\u6761\u4ef6\u8c03\u6574\u7f51\u7edc|Aoran Xiao, Weihao Xuan, Heli Qi, Yun Xing, Ruijie Ren, Xiaoqin Zhang, Shijian Lu|The recent Segment Anything Model (SAM) has demonstrated remarkable zero-shot capability and flexible geometric prompting in general image segmentation. However, SAM often struggles when handling various unconventional images, such as aerial, medical, and non-RGB images. This paper presents CAT-SAM, a ConditionAl Tuning network that adapts SAM toward various unconventional target tasks with just few-shot target samples. CAT-SAM freezes the entire SAM and adapts its mask decoder and image encoder simultaneously with a small number of learnable parameters. The core design is a prompt bridge structure that enables decoder-conditioned joint tuning of the heavyweight image encoder and the lightweight mask decoder. The bridging maps the prompt token of the mask decoder to the image encoder, fostering synergic adaptation of the encoder and the decoder with mutual benefits. We develop two representative tuning strategies for the image encoder which leads to two CAT-SAM variants: one injecting learnable prompt tokens in the input space and the other inserting lightweight adapter networks. Extensive experiments over 11 unconventional tasks show that both CAT-SAM variants achieve superior target segmentation performance consistently even under the very challenging one-shot adaptation setup. Project page: \\url{https://xiaoaoran.github.io/projects/CAT-SAM}|\u6700\u8fd1\u7684Segment Anything Model\uff08SAM\uff09\u5728\u4e00\u822c\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u96f6\u6837\u672c\u80fd\u529b\u548c\u7075\u6d3b\u7684\u51e0\u4f55\u63d0\u793a\u3002\u7136\u800c\uff0cSAM \u5728\u5904\u7406\u5404\u79cd\u975e\u5e38\u89c4\u56fe\u50cf\uff08\u4f8b\u5982\u822a\u7a7a\u56fe\u50cf\u3001\u533b\u5b66\u56fe\u50cf\u548c\u975e RGB \u56fe\u50cf\uff09\u65f6\u5e38\u5e38\u9047\u5230\u56f0\u96be\u3002\u672c\u6587\u63d0\u51fa\u4e86 CAT-SAM\uff0c\u8fd9\u662f\u4e00\u79cd ConditionAl Tuning \u7f51\u7edc\uff0c\u53ea\u9700\u5c11\u91cf\u76ee\u6807\u6837\u672c\u5373\u53ef\u4f7f SAM \u9002\u5e94\u5404\u79cd\u975e\u5e38\u89c4\u76ee\u6807\u4efb\u52a1\u3002 CAT-SAM \u51bb\u7ed3\u6574\u4e2a SAM\uff0c\u5e76\u4f7f\u7528\u5c11\u91cf\u53ef\u5b66\u4e60\u53c2\u6570\u540c\u65f6\u8c03\u6574\u5176\u63a9\u6a21\u89e3\u7801\u5668\u548c\u56fe\u50cf\u7f16\u7801\u5668\u3002\u6838\u5fc3\u8bbe\u8ba1\u662f\u4e00\u4e2a\u5373\u65f6\u6865\u7ed3\u6784\uff0c\u53ef\u4ee5\u5b9e\u73b0\u91cd\u91cf\u7ea7\u56fe\u50cf\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7\u63a9\u6a21\u89e3\u7801\u5668\u7684\u89e3\u7801\u5668\u6761\u4ef6\u8054\u5408\u8c03\u6574\u3002\u6865\u63a5\u5c06\u63a9\u6a21\u89e3\u7801\u5668\u7684\u63d0\u793a\u4ee4\u724c\u6620\u5c04\u5230\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u4fc3\u8fdb\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u534f\u540c\u9002\u5e94\uff0c\u4e92\u60e0\u4e92\u5229\u3002\u6211\u4eec\u4e3a\u56fe\u50cf\u7f16\u7801\u5668\u5f00\u53d1\u4e86\u4e24\u79cd\u4ee3\u8868\u6027\u7684\u8c03\u6574\u7b56\u7565\uff0c\u8fd9\u5bfc\u81f4\u4e86\u4e24\u79cd CAT-SAM \u53d8\u4f53\uff1a\u4e00\u79cd\u5728\u8f93\u5165\u7a7a\u95f4\u4e2d\u6ce8\u5165\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u6807\u8bb0\uff0c\u53e6\u4e00\u79cd\u63d2\u5165\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u7f51\u7edc\u3002\u8d85\u8fc7 11 \u9879\u975e\u5e38\u89c4\u4efb\u52a1\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u7684\u4e00\u6b21\u6027\u9002\u5e94\u8bbe\u7f6e\u4e0b\uff0c\u4e24\u79cd CAT-SAM \u53d8\u4f53\u4e5f\u80fd\u59cb\u7ec8\u5982\u4e00\u5730\u5b9e\u73b0\u5353\u8d8a\u7684\u76ee\u6807\u5206\u5272\u6027\u80fd\u3002\u9879\u76ee\u9875\u9762\uff1a\\url{https://xiaoaoran.github.io/projects/CAT-SAM}|[2402.03631v1](http://arxiv.org/pdf/2402.03631v1)|null|\n", "2402.03607": "|**2024-02-06**|**Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning**|\u5229\u7528\u6ce8\u5165\u77e5\u8bc6\u7684\u5b66\u4e60\u63d0\u9ad8\u8de8\u6a21\u5f0f\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u591a\u6a21\u5f0f\u8425\u9500|Trilok Padhi, Ugur Kursuncu, Yaman Kumar, Valerie L. Shalin, Lane Peterson Fronczek|The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online. However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory.|\u80fd\u591f\u6355\u6349\u591a\u79cd\u6a21\u5f0f\u77ac\u95f4\u7684\u667a\u80fd\u8bbe\u5907\u7684\u666e\u53ca\u4f7f\u7528\u6237\u80fd\u591f\u5728\u7ebf\u4f53\u9a8c\u591a\u6a21\u5f0f\u4fe1\u606f\u3002\u7136\u800c\uff0c\u5927\u578b\u8bed\u8a00\uff08LLM\uff09\u548c\u89c6\u89c9\u6a21\u578b\uff08LVM\uff09\u5728\u6355\u83b7\u5177\u6709\u8de8\u6a21\u6001\u8bed\u4e49\u5173\u7cfb\u7684\u6574\u4f53\u610f\u4e49\u65b9\u9762\u4ecd\u7136\u53d7\u5230\u9650\u5236\u3002\u5982\u679c\u6ca1\u6709\u660e\u786e\u7684\u5e38\u8bc6\u77e5\u8bc6\uff08\u4f8b\u5982\uff0c\u4f5c\u4e3a\u77e5\u8bc6\u56fe\uff09\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u53ea\u80fd\u901a\u8fc7\u6355\u83b7\u5927\u91cf\u8bed\u6599\u5e93\u4e2d\u7684\u9ad8\u7ea7\u6a21\u5f0f\u6765\u5b66\u4e60\u9690\u5f0f\u8868\u793a\uff0c\u4ece\u800c\u7f3a\u5c11\u5fc5\u8981\u7684\u4e0a\u4e0b\u6587\u8de8\u6a21\u5f0f\u7ebf\u7d22\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u77e5\u8bc6\u56fe\u5f62\u5f0f\u7684\u660e\u786e\u5e38\u8bc6\u77e5\u8bc6\u4e0e\u5927\u578b VLM \u7ed3\u5408\u8d77\u6765\uff0c\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u9884\u6d4b\u591a\u6a21\u5f0f\u8425\u9500\u6d3b\u52a8\u7684\u6709\u6548\u6027\u3002\u867d\u7136\u8425\u9500\u5e94\u7528\u7a0b\u5e8f\u4e3a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u6307\u6807\uff0c\u4f46\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u53ca\u65e9\u53d1\u73b0\u53ef\u80fd\u6709\u8bf4\u670d\u529b\u7684\u591a\u6a21\u5f0f\u6d3b\u52a8\u4ee5\u53ca\u8bc4\u4f30\u548c\u589e\u5f3a\u8425\u9500\u7406\u8bba\u3002|[2402.03607v1](http://arxiv.org/pdf/2402.03607v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2402.04101": "|**2024-02-06**|**VRMM: A Volumetric Relightable Morphable Head Model**|VRMM\uff1a\u4f53\u79ef\u53ef\u91cd\u590d\u7167\u660e\u53ef\u53d8\u5f62\u5934\u90e8\u6a21\u578b|Haotian Yang, Mingwu Zheng, Chongyang Ma, Yu-Kun Lai, Pengfei Wan, Haibin Huang|In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations. This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation. Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach enables accurate 3D face reconstruction from even a single portrait input. Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4f53\u79ef\u53ef\u91cd\u5149\u7167\u53d8\u5f62\u6a21\u578b (VRMM)\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e 3D \u9762\u90e8\u5efa\u6a21\u7684\u65b0\u578b\u4f53\u79ef\u548c\u53c2\u6570\u5316\u9762\u90e8\u5148\u9a8c\u3002\u867d\u7136\u6700\u8fd1\u7684\u4f53\u79ef\u5148\u9a8c\u6a21\u578b\u6bd4 3D \u53ef\u53d8\u5f62\u6a21\u578b (3DMM) \u7b49\u4f20\u7edf\u65b9\u6cd5\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5b83\u4eec\u5728\u6a21\u578b\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u91cd\u5efa\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u6211\u4eec\u7684 VRMM \u901a\u8fc7\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u8eab\u4efd\u3001\u8868\u8fbe\u548c\u7167\u660e\u7684\u6f5c\u5728\u7a7a\u95f4\u89e3\u5f00\u5e76\u7f16\u7801\u4e3a\u4f4e\u7ef4\u8868\u793a\u3002\u8be5\u6846\u67b6\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u8bbe\u8ba1\uff0c\u5927\u5927\u51cf\u5c11\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u7ea6\u675f\uff0c\u4f7f\u5176\u5728\u5b9e\u8df5\u4e2d\u66f4\u52a0\u53ef\u884c\u3002\u5b66\u4e60\u540e\u7684 VRMM \u63d0\u4f9b\u91cd\u65b0\u7167\u660e\u529f\u80fd\u5e76\u5305\u542b\u5168\u9762\u7684\u8868\u8fbe\u5f0f\u3002\u6211\u4eec\u901a\u8fc7\u5934\u50cf\u751f\u6210\u3001\u9762\u90e8\u91cd\u5efa\u548c\u52a8\u753b\u7b49\u5404\u79cd\u5e94\u7528\u5c55\u793a\u4e86 VRMM \u7684\u591a\u529f\u80fd\u6027\u548c\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u57fa\u4e8e VRMM \u7684\u65b0\u9896\u7684\u4fdd\u7559\u5148\u9a8c\u4e2a\u6027\u5316\u6846\u67b6\u89e3\u51b3\u4e86\u751f\u6210\u4f53\u79ef\u6a21\u578b\u4e2d\u8fc7\u5ea6\u62df\u5408\u7684\u5e38\u89c1\u95ee\u9898\u3002\u8fd9\u79cd\u65b9\u6cd5\u751a\u81f3\u53ef\u4ee5\u6839\u636e\u5355\u4e2a\u8096\u50cf\u8f93\u5165\u8fdb\u884c\u51c6\u786e\u7684 3D \u9762\u90e8\u91cd\u5efa\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u5c55\u793a\u4e86 VRMM \u663e\u7740\u589e\u5f3a 3D \u9762\u90e8\u5efa\u6a21\u9886\u57df\u7684\u6f5c\u529b\u3002|[2402.04101v1](http://arxiv.org/pdf/2402.04101v1)|null|\n"}, "LLM": {"2402.04249": "|**2024-02-06**|**HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal**|HarmBench\uff1a\u81ea\u52a8\u5316\u7ea2\u961f\u548c\u5f3a\u529b\u62d2\u7edd\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6|Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et.al.|Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.|\u81ea\u52a8\u5316\u7ea2\u961f\u5bf9\u4e8e\u53d1\u73b0\u548c\u51cf\u8f7b\u4e0e\u6076\u610f\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u76f8\u5173\u7684\u98ce\u9669\u6709\u7740\u5de8\u5927\u7684\u5e0c\u671b\uff0c\u4f46\u8be5\u9886\u57df\u7f3a\u4e4f\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u4e25\u683c\u8bc4\u4f30\u65b0\u65b9\u6cd5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 HarmBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u5316\u7ea2\u961f\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002\u6211\u4eec\u786e\u5b9a\u4e86\u4ee5\u524d\u5728\u7ea2\u961f\u8bc4\u4f30\u4e2d\u672a\u8003\u8651\u5230\u7684\u51e0\u4e2a\u7406\u60f3\u5c5e\u6027\uff0c\u5e76\u7cfb\u7edf\u5730\u8bbe\u8ba1 HarmBench \u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u6807\u51c6\u3002\u4f7f\u7528 HarmBench\uff0c\u6211\u4eec\u5bf9 18 \u79cd\u7ea2\u961f\u65b9\u6cd5\u548c 33 \u79cd\u76ee\u6807\u6cd5\u5b66\u7855\u58eb\u548c\u9632\u5fa1\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u6bd4\u8f83\uff0c\u5f97\u51fa\u4e86\u65b0\u9896\u7684\u89c1\u89e3\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6781\u5927\u5730\u589e\u5f3a\u4e86 LLM \u5728\u5404\u79cd\u653b\u51fb\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86 HarmBench \u5982\u4f55\u5b9e\u73b0\u653b\u51fb\u548c\u9632\u5fa1\u7684\u534f\u540c\u5f00\u53d1\u3002\u6211\u4eec\u5728 https://github.com/centerforaisafety/HarmBench \u5f00\u6e90 HarmBench\u3002|[2402.04249v1](http://arxiv.org/pdf/2402.04249v1)|null|\n", "2402.03757": "|**2024-02-06**|**The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs**|\u672c\u80fd\u504f\u89c1\uff1a\u865a\u5047\u56fe\u50cf\u5bfc\u81f4 MLLM \u4ea7\u751f\u5e7b\u89c9|Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, Tong Zhang|Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs' robustness in the presence of misleading images. The resource is available in https://github.com/MasaiahHan/CorrelationQA.|\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6700\u8fd1\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u5c55\uff0c\u5176\u4e2d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u51fa\u73b0\u8d4b\u4e88\u4e86LLM\u89c6\u89c9\u80fd\u529b\uff0c\u5728\u5404\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8868\u73b0\u3002\u7136\u800c\uff0c\u5f53\u51fa\u73b0\u67d0\u4e9b\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\u65f6\uff0c\u90a3\u4e9b\u5f3a\u5927\u7684 MLLM\uff08\u4f8b\u5982 GPT-4V\uff09\u4ecd\u7136\u4f1a\u4e25\u91cd\u5931\u8d25\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u56f0\u6270 MLLM \u7684\u4e00\u7c7b\u5178\u578b\u8f93\u5165\uff0c\u8fd9\u4e9b\u8f93\u5165\u7531\u9ad8\u5ea6\u76f8\u5173\u4f46\u4e0e\u7b54\u6848\u4e0d\u4e00\u81f4\u7684\u56fe\u50cf\u7ec4\u6210\uff0c\u5bfc\u81f4 MLLM \u4ea7\u751f\u5e7b\u89c9\u3002\u4e3a\u4e86\u91cf\u5316\u6548\u679c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 CorrelationQA\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u8bc4\u4f30\u7ed9\u5b9a\u865a\u5047\u56fe\u50cf\u7684\u5e7b\u89c9\u6c34\u5e73\u7684\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b 13 \u4e2a\u7c7b\u522b\u7684 7,308 \u4e2a\u6587\u672c-\u56fe\u50cf\u5bf9\u3002\u57fa\u4e8e\u6240\u63d0\u51fa\u7684 CorrelationQA\uff0c\u6211\u4eec\u5bf9 9 \u4e2a\u4e3b\u6d41 MLLM \u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u8868\u660e\u5b83\u4eec\u666e\u904d\u4e0d\u540c\u7a0b\u5ea6\u5730\u906d\u53d7\u8fd9\u79cd\u672c\u80fd\u504f\u89c1\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7b56\u5212\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u7ed3\u679c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u8bc4\u4f30 MLLM \u5728\u5b58\u5728\u8bef\u5bfc\u6027\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u7684\u7a33\u5065\u6027\u3002\u8be5\u8d44\u6e90\u4f4d\u4e8e https://github.com/MasaiahHan/CorrelationQA\u3002|[2402.03757v1](http://arxiv.org/pdf/2402.03757v1)|null|\n", "2402.03746": "|**2024-02-06**|**Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback**|\u4f7f\u7528 AI \u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u6765\u8c03\u6574\u89c6\u9891\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b|Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, Jonghyun Choi|Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). The previous approaches for VLMMs involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and adding additional learnable modules. Video and text multimodal alignment remains challenging, primarily due to the deficient volume and quality of multimodal instruction-tune data compared to text-only data. We present a novel alignment strategy that employs multimodal AI system to oversee itself called Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. In specific, we propose context-aware reward modeling by providing detailed video descriptions as context during the generation of preference feedback in order to enrich the understanding of video content. Demonstrating enhanced performance across diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.|\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5f71\u54cd\u4e86\u89c6\u9891\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08VLMM\uff09\u7684\u53d1\u5c55\u3002\u4e4b\u524d\u7684 VLMM \u65b9\u6cd5\u6d89\u53ca\u4f7f\u7528\u6307\u4ee4\u8c03\u6574\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03 (SFT)\u3001\u5c06 LLM \u4e0e\u89c6\u89c9\u7f16\u7801\u5668\u96c6\u6210\u4ee5\u53ca\u6dfb\u52a0\u989d\u5916\u7684\u53ef\u5b66\u4e60\u6a21\u5757\u3002\u89c6\u9891\u548c\u6587\u672c\u591a\u6a21\u5f0f\u5bf9\u9f50\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u4e0e\u7eaf\u6587\u672c\u6570\u636e\u76f8\u6bd4\uff0c\u591a\u6a21\u5f0f\u6307\u4ee4\u8c03\u6574\u6570\u636e\u7684\u6570\u91cf\u548c\u8d28\u91cf\u4e0d\u8db3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u9f50\u7b56\u7565\uff0c\u91c7\u7528\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6765\u76d1\u7763\u81ea\u8eab\uff0c\u79f0\u4e3a\u4eba\u5de5\u667a\u80fd\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLAIF\uff09\uff0c\u63d0\u4f9b\u81ea\u6211\u504f\u597d\u53cd\u9988\u6765\u5b8c\u5584\u81ea\u200b\u200b\u8eab\u5e76\u4fc3\u8fdb\u89c6\u9891\u548c\u6587\u672c\u6a21\u6001\u7684\u5bf9\u9f50\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u5956\u52b1\u5efa\u6a21\uff0c\u901a\u8fc7\u5728\u751f\u6210\u504f\u597d\u53cd\u9988\u671f\u95f4\u63d0\u4f9b\u8be6\u7ec6\u7684\u89c6\u9891\u63cf\u8ff0\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u4ee5\u4e30\u5bcc\u5bf9\u89c6\u9891\u5185\u5bb9\u7684\u7406\u89e3\u3002\u6211\u4eec\u7684\u591a\u6a21\u6001 RLAIF \u65b9\u6cd5 VLM-RLAIF \u5728\u4e0d\u540c\u7684\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u589e\u5f3a\u7684\u6027\u80fd\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5305\u62ec SFT \u6a21\u578b\u5728\u5185\u7684\u73b0\u6709\u65b9\u6cd5\u3002\u6211\u4eec\u81f4\u529b\u4e8e\u5f00\u6e90\u6211\u4eec\u7684\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002|[2402.03746v1](http://arxiv.org/pdf/2402.03746v1)|null|\n", "2402.03699": "|**2024-02-06**|**Automatic Robotic Development through Collaborative Framework by Large Language Models**|\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u534f\u4f5c\u6846\u67b6\u8fdb\u884c\u81ea\u52a8\u673a\u5668\u4eba\u5f00\u53d1|Zhirong Luan, Yujun Lai|Despite the remarkable code generation abilities of large language models LLMs, they still face challenges in complex task handling. Robot development, a highly intricate field, inherently demands human involvement in task allocation and collaborative teamwork . To enhance robot development, we propose an innovative automated collaboration framework inspired by real-world robot developers. This framework employs multiple LLMs in distinct roles analysts, programmers, and testers. Analysts delve deep into user requirements, enabling programmers to produce precise code, while testers fine-tune the parameters based on user feedback for practical robot application. Each LLM tackles diverse, critical tasks within the development process. Clear collaboration rules emulate real world teamwork among LLMs. Analysts, programmers, and testers form a cohesive team overseeing strategy, code, and parameter adjustments . Through this framework, we achieve complex robot development without requiring specialized knowledge, relying solely on non experts participation.|\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6cd5\u5b66\u7855\u58eb\u5177\u6709\u51fa\u8272\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u590d\u6742\u4efb\u52a1\u5904\u7406\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002\u673a\u5668\u4eba\u5f00\u53d1\u662f\u4e00\u4e2a\u9ad8\u5ea6\u590d\u6742\u7684\u9886\u57df\uff0c\u672c\u8d28\u4e0a\u9700\u8981\u4eba\u7c7b\u53c2\u4e0e\u4efb\u52a1\u5206\u914d\u548c\u534f\u4f5c\u56e2\u961f\u5408\u4f5c\u3002\u4e3a\u4e86\u52a0\u5f3a\u673a\u5668\u4eba\u5f00\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u53d7\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5f00\u53d1\u4eba\u5458\u542f\u53d1\u7684\u521b\u65b0\u81ea\u52a8\u5316\u534f\u4f5c\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e86\u591a\u4e2a\u6cd5\u5b66\u7855\u58eb\u62c5\u4efb\u4e0d\u540c\u89d2\u8272\u7684\u5206\u6790\u5e08\u3001\u7a0b\u5e8f\u5458\u548c\u6d4b\u8bd5\u4eba\u5458\u3002\u5206\u6790\u5e08\u6df1\u5165\u7814\u7a76\u7528\u6237\u9700\u6c42\uff0c\u4f7f\u7a0b\u5e8f\u5458\u80fd\u591f\u7f16\u5199\u51fa\u7cbe\u786e\u7684\u4ee3\u7801\uff0c\u800c\u6d4b\u8bd5\u4eba\u5458\u5219\u6839\u636e\u7528\u6237\u53cd\u9988\u5fae\u8c03\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u5b9e\u9645\u7684\u673a\u5668\u4eba\u5e94\u7528\u3002\u6bcf\u4e2a\u6cd5\u5b66\u7855\u58eb\u90fd\u5904\u7406\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u5404\u79cd\u5173\u952e\u4efb\u52a1\u3002\u6e05\u6670\u7684\u534f\u4f5c\u89c4\u5219\u6a21\u62df\u4e86\u6cd5\u5b66\u7855\u58eb\u4e4b\u95f4\u73b0\u5b9e\u4e16\u754c\u7684\u56e2\u961f\u5408\u4f5c\u3002\u5206\u6790\u5e08\u3001\u7a0b\u5e8f\u5458\u548c\u6d4b\u8bd5\u4eba\u5458\u7ec4\u6210\u4e86\u4e00\u4e2a\u6709\u51dd\u805a\u529b\u7684\u56e2\u961f\uff0c\u8d1f\u8d23\u76d1\u7763\u7b56\u7565\u3001\u4ee3\u7801\u548c\u53c2\u6570\u8c03\u6574\u3002\u901a\u8fc7\u8fd9\u4e2a\u6846\u67b6\uff0c\u6211\u4eec\u65e0\u9700\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4ec5\u4f9d\u9760\u975e\u4e13\u5bb6\u53c2\u4e0e\u5373\u53ef\u5b9e\u73b0\u590d\u6742\u7684\u673a\u5668\u4eba\u5f00\u53d1\u3002|[2402.03699v1](http://arxiv.org/pdf/2402.03699v1)|null|\n"}, "Transformer": {"2402.04139": "|**2024-02-06**|**U-shaped Vision Mamba for Single Image Dehazing**|\u7528\u4e8e\u5355\u56fe\u50cf\u53bb\u96fe\u7684 U \u5f62 Vision Mamba|Zhuoran Zheng, Chen Wu|Currently, Transformer is the most popular architecture for image dehazing, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices. To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image dehazing network. Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies. Extensive experimental results demonstrate the effectiveness of our method. Our method provides a more highly efficient idea of long-range dependency modeling for image dehazing as well as other image restoration tasks. The URL of the code is \\url{https://github.com/zzr-idam}.|\u76ee\u524d\uff0cTransformer \u662f\u6700\u6d41\u884c\u7684\u56fe\u50cf\u53bb\u96fe\u67b6\u6784\uff0c\u4f46\u7531\u4e8e\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u5176\u5904\u7406\u8fdc\u7a0b\u4f9d\u8d56\u7684\u80fd\u529b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u53d7\u5230\u9650\u5236\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 U \u5f62 Vision Mamba (UVM-Net)\uff0c\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u56fe\u50cf\u53bb\u96fe\u7f51\u7edc\u3002\u53d7\u72b6\u6001\u7a7a\u95f4\u5e8f\u5217\u6a21\u578b\uff08SSM\uff09\uff08\u4e00\u79cd\u4ee5\u5176\u5904\u7406\u957f\u5e8f\u5217\u7684\u80fd\u529b\u800c\u95fb\u540d\u7684\u65b0\u578b\u6df1\u5ea6\u5e8f\u5217\u6a21\u578b\uff09\u7684\u542f\u53d1\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a Bi-SSM \u6a21\u5757\uff0c\u5b83\u5c06\u5377\u79ef\u5c42\u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0e SSM \u7684\u80fd\u529b\u96c6\u6210\u5728\u4e00\u8d77\u3002\u6355\u83b7\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u56fe\u50cf\u53bb\u96fe\u4ee5\u53ca\u5176\u4ed6\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8fdc\u7a0b\u4f9d\u8d56\u5efa\u6a21\u601d\u60f3\u3002\u4ee3\u7801\u7684 URL \u662f \\url{https://github.com/zzr-idam}\u3002|[2402.04139v1](http://arxiv.org/pdf/2402.04139v1)|null|\n", "2402.04009": "|**2024-02-06**|**Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning**|\u7528\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u4f4e\u9636\u6ce8\u610f\u529b\u4fa7\u8c03|Ningyuan Tang, Minghao Fu, Ke Zhu, Jianxin Wu|In finetuning a large pretrained model to downstream tasks, parameter-efficient fine-tuning (PEFT) methods can effectively finetune pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed. Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model's parameters have to be computed and stored during finetuning. We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network. LAST trains a side-network composed of only low-rank self-attention modules. By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge. We also show that LAST can be highly parallel across multiple optimization objectives, making it very efficient in downstream task adaptation, for example, in finding optimal hyperparameters. LAST outperforms previous state-of-the-art methods on VTAB-1K and other visual adaptation tasks with roughly only 30\\% of GPU memory footprint and 60\\% of training time compared to existing PEFT methods, but achieves significantly higher accuracy.|\u5728\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u5230\u4e0b\u6e38\u4efb\u52a1\u65f6\uff0c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5fae\u8c03\u53ef\u8bad\u7ec3\u53c2\u6570\u8f83\u5c11\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u4f1a\u9047\u5230 GPU \u5185\u5b58\u6d88\u8017\u9ad8\u548c\u8bad\u7ec3\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002\u7531\u4e8e\u8fd9\u4e9b\u65b9\u6cd5\u4e2d\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u56e0\u6b64\u5728\u5fae\u8c03\u671f\u95f4\u5fc5\u987b\u8ba1\u7b97\u5e76\u5b58\u50a8\u4e0e\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u76f8\u5173\u7684\u68af\u5ea6\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4f4e\u79e9\u6ce8\u610f\u529b\u4fa7\u8c03\uff08LAST\uff09\uff0c\u5b83\u4e0d\u4ec5\u51bb\u7ed3\u9884\u8bad\u7ec3\u7f51\u7edc\u7684\u53c2\u6570\uff0c\u8fd8\u51bb\u7ed3\u9884\u8bad\u7ec3\u7f51\u7edc\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u5c06\u53ef\u8bad\u7ec3\u6a21\u5757\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u5206\u5f00\u3002 LAST \u8bad\u7ec3\u4ec5\u7531\u4f4e\u79e9\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u7ec4\u6210\u7684\u4fa7\u7f51\u7edc\u3002\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u89c6\u4e3a\u51bb\u7ed3\u7684\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4fa7\u7f51\u7edc\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u83b7\u53d6\u4e2d\u95f4\u8f93\u51fa\uff0c\u5e76\u4e13\u6ce8\u4e8e\u5b66\u4e60\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u77e5\u8bc6\u3002\u6211\u4eec\u8fd8\u8868\u660e\uff0cLAST \u53ef\u4ee5\u5728\u591a\u4e2a\u4f18\u5316\u76ee\u6807\u4e0a\u9ad8\u5ea6\u5e76\u884c\uff0c\u4f7f\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u65b9\u9762\u975e\u5e38\u9ad8\u6548\uff0c\u4f8b\u5982\u5728\u5bfb\u627e\u6700\u4f73\u8d85\u53c2\u6570\u65b9\u9762\u3002\u4e0e\u73b0\u6709 PEFT \u65b9\u6cd5\u76f8\u6bd4\uff0cLAST \u5728 VTAB-1K \u548c\u5176\u4ed6\u89c6\u89c9\u9002\u5e94\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5927\u7ea6\u4ec5\u5360\u7528 30% \u7684 GPU \u5185\u5b58\u5360\u7528\u548c 60% \u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u4f46\u7cbe\u5ea6\u5374\u663e\u7740\u63d0\u9ad8\u3002|[2402.04009v1](http://arxiv.org/pdf/2402.04009v1)|null|\n", "2402.03981": "|**2024-02-06**|**Controllable Diverse Sampling for Diffusion Based Motion Behavior Forecasting**|\u57fa\u4e8e\u6269\u6563\u7684\u8fd0\u52a8\u884c\u4e3a\u9884\u6d4b\u7684\u53ef\u63a7\u591a\u6837\u5316\u91c7\u6837|Yiming Xu, Hao Cheng, Monika Sester|In autonomous driving tasks, trajectory prediction in complex traffic environments requires adherence to real-world context conditions and behavior multimodalities. Existing methods predominantly rely on prior assumptions or generative models trained on curated data to learn road agents' stochastic behavior bounded by scene constraints. However, they often face mode averaging issues due to data imbalance and simplistic priors, and could even suffer from mode collapse due to unstable training and single ground truth supervision. These issues lead the existing methods to a loss of predictive diversity and adherence to the scene constraints. To address these challenges, we introduce a novel trajectory generator named Controllable Diffusion Trajectory (CDT), which integrates map information and social interactions into a Transformer-based conditional denoising diffusion model to guide the prediction of future trajectories. To ensure multimodality, we incorporate behavioral tokens to direct the trajectory's modes, such as going straight, turning right or left. Moreover, we incorporate the predicted endpoints as an alternative behavioral token into the CDT model to facilitate the prediction of accurate trajectories. Extensive experiments on the Argoverse 2 benchmark demonstrate that CDT excels in generating diverse and scene-compliant trajectories in complex urban settings.|\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\uff0c\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u9884\u6d4b\u9700\u8981\u9075\u5b88\u73b0\u5b9e\u4e16\u754c\u7684\u4e0a\u4e0b\u6587\u6761\u4ef6\u548c\u884c\u4e3a\u591a\u6a21\u6001\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5148\u9a8c\u5047\u8bbe\u6216\u57fa\u4e8e\u7cbe\u9009\u6570\u636e\u8bad\u7ec3\u7684\u751f\u6210\u6a21\u578b\u6765\u5b66\u4e60\u9053\u8def\u4ee3\u7406\u53d7\u573a\u666f\u7ea6\u675f\u7684\u968f\u673a\u884c\u4e3a\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6570\u636e\u4e0d\u5e73\u8861\u548c\u7b80\u5355\u5316\u5148\u9a8c\uff0c\u5b83\u4eec\u7ecf\u5e38\u9762\u4e34\u6a21\u5f0f\u5e73\u5747\u95ee\u9898\uff0c\u751a\u81f3\u53ef\u80fd\u7531\u4e8e\u4e0d\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u5355\u4e00\u7684\u5730\u9762\u5b9e\u51b5\u76d1\u7763\u800c\u906d\u53d7\u6a21\u5f0f\u5d29\u6e83\u3002\u8fd9\u4e9b\u95ee\u9898\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u5931\u53bb\u9884\u6d4b\u591a\u6837\u6027\u548c\u9075\u5b88\u573a\u666f\u7ea6\u675f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u53ef\u63a7\u6269\u6563\u8f68\u8ff9\uff08CDT\uff09\u7684\u65b0\u578b\u8f68\u8ff9\u751f\u6210\u5668\uff0c\u5b83\u5c06\u5730\u56fe\u4fe1\u606f\u548c\u793e\u4ea4\u4e92\u52a8\u96c6\u6210\u5230\u57fa\u4e8e Transformer \u7684\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u8f68\u8ff9\u7684\u9884\u6d4b\u3002\u4e3a\u4e86\u786e\u4fdd\u591a\u6a21\u6001\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u884c\u4e3a\u6807\u8bb0\u6765\u6307\u5bfc\u8f68\u8ff9\u7684\u6a21\u5f0f\uff0c\u4f8b\u5982\u76f4\u884c\u3001\u53f3\u8f6c\u6216\u5de6\u8f6c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u9884\u6d4b\u7684\u7aef\u70b9\u4f5c\u4e3a\u66ff\u4ee3\u884c\u4e3a\u6807\u8bb0\u7eb3\u5165 CDT \u6a21\u578b\u4e2d\uff0c\u4ee5\u4fc3\u8fdb\u51c6\u786e\u8f68\u8ff9\u7684\u9884\u6d4b\u3002\u5bf9 Argoverse 2 \u57fa\u51c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCDT \u64c5\u957f\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u751f\u6210\u591a\u6837\u5316\u4e14\u7b26\u5408\u573a\u666f\u7684\u8f68\u8ff9\u3002|[2402.03981v1](http://arxiv.org/pdf/2402.03981v1)|null|\n", "2402.03754": "|**2024-02-06**|**Intensive Vision-guided Network for Radiology Report Generation**|\u7528\u4e8e\u751f\u6210\u653e\u5c04\u5b66\u62a5\u544a\u7684\u5f3a\u5316\u89c6\u89c9\u5f15\u5bfc\u7f51\u7edc|Fudan Zheng, Mengfei Li, Ying Wang, Weijiang Yu, Ruixuan Wang, Zhiguang Chen, Nong Xiao, Yutong Lu|Automatic radiology report generation is booming due to its huge application potential for the healthcare industry. However, existing computer vision and natural language processing approaches to tackle this problem are limited in two aspects. First, when extracting image features, most of them neglect multi-view reasoning in vision and model single-view structure of medical images, such as space-view or channel-view. However, clinicians rely on multi-view imaging information for comprehensive judgment in daily clinical diagnosis. Second, when generating reports, they overlook context reasoning with multi-modal information and focus on pure textual optimization utilizing retrieval-based methods. We aim to address these two issues by proposing a model that better simulates clinicians' perspectives and generates more accurate reports. Given the above limitation in feature extraction, we propose a Globally-intensive Attention (GIA) module in the medical image encoder to simulate and integrate multi-view vision perception. GIA aims to learn three types of vision perception: depth view, space view, and pixel view. On the other hand, to address the above problem in report generation, we explore how to involve multi-modal signals to generate precisely matched reports, i.e., how to integrate previously predicted words with region-aware visual content in next word prediction. Specifically, we design a Visual Knowledge-guided Decoder (VKGD), which can adaptively consider how much the model needs to rely on visual information and previously predicted text to assist next word prediction. Hence, our final Intensive Vision-guided Network (IVGN) framework includes a GIA-guided Visual Encoder and the VKGD. Experiments on two commonly-used datasets IU X-Ray and MIMIC-CXR demonstrate the superior ability of our method compared with other state-of-the-art approaches.|\u81ea\u52a8\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7531\u4e8e\u5176\u5728\u533b\u7597\u4fdd\u5065\u884c\u4e1a\u7684\u5de8\u5927\u5e94\u7528\u6f5c\u529b\u800c\u84ec\u52c3\u53d1\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u5728\u4e24\u4e2a\u65b9\u9762\u53d7\u5230\u9650\u5236\u3002\u9996\u5148\uff0c\u5728\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\u65f6\uff0c\u5927\u591a\u6570\u4eba\u5ffd\u7565\u4e86\u89c6\u89c9\u4e2d\u7684\u591a\u89c6\u56fe\u63a8\u7406\uff0c\u800c\u5bf9\u533b\u5b66\u56fe\u50cf\u7684\u5355\u89c6\u56fe\u7ed3\u6784\u8fdb\u884c\u5efa\u6a21\uff0c\u4f8b\u5982\u7a7a\u95f4\u89c6\u56fe\u6216\u901a\u9053\u89c6\u56fe\u3002\u7136\u800c\uff0c\u4e34\u5e8a\u533b\u751f\u5728\u65e5\u5e38\u4e34\u5e8a\u8bca\u65ad\u4e2d\u4f9d\u9760\u591a\u89c6\u89d2\u5f71\u50cf\u4fe1\u606f\u8fdb\u884c\u7efc\u5408\u5224\u65ad\u3002\u5176\u6b21\uff0c\u5728\u751f\u6210\u62a5\u544a\u65f6\uff0c\u4ed6\u4eec\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u800c\u4e13\u6ce8\u4e8e\u5229\u7528\u57fa\u4e8e\u68c0\u7d22\u7684\u65b9\u6cd5\u8fdb\u884c\u7eaf\u6587\u672c\u4f18\u5316\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u66f4\u597d\u5730\u6a21\u62df\u4e34\u5e8a\u533b\u751f\u7684\u89c2\u70b9\u5e76\u751f\u6210\u66f4\u51c6\u786e\u7684\u62a5\u544a\u3002\u9274\u4e8e\u7279\u5f81\u63d0\u53d6\u4e2d\u7684\u4e0a\u8ff0\u9650\u5236\uff0c\u6211\u4eec\u5728\u533b\u5b66\u56fe\u50cf\u7f16\u7801\u5668\u4e2d\u63d0\u51fa\u4e86\u5168\u5c40\u5bc6\u96c6\u6ce8\u610f\u529b\uff08GIA\uff09\u6a21\u5757\u6765\u6a21\u62df\u548c\u96c6\u6210\u591a\u89c6\u56fe\u89c6\u89c9\u611f\u77e5\u3002 GIA \u65e8\u5728\u5b66\u4e60\u4e09\u79cd\u7c7b\u578b\u7684\u89c6\u89c9\u611f\u77e5\uff1a\u6df1\u5ea6\u89c6\u56fe\u3001\u7a7a\u95f4\u89c6\u56fe\u548c\u50cf\u7d20\u89c6\u56fe\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u4e3a\u4e86\u89e3\u51b3\u62a5\u544a\u751f\u6210\u4e2d\u7684\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63a2\u7d22\u5982\u4f55\u6d89\u53ca\u591a\u6a21\u6001\u4fe1\u53f7\u6765\u751f\u6210\u7cbe\u786e\u5339\u914d\u7684\u62a5\u544a\uff0c\u5373\u5982\u4f55\u5728\u4e0b\u4e00\u4e2a\u5355\u8bcd\u9884\u6d4b\u4e2d\u5c06\u5148\u524d\u9884\u6d4b\u7684\u5355\u8bcd\u4e0e\u533a\u57df\u611f\u77e5\u7684\u89c6\u89c9\u5185\u5bb9\u96c6\u6210\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u89c6\u89c9\u77e5\u8bc6\u5f15\u5bfc\u89e3\u7801\u5668\uff08VKGD\uff09\uff0c\u5b83\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u8003\u8651\u6a21\u578b\u9700\u8981\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u89c6\u89c9\u4fe1\u606f\u548c\u5148\u524d\u9884\u6d4b\u7684\u6587\u672c\u6765\u8f85\u52a9\u4e0b\u4e00\u4e2a\u5355\u8bcd\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u6700\u7ec8\u7684\u5f3a\u5316\u89c6\u89c9\u5f15\u5bfc\u7f51\u7edc (IVGN) \u6846\u67b6\u5305\u62ec GIA \u5f15\u5bfc\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c VKGD\u3002\u5bf9\u4e24\u4e2a\u5e38\u7528\u6570\u636e\u96c6 IU X-Ray \u548c MIMIC-CXR \u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u5353\u8d8a\u7684\u80fd\u529b\u3002|[2402.03754v1](http://arxiv.org/pdf/2402.03754v1)|null|\n", "2402.03752": "|**2024-02-06**|**Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images**|\u5728\u5177\u6709\u6700\u5c0f\u7f29\u653e\u56fe\u50cf\u7684\u5c0f\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u89c6\u89c9\u53d8\u538b\u5668|Jen Hong Tan|Can a lightweight Vision Transformer (ViT) match or exceed the performance of Convolutional Neural Networks (CNNs) like ResNet on small datasets with small image resolutions? This report demonstrates that a pure ViT can indeed achieve superior performance through pre-training, using a masked auto-encoder technique with minimal image scaling. Our experiments on the CIFAR-10 and CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters and a multiply-accumulate (MAC) count below 0.27G, qualifying them as 'lightweight' models. Unlike previous approaches, our method attains state-of-the-art performance among similar lightweight transformer-based architectures without significantly scaling up images from CIFAR-10 and CIFAR-100. This achievement underscores the efficiency of our model, not only in handling small datasets but also in effectively processing images close to their original scale.|\u5728\u5c0f\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u5c0f\u6570\u636e\u96c6\u4e0a\uff0c\u8f7b\u91cf\u7ea7 Vision Transformer (ViT) \u80fd\u5426\u5339\u914d\u6216\u8d85\u8fc7 ResNet \u7b49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7684\u6027\u80fd\uff1f\u8be5\u62a5\u544a\u8868\u660e\uff0c\u7eaf ViT \u786e\u5b9e\u53ef\u4ee5\u901a\u8fc7\u9884\u8bad\u7ec3\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4f7f\u7528\u5e26\u6709\u6700\u5c0f\u56fe\u50cf\u7f29\u653e\u7684\u63a9\u6a21\u81ea\u52a8\u7f16\u7801\u5668\u6280\u672f\u3002\u6211\u4eec\u5728 CIFAR-10 \u548c CIFAR-100 \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u6d89\u53ca ViT \u6a21\u578b\uff0c\u53c2\u6570\u5c11\u4e8e 365 \u4e07\u4e2a\uff0c\u4e58\u6cd5\u7d2f\u52a0 (MAC) \u8ba1\u6570\u4f4e\u4e8e 0.27G\uff0c\u4f7f\u5b83\u4eec\u7b26\u5408\u201c\u8f7b\u91cf\u7ea7\u201d\u6a21\u578b\u7684\u8d44\u683c\u3002\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7c7b\u4f3c\u7684\u57fa\u4e8e Transformer \u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\u4e2d\u83b7\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u663e\u7740\u653e\u5927 CIFAR-10 \u548c CIFAR-100 \u7684\u56fe\u50cf\u3002\u8fd9\u4e00\u6210\u5c31\u5f3a\u8c03\u4e86\u6211\u4eec\u6a21\u578b\u7684\u6548\u7387\uff0c\u4e0d\u4ec5\u5728\u5904\u7406\u5c0f\u6570\u636e\u96c6\u65b9\u9762\uff0c\u800c\u4e14\u5728\u6709\u6548\u5904\u7406\u63a5\u8fd1\u539f\u59cb\u6bd4\u4f8b\u7684\u56fe\u50cf\u65b9\u9762\u3002|[2402.03752v1](http://arxiv.org/pdf/2402.03752v1)|null|\n", "2402.03716": "|**2024-02-06**|**Attention-based Shape and Gait Representations Learning for Video-based Cloth-Changing Person Re-Identification**|\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5f62\u72b6\u548c\u6b65\u6001\u8868\u793a\u5b66\u4e60\uff0c\u7528\u4e8e\u57fa\u4e8e\u89c6\u9891\u7684\u6362\u8863\u4eba\u5458\u91cd\u65b0\u8bc6\u522b|Vuong D. Nguyen, Samiha Mirza, Pranav Mantini, Shishir K. Shah|Current state-of-the-art Video-based Person Re-Identification (Re-ID) primarily relies on appearance features extracted by deep learning models. These methods are not applicable for long-term analysis in real-world scenarios where persons have changed clothes, making appearance information unreliable. In this work, we deal with the practical problem of Video-based Cloth-Changing Person Re-ID (VCCRe-ID) by proposing \"Attention-based Shape and Gait Representations Learning\" (ASGL) for VCCRe-ID. Our ASGL framework improves Re-ID performance under clothing variations by learning clothing-invariant gait cues using a Spatial-Temporal Graph Attention Network (ST-GAT). Given the 3D-skeleton-based spatial-temporal graph, our proposed ST-GAT comprises multi-head attention modules, which are able to enhance the robustness of gait embeddings under viewpoint changes and occlusions. The ST-GAT amplifies the important motion ranges and reduces the influence of noisy poses. Then, the multi-head learning module effectively reserves beneficial local temporal dynamics of movement. We also boost discriminative power of person representations by learning body shape cues using a GAT. Experiments on two large-scale VCCRe-ID datasets demonstrate that our proposed framework outperforms state-of-the-art methods by 12.2% in rank-1 accuracy and 7.0% in mAP.|\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u89c6\u9891\u7684\u884c\u4eba\u91cd\u65b0\u8bc6\u522b\uff08Re-ID\uff09\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u7684\u5916\u89c2\u7279\u5f81\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u4eba\u4eec\u6362\u8863\u670d\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u957f\u671f\u5206\u6790\uff0c\u4f7f\u5f97\u5916\u89c2\u4fe1\u606f\u4e0d\u53ef\u9760\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u4e3a VCCRe-ID \u63d0\u51fa\u201c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5f62\u72b6\u548c\u6b65\u6001\u8868\u793a\u5b66\u4e60\u201d\uff08ASGL\uff09\u6765\u89e3\u51b3\u57fa\u4e8e\u89c6\u9891\u7684\u6362\u8863\u4eba\u91cd\u8bc6\u522b\uff08VCCRe-ID\uff09\u7684\u5b9e\u9645\u95ee\u9898\u3002\u6211\u4eec\u7684 ASGL \u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u65f6\u7a7a\u56fe\u6ce8\u610f\u7f51\u7edc (ST-GAT) \u5b66\u4e60\u670d\u88c5\u4e0d\u53d8\u7684\u6b65\u6001\u7ebf\u7d22\uff0c\u63d0\u9ad8\u4e86\u670d\u88c5\u53d8\u5316\u4e0b\u7684 Re-ID \u6027\u80fd\u3002\u8003\u8651\u5230\u57fa\u4e8e 3D \u9aa8\u67b6\u7684\u65f6\u7a7a\u56fe\uff0c\u6211\u4eec\u63d0\u51fa\u7684 ST-GAT \u5305\u542b\u591a\u5934\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u80fd\u591f\u589e\u5f3a\u6b65\u6001\u5d4c\u5165\u5728\u89c6\u70b9\u53d8\u5316\u548c\u906e\u6321\u4e0b\u7684\u9c81\u68d2\u6027\u3002 ST-GAT \u653e\u5927\u4e86\u91cd\u8981\u7684\u8fd0\u52a8\u8303\u56f4\u5e76\u51cf\u5c11\u4e86\u566a\u58f0\u59ff\u52bf\u7684\u5f71\u54cd\u3002\u7136\u540e\uff0c\u591a\u5934\u5b66\u4e60\u6a21\u5757\u6709\u6548\u5730\u4fdd\u7559\u6709\u76ca\u7684\u5c40\u90e8\u8fd0\u52a8\u52a8\u6001\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u4f7f\u7528 GAT \u5b66\u4e60\u4f53\u5f62\u7ebf\u7d22\u6765\u589e\u5f3a\u4eba\u7269\u8868\u5f81\u7684\u8fa8\u522b\u80fd\u529b\u3002\u5728\u4e24\u4e2a\u5927\u578b VCCRe-ID \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u5728 1 \u7ea7\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5 12.2%\uff0c\u5728 mAP \u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5 7.0%\u3002|[2402.03716v1](http://arxiv.org/pdf/2402.03716v1)|null|\n"}, "3D/CG": {"2402.04195": "|**2024-02-06**|**Instance by Instance: An Iterative Framework for Multi-instance 3D Registration**|\u9010\u4e2a\u5b9e\u4f8b\uff1a\u591a\u5b9e\u4f8b 3D \u914d\u51c6\u7684\u8fed\u4ee3\u6846\u67b6|Xinyue Cao, Xiyu Zhang, Yuxin Cheng, Zhaoshuai Qi, Yanning Zhang, Jiaqi Yang|Multi-instance registration is a challenging problem in computer vision and robotics, where multiple instances of an object need to be registered in a standard coordinate system. In this work, we propose the first iterative framework called instance-by-instance (IBI) for multi-instance 3D registration (MI-3DReg). It successively registers all instances in a given scenario, starting from the easiest and progressing to more challenging ones. Throughout the iterative process, outliers are eliminated continuously, leading to an increasing inlier rate for the remaining and more challenging instances. Under the IBI framework, we further propose a sparse-to-dense-correspondence-based multi-instance registration method (IBI-S2DC) to achieve robust MI-3DReg. Experiments on the synthetic and real datasets have demonstrated the effectiveness of IBI and suggested the new state-of-the-art performance of IBI-S2DC, e.g., our MHF1 is 12.02%/12.35% higher than the existing state-of-the-art method ECC on the synthetic/real datasets.|\u591a\u5b9e\u4f8b\u6ce8\u518c\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5176\u4e2d\u4e00\u4e2a\u5bf9\u8c61\u7684\u591a\u4e2a\u5b9e\u4f8b\u9700\u8981\u5728\u6807\u51c6\u5750\u6807\u7cfb\u4e2d\u6ce8\u518c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u540d\u4e3a\u9010\u5b9e\u4f8b\uff08IBI\uff09\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u5b9e\u4f8b 3D \u914d\u51c6\uff08MI-3DReg\uff09\u3002\u5b83\u8fde\u7eed\u6ce8\u518c\u7ed9\u5b9a\u573a\u666f\u4e2d\u7684\u6240\u6709\u5b9e\u4f8b\uff0c\u4ece\u6700\u7b80\u5355\u7684\u5f00\u59cb\uff0c\u9010\u6e10\u5230\u66f4\u5177\u6311\u6218\u6027\u7684\u5b9e\u4f8b\u3002\u5728\u6574\u4e2a\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\uff0c\u5f02\u5e38\u503c\u4e0d\u65ad\u88ab\u6d88\u9664\uff0c\u5bfc\u81f4\u5269\u4f59\u7684\u548c\u66f4\u5177\u6311\u6218\u6027\u7684\u5b9e\u4f8b\u7684\u5185\u90e8\u503c\u7387\u4e0d\u65ad\u589e\u52a0\u3002\u5728IBI\u6846\u67b6\u4e0b\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u5230\u5bc6\u96c6\u5bf9\u5e94\u7684\u591a\u5b9e\u4f8b\u6ce8\u518c\u65b9\u6cd5\uff08IBI-S2DC\uff09\u6765\u5b9e\u73b0\u9c81\u68d2\u7684MI-3DReg\u3002\u5bf9\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86 IBI \u7684\u6709\u6548\u6027\uff0c\u5e76\u8868\u660e\u4e86 IBI-S2DC \u7684\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f8b\u5982\uff0c\u6211\u4eec\u7684 MHF1 \u6bd4\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6027\u80fd\u9ad8 12.02%/12.35%\u5408\u6210/\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u827a\u672f\u65b9\u6cd5 ECC\u3002|[2402.04195v1](http://arxiv.org/pdf/2402.04195v1)|null|\n", "2402.04171": "|**2024-02-06**|**3D Volumetric Super-Resolution in Radiology Using 3D RRDB-GAN**|\u4f7f\u7528 3D RRDB-GAN \u5b9e\u73b0\u653e\u5c04\u5b66\u4e2d\u7684 3D \u4f53\u79ef\u8d85\u5206\u8fa8\u7387|Juhyung Ha, Nian Wang, Surendra Maharjan, Xuhong Zhang|This study introduces the 3D Residual-in-Residual Dense Block GAN (3D RRDB-GAN) for 3D super-resolution for radiology imagery. A key aspect of 3D RRDB-GAN is the integration of a 2.5D perceptual loss function, which contributes to improved volumetric image quality and realism. The effectiveness of our model was evaluated through 4x super-resolution experiments across diverse datasets, including Mice Brain MRH, OASIS, HCP1200, and MSD-Task-6. These evaluations, encompassing both quantitative metrics like LPIPS and FID and qualitative assessments through sample visualizations, demonstrate the models effectiveness in detailed image analysis. The 3D RRDB-GAN offers a significant contribution to medical imaging, particularly by enriching the depth, clarity, and volumetric detail of medical images. Its application shows promise in enhancing the interpretation and analysis of complex medical imagery from a comprehensive 3D perspective.|\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u7528\u4e8e\u653e\u5c04\u5b66\u56fe\u50cf 3D \u8d85\u5206\u8fa8\u7387\u7684 3D \u6b8b\u5dee\u5bc6\u96c6\u5757 GAN (3D RRDB-GAN)\u3002 3D RRDB-GAN \u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u662f\u96c6\u6210\u4e86 2.5D \u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u9ad8\u4f53\u79ef\u56fe\u50cf\u8d28\u91cf\u548c\u771f\u5b9e\u611f\u3002\u6211\u4eec\u7684\u6a21\u578b\u7684\u6709\u6548\u6027\u901a\u8fc7\u4e0d\u540c\u6570\u636e\u96c6\uff08\u5305\u62ec Mice Brain MRH\u3001OASIS\u3001HCP1200 \u548c MSD-Task-6\uff09\u7684 4 \u500d\u8d85\u5206\u8fa8\u7387\u5b9e\u9a8c\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u8fd9\u4e9b\u8bc4\u4f30\u5305\u62ec LPIPS \u548c FID \u7b49\u5b9a\u91cf\u6307\u6807\u4ee5\u53ca\u901a\u8fc7\u6837\u672c\u53ef\u89c6\u5316\u8fdb\u884c\u7684\u5b9a\u6027\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u5728\u8be6\u7ec6\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002 3D RRDB-GAN \u4e3a\u533b\u5b66\u6210\u50cf\u505a\u51fa\u4e86\u91cd\u5927\u8d21\u732e\uff0c\u7279\u522b\u662f\u4e30\u5bcc\u4e86\u533b\u5b66\u56fe\u50cf\u7684\u6df1\u5ea6\u3001\u6e05\u6670\u5ea6\u548c\u4f53\u79ef\u7ec6\u8282\u3002\u5b83\u7684\u5e94\u7528\u6709\u671b\u4ece\u5168\u9762\u7684 3D \u89d2\u5ea6\u589e\u5f3a\u590d\u6742\u533b\u5b66\u56fe\u50cf\u7684\u89e3\u91ca\u548c\u5206\u6790\u3002|[2402.04171v1](http://arxiv.org/pdf/2402.04171v1)|null|\n", "2402.03908": "|**2024-02-06**|**EscherNet: A Generative Model for Scalable View Synthesis**|EscherNet\uff1a\u53ef\u6269\u5c55\u89c6\u56fe\u5408\u6210\u7684\u751f\u6210\u6a21\u578b|Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison|We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis -- it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: \\url{https://kxhit.github.io/EscherNet}.|\u6211\u4eec\u4ecb\u7ecd EscherNet\uff0c\u4e00\u79cd\u7528\u4e8e\u89c6\u56fe\u5408\u6210\u7684\u591a\u89c6\u56fe\u6761\u4ef6\u6269\u6563\u6a21\u578b\u3002 EscherNet \u5b66\u4e60\u9690\u5f0f\u548c\u751f\u6210 3D \u8868\u793a\uff0c\u5e76\u7ed3\u5408\u4e13\u95e8\u7684\u76f8\u673a\u4f4d\u7f6e\u7f16\u7801\uff0c\u4ece\u800c\u5141\u8bb8\u5bf9\u4efb\u610f\u6570\u91cf\u7684\u53c2\u8003\u89c6\u56fe\u548c\u76ee\u6807\u89c6\u56fe\u4e4b\u95f4\u7684\u76f8\u673a\u53d8\u6362\u8fdb\u884c\u7cbe\u786e\u548c\u8fde\u7eed\u7684\u76f8\u5bf9\u63a7\u5236\u3002 EscherNet \u5728\u89c6\u56fe\u5408\u6210\u65b9\u9762\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u901a\u7528\u6027\u3001\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u2014\u2014\u5c3d\u7ba1\u4f7f\u7528\u56fa\u5b9a\u6570\u91cf\u7684 3 \u4e2a\u53c2\u8003\u89c6\u56fe\u5230 3 \u4e2a\u76ee\u6807\u89c6\u56fe\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u5b83\u53ef\u4ee5\u5728\u5355\u4e2a\u6d88\u8d39\u7ea7 GPU \u4e0a\u540c\u65f6\u751f\u6210 100 \u591a\u4e2a\u4e00\u81f4\u7684\u76ee\u6807\u89c6\u56fe\u3002\u56e0\u6b64\uff0cEscherNet \u4e0d\u4ec5\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u65b0\u9896\u89c6\u56fe\u5408\u6210\u7684\u95ee\u9898\uff0c\u800c\u4e14\u8fd8\u81ea\u7136\u5730\u7edf\u4e00\u4e86\u5355\u56fe\u50cf\u548c\u591a\u56fe\u50cf 3D \u91cd\u5efa\uff0c\u5c06\u8fd9\u4e9b\u4e0d\u540c\u7684\u4efb\u52a1\u7ec4\u5408\u5230\u4e00\u4e2a\u5355\u4e00\u7684\u3001\u6709\u51dd\u805a\u529b\u7684\u6846\u67b6\u4e2d\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u4e0e\u4e13\u95e8\u9488\u5bf9\u6bcf\u4e2a\u95ee\u9898\u91cf\u8eab\u5b9a\u5236\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cEscherNet \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u8fd9\u79cd\u975e\u51e1\u7684\u591a\u529f\u80fd\u6027\u4e3a\u8bbe\u8ba1 3D \u89c6\u89c9\u7684\u53ef\u6269\u5c55\u795e\u7ecf\u67b6\u6784\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002\u9879\u76ee\u9875\u9762\uff1a\\url{https://kxhit.github.io/EscherNet}\u3002|[2402.03908v1](http://arxiv.org/pdf/2402.03908v1)|null|\n", "2402.03840": "|**2024-02-06**|**Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation**|\u4fe1\u5ff5\u573a\u666f\u56fe\uff1a\u901a\u8fc7\u671f\u671b\u8ba1\u7b97\u200b\u200b\u7528\u5bf9\u8c61\u6269\u5c55\u90e8\u5206\u573a\u666f|Mario A. V. Saucedo, Akash Patel, Akshit Saradagi, Christoforos Kanellakis, George Nikolakopoulos|In this article, we propose the novel concept of Belief Scene Graphs, which are utility-driven extensions of partial 3D scene graphs, that enable efficient high-level task planning with partial information. We propose a graph-based learning methodology for the computation of belief (also referred to as expectation) on any given 3D scene graph, which is then used to strategically add new nodes (referred to as blind nodes) that are relevant for a robotic mission. We propose the method of Computation of Expectation based on Correlation Information (CECI), to reasonably approximate real Belief/Expectation, by learning histograms from available training data. A novel Graph Convolutional Neural Network (GCN) model is developed, to learn CECI from a repository of 3D scene graphs. As no database of 3D scene graphs exists for the training of the novel CECI model, we present a novel methodology for generating a 3D scene graph dataset based on semantically annotated real-life 3D spaces. The generated dataset is then utilized to train the proposed CECI model and for extensive validation of the proposed method. We establish the novel concept of \\textit{Belief Scene Graphs} (BSG), as a core component to integrate expectations into abstract representations. This new concept is an evolution of the classical 3D scene graph concept and aims to enable high-level reasoning for the task planning and optimization of a variety of robotics missions. The efficacy of the overall framework has been evaluated in an object search scenario, and has also been tested on a real-life experiment to emulate human common sense of unseen-objects.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4fe1\u5ff5\u573a\u666f\u56fe\u7684\u65b0\u6982\u5ff5\uff0c\u5b83\u662f\u90e8\u5206 3D \u573a\u666f\u56fe\u7684\u5b9e\u7528\u9a71\u52a8\u6269\u5c55\uff0c\u53ef\u4ee5\u5229\u7528\u90e8\u5206\u4fe1\u606f\u8fdb\u884c\u9ad8\u6548\u7684\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5b66\u200b\u200b\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u4efb\u4f55\u7ed9\u5b9a 3D \u573a\u666f\u56fe\u4e0a\u7684\u7f6e\u4fe1\u5ea6\uff08\u4e5f\u79f0\u4e3a\u671f\u671b\uff09\uff0c\u7136\u540e\u7528\u4e8e\u7b56\u7565\u6027\u5730\u6dfb\u52a0\u4e0e\u673a\u5668\u4eba\u4efb\u52a1\u76f8\u5173\u7684\u65b0\u8282\u70b9\uff08\u79f0\u4e3a\u76f2\u8282\u70b9\uff09 \u3002\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u76f8\u5173\u4fe1\u606f\uff08CECI\uff09\u7684\u671f\u671b\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u53ef\u7528\u7684\u8bad\u7ec3\u6570\u636e\u4e2d\u5b66\u4e60\u76f4\u65b9\u56fe\u6765\u5408\u7406\u5730\u8fd1\u4f3c\u771f\u5b9e\u7684\u4fe1\u5ff5/\u671f\u671b\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (GCN) \u6a21\u578b\uff0c\u7528\u4e8e\u4ece 3D \u573a\u666f\u56fe\u5b58\u50a8\u5e93\u4e2d\u5b66\u4e60 CECI\u3002\u7531\u4e8e\u4e0d\u5b58\u5728\u7528\u4e8e\u8bad\u7ec3\u65b0\u9896\u7684 CECI \u6a21\u578b\u7684 3D \u573a\u666f\u56fe\u6570\u636e\u5e93\uff0c\u56e0\u6b64\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u6ce8\u91ca\u7684\u73b0\u5b9e\u751f\u6d3b 3D \u7a7a\u95f4\u751f\u6210 3D \u573a\u666f\u56fe\u6570\u636e\u96c6\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u7136\u540e\uff0c\u751f\u6210\u7684\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u6240\u63d0\u51fa\u7684 CECI \u6a21\u578b\u5e76\u5bf9\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8fdb\u884c\u5e7f\u6cdb\u9a8c\u8bc1\u3002\u6211\u4eec\u5efa\u7acb\u4e86 \\textit{Belief Scene Graphs} (BSG) \u7684\u65b0\u6982\u5ff5\uff0c\u4f5c\u4e3a\u5c06\u671f\u671b\u96c6\u6210\u5230\u62bd\u8c61\u8868\u793a\u4e2d\u7684\u6838\u5fc3\u7ec4\u4ef6\u3002\u8fd9\u4e2a\u65b0\u6982\u5ff5\u662f\u7ecf\u5178 3D \u573a\u666f\u56fe\u6982\u5ff5\u7684\u6f14\u53d8\uff0c\u65e8\u5728\u4e3a\u5404\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u7684\u4efb\u52a1\u89c4\u5212\u548c\u4f18\u5316\u63d0\u4f9b\u9ad8\u7ea7\u63a8\u7406\u3002\u6574\u4e2a\u6846\u67b6\u7684\u529f\u6548\u5df2\u7ecf\u5728\u5bf9\u8c61\u641c\u7d22\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e14\u8fd8\u5728\u73b0\u5b9e\u751f\u6d3b\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4ee5\u6a21\u62df\u4eba\u7c7b\u5bf9\u770b\u4e0d\u89c1\u7684\u5bf9\u8c61\u7684\u5e38\u8bc6\u3002|[2402.03840v1](http://arxiv.org/pdf/2402.03840v1)|null|\n", "2402.03830": "|**2024-02-06**|**OASim: an Open and Adaptive Simulator based on Neural Rendering for Autonomous Driving**|OASim\uff1a\u57fa\u4e8e\u795e\u7ecf\u6e32\u67d3\u7684\u81ea\u52a8\u9a7e\u9a76\u5f00\u653e\u81ea\u9002\u5e94\u6a21\u62df\u5668|Guohang Yan, Jiahao Pi, Jianfei Guo, Zhaotong Luo, Min Dou, Nianchen Deng, Qiusheng Huang, Daocheng Fu, Licheng Wen, Pinlong Cai, et.al.|With deep learning and computer vision technology development, autonomous driving provides new solutions to improve traffic safety and efficiency. The importance of building high-quality datasets is self-evident, especially with the rise of end-to-end autonomous driving algorithms in recent years. Data plays a core role in the algorithm closed-loop system. However, collecting real-world data is expensive, time-consuming, and unsafe. With the development of implicit rendering technology and in-depth research on using generative models to produce data at scale, we propose OASim, an open and adaptive simulator and autonomous driving data generator based on implicit neural rendering. It has the following characteristics: (1) High-quality scene reconstruction through neural implicit surface reconstruction technology. (2) Trajectory editing of the ego vehicle and participating vehicles. (3) Rich vehicle model library that can be freely selected and inserted into the scene. (4) Rich sensors model library where you can select specified sensors to generate data. (5) A highly customizable data generation system can generate data according to user needs. We demonstrate the high quality and fidelity of the generated data through perception performance evaluation on the Carla simulator and real-world data acquisition. Code is available at https://github.com/PJLab-ADG/OASim.|\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u53d1\u5c55\uff0c\u81ea\u52a8\u9a7e\u9a76\u4e3a\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u548c\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u4e0d\u8a00\u800c\u55bb\uff0c\u5c24\u5176\u662f\u968f\u7740\u8fd1\u5e74\u6765\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u7684\u5174\u8d77\u3002\u6570\u636e\u5728\u7b97\u6cd5\u95ed\u73af\u7cfb\u7edf\u4e2d\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\u3002\u7136\u800c\uff0c\u6536\u96c6\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u65e2\u6602\u8d35\u3001\u8017\u65f6\u53c8\u4e0d\u5b89\u5168\u3002\u968f\u7740\u9690\u5f0f\u6e32\u67d3\u6280\u672f\u7684\u53d1\u5c55\u4ee5\u53ca\u4f7f\u7528\u751f\u6210\u6a21\u578b\u5927\u89c4\u6a21\u751f\u6210\u6570\u636e\u7684\u6df1\u5165\u7814\u7a76\uff0c\u6211\u4eec\u63d0\u51fa\u4e86OASim\uff0c\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u6e32\u67d3\u7684\u5f00\u653e\u5f0f\u81ea\u9002\u5e94\u6a21\u62df\u5668\u548c\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u751f\u6210\u5668\u3002\u5b83\u5177\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a\uff081\uff09\u901a\u8fc7\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u91cd\u5efa\u6280\u672f\u8fdb\u884c\u9ad8\u8d28\u91cf\u7684\u573a\u666f\u91cd\u5efa\u3002 (2)\u81ea\u6211\u8f66\u8f86\u548c\u53c2\u4e0e\u8f66\u8f86\u7684\u8f68\u8ff9\u7f16\u8f91\u3002 (3)\u4e30\u5bcc\u7684\u8f66\u8f86\u6a21\u578b\u5e93\uff0c\u53ef\u81ea\u7531\u9009\u62e9\u63d2\u5165\u573a\u666f\u3002 (4)\u4e30\u5bcc\u7684\u4f20\u611f\u5668\u6a21\u578b\u5e93\uff0c\u53ef\u4ee5\u9009\u62e9\u6307\u5b9a\u7684\u4f20\u611f\u5668\u6765\u751f\u6210\u6570\u636e\u3002 (5)\u9ad8\u5ea6\u53ef\u5b9a\u5236\u7684\u6570\u636e\u751f\u6210\u7cfb\u7edf\uff0c\u53ef\u4ee5\u6839\u636e\u7528\u6237\u9700\u6c42\u751f\u6210\u6570\u636e\u3002\u6211\u4eec\u901a\u8fc7 Carla \u6a21\u62df\u5668\u7684\u611f\u77e5\u6027\u80fd\u8bc4\u4f30\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u91c7\u96c6\u6765\u8bc1\u660e\u6240\u751f\u6210\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/PJLab-ADG/OASim \u83b7\u53d6\u3002|[2402.03830v1](http://arxiv.org/pdf/2402.03830v1)|null|\n", "2402.03723": "|**2024-02-06**|**Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos**|Rig3DGS\uff1a\u4ece\u4f11\u95f2\u5355\u76ee\u89c6\u9891\u521b\u5efa\u53ef\u63a7\u8096\u50cf|Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras|Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications. The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency. However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS to address this challenge. We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space. Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering. Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model. This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures. We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments. The project page can be found at http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html|\u4ece\u4f11\u95f2\u667a\u80fd\u624b\u673a\u89c6\u9891\u4e2d\u521b\u5efa\u53ef\u63a7\u7684 3D \u4eba\u7269\u8096\u50cf\u662f\u975e\u5e38\u7406\u60f3\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728 AR/VR \u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u7684\u4ef7\u503c\u3002 3D Gaussian Splatting (3DGS) \u7684\u6700\u65b0\u53d1\u5c55\u663e\u793a\u51fa\u6e32\u67d3\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u7684\u63d0\u9ad8\u3002\u7136\u800c\uff0c\u4ece\u5355\u89c6\u56fe\u6355\u83b7\u4e2d\u51c6\u786e\u5efa\u6a21\u548c\u5206\u79bb\u5934\u90e8\u8fd0\u52a8\u548c\u9762\u90e8\u8868\u60c5\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6e32\u67d3\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165 Rig3DGS \u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002\u6211\u4eec\u5728\u89c4\u8303\u7a7a\u95f4\u4e2d\u4f7f\u7528\u4e00\u7ec4 3D \u9ad8\u65af\u51fd\u6570\u6765\u8868\u793a\u6574\u4e2a\u573a\u666f\uff0c\u5305\u62ec\u52a8\u6001\u4e3b\u9898\u3002\u4f7f\u7528\u4e00\u7ec4\u63a7\u5236\u4fe1\u53f7\uff08\u4f8b\u5982\u5934\u90e8\u59ff\u52bf\u548c\u8868\u60c5\uff09\uff0c\u6211\u4eec\u5c06\u5b83\u4eec\u8f6c\u6362\u5230\u5177\u6709\u5b66\u4e60\u53d8\u5f62\u7684 3D \u7a7a\u95f4\uff0c\u4ee5\u751f\u6210\u6240\u9700\u7684\u6e32\u67d3\u3002\u6211\u4eec\u7684\u5173\u952e\u521b\u65b0\u662f\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53d8\u5f62\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ee5\u6e90\u81ea 3D \u53ef\u53d8\u5f62\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u5148\u9a8c\u4e3a\u6307\u5bfc\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bad\u7ec3\u4e2d\u975e\u5e38\u9ad8\u6548\uff0c\u5e76\u4e14\u5728\u63a7\u5236\u5404\u79cd\u6355\u83b7\u7684\u9762\u90e8\u8868\u60c5\u3001\u5934\u90e8\u4f4d\u7f6e\u548c\u89c6\u56fe\u5408\u6210\u65b9\u9762\u975e\u5e38\u6709\u6548\u3002\u6211\u4eec\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u5b66\u53d8\u5f62\u7684\u6709\u6548\u6027\u3002\u9879\u76ee\u9875\u9762\u53ef\u4ee5\u5728http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html\u627e\u5230|[2402.03723v1](http://arxiv.org/pdf/2402.03723v1)|null|\n", "2402.03690": "|**2024-02-06**|**3Doodle: Compact Abstraction of Objects with 3D Strokes**|3Doodle\uff1a\u4f7f\u7528 3D \u7b14\u753b\u5bf9\u5bf9\u8c61\u8fdb\u884c\u7d27\u51d1\u62bd\u8c61|Changwoon Choi, Jaeah Lee, Jaesik Park, Young Min Kim|While free-hand sketching has long served as an efficient representation to convey characteristics of an object, they are often subjective, deviating significantly from realistic representations. Moreover, sketches are not consistent for arbitrary viewpoints, making it hard to catch 3D shapes. We propose 3Dooole, generating descriptive and view-consistent sketch images given multi-view images of the target object. Our method is based on the idea that a set of 3D strokes can efficiently represent 3D structural information and render view-consistent 2D sketches. We express 2D sketches as a union of view-independent and view-dependent components. 3D cubic B ezier curves indicate view-independent 3D feature lines, while contours of superquadrics express a smooth outline of the volume of varying viewpoints. Our pipeline directly optimizes the parameters of 3D stroke primitives to minimize perceptual losses in a fully differentiable manner. The resulting sparse set of 3D strokes can be rendered as abstract sketches containing essential 3D characteristic shapes of various objects. We demonstrate that 3Doodle can faithfully express concepts of the original images compared with recent sketch generation approaches.|\u867d\u7136\u624b\u7ed8\u8349\u56fe\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u662f\u4f20\u8fbe\u7269\u4f53\u7279\u5f81\u7684\u6709\u6548\u8868\u793a\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u662f\u4e3b\u89c2\u7684\uff0c\u4e0e\u73b0\u5b9e\u7684\u8868\u793a\u6709\u5f88\u5927\u504f\u5dee\u3002\u6b64\u5916\uff0c\u8349\u56fe\u5bf9\u4e8e\u4efb\u610f\u89c6\u70b9\u6765\u8bf4\u5e76\u4e0d\u4e00\u81f4\uff0c\u4f7f\u5f97\u6355\u6349 3D \u5f62\u72b6\u53d8\u5f97\u56f0\u96be\u3002\u6211\u4eec\u63d0\u51fa 3Dooole\uff0c\u5728\u7ed9\u5b9a\u76ee\u6807\u5bf9\u8c61\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u63cf\u8ff0\u6027\u4e14\u89c6\u56fe\u4e00\u81f4\u7684\u8349\u56fe\u56fe\u50cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u8fd9\u6837\u7684\u60f3\u6cd5\uff1a\u4e00\u7ec4 3D \u7b14\u5212\u53ef\u4ee5\u6709\u6548\u5730\u8868\u793a 3D \u7ed3\u6784\u4fe1\u606f\u5e76\u6e32\u67d3\u89c6\u56fe\u4e00\u81f4\u7684 2D \u8349\u56fe\u3002\u6211\u4eec\u5c06 2D \u8349\u56fe\u8868\u793a\u4e3a\u89c6\u56fe\u65e0\u5173\u7ec4\u4ef6\u548c\u89c6\u56fe\u76f8\u5173\u7ec4\u4ef6\u7684\u8054\u5408\u3002 3D \u4e09\u6b21 B ezier \u66f2\u7ebf\u8868\u793a\u4e0e\u89c6\u56fe\u65e0\u5173\u7684 3D \u7279\u5f81\u7ebf\uff0c\u800c\u8d85\u4e8c\u6b21\u66f2\u9762\u7684\u8f6e\u5ed3\u8868\u793a\u4e0d\u540c\u89c6\u70b9\u4f53\u79ef\u7684\u5e73\u6ed1\u8f6e\u5ed3\u3002\u6211\u4eec\u7684\u6d41\u7a0b\u76f4\u63a5\u4f18\u5316 3D \u7b14\u753b\u57fa\u5143\u7684\u53c2\u6570\uff0c\u4ee5\u5b8c\u5168\u53ef\u5fae\u5206\u7684\u65b9\u5f0f\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u611f\u77e5\u635f\u5931\u3002\u7531\u6b64\u4ea7\u751f\u7684\u7a00\u758f 3D \u7b14\u753b\u96c6\u53ef\u4ee5\u6e32\u67d3\u4e3a\u5305\u542b\u5404\u79cd\u5bf9\u8c61\u7684\u57fa\u672c 3D \u7279\u5f81\u5f62\u72b6\u7684\u62bd\u8c61\u8349\u56fe\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u4e0e\u6700\u8fd1\u7684\u8349\u56fe\u751f\u6210\u65b9\u6cd5\u76f8\u6bd4\uff0c3Doodle \u53ef\u4ee5\u5fe0\u5b9e\u5730\u8868\u8fbe\u539f\u59cb\u56fe\u50cf\u7684\u6982\u5ff5\u3002|[2402.03690v1](http://arxiv.org/pdf/2402.03690v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.04129": "|**2024-02-06**|**OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning**|OVOR\uff1aOnePrompt \u5177\u6709\u865a\u62df\u79bb\u7fa4\u503c\u6b63\u5219\u5316\u529f\u80fd\uff0c\u53ef\u5b9e\u73b0\u514d\u6392\u7ec3\u7684\u8bfe\u5802\u589e\u91cf\u5b66\u4e60|Wei-Cheng Huang, Chun-Fu Chen, Hsiang Hsu|Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together. In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool. This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper. We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the-art (SOTA) methods equipped with a prompt pool, using much less learnable parameters and lower inference cost. Our regularization method has demonstrated its compatibility with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and CIFAR-100 benchmarks. Our source code is available at https://github.com/jpmorganchase/ovor.|\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4f7f\u7528\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u53ef\u5b66\u4e60\u7684\u63d0\u793a\uff0c\u7528\u4e8e\u8bfe\u5802\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\u8bbe\u7f6e\u7684\u514d\u6392\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u4f18\u4e8e\u57fa\u4e8e\u6392\u7ec3\u7684\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u65e0\u9700\u6392\u7ec3\u7684 CIL \u65b9\u6cd5\u5f88\u96be\u533a\u5206\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u7c7b\uff0c\u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u4e00\u8d77\u8bad\u7ec3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u865a\u62df\u5f02\u5e38\u503c\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u4ee5\u6536\u7d27\u5206\u7c7b\u5668\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u4ece\u800c\u51cf\u8f7b\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u7684\u7c7b\u6df7\u6dc6\u3002\u6700\u8fd1\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e00\u7ec4\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u63d0\u793a\uff0c\u4ee5\u9632\u6b62\u7528\u65b0\u4efb\u52a1\u7684\u77e5\u8bc6\u8986\u76d6\u5148\u524d\u4efb\u52a1\u7684\u77e5\u8bc6\uff0c\u4ece\u800c\u5bfc\u81f4\u5728\u4ece\u6c60\u4e2d\u67e5\u8be2\u548c\u7f16\u5199\u9002\u5f53\u7684\u63d0\u793a\u65f6\u8fdb\u884c\u989d\u5916\u7684\u8ba1\u7b97\u3002\u6b63\u5982\u6211\u4eec\u5728\u8bba\u6587\u4e2d\u6240\u63ed\u793a\u7684\uff0c\u8fd9\u79cd\u989d\u5916\u6210\u672c\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u6d88\u9664\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u57fa\u4e8e\u7b80\u5316\u63d0\u793a\u7684\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u7528\u66f4\u5c11\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u548c\u66f4\u4f4e\u7684\u63a8\u7406\u6210\u672c\uff0c\u83b7\u5f97\u4e0e\u4e4b\u524d\u914d\u5907\u63d0\u793a\u6c60\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u6211\u4eec\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u5df2\u7ecf\u8bc1\u660e\u4e86\u5b83\u4e0e\u4e0d\u540c\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u7684\u517c\u5bb9\u6027\uff0c\u63d0\u9ad8\u4e86\u4ee5\u524d\u7684 SOTA \u514d\u6f14\u7ec3 CIL \u65b9\u6cd5\u5728 ImageNet-R \u548c CIFAR-100 \u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/jpmorganchase/ovor \u83b7\u53d6\u3002|[2402.04129v1](http://arxiv.org/pdf/2402.04129v1)|null|\n", "2402.03917": "|**2024-02-06**|**Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning**|\u7528\u4e8e\u51b7\u542f\u52a8\u65e0\u8303\u4f8b\u589e\u91cf\u5b66\u4e60\u7684\u5f39\u6027\u7279\u5f81\u6574\u5408|Simone Magistri, Tomaso Trinci, Albin Soutif-Cormerais, Joost van de Weijer, Andrew D. Bagdanov|Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prototypes used in a novel asymmetric cross entropy loss which effectively balances prototype rehearsal with data from new tasks. Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and ImageNet-1K demonstrate that Elastic Feature Consolidation is better able to learn new tasks by maintaining model plasticity and significantly outperform the state-of-the-art.|\u65e0\u8303\u4f8b\u7c7b\u589e\u91cf\u5b66\u4e60 (EFCIL) \u65e8\u5728\u4ece\u4e00\u7cfb\u5217\u4efb\u52a1\u4e2d\u5b66\u4e60\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u5148\u524d\u7684\u4efb\u52a1\u6570\u636e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8003\u8651\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u51b7\u542f\u52a8\u573a\u666f\uff0c\u5176\u4e2d\u7b2c\u4e00\u4e2a\u4efb\u52a1\u4e2d\u6ca1\u6709\u8db3\u591f\u7684\u6570\u636e\u6765\u5b66\u4e60\u9ad8\u8d28\u91cf\u7684\u9aa8\u5e72\u7f51\u3002\u8fd9\u5bf9\u4e8e EFCIL \u6765\u8bf4\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u9ad8\u53ef\u5851\u6027\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u7279\u5f81\u6f02\u79fb\uff0c\u800c\u5728\u65e0\u6837\u672c\u8bbe\u7f6e\u4e2d\u5f88\u96be\u8865\u507f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c4\u8303\u4e0e\u5148\u524d\u4efb\u52a1\u9ad8\u5ea6\u76f8\u5173\u7684\u65b9\u5411\u4e0a\u7684\u6f02\u79fb\u6765\u5de9\u56fa\u7279\u5f81\u8868\u793a\uff0c\u5e76\u91c7\u7528\u539f\u578b\u6765\u51cf\u5c11\u4efb\u52a1\u65b0\u8fd1\u5ea6\u504f\u5dee\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3a\u5f39\u6027\u7279\u5f81\u5408\u5e76\uff08EFC\uff09\uff0c\u5229\u7528\u57fa\u4e8e\u7ecf\u9a8c\u7279\u5f81\u77e9\u9635\uff08EFM\uff09\u7684\u7279\u5f81\u6f02\u79fb\u7684\u6613\u4e8e\u5904\u7406\u7684\u4e8c\u9636\u8fd1\u4f3c\u3002 EFM \u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5f15\u5165\u4e86\u4f2a\u5ea6\u91cf\uff0c\u6211\u4eec\u7528\u5b83\u6765\u89c4\u8303\u91cd\u8981\u65b9\u5411\u4e0a\u7684\u7279\u5f81\u6f02\u79fb\uff0c\u5e76\u66f4\u65b0\u7528\u4e8e\u65b0\u578b\u975e\u5bf9\u79f0\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u9ad8\u65af\u539f\u578b\uff0c\u8be5\u539f\u578b\u6709\u6548\u5730\u5e73\u8861\u4e86\u539f\u578b\u6392\u7ec3\u4e0e\u65b0\u4efb\u52a1\u7684\u6570\u636e\u3002 CIFAR-100\u3001Tiny-ImageNet\u3001ImageNet-Subset \u548c ImageNet-1K \u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f39\u6027\u7279\u5f81\u5408\u5e76\u80fd\u591f\u901a\u8fc7\u4fdd\u6301\u6a21\u578b\u53ef\u5851\u6027\u66f4\u597d\u5730\u5b66\u4e60\u65b0\u4efb\u52a1\uff0c\u5e76\u4e14\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6280\u672f\u3002|[2402.03917v1](http://arxiv.org/pdf/2402.03917v1)|null|\n", "2402.03904": "|**2024-02-06**|**Deep MSFOP: Multiple Spectral filter Operators Preservation in Deep Functional Maps for Unsupervised Shape Matching**|Deep MSFOP\uff1a\u5728\u6df1\u5ea6\u51fd\u6570\u56fe\u4e2d\u4fdd\u7559\u591a\u4e2a\u5149\u8c31\u6ee4\u6ce2\u5668\u7b97\u5b50\u4ee5\u5b9e\u73b0\u65e0\u76d1\u7763\u5f62\u72b6\u5339\u914d|Feifan Luo, Qingsong Li, Ling Hu, Xinru Liu, Haojun Xu, Haibo Wang, Ting Li, Shengjun Liu|We propose a novel constraint called Multiple Spectral filter Operators Preservation (MSFOR) to compute functional maps and based on it, develop an efficient deep functional map architecture called Deep MSFOP for shape matching. The core idea is that, instead of using the general descriptor preservation constraint, we require our maps to preserve multiple spectral filter operators. This allows us to incorporate more informative geometrical information, contained in different frequency bands of functions, into the functional map computing. This can be confirmed by that some previous techniques like wavelet preservation and LBO commutativity are actually our special cases. Moreover, we also develop a very efficient way to compute the maps with MSFOP constraint, which can be conveniently embedded into the deep learning, especially having learnable filter operators. Utilizing the above results, we finally design our Deep MSFOP pipeline, equipped with a suitable unsupervised loss jointly penalizing the functional map and the underlying pointwise map. Our deep functional map has notable advantages, including that the functional map is more geometrically informative and guaranteed to be proper, and the computing is numerically stable. Extensive experimental results on different datasets demonstrate that our approach outperforms the existing state-of-the-art methods, especially in challenging settings like non-isometric and inconsistent topology datasets.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u591a\u5149\u8c31\u6ee4\u6ce2\u5668\u7b97\u5b50\u4fdd\u7559\uff08MSFOR\uff09\u7684\u65b0\u9896\u7ea6\u675f\u6765\u8ba1\u7b97\u529f\u80fd\u56fe\uff0c\u5e76\u57fa\u4e8e\u5b83\u5f00\u53d1\u4e86\u4e00\u79cd\u79f0\u4e3a\u6df1\u5ea6 MSFOP \u7684\u9ad8\u6548\u6df1\u5ea6\u529f\u80fd\u56fe\u67b6\u6784\uff0c\u7528\u4e8e\u5f62\u72b6\u5339\u914d\u3002\u6838\u5fc3\u601d\u60f3\u662f\uff0c\u6211\u4eec\u4e0d\u4f7f\u7528\u901a\u7528\u63cf\u8ff0\u7b26\u4fdd\u7559\u7ea6\u675f\uff0c\u800c\u662f\u8981\u6c42\u6211\u4eec\u7684\u6620\u5c04\u4fdd\u7559\u591a\u4e2a\u5149\u8c31\u6ee4\u6ce2\u5668\u7b97\u5b50\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u5c06\u5305\u542b\u5728\u51fd\u6570\u4e0d\u540c\u9891\u5e26\u4e2d\u7684\u66f4\u591a\u51e0\u4f55\u4fe1\u606f\u5408\u5e76\u5230\u51fd\u6570\u56fe\u8ba1\u7b97\u4e2d\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u4e4b\u524d\u7684\u4e00\u4e9b\u6280\u672f\uff08\u4f8b\u5982\u5c0f\u6ce2\u4fdd\u5b58\u548c LBO \u4ea4\u6362\u6027\uff09\u5b9e\u9645\u4e0a\u662f\u6211\u4eec\u7684\u7279\u4f8b\u6765\u8bc1\u5b9e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u975e\u5e38\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u5177\u6709 MSFOP \u7ea6\u675f\u7684\u6620\u5c04\uff0c\u5b83\u53ef\u4ee5\u65b9\u4fbf\u5730\u5d4c\u5165\u5230\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u7279\u522b\u662f\u5177\u6709\u53ef\u5b66\u4e60\u7684\u8fc7\u6ee4\u5668\u7b97\u5b50\u3002\u5229\u7528\u4e0a\u8ff0\u7ed3\u679c\uff0c\u6211\u4eec\u6700\u7ec8\u8bbe\u8ba1\u4e86 Deep MSFOP \u7ba1\u9053\uff0c\u914d\u5907\u4e86\u5408\u9002\u7684\u65e0\u76d1\u7763\u635f\u5931\uff0c\u5171\u540c\u60e9\u7f5a\u529f\u80fd\u56fe\u548c\u5e95\u5c42\u7684\u9010\u70b9\u56fe\u3002\u6211\u4eec\u7684\u6df1\u5ea6\u51fd\u6570\u56fe\u5177\u6709\u663e\u7740\u7684\u4f18\u52bf\uff0c\u5305\u62ec\u51fd\u6570\u56fe\u7684\u51e0\u4f55\u4fe1\u606f\u66f4\u4e30\u5bcc\u4e14\u4fdd\u8bc1\u6b63\u786e\uff0c\u5e76\u4e14\u8ba1\u7b97\u5728\u6570\u503c\u4e0a\u7a33\u5b9a\u3002\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u975e\u7b49\u8ddd\u548c\u4e0d\u4e00\u81f4\u7684\u62d3\u6251\u6570\u636e\u96c6\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u8bbe\u7f6e\u4e2d\u3002|[2402.03904v1](http://arxiv.org/pdf/2402.03904v1)|null|\n", "2402.03896": "|**2024-02-06**|**Convincing Rationales for Visual Question Answering Reasoning**|\u89c6\u89c9\u95ee\u7b54\u63a8\u7406\u7684\u4ee4\u4eba\u4fe1\u670d\u7684\u7406\u7531|Kun Li, George Vosselman, Michael Ying Yang|Visual Question Answering (VQA) is a challenging task of predicting the answer to a question about the content of an image. It requires deep understanding of both the textual question and visual image. Prior works directly evaluate the answering models by simply calculating the accuracy of the predicted answers. However, the inner reasoning behind the prediction is disregarded in such a \"black box\" system, and we do not even know if one can trust the predictions. In some cases, the models still get the correct answers even when they focus on irrelevant visual regions or textual tokens, which makes the models unreliable and illogical. To generate both visual and textual rationales next to the predicted answer to the given image/question pair, we propose Convincing Rationales for VQA, CRVQA. Considering the extra annotations brought by the new outputs, {CRVQA} is trained and evaluated by samples converted from some existing VQA datasets and their visual labels. The extensive experiments demonstrate that the visual and textual rationales support the prediction of the answers, and further improve the accuracy. Furthermore, {CRVQA} achieves competitive performance on generic VQA datatsets in the zero-shot evaluation setting. The dataset and source code will be released under https://github.com/lik1996/CRVQA2024.|\u89c6\u89c9\u95ee\u7b54 (VQA) \u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u7528\u4e8e\u9884\u6d4b\u6709\u5173\u56fe\u50cf\u5185\u5bb9\u7684\u95ee\u9898\u7684\u7b54\u6848\u3002\u5b83\u9700\u8981\u5bf9\u6587\u672c\u95ee\u9898\u548c\u89c6\u89c9\u56fe\u50cf\u90fd\u6709\u6df1\u523b\u7684\u7406\u89e3\u3002\u5148\u524d\u7684\u5de5\u4f5c\u901a\u8fc7\u7b80\u5355\u5730\u8ba1\u7b97\u9884\u6d4b\u7b54\u6848\u7684\u51c6\u786e\u6027\u6765\u76f4\u63a5\u8bc4\u4f30\u7b54\u6848\u6a21\u578b\u3002\u7136\u800c\uff0c\u5728\u8fd9\u6837\u4e00\u4e2a\u201c\u9ed1\u5323\u5b50\u201d\u7cfb\u7edf\u4e2d\uff0c\u9884\u6d4b\u80cc\u540e\u7684\u5185\u5728\u63a8\u7406\u88ab\u5ffd\u89c6\uff0c\u6211\u4eec\u751a\u81f3\u4e0d\u77e5\u9053\u9884\u6d4b\u662f\u5426\u53ef\u4fe1\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u6a21\u578b\u5173\u6ce8\u4e0d\u76f8\u5173\u7684\u89c6\u89c9\u533a\u57df\u6216\u6587\u672c\u6807\u8bb0\uff0c\u6a21\u578b\u4ecd\u7136\u53ef\u4ee5\u83b7\u5f97\u6b63\u786e\u7684\u7b54\u6848\uff0c\u8fd9\u4f7f\u5f97\u6a21\u578b\u4e0d\u53ef\u9760\u4e14\u4e0d\u5408\u903b\u8f91\u3002\u4e3a\u4e86\u5728\u7ed9\u5b9a\u56fe\u50cf/\u95ee\u9898\u5bf9\u7684\u9884\u6d4b\u7b54\u6848\u65c1\u8fb9\u751f\u6210\u89c6\u89c9\u548c\u6587\u672c\u57fa\u672c\u539f\u7406\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 VQA\u3001CRVQA \u7684\u4ee4\u4eba\u4fe1\u670d\u7684\u57fa\u672c\u539f\u7406\u3002\u8003\u8651\u5230\u65b0\u8f93\u51fa\u5e26\u6765\u7684\u989d\u5916\u6ce8\u91ca\uff0c{CRVQA} \u901a\u8fc7\u4ece\u4e00\u4e9b\u73b0\u6709 VQA \u6570\u636e\u96c6\u53ca\u5176\u89c6\u89c9\u6807\u7b7e\u8f6c\u6362\u800c\u6765\u7684\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u89c6\u89c9\u548c\u6587\u672c\u7684\u57fa\u672c\u539f\u7406\u652f\u6301\u5bf9\u7b54\u6848\u7684\u9884\u6d4b\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c{CRVQA} \u5728\u96f6\u6837\u672c\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u5728\u901a\u7528 VQA \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u5c06\u5728https://github.com/lik1996/CRVQA2024\u4e0b\u53d1\u5e03\u3002|[2402.03896v1](http://arxiv.org/pdf/2402.03896v1)|null|\n", "2402.03749": "|**2024-02-06**|**Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models**|\u89c6\u89c9\u8d85\u5bf9\u9f50\uff1a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5f31\u5230\u5f3a\u6cdb\u5316|Jianyuan Guo, Hanting Chen, Chengcheng Wang, Kai Han, Chang Xu, Yunhe Wang|Recent advancements in large language models have sparked interest in their extraordinary and near-superhuman capabilities, leading researchers to explore methods for evaluating and optimizing these abilities, which is called superalignment. In this context, our paper delves into the realm of vision foundation models, focusing on the concept of weak-to-strong generalization, which involves using a weaker model to supervise a stronger one, aiming to enhance the latter's capabilities beyond the former's limits. We introduce a novel and adaptively adjustable loss function for weak-to-strong supervision. Our comprehensive experiments span various scenarios, including few-shot learning, transfer learning, noisy label learning, and common knowledge distillation settings. The results are striking: our approach not only exceeds the performance benchmarks set by strong-to-strong generalization but also surpasses the outcomes of fine-tuning strong models with whole datasets. This compelling evidence underscores the significant potential of weak-to-strong generalization, showcasing its capability to substantially elevate the performance of vision foundation models. The code is available at https://github.com/ggjy/vision_weak_to_strong.|\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5f15\u53d1\u4e86\u4eba\u4eec\u5bf9\u5176\u975e\u51e1\u4e14\u8fd1\u4e4e\u8d85\u4eba\u7684\u80fd\u529b\u7684\u5174\u8da3\uff0c\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u8bc4\u4f30\u548c\u4f18\u5316\u8fd9\u4e9b\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u8fd9\u88ab\u79f0\u4e3a\u8d85\u5bf9\u9f50\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u7684\u8bba\u6587\u6df1\u5165\u7814\u7a76\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u9886\u57df\uff0c\u91cd\u70b9\u5173\u6ce8\u5f31\u5230\u5f3a\u6cdb\u5316\u7684\u6982\u5ff5\uff0c\u5373\u4f7f\u7528\u8f83\u5f31\u7684\u6a21\u578b\u6765\u76d1\u7763\u8f83\u5f3a\u7684\u6a21\u578b\uff0c\u65e8\u5728\u589e\u5f3a\u540e\u8005\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u8d85\u8d8a\u524d\u8005\u7684\u9650\u5236\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u81ea\u9002\u5e94\u53ef\u8c03\u7684\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u5f31\u5230\u5f3a\u7684\u76d1\u7763\u3002\u6211\u4eec\u7684\u7efc\u5408\u5b9e\u9a8c\u6db5\u76d6\u5404\u79cd\u573a\u666f\uff0c\u5305\u62ec\u5c0f\u6837\u672c\u5b66\u4e60\u3001\u8fc1\u79fb\u5b66\u4e60\u3001\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u548c\u516c\u5171\u77e5\u8bc6\u84b8\u998f\u8bbe\u7f6e\u3002\u7ed3\u679c\u662f\u60ca\u4eba\u7684\uff1a\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u8d85\u8fc7\u4e86\u5f3a\u5bf9\u5f3a\u6cdb\u5316\u8bbe\u5b9a\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u800c\u4e14\u8d85\u8fc7\u4e86\u4f7f\u7528\u6574\u4e2a\u6570\u636e\u96c6\u5fae\u8c03\u5f3a\u6a21\u578b\u7684\u7ed3\u679c\u3002\u8fd9\u4e00\u4ee4\u4eba\u4fe1\u670d\u7684\u8bc1\u636e\u5f3a\u8c03\u4e86\u4ece\u5f31\u5230\u5f3a\u6cdb\u5316\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5927\u5e45\u63d0\u5347\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6027\u80fd\u7684\u80fd\u529b\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/ggjy/vision_weak_to_strong \u83b7\u53d6\u3002|[2402.03749v1](http://arxiv.org/pdf/2402.03749v1)|null|\n"}, "\u5176\u4ed6": {"2402.04236": "|**2024-02-06**|**CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations**|CogCoM\uff1a\u8bad\u7ec3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u64cd\u4f5c\u94fe\u6df1\u5165\u7ec6\u8282|Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et.al.|Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art performance across 8 benchmarks from 3 categories, and a limited number of training steps with the data swiftly gains a competitive performance. The code and data are publicly available at https://github.com/THUDM/CogCoM.|\u5f97\u76ca\u4e8e\u5c06\u89c6\u89c9\u6307\u4ee4\u4e0e\u7b54\u6848\u76f8\u7ed3\u5408\u7684\u5927\u91cf\u8bad\u7ec3\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5df2\u7ecf\u8bc1\u660e\u4e86\u5176\u5e7f\u6cdb\u7684\u53ef\u884c\u6027\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u51b3\u5b9a\u6027\u7684\u4e00\u81f4\u6027\u5bfc\u81f4\u6a21\u578b\u5ffd\u7565\u5173\u952e\u7684\u89c6\u89c9\u63a8\u7406\uff0c\u5e76\u8fdb\u4e00\u6b65\u5bfc\u81f4\u7ec6\u81f4\u7684\u89c6\u89c9\u95ee\u9898\u548c\u4e0d\u5fe0\u5b9e\u7684\u53cd\u5e94\u7684\u5931\u8d25\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u64cd\u4f5c\u94fe\uff08Chain of Manipulations\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4f7f VLM \u80fd\u591f\u901a\u8fc7\u4e00\u7cfb\u5217\u64cd\u4f5c\u6765\u89e3\u51b3\u95ee\u9898\u7684\u673a\u5236\uff0c\u5176\u4e2d\u6bcf\u4e2a\u64cd\u4f5c\u662f\u6307\u5bf9\u89c6\u89c9\u8f93\u5165\u7684\u64cd\u4f5c\uff0c\u6216\u8005\u6765\u81ea\u901a\u8fc7\u5148\u524d\u8bad\u7ec3\u83b7\u5f97\u7684\u5185\u5728\u80fd\u529b\uff08\u4f8b\u5982\uff0c\u57fa\u7840\uff09\uff0c\u6216\u8005\u6765\u81ea\u6a21\u4eff\u7c7b\u4eba\u884c\u4e3a\uff08\u4f8b\u5982\u653e\u5927\uff09\u3002\u8fd9\u79cd\u673a\u5236\u9f13\u52b1 VLM \u901a\u8fc7\u8bc1\u636e\u89c6\u89c9\u63a8\u7406\u751f\u6210\u5fe0\u5b9e\u7684\u54cd\u5e94\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u5728\u53ef\u89e3\u91ca\u7684\u8def\u5f84\u4e2d\u8ffd\u8e2a\u9519\u8bef\u539f\u56e0\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86 CogCoM\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u7528\u7684 17B VLM\uff0c\u5177\u6709\u57fa\u4e8e\u5185\u5b58\u7684\u517c\u5bb9\u67b6\u6784\uff0c\u8d4b\u4e88\u4e86\u8fd9\u79cd\u63a8\u7406\u673a\u5236\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728 3 \u4e2a\u7c7b\u522b\u7684 8 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u6570\u636e\u8bad\u7ec3\u6b65\u9aa4\u5373\u53ef\u5feb\u901f\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728 https://github.com/THUDM/CogCoM \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2402.04236v1](http://arxiv.org/pdf/2402.04236v1)|null|\n", "2402.04168": "|**2024-02-06**|**Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions**|\u9488\u5bf9\u60c5\u5883\u611f\u77e5\u4ea4\u901a\u89c4\u5219\u5f02\u5e38\u7684\u77e5\u60c5\u5f3a\u5316\u5b66\u4e60|Daniel Bogdoll, Jing Qin, Moritz Nekolla, Ahmed Abouelazm, Tim Joseph, J. Marius Z\u00f6llner|Reinforcement Learning is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.|\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u4e2a\u9ad8\u5ea6\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\uff0c\u5177\u6709\u5e7f\u9614\u7684\u524d\u666f\u3002\u7136\u800c\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u901a\u5e38\u6b63\u5728\u68c0\u67e5\u975e\u5e38\u7b80\u5355\u7684\u573a\u666f\u3002\u5e38\u89c1\u7684\u65b9\u6cd5\u4f7f\u7528\u4e0d\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u547d\u4ee4\u4f5c\u4e3a\u52a8\u4f5c\u7a7a\u95f4\u548c\u7f3a\u4e4f\u7ed3\u6784\u7684\u975e\u7ed3\u6784\u5316\u5956\u52b1\u8bbe\u8ba1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u77e5\u60c5\u5f3a\u5316\u5b66\u4e60\uff0c\u5176\u4e2d\u7ed3\u6784\u5316\u89c4\u5219\u624b\u518c\u88ab\u96c6\u6210\u4e3a\u77e5\u8bc6\u6e90\u3002\u6211\u4eec\u5b66\u4e60\u8f68\u8ff9\u5e76\u901a\u8fc7\u60c5\u5883\u611f\u77e5\u5956\u52b1\u8bbe\u8ba1\u5bf9\u5176\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ece\u800c\u4ea7\u751f\u52a8\u6001\u5956\u52b1\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u4e86\u89e3\u9700\u8981\u53d7\u63a7\u4ea4\u901a\u89c4\u5219\u4f8b\u5916\u7684\u60c5\u51b5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u610f RL \u6a21\u578b\u3002\u6211\u4eec\u4f7f\u7528\u6700\u65b0\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u4ee3\u7406\u6210\u529f\u5730\u5c55\u793a\u4e86\u590d\u6742\u573a\u666f\u7684\u9ad8\u5b8c\u6210\u7387\u3002|[2402.04168v1](http://arxiv.org/pdf/2402.04168v1)|null|\n", "2402.04097": "|**2024-02-06**|**Analysis of Deep Image Prior and Exploiting Self-Guidance for Image Reconstruction**|\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u5206\u6790\u548c\u5229\u7528\u81ea\u5f15\u5bfc\u8fdb\u884c\u56fe\u50cf\u91cd\u5efa|Shijun Liang, Evan Bell, Qing Qu, Rongrong Wang, Saiprasad Ravishankar|The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI). However, conventional DIP suffers from severe overfitting and spectral bias effects.In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures.This study sheds light on important underlying properties for DIP-based recovery.Current research suggests that incorporating a reference image as network input can enhance DIP's performance in image reconstruction compared to using random inputs. However, obtaining suitable reference images requires supervision, and raises practical difficulties. In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data. Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image.We demonstrate that our self-guided method surpasses both the original DIP and modern supervised methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting.|\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c (DIP) \u4ece\u4e0d\u5b8c\u6574\u6216\u635f\u574f\u7684\u6d4b\u91cf\u4e2d\u6062\u590d\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u80fd\u529b\u4f7f\u5176\u5728\u56fe\u50cf\u6062\u590d\u548c\u533b\u5b66\u6210\u50cf\uff08\u5305\u62ec\u78c1\u5171\u632f\u6210\u50cf (MRI)\uff09\u7684\u9006\u95ee\u9898\u4e2d\u5f88\u53d7\u6b22\u8fce\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684 DIP \u906d\u53d7\u4e25\u91cd\u7684\u8fc7\u62df\u5408\u548c\u8c31\u504f\u5dee\u6548\u5e94\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u67b6\u6784\u7684\u5185\u6838\u673a\u5236\u4e2d\u5e95\u5c42\u7f51\u7edc\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5206\u6790 DIP \u5982\u4f55\u4ece\u6b20\u91c7\u6837\u6210\u50cf\u6d4b\u91cf\u4e2d\u6062\u590d\u4fe1\u606f\u3002\u63ed\u793a\u4e86\u57fa\u4e8e DIP \u7684\u6062\u590d\u7684\u91cd\u8981\u57fa\u672c\u7279\u6027\u3002\u5f53\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4f7f\u7528\u968f\u673a\u8f93\u5165\u76f8\u6bd4\uff0c\u5c06\u53c2\u8003\u56fe\u50cf\u5408\u5e76\u4e3a\u7f51\u7edc\u8f93\u5165\u53ef\u4ee5\u589e\u5f3a DIP \u5728\u56fe\u50cf\u91cd\u5efa\u4e2d\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u83b7\u5f97\u5408\u9002\u7684\u53c2\u8003\u56fe\u50cf\u9700\u8981\u76d1\u7763\uff0c\u5e76\u4e14\u5e26\u6765\u4e86\u5b9e\u9645\u56f0\u96be\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u969c\u788d\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u9a71\u52a8\u7684\u91cd\u5efa\u8fc7\u7a0b\uff0c\u8be5\u8fc7\u7a0b\u53ef\u4ee5\u540c\u65f6\u4f18\u5316\u7f51\u7edc\u6743\u91cd\u548c\u8f93\u5165\uff0c\u540c\u65f6\u6d88\u9664\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u9700\u6c42\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u964d\u566a\u6b63\u5219\u5316\u9879\uff0c\u53ef\u4ee5\u5bf9\u7f51\u7edc\u8f93\u5165\u548c\u91cd\u5efa\u56fe\u50cf\u8fdb\u884c\u9c81\u68d2\u4e14\u7a33\u5b9a\u7684\u8054\u5408\u4f30\u8ba1\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u81ea\u5f15\u5bfc\u65b9\u6cd5\u5728 MR \u56fe\u50cf\u91cd\u5efa\u6027\u80fd\u65b9\u9762\u8d85\u8d8a\u4e86\u539f\u59cb DIP \u548c\u73b0\u4ee3\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e14\u8868\u73b0\u4f18\u4e8e\u539f\u59cb DIP \u548c\u73b0\u4ee3\u76d1\u7763\u65b9\u6cd5\u3002\u4e4b\u524d\u57fa\u4e8e DIP \u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6848\u3002|[2402.04097v1](http://arxiv.org/pdf/2402.04097v1)|null|\n", "2402.04013": "|**2024-02-06**|**Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses**|DNN \u4e0a\u7684\u9690\u79c1\u6cc4\u9732\uff1a\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u548c\u9632\u5fa1\u7684\u8c03\u67e5|Hao Fang, Yixiang Qiu, Hongyao Yu, Wenbo Yu, Jiawei Kong, Baoli Chong, Bin Chen, Xuan Wang, Shu-Tao Xia|Model Inversion (MI) attacks aim to disclose private information about the training data by abusing access to the pre-trained models. These attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training data, which has raised significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this field and presents a holistic survey. Firstly, our work briefly reviews the traditional MI on machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on \\textbf{D}eep \\textbf{N}eural \\textbf{N}etworks (DNNs) across multiple modalities and learning tasks.|\u6a21\u578b\u53cd\u8f6c (MI) \u653b\u51fb\u65e8\u5728\u901a\u8fc7\u6ee5\u7528\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bbf\u95ee\u6765\u6cc4\u9732\u6709\u5173\u8bad\u7ec3\u6570\u636e\u7684\u79c1\u4eba\u4fe1\u606f\u3002\u8fd9\u4e9b\u653b\u51fb\u4f7f\u5bf9\u624b\u80fd\u591f\u91cd\u5efa\u4e0e\u79c1\u4eba\u8bad\u7ec3\u6570\u636e\u5bc6\u5207\u76f8\u5173\u7684\u9ad8\u4fdd\u771f\u6570\u636e\uff0c\u8fd9\u5f15\u8d77\u4e86\u4e25\u91cd\u7684\u9690\u79c1\u95ee\u9898\u3002\u5c3d\u7ba1\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u6211\u4eec\u7f3a\u4e4f\u5bf9\u73b0\u6709 MI \u653b\u51fb\u548c\u9632\u5fa1\u7684\u5168\u9762\u6982\u8ff0\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u6df1\u5165\u7814\u7a76\u4e86\u8fd9\u4e00\u9886\u57df\u5e76\u63d0\u51fa\u4e86\u5168\u9762\u7684\u8c03\u67e5\u3002\u9996\u5148\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u7b80\u8981\u56de\u987e\u4e86\u673a\u5668\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u4f20\u7edf MI\u3002\u7136\u540e\uff0c\u6211\u4eec\u7cbe\u5fc3\u5206\u6790\u548c\u6bd4\u8f83\u4e86\u8de8\u591a\u79cd\u6a21\u5f0f\u548c\u5b66\u4e60\u4efb\u52a1\u7684\u5bf9 \\textbf{D}eep \\textbf{N}eural \\textbf{N} \u7f51\u7edc\uff08DNN\uff09\u7684\u5927\u91cf\u6700\u65b0\u653b\u51fb\u548c\u9632\u5fa1\u3002|[2402.04013v1](http://arxiv.org/pdf/2402.04013v1)|null|\n", "2402.03766": "|**2024-02-06**|**MobileVLM V2: Faster and Stronger Baseline for Vision Language Model**|MobileVLM V2\uff1a\u66f4\u5feb\u66f4\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf|Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et.al.|We introduce MobileVLM V2, a family of significantly improved vision language models upon MobileVLM, which proves that a delicate orchestration of novel architectural design, an improved training scheme tailored for mobile VLMs, and rich high-quality dataset curation can substantially benefit VLMs' performance. Specifically, MobileVLM V2 1.7B achieves better or on-par performance on standard VLM benchmarks compared with much larger VLMs at the 3B scale. Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our models will be released at https://github.com/Meituan-AutoML/MobileVLM .|\u6211\u4eec\u63a8\u51fa\u4e86 MobileVLM V2\uff0c\u8fd9\u662f\u5728 MobileVLM \u4e0a\u8fdb\u884c\u663e\u7740\u6539\u8fdb\u7684\u4e00\u7cfb\u5217\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u8bc1\u660e\u65b0\u9896\u7684\u67b6\u6784\u8bbe\u8ba1\u7684\u7cbe\u5fc3\u7f16\u6392\u3001\u9488\u5bf9\u79fb\u52a8 VLM \u91cf\u8eab\u5b9a\u5236\u7684\u6539\u8fdb\u8bad\u7ec3\u65b9\u6848\u4ee5\u53ca\u4e30\u5bcc\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7ba1\u7406\u53ef\u4ee5\u6781\u5927\u5730\u63d0\u9ad8 VLM \u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4e0e 3B \u89c4\u6a21\u7684\u66f4\u5927 VLM \u76f8\u6bd4\uff0cMobileVLM V2 1.7B \u5728\u6807\u51c6 VLM \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u6216\u540c\u7b49\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684 3B \u6a21\u578b\u5728 7B+ \u89c4\u6a21\u4e0a\u4f18\u4e8e\u591a\u79cd VLM\u3002\u6211\u4eec\u7684\u6a21\u578b\u5c06\u5728 https://github.com/Meituan-AutoML/MobileVLM \u53d1\u5e03\u3002|[2402.03766v1](http://arxiv.org/pdf/2402.03766v1)|null|\n", "2402.03738": "|**2024-02-06**|**AoSRNet: All-in-One Scene Recovery Networks via Multi-knowledge Integration**|AoSRNet\uff1a\u901a\u8fc7\u591a\u77e5\u8bc6\u96c6\u6210\u7684\u591a\u5408\u4e00\u573a\u666f\u6062\u590d\u7f51\u7edc|Yuxu Lu, Dong Yang, Yuan Gao, Ryan Wen Liu, Jun Liu, Yu Guo|Scattering and attenuation of light in no-homogeneous imaging media or inconsistent light intensity will cause insufficient contrast and color distortion in the collected images, which limits the developments such as vision-driven smart urban, autonomous vehicles, and intelligent robots. In this paper, we propose an all-in-one scene recovery network via multi-knowledge integration (termed AoSRNet) to improve the visibility of imaging devices in typical low-visibility imaging scenes (e.g., haze, sand dust, and low light). It combines gamma correction (GC) and optimized linear stretching (OLS) to create the detail enhancement module (DEM) and color restoration module (CRM). Additionally, we suggest a multi-receptive field extraction module (MEM) to attenuate the loss of image texture details caused by GC nonlinear and OLS linear transformations. Finally, we refine the coarse features generated by DEM, CRM, and MEM through Encoder-Decoder to generate the final restored image. Comprehensive experimental results demonstrate the effectiveness and stability of AoSRNet compared to other state-of-the-art methods. The source code is available at \\url{https://github.com/LouisYuxuLu/AoSRNet}.|\u975e\u5747\u5300\u6210\u50cf\u4ecb\u8d28\u4e2d\u7684\u5149\u6563\u5c04\u548c\u8870\u51cf\u6216\u5149\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u4f1a\u5bfc\u81f4\u6536\u96c6\u5230\u7684\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u4e0d\u8db3\u548c\u989c\u8272\u5931\u771f\uff0c\u4ece\u800c\u9650\u5236\u4e86\u89c6\u89c9\u9a71\u52a8\u7684\u667a\u6167\u57ce\u5e02\u3001\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u548c\u667a\u80fd\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u53d1\u5c55\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u591a\u77e5\u8bc6\u96c6\u6210\u7684\u4e00\u4f53\u5316\u573a\u666f\u6062\u590d\u7f51\u7edc\uff08\u79f0\u4e3a AoSRNet\uff09\uff0c\u4ee5\u63d0\u9ad8\u6210\u50cf\u8bbe\u5907\u5728\u5178\u578b\u4f4e\u53ef\u89c1\u5ea6\u6210\u50cf\u573a\u666f\uff08\u4f8b\u5982\u96fe\u973e\u3001\u6c99\u5c18\u548c\u4f4e\u5149\uff09\u4e2d\u7684\u53ef\u89c1\u5ea6\u3002\u5b83\u7ed3\u5408\u4e86\u4f3d\u739b\u6821\u6b63\uff08GC\uff09\u548c\u4f18\u5316\u7684\u7ebf\u6027\u62c9\u4f38\uff08OLS\uff09\u6765\u521b\u5efa\u7ec6\u8282\u589e\u5f3a\u6a21\u5757\uff08DEM\uff09\u548c\u8272\u5f69\u6062\u590d\u6a21\u5757\uff08CRM\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u591a\u611f\u53d7\u91ce\u63d0\u53d6\u6a21\u5757\uff08MEM\uff09\u6765\u51cf\u5c11\u7531 GC \u975e\u7ebf\u6027\u548c OLS \u7ebf\u6027\u53d8\u6362\u5f15\u8d77\u7684\u56fe\u50cf\u7eb9\u7406\u7ec6\u8282\u7684\u635f\u5931\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7Encoder-Decoder\u5bf9DEM\u3001CRM\u548cMEM\u751f\u6210\u7684\u7c97\u7565\u7279\u5f81\u8fdb\u884c\u7ec6\u5316\uff0c\u751f\u6210\u6700\u7ec8\u7684\u6062\u590d\u56fe\u50cf\u3002\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86 AoSRNet \u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u7684\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\u3002\u6e90\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/LouisYuxuLu/AoSRNet} \u83b7\u53d6\u3002|[2402.03738v1](http://arxiv.org/pdf/2402.03738v1)|null|\n", "2402.03705": "|**2024-02-06**|**FoolSDEdit: Deceptively Steering Your Edits Towards Targeted Attribute-aware Distribution**|FoolSDEdit\uff1a\u6b3a\u9a97\u6027\u5730\u5c06\u60a8\u7684\u7f16\u8f91\u5f15\u5411\u6709\u9488\u5bf9\u6027\u7684\u5c5e\u6027\u611f\u77e5\u5206\u53d1|Qi Zhou, Dongxia Wang, Tianlin Li, Zhihong Xu, Yang Liu, Kui Ren, Wenhai Wang, Qing Guo|Guided image synthesis methods, like SDEdit based on the diffusion model, excel at creating realistic images from user inputs such as stroke paintings. However, existing efforts mainly focus on image quality, often overlooking a key point: the diffusion model represents a data distribution, not individual images. This introduces a low but critical chance of generating images that contradict user intentions, raising ethical concerns. For example, a user inputting a stroke painting with female characteristics might, with some probability, get male faces from SDEdit. To expose this potential vulnerability, we aim to build an adversarial attack forcing SDEdit to generate a specific data distribution aligned with a specified attribute (e.g., female), without changing the input's attribute characteristics. We propose the Targeted Attribute Generative Attack (TAGA), using an attribute-aware objective function and optimizing the adversarial noise added to the input stroke painting. Empirical studies reveal that traditional adversarial noise struggles with TAGA, while natural perturbations like exposure and motion blur easily alter generated images' attributes. To execute effective attacks, we introduce FoolSDEdit: We design a joint adversarial exposure and blur attack, adding exposure and motion blur to the stroke painting and optimizing them together. We optimize the execution strategy of various perturbations, framing it as a network architecture search problem. We create the SuperPert, a graph representing diverse execution strategies for different perturbations. After training, we obtain the optimized execution strategy for effective TAGA against SDEdit. Comprehensive experiments on two datasets show our method compelling SDEdit to generate a targeted attribute-aware data distribution, significantly outperforming baselines.|\u5f15\u5bfc\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff08\u4f8b\u5982\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684 SDEdit\uff09\u64c5\u957f\u6839\u636e\u7528\u6237\u8f93\u5165\uff08\u4f8b\u5982\u7b14\u753b\uff09\u521b\u5efa\u903c\u771f\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u52aa\u529b\u4e3b\u8981\u96c6\u4e2d\u5728\u56fe\u50cf\u8d28\u91cf\u4e0a\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u4e00\u4e2a\u5173\u952e\u70b9\uff1a\u6269\u6563\u6a21\u578b\u4ee3\u8868\u6570\u636e\u5206\u5e03\uff0c\u800c\u4e0d\u662f\u5355\u4e2a\u56fe\u50cf\u3002\u8fd9\u5bfc\u81f4\u751f\u6210\u4e0e\u7528\u6237\u610f\u56fe\u76f8\u77db\u76fe\u7684\u56fe\u50cf\u7684\u53ef\u80fd\u6027\u5f88\u4f4e\uff0c\u4f46\u5f88\u5173\u952e\uff0c\u4ece\u800c\u5f15\u8d77\u9053\u5fb7\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u7528\u6237\u8f93\u5165\u5177\u6709\u5973\u6027\u7279\u5f81\u7684\u7b14\u753b\u53ef\u80fd\u6709\u4e00\u5b9a\u6982\u7387\u4ece SDEdit \u4e2d\u83b7\u5f97\u7537\u6027\u9762\u5b54\u3002\u4e3a\u4e86\u66b4\u9732\u8fd9\u4e2a\u6f5c\u5728\u7684\u6f0f\u6d1e\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u6784\u5efa\u4e00\u4e2a\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u8feb\u4f7f SDEdit \u751f\u6210\u4e0e\u6307\u5b9a\u5c5e\u6027\uff08\u4f8b\u5982\u5973\u6027\uff09\u4e00\u81f4\u7684\u7279\u5b9a\u6570\u636e\u5206\u5e03\uff0c\u800c\u4e0d\u6539\u53d8\u8f93\u5165\u7684\u5c5e\u6027\u7279\u5f81\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u76ee\u6807\u5c5e\u6027\u751f\u6210\u653b\u51fb\uff08TAGA\uff09\uff0c\u4f7f\u7528\u5c5e\u6027\u611f\u77e5\u76ee\u6807\u51fd\u6570\u5e76\u4f18\u5316\u6dfb\u52a0\u5230\u8f93\u5165\u7b14\u5212\u7ed8\u753b\u4e2d\u7684\u5bf9\u6297\u6027\u566a\u58f0\u3002\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u4f20\u7edf\u7684\u5bf9\u6297\u6027\u566a\u58f0\u4e0e TAGA \u76f8\u6297\u8861\uff0c\u800c\u66dd\u5149\u548c\u8fd0\u52a8\u6a21\u7cca\u7b49\u81ea\u7136\u6270\u52a8\u5f88\u5bb9\u6613\u6539\u53d8\u751f\u6210\u56fe\u50cf\u7684\u5c5e\u6027\u3002\u4e3a\u4e86\u6267\u884c\u6709\u6548\u7684\u653b\u51fb\uff0c\u6211\u4eec\u5f15\u5165\u4e86 FoolSDEdit\uff1a\u6211\u4eec\u8bbe\u8ba1\u4e86\u8054\u5408\u5bf9\u6297\u6027\u66dd\u5149\u548c\u6a21\u7cca\u653b\u51fb\uff0c\u5c06\u66dd\u5149\u548c\u8fd0\u52a8\u6a21\u7cca\u6dfb\u52a0\u5230\u7b14\u753b\u4e2d\u5e76\u4e00\u8d77\u4f18\u5316\u5b83\u4eec\u3002\u6211\u4eec\u4f18\u5316\u5404\u79cd\u6270\u52a8\u7684\u6267\u884c\u7b56\u7565\uff0c\u5c06\u5176\u89c6\u4e3a\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u95ee\u9898\u3002\u6211\u4eec\u521b\u5efa\u4e86 SuperPert\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee3\u8868\u4e0d\u540c\u6270\u52a8\u7684\u4e0d\u540c\u6267\u884c\u7b56\u7565\u7684\u56fe\u8868\u3002\u7ecf\u8fc7\u8bad\u7ec3\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u9488\u5bf9 SDEdit \u6709\u6548 TAGA \u7684\u4f18\u5316\u6267\u884c\u7b56\u7565\u3002\u5bf9\u4e24\u4e2a\u6570\u636e\u96c6\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8feb\u4f7f SDEdit \u751f\u6210\u76ee\u6807\u5c5e\u6027\u611f\u77e5\u6570\u636e\u5206\u5e03\uff0c\u5176\u6027\u80fd\u663e\u7740\u4f18\u4e8e\u57fa\u7ebf\u3002|[2402.03705v1](http://arxiv.org/pdf/2402.03705v1)|null|\n", "2402.03666": "|**2024-02-06**|**QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning**|QuEST\uff1a\u901a\u8fc7\u9ad8\u6548\u9009\u62e9\u6027\u5fae\u8c03\u8fdb\u884c\u4f4e\u4f4d\u6269\u6563\u6a21\u578b\u91cf\u5316|Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan|Diffusion models have achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption. While quantization paves a way for diffusion model compression and acceleration, existing methods totally fail when the models are quantized to low-bits. In this paper, we unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules. To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution. Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency. We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization. Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion.|\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u529f\uff0c\u4f46\u5176\u5b9e\u9645\u90e8\u7f72\u53d7\u5230\u9ad8\u5185\u5b58\u548c\u65f6\u95f4\u6d88\u8017\u7684\u9650\u5236\u3002\u867d\u7136\u91cf\u5316\u4e3a\u6269\u6563\u6a21\u578b\u538b\u7f29\u548c\u52a0\u901f\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4f46\u5f53\u6a21\u578b\u91cf\u5316\u5230\u4f4e\u4f4d\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u5b8c\u5168\u5931\u8d25\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u91cf\u5316\u6269\u6563\u6a21\u578b\u4e2d\u635f\u5bb3\u5f53\u524d\u65b9\u6cd5\u6709\u6548\u6027\u7684\u4e09\u4e2a\u5c5e\u6027\uff1a\u4e0d\u5e73\u8861\u7684\u6fc0\u6d3b\u5206\u5e03\u3001\u4e0d\u7cbe\u786e\u7684\u65f6\u95f4\u4fe1\u606f\u4ee5\u53ca\u6613\u53d7\u7279\u5b9a\u6a21\u5757\u6270\u52a8\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u7f13\u89e3\u7531\u4e8e\u5206\u5e03\u4e0d\u5e73\u8861\u800c\u52a0\u5267\u7684\u4f4e\u4f4d\u91cf\u5316\u96be\u5ea6\uff0c\u6211\u4eec\u5efa\u8bae\u5bf9\u91cf\u5316\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u6fc0\u6d3b\u5206\u5e03\u3002\u57fa\u4e8e\u8fd9\u4e2a\u60f3\u6cd5\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u4e24\u79cd\u5173\u952e\u7c7b\u578b\u7684\u91cf\u5316\u5c42\uff1a\u4fdd\u5b58\u91cd\u8981\u65f6\u95f4\u4fe1\u606f\u7684\u5c42\u548c\u5bf9\u51cf\u5c11\u4f4d\u5bbd\u654f\u611f\u7684\u5c42\uff0c\u5e76\u5bf9\u5b83\u4eec\u8fdb\u884c\u5fae\u8c03\u4ee5\u6709\u6548\u51cf\u8f7b\u6027\u80fd\u4e0b\u964d\u3002\u6211\u4eec\u51ed\u7ecf\u9a8c\u9a8c\u8bc1\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u4fee\u6539\u6fc0\u6d3b\u5206\u5e03\u5e76\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u5bb9\u6613\u3001\u66f4\u51c6\u786e\u7684\u91cf\u5316\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u4e09\u4e2a\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5728\u5404\u79cd\u4f4d\u5bbd\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u662f\u7b2c\u4e00\u4e2a\u5728\u5168 4 \u4f4d\uff08\u5373 W4A4\uff09\u4e0a\u751f\u6210\u53ef\u8bfb\u56fe\u50cf\u7684\u65b9\u6cd5\uff08\u7a33\u5b9a\uff09\u6269\u6563\u3002|[2402.03666v1](http://arxiv.org/pdf/2402.03666v1)|null|\n", "2402.03654": "|**2024-02-06**|**Reviewing FID and SID Metrics on Generative Adversarial Networks**|\u5ba1\u67e5\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684 FID \u548c SID \u6307\u6807|Ricardo de Deijn, Aishwarya Batra, Brandon Koch, Naseef Mansoor, Hema Makkena|The growth of generative adversarial network (GAN) models has increased the ability of image processing and provides numerous industries with the technology to produce realistic image transformations. However, with the field being recently established there are new evaluation metrics that can further this research. Previous research has shown the Fr\\'echet Inception Distance (FID) to be an effective metric when testing these image-to-image GANs in real-world applications. Signed Inception Distance (SID), a founded metric in 2023, expands on FID by allowing unsigned distances. This paper uses public datasets that consist of fa\\c{c}ades, cityscapes, and maps within Pix2Pix and CycleGAN models. After training these models are evaluated on both inception distance metrics which measure the generating performance of the trained models. Our findings indicate that usage of the metric SID incorporates an efficient and effective metric to complement, or even exceed the ability shown using the FID for the image-to-image GANs|\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u9ad8\u4e86\u56fe\u50cf\u5904\u7406\u7684\u80fd\u529b\uff0c\u5e76\u4e3a\u4f17\u591a\u884c\u4e1a\u63d0\u4f9b\u4e86\u4ea7\u751f\u903c\u771f\u56fe\u50cf\u8f6c\u6362\u7684\u6280\u672f\u3002\u7136\u800c\uff0c\u968f\u7740\u8be5\u9886\u57df\u6700\u8fd1\u7684\u5efa\u7acb\uff0c\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a8\u8fdb\u8fd9\u9879\u7814\u7a76\u3002\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0cFr'echet \u8d77\u59cb\u8ddd\u79bb (FID) \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6d4b\u8bd5\u8fd9\u4e9b\u56fe\u50cf\u5230\u56fe\u50cf GAN \u65f6\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6307\u6807\u3002\u6709\u7b26\u53f7\u8d77\u59cb\u8ddd\u79bb (SID) \u662f 2023 \u5e74\u5efa\u7acb\u7684\u6307\u6807\uff0c\u901a\u8fc7\u5141\u8bb8\u65e0\u7b26\u53f7\u8ddd\u79bb\u6765\u6269\u5c55 FID\u3002\u672c\u6587\u4f7f\u7528\u7684\u516c\u5171\u6570\u636e\u96c6\u5305\u62ec Pix2Pix \u548c CycleGAN \u6a21\u578b\u4e2d\u7684\u7acb\u9762\u3001\u57ce\u5e02\u666f\u89c2\u548c\u5730\u56fe\u3002\u8bad\u7ec3\u540e\uff0c\u6839\u636e\u4e24\u4e2a\u521d\u59cb\u8ddd\u79bb\u6307\u6807\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u6307\u6807\u8861\u91cf\u8bad\u7ec3\u6a21\u578b\u7684\u751f\u6210\u6027\u80fd\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5ea6\u91cf SID \u7684\u4f7f\u7528\u5305\u542b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u5ea6\u91cf\uff0c\u53ef\u4ee5\u8865\u5145\u751a\u81f3\u8d85\u8fc7\u4f7f\u7528 FID \u8fdb\u884c\u56fe\u50cf\u5230\u56fe\u50cf GAN \u6240\u663e\u793a\u7684\u80fd\u529b|[2402.03654v1](http://arxiv.org/pdf/2402.03654v1)|null|\n", "2402.03592": "|**2024-02-06**|**GRASP: GRAph-Structured Pyramidal Whole Slide Image Representation**|GRASP\uff1a\u56fe\u7ed3\u6784\u91d1\u5b57\u5854\u6574\u4f53\u5e7b\u706f\u7247\u56fe\u50cf\u8868\u793a|Ali Khajegili Mirabadi, Graham Archibald, Amirali Darbandsari, Alberto Contreras-Sanz, Ramin Ebrahim Nakhli, Maryam Asadi, Allen Zhang, C. Blake Gilks, Peter Black, Gang Wang, et.al.|Cancer subtyping is one of the most challenging tasks in digital pathology, where Multiple Instance Learning (MIL) by processing gigapixel whole slide images (WSIs) has been in the spotlight of recent research. However, MIL approaches do not take advantage of inter- and intra-magnification information contained in WSIs. In this work, we present GRASP, a novel graph-structured multi-magnification framework for processing WSIs in digital pathology. Our approach is designed to dynamically emulate the pathologist's behavior in handling WSIs and benefits from the hierarchical structure of WSIs. GRASP, which introduces a convergence-based node aggregation instead of traditional pooling mechanisms, outperforms state-of-the-art methods over two distinct cancer datasets by a margin of up to 10% balanced accuracy, while being 7 times smaller than the closest-performing state-of-the-art model in terms of the number of parameters. Our results show that GRASP is dynamic in finding and consulting with different magnifications for subtyping cancers and is reliable and stable across different hyperparameters. The model's behavior has been evaluated by two expert pathologists confirming the interpretability of the model's dynamic. We also provide a theoretical foundation, along with empirical evidence, for our work, explaining how GRASP interacts with different magnifications and nodes in the graph to make predictions. We believe that the strong characteristics yet simple structure of GRASP will encourage the development of interpretable, structure-based designs for WSI representation in digital pathology. Furthermore, we publish two large graph datasets of rare Ovarian and Bladder cancers to contribute to the field.|\u764c\u75c7\u4e9a\u578b\u5206\u578b\u662f\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e4b\u4e00\uff0c\u5176\u4e2d\u901a\u8fc7\u5904\u7406\u5341\u4ebf\u50cf\u7d20\u5168\u5e7b\u706f\u7247\u56fe\u50cf (WSI) \u7684\u591a\u5b9e\u4f8b\u5b66\u4e60 (MIL) \u4e00\u76f4\u662f\u6700\u8fd1\u7814\u7a76\u7684\u7126\u70b9\u3002\u7136\u800c\uff0cMIL \u65b9\u6cd5\u6ca1\u6709\u5229\u7528 WSI \u4e2d\u5305\u542b\u7684\u5185\u90e8\u548c\u5185\u90e8\u653e\u5927\u4fe1\u606f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GRASP\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u7ed3\u6784\u591a\u653e\u5927\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684 WSI\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u65e8\u5728\u52a8\u6001\u6a21\u62df\u75c5\u7406\u5b66\u5bb6\u5728\u5904\u7406 WSI \u65f6\u7684\u884c\u4e3a\uff0c\u5e76\u4ece WSI \u7684\u5c42\u6b21\u7ed3\u6784\u4e2d\u53d7\u76ca\u3002 GRASP \u5f15\u5165\u4e86\u57fa\u4e8e\u6536\u655b\u7684\u8282\u70b9\u805a\u5408\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u6c60\u5316\u673a\u5236\uff0c\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u764c\u75c7\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e73\u8861\u7cbe\u5ea6\u9ad8\u8fbe 10%\uff0c\u540c\u65f6\u6bd4\u6700\u63a5\u8fd1\u7684\u65b9\u6cd5\u5c0f 7 \u500d\u3002\u5728\u53c2\u6570\u6570\u91cf\u65b9\u9762\u6267\u884c\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cGRASP \u5728\u5bfb\u627e\u548c\u54a8\u8be2\u4e0d\u540c\u653e\u5927\u500d\u6570\u7684\u764c\u75c7\u4e9a\u578b\u65b9\u9762\u662f\u52a8\u6001\u7684\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u8d85\u53c2\u6570\u4e0b\u90fd\u662f\u53ef\u9760\u548c\u7a33\u5b9a\u7684\u3002\u8be5\u6a21\u578b\u7684\u884c\u4e3a\u5df2\u7531\u4e24\u4f4d\u75c5\u7406\u5b66\u5bb6\u4e13\u5bb6\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u786e\u8ba4\u4e86\u6a21\u578b\u52a8\u6001\u7684\u53ef\u89e3\u91ca\u6027\u3002\u6211\u4eec\u8fd8\u4e3a\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u7ecf\u9a8c\u8bc1\u636e\uff0c\u89e3\u91ca\u4e86 GRASP \u5982\u4f55\u4e0e\u56fe\u4e2d\u7684\u4e0d\u540c\u653e\u5927\u500d\u6570\u548c\u8282\u70b9\u4ea4\u4e92\u6765\u505a\u51fa\u9884\u6d4b\u3002\u6211\u4eec\u76f8\u4fe1\uff0cGRASP \u5f3a\u5927\u7684\u7279\u6027\u548c\u7b80\u5355\u7684\u7ed3\u6784\u5c06\u9f13\u52b1\u5f00\u53d1\u53ef\u89e3\u91ca\u7684\u3001\u57fa\u4e8e\u7ed3\u6784\u7684\u8bbe\u8ba1\uff0c\u7528\u4e8e\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684 WSI \u8868\u793a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u4e24\u4e2a\u7f55\u89c1\u5375\u5de2\u764c\u548c\u8180\u80f1\u764c\u7684\u5927\u578b\u56fe\u5f62\u6570\u636e\u96c6\uff0c\u4e3a\u8be5\u9886\u57df\u505a\u51fa\u8d21\u732e\u3002|[2402.03592v1](http://arxiv.org/pdf/2402.03592v1)|null|\n"}}