{"\u751f\u6210\u6a21\u578b": {}, "\u591a\u6a21\u6001": {}, "Nerf": {}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.17766": "|**2024-02-27**|**ShapeLLM: Universal 3D Object Understanding for Embodied Interaction**|ShapeLLM\uff1a\u7528\u4e8e\u5b9e\u4f53\u4ea4\u4e92\u7684\u901a\u7528 3D \u5bf9\u8c61\u7406\u89e3|Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, Kaisheng Ma|This paper presents ShapeLLM, the first 3D Multimodal Large Language Model (LLM) designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages. ShapeLLM is built upon an improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view image distillation for enhanced geometry understanding. By utilizing ReCon++ as the 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed instruction-following data and tested on our newly human-curated evaluation benchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3D geometry understanding and language-unified 3D interaction tasks, such as embodied visual grounding.|\u672c\u6587\u4ecb\u7ecd\u4e86 ShapeLLM\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u4e3a\u4f53\u73b0\u4ea4\u4e92\u800c\u8bbe\u8ba1\u7684 3D \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (LLM)\uff0c\u63a2\u7d22\u4f7f\u7528 3D \u70b9\u4e91\u548c\u8bed\u8a00\u7684\u901a\u7528 3D \u5bf9\u8c61\u7406\u89e3\u3002 ShapeLLM \u57fa\u4e8e\u6539\u8fdb\u7684 3D \u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5c06 ReCon \u6269\u5c55\u5230 ReCon++\uff0c\u53d7\u76ca\u4e8e\u591a\u89c6\u56fe\u56fe\u50cf\u84b8\u998f\u4ee5\u589e\u5f3a\u51e0\u4f55\u7406\u89e3\u3002\u901a\u8fc7\u5229\u7528 ReCon++ \u4f5c\u4e3a LLM \u7684 3D \u70b9\u4e91\u8f93\u5165\u7f16\u7801\u5668\uff0cShapeLLM \u5728\u6784\u5efa\u7684\u6307\u4ee4\u8ddf\u8e2a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u6211\u4eec\u65b0\u7684\u4eba\u5de5\u7b56\u5212\u7684\u8bc4\u4f30\u57fa\u51c6 3D MM-Vet \u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002 ReCon++ \u548c ShapeLLM \u5728 3D \u51e0\u4f55\u7406\u89e3\u548c\u8bed\u8a00\u7edf\u4e00\u7684 3D \u4ea4\u4e92\u4efb\u52a1\uff08\u4f8b\u5982\u5177\u4f53\u89c6\u89c9\u57fa\u7840\uff09\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2402.17766v1](http://arxiv.org/pdf/2402.17766v1)|null|\n", "2402.17706": "|**2024-02-27**|**Adaptive quantization with mixed-precision based on low-cost proxy**|\u57fa\u4e8e\u4f4e\u6210\u672c\u4ee3\u7406\u7684\u6df7\u5408\u7cbe\u5ea6\u81ea\u9002\u5e94\u91cf\u5316|Junzhe Chen, Qiao Yang, Senmao Tian, Shunli Zhang|It is critical to deploy complicated neural network models on hardware with limited resources. This paper proposes a novel model quantization method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ), which contains three key modules. The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision quantization module is developed to evaluate the quantization sensitivity by using the Hessian matrix and Pareto frontier techniques. Integer linear programming is used to fine-tune the quantization across different layers. Then the low-cost proxy neural architecture search module efficiently explores the ideal quantization hyperparameters. Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior quantization accuracy to existing mixed-precision models. Notably, LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical quantization use for resource-limited devices.|\u5728\u8d44\u6e90\u6709\u9650\u7684\u786c\u4ef6\u4e0a\u90e8\u7f72\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\u91cf\u5316\u65b9\u6cd5\uff0c\u79f0\u4e3a\u57fa\u4e8e\u4f4e\u6210\u672c\u4ee3\u7406\u7684\u81ea\u9002\u5e94\u6df7\u5408\u7cbe\u5ea6\u6a21\u578b\u91cf\u5316\uff08LCPAQ\uff09\uff0c\u8be5\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\u3002\u8003\u8651\u5230\u786c\u4ef6\u9650\u5236\uff0c\u8bbe\u8ba1\u4e86\u786c\u4ef6\u611f\u77e5\u6a21\u5757\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u81ea\u9002\u5e94\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6a21\u5757\uff0c\u5229\u7528 Hessian \u77e9\u9635\u548c Pareto \u524d\u6cbf\u6280\u672f\u6765\u8bc4\u4f30\u91cf\u5316\u7075\u654f\u5ea6\u3002\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7528\u4e8e\u5fae\u8c03\u4e0d\u540c\u5c42\u7684\u91cf\u5316\u3002\u7136\u540e\uff0c\u4f4e\u6210\u672c\u4ee3\u7406\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u6a21\u5757\u6709\u6548\u5730\u63a2\u7d22\u7406\u60f3\u7684\u91cf\u5316\u8d85\u53c2\u6570\u3002 ImageNet \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 LCPAQ \u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6df7\u5408\u7cbe\u5ea6\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u9ad8\u7684\u91cf\u5316\u7cbe\u5ea6\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cLCPAQ \u7684\u641c\u7d22\u65f6\u95f4\u7f29\u77ed\u4e86 1/200\uff0c\u8fd9\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u8bbe\u5907\u7684\u5b9e\u9645\u91cf\u5316\u4f7f\u7528\u63d0\u4f9b\u4e86\u6377\u5f84\u3002|[2402.17706v1](http://arxiv.org/pdf/2402.17706v1)|null|\n", "2402.17680": "|**2024-02-27**|**MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning**|MCF-VC\uff1a\u51cf\u8f7b\u591a\u6a21\u6001\u89c6\u9891\u5b57\u5e55\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8|Huiyu Xiong, Lanxiao Wang, Heqian Qiu, Taijin Zhao, Benliu Qiu, Hongliang Li|To address the problem of catastrophic forgetting due to the invisibility of old categories in sequential input, existing work based on relatively simple categorization tasks has made some progress. In contrast, video captioning is a more complex task in multimodal scenario, which has not been explored in the field of incremental learning. After identifying this stability-plasticity problem when analyzing video with sequential input, we originally propose a method to Mitigate Catastrophic Forgetting in class-incremental learning for multimodal Video Captioning (MCF-VC). As for effectively maintaining good performance on old tasks at the macro level, we design Fine-grained Sensitivity Selection (FgSS) based on the Mask of Linear's Parameters and Fisher Sensitivity to pick useful knowledge from old tasks. Further, in order to better constrain the knowledge characteristics of old and new tasks at the specific feature level, we have created the Two-stage Knowledge Distillation (TsKD), which is able to learn the new task well while weighing the old task. Specifically, we design two distillation losses, which constrain the cross modal semantic information of semantic attention feature map and the textual information of the final outputs respectively, so that the inter-model and intra-model stylized knowledge of the old class is retained while learning the new class. In order to illustrate the ability of our model to resist forgetting, we designed a metric CIDER_t to detect the stage forgetting rate. Our experiments on the public dataset MSR-VTT show that the proposed method significantly resists the forgetting of previous tasks without replaying old samples, and performs well on the new task.|\u4e3a\u4e86\u89e3\u51b3\u7531\u4e8e\u987a\u5e8f\u8f93\u5165\u4e2d\u65e7\u7c7b\u522b\u4e0d\u53ef\u89c1\u800c\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u57fa\u4e8e\u76f8\u5bf9\u7b80\u5355\u7684\u5206\u7c7b\u4efb\u52a1\u7684\u73b0\u6709\u5de5\u4f5c\u5df2\u7ecf\u53d6\u5f97\u4e86\u4e00\u4e9b\u8fdb\u5c55\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u89c6\u9891\u5b57\u5e55\u5728\u591a\u6a21\u6001\u573a\u666f\u4e2d\u662f\u4e00\u4e2a\u66f4\u590d\u6742\u7684\u4efb\u52a1\uff0c\u5728\u589e\u91cf\u5b66\u4e60\u9886\u57df\u5c1a\u672a\u5f97\u5230\u63a2\u7d22\u3002\u5728\u5206\u6790\u5177\u6709\u987a\u5e8f\u8f93\u5165\u7684\u89c6\u9891\u65f6\u8bc6\u522b\u51fa\u8fd9\u79cd\u7a33\u5b9a\u6027\u53ef\u5851\u6027\u95ee\u9898\u540e\uff0c\u6211\u4eec\u6700\u521d\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u6a21\u6001\u89c6\u9891\u5b57\u5e55\uff08MCF-VC\uff09\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u7684\u65b9\u6cd5\u3002\u4e3a\u4e86\u5728\u5b8f\u89c2\u5c42\u9762\u6709\u6548\u4fdd\u6301\u65e7\u4efb\u52a1\u7684\u826f\u597d\u6027\u80fd\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u7ebf\u6027\u53c2\u6570\u63a9\u6a21\u548c\u8d39\u820d\u5c14\u7075\u654f\u5ea6\u7684\u7ec6\u7c92\u5ea6\u7075\u654f\u5ea6\u9009\u62e9\uff08FgSS\uff09\uff0c\u4ee5\u4ece\u65e7\u4efb\u52a1\u4e2d\u6311\u9009\u6709\u7528\u7684\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u66f4\u597d\u5730\u5728\u7279\u5b9a\u7279\u5f81\u7ea7\u522b\u4e0a\u7ea6\u675f\u65b0\u65e7\u4efb\u52a1\u7684\u77e5\u8bc6\u7279\u5f81\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\uff08TsKD\uff09\uff0c\u5b83\u80fd\u591f\u5728\u6743\u8861\u65e7\u4efb\u52a1\u7684\u540c\u65f6\u5f88\u597d\u5730\u5b66\u4e60\u65b0\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u84b8\u998f\u635f\u5931\uff0c\u5206\u522b\u7ea6\u675f\u8bed\u4e49\u6ce8\u610f\u7279\u5f81\u56fe\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u4fe1\u606f\u548c\u6700\u7ec8\u8f93\u51fa\u7684\u6587\u672c\u4fe1\u606f\uff0c\u4ece\u800c\u5728\u5b66\u4e60\u65f6\u4fdd\u7559\u65e7\u7c7b\u7684\u6a21\u578b\u95f4\u548c\u6a21\u578b\u5185\u7684\u98ce\u683c\u5316\u77e5\u8bc6\u65b0\u73ed\u7ea7\u3002\u4e3a\u4e86\u8bf4\u660e\u6211\u4eec\u7684\u6a21\u578b\u62b5\u6297\u9057\u5fd8\u7684\u80fd\u529b\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5ea6\u91cfCIDER_t\u6765\u68c0\u6d4b\u9636\u6bb5\u9057\u5fd8\u7387\u3002\u6211\u4eec\u5728\u516c\u5171\u6570\u636e\u96c6MSR-VTT\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u91cd\u653e\u65e7\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u663e\u7740\u5730\u62b5\u6297\u4e86\u5148\u524d\u4efb\u52a1\u7684\u9057\u5fd8\uff0c\u5e76\u4e14\u5728\u65b0\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\u3002|[2402.17680v1](http://arxiv.org/pdf/2402.17680v1)|null|\n", "2402.17414": "|**2024-02-27**|**Neural Video Compression with Feature Modulation**|\u5177\u6709\u7279\u5f81\u8c03\u5236\u7684\u795e\u7ecf\u89c6\u9891\u538b\u7f29|Jiahao Li, Bin Li, Yan Lu|The emerging conditional coding-based neural video codec (NVC) shows superiority over commonly-used residual coding-based codec and the latest NVC already claims to outperform the best traditional codec. However, there still exist critical problems blocking the practicality of NVC. In this paper, we propose a powerful conditional coding-based NVC that solves two critical problems via feature modulation. The first is how to support a wide quality range in a single model. Previous NVC with this capability only supports about 3.8 dB PSNR range on average. To tackle this limitation, we modulate the latent feature of the current frame via the learnable quantization scaler. During the training, we specially design the uniform quantization parameter sampling mechanism to improve the harmonization of encoding and quantization. This results in a better learning of the quantization scaler and helps our NVC support about 11.4 dB PSNR range. The second is how to make NVC still work under a long prediction chain. We expose that the previous SOTA NVC has an obvious quality degradation problem when using a large intra-period setting. To this end, we propose modulating the temporal feature with a periodically refreshing mechanism to boost the quality. %Besides solving the above two problems, we also design a single model that can support both RGB and YUV colorspaces. Notably, under single intra-frame setting, our codec can achieve 29.7\\% bitrate saving over previous SOTA NVC with 16\\% MACs reduction. Our codec serves as a notable landmark in the journey of NVC evolution. The codes are at https://github.com/microsoft/DCVC.|\u65b0\u5174\u7684\u57fa\u4e8e\u6761\u4ef6\u7f16\u7801\u7684\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u5668 (NVC) \u663e\u793a\u51fa\u4f18\u4e8e\u5e38\u7528\u7684\u57fa\u4e8e\u6b8b\u5dee\u7f16\u7801\u7684\u7f16\u89e3\u7801\u5668\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u4e14\u6700\u65b0\u7684 NVC \u5df2\u7ecf\u58f0\u79f0\u4f18\u4e8e\u6700\u597d\u7684\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u3002\u7136\u800c\uff0c\u4ecd\u7136\u5b58\u5728\u963b\u788d NVC \u5b9e\u7528\u5316\u7684\u5173\u952e\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u57fa\u4e8e\u6761\u4ef6\u7f16\u7801\u7684 NVC\uff0c\u5b83\u901a\u8fc7\u7279\u5f81\u8c03\u5236\u89e3\u51b3\u4e86\u4e24\u4e2a\u5173\u952e\u95ee\u9898\u3002\u9996\u5148\u662f\u5982\u4f55\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u652f\u6301\u5e7f\u6cdb\u7684\u8d28\u91cf\u8303\u56f4\u3002\u4ee5\u524d\u5177\u6709\u6b64\u529f\u80fd\u7684 NVC \u5e73\u5747\u4ec5\u652f\u6301\u7ea6 3.8 dB PSNR \u8303\u56f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u91cf\u5316\u7f29\u653e\u5668\u6765\u8c03\u5236\u5f53\u524d\u5e27\u7684\u6f5c\u5728\u7279\u5f81\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4e13\u95e8\u8bbe\u8ba1\u4e86\u7edf\u4e00\u91cf\u5316\u53c2\u6570\u91c7\u6837\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u7f16\u7801\u548c\u91cf\u5316\u7684\u534f\u8c03\u6027\u3002\u8fd9\u53ef\u4ee5\u66f4\u597d\u5730\u5b66\u4e60\u91cf\u5316\u7f29\u653e\u5668\uff0c\u5e76\u5e2e\u52a9\u6211\u4eec\u7684 NVC \u652f\u6301\u7ea6 11.4 dB PSNR \u8303\u56f4\u3002\u7b2c\u4e8c\u4e2a\u662f\u5982\u4f55\u8ba9NVC\u5728\u957f\u9884\u6d4b\u94fe\u4e0b\u4ecd\u7136\u5de5\u4f5c\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4e4b\u524d\u7684 SOTA NVC \u5728\u4f7f\u7528\u8f83\u5927\u7684\u5468\u671f\u5185\u8bbe\u7f6e\u65f6\u5b58\u5728\u660e\u663e\u7684\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u5b9a\u671f\u5237\u65b0\u673a\u5236\u6765\u8c03\u5236\u65f6\u95f4\u7279\u5f81\u4ee5\u63d0\u9ad8\u8d28\u91cf\u3002 %\u9664\u4e86\u89e3\u51b3\u4e0a\u8ff0\u4e24\u4e2a\u95ee\u9898\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u4ee5\u540c\u65f6\u652f\u6301RGB\u548cYUV\u8272\u5f69\u7a7a\u95f4\u7684\u5355\u4e00\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u5355\u5e27\u5185\u8bbe\u7f6e\u4e0b\uff0c\u6211\u4eec\u7684\u7f16\u89e3\u7801\u5668\u6bd4\u4e4b\u524d\u7684 SOTA NVC \u53ef\u4ee5\u8282\u7701 29.7% \u7684\u6bd4\u7279\u7387\uff0c\u5e76\u51cf\u5c11 16% \u7684 MAC\u3002\u6211\u4eec\u7684\u7f16\u89e3\u7801\u5668\u662f NVC \u53d1\u5c55\u5386\u7a0b\u4e2d\u7684\u4e00\u4e2a\u663e\u7740\u91cc\u7a0b\u7891\u3002\u4ee3\u7801\u4f4d\u4e8e https://github.com/microsoft/DCVC\u3002|[2402.17414v1](http://arxiv.org/pdf/2402.17414v1)|null|\n", "2402.17316": "|**2024-02-27**|**Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation**|\u901a\u8fc7\u9009\u62e9\u6027\u71b5\u84b8\u998f\u5b9e\u73b0\u7a33\u5065\u9ad8\u6548\u7684\u4e91\u8fb9\u7f18\u5f39\u6027\u6a21\u578b\u9002\u5e94|Yaofo Chen, Shuaicheng Niu, Shoukai Xu, Hengjie Song, Yaowei Wang, Mingkui Tan|The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.|\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\u901a\u5e38\u6d89\u53ca\u5728\u670d\u52a1\u5668\u4e0a\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\uff0c\u7136\u540e\u5c06\u8be5\u6a21\u578b\u6216\u5176\u7cbe\u70bc\u6a21\u578b\u90e8\u7f72\u5230\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002\u901a\u5e38\uff0c\u7531\u4e8e\u670d\u52a1\u5668\u7aef\u548c\u8fb9\u7f18\u7aef\u7684\u6a21\u578b\u9002\u914d\u6210\u672c\u53ef\u80fd\u5f88\u9ad8\uff0c\u6a21\u578b\u4e00\u65e6\u90e8\u7f72\uff08\u81f3\u5c11\u5728\u4e00\u6bb5\u65f6\u95f4\u5185\uff09\u5c31\u5e94\u4fdd\u6301\u56fa\u5b9a\u3002\u7136\u800c\uff0c\u5728\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u6d4b\u8bd5\u73af\u5883\u53ef\u80fd\u4f1a\u52a8\u6001\u53d8\u5316\uff08\u79f0\u4e3a\u5206\u5e03\u53d8\u5316\uff09\uff0c\u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u4eba\u4eec\u5fc5\u987b\u53ca\u65f6\u8c03\u6574\u8fb9\u7f18\u6a21\u578b\u4ee5\u83b7\u5f97\u6709\u5e0c\u671b\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u968f\u7740\u5728\u8fb9\u7f18\u6536\u96c6\u7684\u6570\u636e\u4e0d\u65ad\u589e\u52a0\uff0c\u8fd9\u79cd\u8303\u5f0f\u4e5f\u65e0\u6cd5\u8fdb\u4e00\u6b65\u9002\u5e94\u4e91\u6a21\u578b\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u9047\u5230\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u8fb9\u7f18\u6a21\u578b\u7684\u8ba1\u7b97\u80fd\u529b\u6709\u9650\uff0c\u53ef\u80fd\u53ea\u652f\u6301\u524d\u5411\u4f20\u64ad\uff1b 2\uff09\u5728\u5ef6\u8fdf\u654f\u611f\u7684\u573a\u666f\u4e2d\uff0c\u4e91\u548c\u8fb9\u7f18\u8bbe\u5907\u4e4b\u95f4\u7684\u6570\u636e\u4f20\u8f93\u9884\u7b97\u662f\u6709\u9650\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u79cd\u4e91\u8fb9\u7f18\u5f39\u6027\u6a21\u578b\u81ea\u9002\u5e94\uff08CEMA\uff09\u8303\u5f0f\uff0c\u5176\u4e2d\u8fb9\u7f18\u6a21\u578b\u53ea\u9700\u8981\u6267\u884c\u524d\u5411\u4f20\u64ad\uff0c\u5e76\u4e14\u8fb9\u7f18\u6a21\u578b\u53ef\u4ee5\u5728\u7ebf\u81ea\u9002\u5e94\u3002\u5728\u6211\u4eec\u7684CEMA\u4e2d\uff0c\u4e3a\u4e86\u51cf\u8f7b\u901a\u4fe1\u8d1f\u62c5\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u6807\u51c6\u6765\u6392\u9664\u4e0d\u5fc5\u8981\u7684\u6837\u672c\u4e0a\u4f20\u5230\u4e91\u7aef\uff0c\u5373\u52a8\u6001\u4e0d\u53ef\u9760\u548c\u4f4e\u4fe1\u606f\u6837\u672c\u6392\u9664\u3002\u57fa\u4e8e\u4e0a\u4f20\u7684\u6837\u672c\uff0c\u6211\u4eec\u901a\u8fc7\u6837\u672c\u91cd\u64ad\u7b56\u7565\u4ece\u66f4\u5f3a\u7684\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u5230\u8fb9\u7f18\u6a21\u578b\u6765\u66f4\u65b0\u548c\u5206\u914d\u5f52\u4e00\u5316\u5c42\u7684\u4eff\u5c04\u53c2\u6570\u3002 ImageNet-C \u548c ImageNet-R \u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec CEMA \u7684\u6709\u6548\u6027\u3002|[2402.17316v1](http://arxiv.org/pdf/2402.17316v1)|null|\n", "2402.17171": "|**2024-02-27**|**LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment**|LiveHPS\uff1a\u81ea\u7531\u73af\u5883\u4e0b\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u7684\u573a\u666f\u7ea7\u4eba\u4f53\u59ff\u6001\u548c\u5f62\u72b6\u4f30\u8ba1|Yiming Ren, Xiao Han, Chengfeng Zhao, Jingya Wang, Lan Xu, Jingyi Yu, Yuexin Ma|For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications. In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level human pose and shape estimation without any limitation of light conditions and wearable devices. In particular, we design a distillation mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance. LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications. Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations. It consists of multi-modal and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs. Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach. We will release our code and dataset soon.|\u5bf9\u4e8e\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5927\u89c4\u6a21\u573a\u666f\uff0c3D \u4eba\u4f53\u5168\u5c40\u59ff\u52bf\u548c\u5f62\u72b6\u7684\u7ec6\u7c92\u5ea6\u5efa\u6a21\u5bf9\u4e8e\u573a\u666f\u7406\u89e3\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u53d7\u76ca\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 LiveHPS\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5355\u6fc0\u5149\u96f7\u8fbe\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u7528\u4e8e\u573a\u666f\u7ea7\u4eba\u4f53\u59ff\u52bf\u548c\u5f62\u72b6\u4f30\u8ba1\uff0c\u4e0d\u53d7\u4efb\u4f55\u5149\u7167\u6761\u4ef6\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u9650\u5236\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u84b8\u998f\u673a\u5236\u6765\u51cf\u8f7b\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u7684\u5206\u5e03\u53d8\u5316\u6548\u5e94\uff0c\u5e76\u5229\u7528\u8fde\u7eed\u5e27\u4e2d\u5b58\u5728\u7684\u65f6\u7a7a\u51e0\u4f55\u548c\u52a8\u6001\u4fe1\u606f\u6765\u89e3\u51b3\u906e\u6321\u548c\u566a\u58f0\u5e72\u6270\u3002 LiveHPS \u4ee5\u5176\u9ad8\u6548\u7684\u914d\u7f6e\u548c\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\uff0c\u975e\u5e38\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5de8\u5927\u7684\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u96c6\uff0c\u540d\u4e3a FreeMotion\uff0c\u5b83\u662f\u5728\u5404\u79cd\u573a\u666f\u4e2d\u6536\u96c6\u7684\u4e0d\u540c\u4eba\u4f53\u59ff\u52bf\u3001\u5f62\u72b6\u548c\u5e73\u79fb\u7684\u6570\u636e\u96c6\u3002\u5b83\u7531\u6765\u81ea\u6821\u51c6\u548c\u540c\u6b65 LiDAR\u3001\u6444\u50cf\u673a\u548c IMU \u7684\u591a\u6a21\u5f0f\u548c\u591a\u89c6\u56fe\u91c7\u96c6\u6570\u636e\u7ec4\u6210\u3002\u5bf9\u6211\u4eec\u7684\u65b0\u6570\u636e\u96c6\u548c\u5176\u4ed6\u516c\u5171\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684 SOTA \u6027\u80fd\u548c\u7a33\u5065\u6027\u3002\u6211\u4eec\u5c06\u5f88\u5feb\u53d1\u5e03\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002|[2402.17171v1](http://arxiv.org/pdf/2402.17171v1)|null|\n", "2402.17091": "|**2024-02-27**|**Structural Teacher-Student Normality Learning for Multi-Class Anomaly Detection and Localization**|\u7528\u4e8e\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u7ed3\u6784\u6027\u5e08\u751f\u6b63\u6001\u5b66\u4e60|Hanqiu Deng, Xingyu Li|Visual anomaly detection is a challenging open-set task aimed at identifying unknown anomalous patterns while modeling normal data. The knowledge distillation paradigm has shown remarkable performance in one-class anomaly detection by leveraging teacher-student network feature comparisons. However, extending this paradigm to multi-class anomaly detection introduces novel scalability challenges. In this study, we address the significant performance degradation observed in previous teacher-student models when applied to multi-class anomaly detection, which we identify as resulting from cross-class interference. To tackle this issue, we introduce a novel approach known as Structural Teacher-Student Normality Learning (SNL): (1) We propose spatial-channel distillation and intra-&inter-affinity distillation techniques to measure structural distance between the teacher and student networks. (2) We introduce a central residual aggregation module (CRAM) to encapsulate the normal representation space of the student network. We evaluate our proposed approach on two anomaly detection datasets, MVTecAD and VisA. Our method surpasses the state-of-the-art distillation-based algorithms by a significant margin of 3.9% and 1.5% on MVTecAD and 1.2% and 2.5% on VisA in the multi-class anomaly detection and localization tasks, respectively. Furthermore, our algorithm outperforms the current state-of-the-art unified models on both MVTecAD and VisA.|\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u5f00\u653e\u96c6\u4efb\u52a1\uff0c\u65e8\u5728\u5728\u5bf9\u6b63\u5e38\u6570\u636e\u8fdb\u884c\u5efa\u6a21\u7684\u540c\u65f6\u8bc6\u522b\u672a\u77e5\u7684\u5f02\u5e38\u6a21\u5f0f\u3002\u901a\u8fc7\u5229\u7528\u5e08\u751f\u7f51\u7edc\u7279\u5f81\u6bd4\u8f83\uff0c\u77e5\u8bc6\u84b8\u998f\u8303\u5f0f\u5728\u4e00\u7c7b\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5c06\u6b64\u8303\u4f8b\u6269\u5c55\u5230\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u4f1a\u5e26\u6765\u65b0\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u4ee5\u524d\u7684\u5e08\u751f\u6a21\u578b\u5728\u5e94\u7528\u4e8e\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u65f6\u89c2\u5bdf\u5230\u7684\u663e\u7740\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u6211\u4eec\u5c06\u5176\u786e\u5b9a\u4e3a\u8de8\u7c7b\u5e72\u6270\u9020\u6210\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u79f0\u4e3a\u7ed3\u6784\u6027\u5e08\u751f\u5e38\u6001\u5b66\u4e60\uff08SNL\uff09\u7684\u65b0\u65b9\u6cd5\uff1a\uff081\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u7a7a\u95f4\u901a\u9053\u84b8\u998f\u548c\u5185\u90e8\u548c\u5185\u90e8\u4eb2\u548c\u529b\u84b8\u998f\u6280\u672f\u6765\u6d4b\u91cf\u6559\u5e08\u548c\u5b66\u751f\u7f51\u7edc\u4e4b\u95f4\u7684\u7ed3\u6784\u8ddd\u79bb\u3002 \uff082\uff09\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u4e2d\u592e\u6b8b\u5dee\u805a\u5408\u6a21\u5757\uff08CRAM\uff09\u6765\u5c01\u88c5\u5b66\u751f\u7f51\u7edc\u7684\u6b63\u5e38\u8868\u793a\u7a7a\u95f4\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6 MVTecAD \u548c VisA \u4e0a\u8bc4\u4f30\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u5728\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 MVTecAD \u4e0a\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u84b8\u998f\u7684\u7b97\u6cd5\uff0c\u5206\u522b\u8d85\u8fc7\u4e86 3.9% \u548c 1.5%\uff0c\u5728 VisA \u4e0a\u8d85\u8fc7\u4e86 1.2% \u548c 2.5%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7b97\u6cd5\u5728 MVTecAD \u548c VisA \u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7edf\u4e00\u6a21\u578b\u3002|[2402.17091v1](http://arxiv.org/pdf/2402.17091v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.17758": "|**2024-02-27**|**ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living**|ADL4D\uff1a\u4e3a\u65e5\u5e38\u751f\u6d3b\u7684 4D \u6d3b\u52a8\u5efa\u7acb\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u6570\u636e\u96c6|Marsil Zakour, Partha Pratim Nath, Ludwig Lohmer, Emre Faik G\u00f6k\u00e7e, Martin Piccolrovazzi, Constantin Patsch, Yuankai Wu, Rahul Chaudhari, Eckehard Steinbach|Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, pre- vious actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject inter- acting with one object only. This restricts the generalization of learning-based HOI methods trained on those datasets. We introduce ADL4D, a dataset of up to two subjects inter- acting with different sets of objects performing Activities of Daily Living (ADL) like breakfast or lunch preparation ac- tivities. The transition between multiple objects to complete a certain task over time introduces a unique context lacking in existing datasets. Our dataset consists of 75 sequences with a total of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action annotations. We develop an automatic system for multi-view multi-hand 3D pose an- notation capable of tracking hand poses over time. We inte- grate and test it against publicly available datasets. Finally, we evaluate our dataset on the tasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).|\u624b-\u7269\u4f53\u4ea4\u4e92\uff08HOI\uff09\u4ee5\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\u4e3a\u6761\u4ef6\uff0c\u4f8b\u5982\u5468\u56f4\u7684\u7269\u4f53\u3001\u5148\u524d\u7684\u52a8\u4f5c\u548c\u672a\u6765\u7684\u610f\u56fe\uff08\u4f8b\u5982\uff0c\u6293\u53d6\u548c\u5207\u6362\u52a8\u4f5c\u6839\u636e\u7269\u4f53\u7684\u63a5\u8fd1\u5ea6\u548c\u8f68\u8ff9\u969c\u788d\u800c\u6709\u5f88\u5927\u5dee\u5f02\uff09\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684 4D HOI\uff08\u968f\u65f6\u95f4\u53d8\u5316\u7684 3D HOI\uff09\u6570\u636e\u96c6\u4ec5\u9650\u4e8e\u4e00\u4e2a\u4e3b\u4f53\u4e0e\u4e00\u4e2a\u5bf9\u8c61\u4ea4\u4e92\u3002\u8fd9\u9650\u5236\u4e86\u5728\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u57fa\u4e8e\u5b66\u4e60\u7684 HOI \u65b9\u6cd5\u7684\u6cdb\u5316\u3002\u6211\u4eec\u5f15\u5165\u4e86 ADL4D\uff0c\u8fd9\u662f\u4e00\u4e2a\u6700\u591a\u5305\u542b\u4e24\u4e2a\u53d7\u8bd5\u8005\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u53d7\u8bd5\u8005\u4e0e\u6267\u884c\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\uff08ADL\uff09\uff08\u4f8b\u5982\u65e9\u9910\u6216\u5348\u9910\u51c6\u5907\u6d3b\u52a8\uff09\u7684\u4e0d\u540c\u5bf9\u8c61\u7ec4\u8fdb\u884c\u4ea4\u4e92\u3002\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u591a\u4e2a\u5bf9\u8c61\u4e4b\u95f4\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1\u7684\u8f6c\u6362\u5f15\u5165\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u7684\u72ec\u7279\u4e0a\u4e0b\u6587\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u7531 75 \u4e2a\u5e8f\u5217\u7ec4\u6210\uff0c\u603b\u5171 110 \u4e07\u4e2a RGB-D \u5e27\u3001\u624b\u90e8\u548c\u7269\u4f53\u59ff\u52bf\u4ee5\u53ca\u6bcf\u53ea\u624b\u7684\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u6ce8\u91ca\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u89c6\u56fe\u591a\u624b 3D \u59ff\u52bf\u6ce8\u91ca\u7684\u81ea\u52a8\u7cfb\u7edf\uff0c\u80fd\u591f\u968f\u65f6\u95f4\u8ddf\u8e2a\u624b\u90e8\u59ff\u52bf\u3002\u6211\u4eec\u9488\u5bf9\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u5bf9\u5176\u8fdb\u884c\u96c6\u6210\u548c\u6d4b\u8bd5\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u624b\u90e8\u7f51\u683c\u6062\u590d\uff08HMR\uff09\u548c\u624b\u90e8\u52a8\u4f5c\u5206\u5272\uff08HAS\uff09\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u6570\u636e\u96c6\u3002|[2402.17758v1](http://arxiv.org/pdf/2402.17758v1)|null|\n", "2402.17726": "|**2024-02-27**|**VRP-SAM: SAM with Visual Reference Prompt**|VRP-SAM\uff1a\u5e26\u6709\u89c6\u89c9\u53c2\u8003\u63d0\u793a\u7684 SAM|Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, Gang Zhang, Errui Ding, Jingdong Wang, Zechao Li|In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image. It is note that the VRP encoder can support a variety of annotation formats for reference images, including \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}. VRP-SAM achieves a breakthrough within the SAM framework by extending its versatility and applicability while preserving SAM's inherent strengths, thus enhancing user-friendliness. To enhance the generalization ability of VRP-SAM, the VRP encoder adopts a meta-learning strategy. To validate the effectiveness of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM demonstrates strong generalization capabilities, allowing it to perform segmentation of unseen objects and enabling cross-domain segmentation.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u53c2\u8003\u63d0\u793a\uff08VRP\uff09\u7f16\u7801\u5668\uff0c\u4f7f\u5206\u6bb5\u4efb\u4f55\u6a21\u578b\uff08SAM\uff09\u80fd\u591f\u5229\u7528\u5e26\u6ce8\u91ca\u7684\u53c2\u8003\u56fe\u50cf\u4f5c\u4e3a\u5206\u5272\u63d0\u793a\uff0c\u4ece\u800c\u521b\u5efa VRP-SAM \u6a21\u578b\u3002\u672c\u8d28\u4e0a\uff0cVRP-SAM\u53ef\u4ee5\u5229\u7528\u5e26\u6ce8\u91ca\u7684\u53c2\u8003\u56fe\u50cf\u6765\u7406\u89e3\u7279\u5b9a\u5bf9\u8c61\u5e76\u5bf9\u76ee\u6807\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u5bf9\u8c61\u8fdb\u884c\u5206\u5272\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cVRP\u7f16\u7801\u5668\u53ef\u4ee5\u652f\u6301\u53c2\u8003\u56fe\u50cf\u7684\u591a\u79cd\u6ce8\u91ca\u683c\u5f0f\uff0c\u5305\u62ec\\textbf{point}\u3001\\textbf{box}\u3001\\textbf{scribble}\u548c\\textbf{mask}\u3002 VRP-SAM\u5728SAM\u6846\u67b6\u5185\u5b9e\u73b0\u4e86\u7a81\u7834\uff0c\u6269\u5c55\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u9002\u7528\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86SAM\u7684\u56fa\u6709\u4f18\u52bf\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u7528\u6237\u53cb\u597d\u6027\u3002\u4e3a\u4e86\u589e\u5f3aVRP-SAM\u7684\u6cdb\u5316\u80fd\u529b\uff0cVRP\u7f16\u7801\u5668\u91c7\u7528\u5143\u5b66\u4e60\u7b56\u7565\u3002\u4e3a\u4e86\u9a8c\u8bc1 VRP-SAM \u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5bf9 Pascal \u548c COCO \u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cVRP-SAM \u5728\u89c6\u89c9\u53c2\u8003\u5206\u5272\u65b9\u9762\u4ee5\u6700\u5c11\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0cVRP-SAM \u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5bf9\u4e0d\u53ef\u89c1\u7684\u5bf9\u8c61\u8fdb\u884c\u5206\u5272\u5e76\u5b9e\u73b0\u8de8\u57df\u5206\u5272\u3002|[2402.17726v1](http://arxiv.org/pdf/2402.17726v1)|null|\n", "2402.17725": "|**2024-02-27**|**MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation**|MedContext\uff1a\u5b66\u4e60\u4e0a\u4e0b\u6587\u7ebf\u7d22\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u4f53\u79ef\u533b\u5b66\u5206\u5272|Hanan Gani, Muzammal Naseer, Fahad Khan, Salman Khan|Volumetric medical segmentation is a critical component of 3D medical image analysis that delineates different semantic regions. Deep neural networks have significantly improved volumetric medical segmentation, but they generally require large-scale annotated data to achieve better performance, which can be expensive and prohibitive to obtain. To address this limitation, existing works typically perform transfer learning or design dedicated pretraining-finetuning stages to learn representative features. However, the mismatch between the source and target domain can make it challenging to learn optimal representation for volumetric data, while the multi-stage training demands higher compute as well as careful selection of stage-specific design choices. In contrast, we propose a universal training framework called MedContext that is architecture-agnostic and can be incorporated into any existing training framework for 3D medical segmentation. Our approach effectively learns self supervised contextual cues jointly with the supervised voxel segmentation task without requiring large-scale annotated volumetric medical data or dedicated pretraining-finetuning stages. The proposed approach induces contextual knowledge in the network by learning to reconstruct the missing organ or parts of an organ in the output segmentation space. The effectiveness of MedContext is validated across multiple 3D medical datasets and four state-of-the-art model architectures. Our approach demonstrates consistent gains in segmentation performance across datasets and different architectures even in few-shot data scenarios. Our code and pretrained models are available at https://github.com/hananshafi/MedContext|\u4f53\u79ef\u533b\u5b66\u5206\u5272\u662f 3D \u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u53ef\u63cf\u7ed8\u4e0d\u540c\u7684\u8bed\u4e49\u533a\u57df\u3002\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u663e\u7740\u6539\u8fdb\u4e86\u4f53\u79ef\u533b\u5b66\u5206\u5272\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u9700\u8981\u5927\u89c4\u6a21\u6ce8\u91ca\u6570\u636e\u624d\u80fd\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\uff0c\u800c\u83b7\u5f97\u8fd9\u4e9b\u6570\u636e\u53ef\u80fd\u6210\u672c\u9ad8\u6602\u4e14\u4ee4\u4eba\u671b\u800c\u5374\u6b65\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u73b0\u6709\u7684\u5de5\u4f5c\u901a\u5e38\u6267\u884c\u8fc1\u79fb\u5b66\u4e60\u6216\u8bbe\u8ba1\u4e13\u7528\u7684\u9884\u8bad\u7ec3\u5fae\u8c03\u9636\u6bb5\u6765\u5b66\u4e60\u4ee3\u8868\u6027\u7279\u5f81\u3002\u7136\u800c\uff0c\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u4f7f\u5f97\u5b66\u4e60\u4f53\u79ef\u6570\u636e\u7684\u6700\u4f73\u8868\u793a\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\uff0c\u800c\u591a\u9636\u6bb5\u8bad\u7ec3\u9700\u8981\u66f4\u9ad8\u7684\u8ba1\u7b97\u80fd\u529b\u4ee5\u53ca\u4ed4\u7ec6\u9009\u62e9\u7279\u5b9a\u4e8e\u9636\u6bb5\u7684\u8bbe\u8ba1\u9009\u62e9\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a MedContext \u7684\u901a\u7528\u8bad\u7ec3\u6846\u67b6\uff0c\u5b83\u4e0e\u67b6\u6784\u65e0\u5173\uff0c\u53ef\u4ee5\u5408\u5e76\u5230\u4efb\u4f55\u73b0\u6709\u7684 3D \u533b\u5b66\u5206\u5272\u8bad\u7ec3\u6846\u67b6\u4e2d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u5b66\u4e60\u81ea\u6211\u76d1\u7763\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u4e0e\u76d1\u7763\u4f53\u7d20\u5206\u5272\u4efb\u52a1\uff0c\u800c\u4e0d\u9700\u8981\u5927\u89c4\u6a21\u6ce8\u91ca\u4f53\u79ef\u533b\u5b66\u6570\u636e\u6216\u4e13\u7528\u7684\u9884\u8bad\u7ec3\u5fae\u8c03\u9636\u6bb5\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u5728\u8f93\u51fa\u5206\u5272\u7a7a\u95f4\u4e2d\u91cd\u5efa\u7f3a\u5931\u7684\u5668\u5b98\u6216\u5668\u5b98\u7684\u4e00\u90e8\u5206\u6765\u5f15\u5165\u7f51\u7edc\u4e2d\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u3002 MedContext \u7684\u6709\u6548\u6027\u5728\u591a\u4e2a 3D \u533b\u5b66\u6570\u636e\u96c6\u548c\u56db\u79cd\u6700\u5148\u8fdb\u7684\u6a21\u578b\u67b6\u6784\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u8de8\u6570\u636e\u96c6\u548c\u4e0d\u540c\u67b6\u6784\u7684\u5206\u5272\u6027\u80fd\u7684\u4e00\u81f4\u589e\u76ca\uff0c\u5373\u4f7f\u5728\u5c11\u91cf\u6570\u636e\u573a\u666f\u4e2d\u4e5f\u662f\u5982\u6b64\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728 https://github.com/hananshafi/MedContext \u83b7\u53d6|[2402.17725v1](http://arxiv.org/pdf/2402.17725v1)|null|\n", "2402.17672": "|**2024-02-27**|**SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification**|SDF2Net\uff1a\u7528\u4e8e PolSAR \u56fe\u50cf\u5206\u7c7b\u7684\u6d45\u5c42\u5230\u6df1\u5c42\u7279\u5f81\u878d\u5408\u7f51\u7edc|Mohammed Q. Alkhatib, M. Sami Zitouni, Mina Al-Saad, Nour Aburaed, Hussain Al-Ahmad|Polarimetric synthetic aperture radar (PolSAR) images encompass valuable information that can facilitate extensive land cover interpretation and generate diverse output products. Extracting meaningful features from PolSAR data poses challenges distinct from those encountered in optical imagery. Deep learning (DL) methods offer effective solutions for overcoming these challenges in PolSAR feature extraction. Convolutional neural networks (CNNs) play a crucial role in capturing PolSAR image characteristics by leveraging kernel capabilities to consider local information and the complex-valued nature of PolSAR data. In this study, a novel three-branch fusion of complex-valued CNN, named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for PolSAR image classification. To validate the performance of the proposed method, classification results are compared against multiple state-of-the-art approaches using the airborne synthetic aperture radar (AIRSAR) datasets of Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. The results indicate that the proposed approach demonstrates improvements in overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a 0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland data underscore the effectiveness of the SDF2Net model, revealing a promising overall accuracy of 96.01% even with only a 1% sampling ratio.|\u504f\u632f\u5408\u6210\u5b54\u5f84\u96f7\u8fbe (PolSAR) \u56fe\u50cf\u5305\u542b\u6709\u4ef7\u503c\u7684\u4fe1\u606f\uff0c\u53ef\u4ee5\u4fc3\u8fdb\u5e7f\u6cdb\u7684\u571f\u5730\u8986\u76d6\u89e3\u91ca\u5e76\u751f\u6210\u4e0d\u540c\u7684\u8f93\u51fa\u4ea7\u54c1\u3002\u4ece PolSAR \u6570\u636e\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u7279\u5f81\u6240\u9762\u4e34\u7684\u6311\u6218\u4e0e\u5149\u5b66\u56fe\u50cf\u4e2d\u9047\u5230\u7684\u6311\u6218\u4e0d\u540c\u3002\u6df1\u5ea6\u5b66\u4e60 (DL) \u65b9\u6cd5\u4e3a\u514b\u670d PolSAR \u7279\u5f81\u63d0\u53d6\u4e2d\u7684\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u5229\u7528\u5185\u6838\u529f\u80fd\u8003\u8651\u5c40\u90e8\u4fe1\u606f\u548c PolSAR \u6570\u636e\u7684\u590d\u503c\u6027\u8d28\uff0c\u5728\u6355\u83b7 PolSAR \u56fe\u50cf\u7279\u5f81\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e09\u5206\u652f\u590d\u503c CNN \u878d\u5408\uff0c\u79f0\u4e3a\u6d45\u5230\u6df1\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08SDF2Net\uff09\uff0c\u7528\u4e8e PolSAR \u56fe\u50cf\u5206\u7c7b\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4f7f\u7528\u5f17\u83b1\u798f\u5170\u548c\u65e7\u91d1\u5c71\u7684\u673a\u8f7d\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08AIRSAR\uff09\u6570\u636e\u96c6\u4ee5\u53ca ESAR Oberpfaffenhofen \u6570\u636e\u96c6\u5c06\u5206\u7c7b\u7ed3\u679c\u4e0e\u591a\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u793a\u4e86\u6574\u4f53\u7cbe\u5ea6\u7684\u63d0\u9ad8\uff0cAIRSAR \u6570\u636e\u96c6\u63d0\u9ad8\u4e86 1.3% \u548c 0.8%\uff0cESAR \u6570\u636e\u96c6\u63d0\u9ad8\u4e86 0.5%\u3002\u5bf9 Flevoland \u6570\u636e\u8fdb\u884c\u7684\u5206\u6790\u5f3a\u8c03\u4e86 SDF2Net \u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5373\u4f7f\u5728\u53ea\u6709 1% \u7684\u91c7\u6837\u7387\u7684\u60c5\u51b5\u4e0b\uff0c\u6574\u4f53\u51c6\u786e\u7387\u4e5f\u9ad8\u8fbe 96.01%\u3002|[2402.17672v1](http://arxiv.org/pdf/2402.17672v1)|null|\n", "2402.17653": "|**2024-02-27**|**Mitigating Distributional Shift in Semantic Segmentation via Uncertainty Estimation from Unlabelled Data**|\u901a\u8fc7\u672a\u6807\u8bb0\u6570\u636e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u51cf\u8f7b\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5206\u5e03\u53d8\u5316|David S. W. Williams, Daniele De Martini, Matthew Gadd, Paul Newman|Knowing when a trained segmentation model is encountering data that is different to its training data is important. Understanding and mitigating the effects of this play an important part in their application from a performance and assurance perspective - this being a safety concern in applications such as autonomous vehicles (AVs). This work presents a segmentation network that can detect errors caused by challenging test domains without any additional annotation in a single forward pass. As annotation costs limit the diversity of labelled datasets, we use easy-to-obtain, uncurated and unlabelled data to learn to perform uncertainty estimation by selectively enforcing consistency over data augmentation. To this end, a novel segmentation benchmark based on the SAX Dataset is used, which includes labelled test data spanning three autonomous-driving domains, ranging in appearance from dense urban to off-road. The proposed method, named Gamma-SSL, consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark - by up to 10.7% in area under the receiver operating characteristic (ROC) curve and 19.2% in area under the precision-recall (PR) curve in the most challenging of the three scenarios.|\u4e86\u89e3\u7ecf\u8fc7\u8bad\u7ec3\u7684\u5206\u5272\u6a21\u578b\u4f55\u65f6\u9047\u5230\u4e0e\u5176\u8bad\u7ec3\u6570\u636e\u4e0d\u540c\u7684\u6570\u636e\u975e\u5e38\u91cd\u8981\u3002\u4ece\u6027\u80fd\u548c\u4fdd\u8bc1\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u7406\u89e3\u548c\u51cf\u8f7b\u5176\u5f71\u54cd\u5728\u5176\u5e94\u7528\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\u2014\u2014\u8fd9\u662f\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66 (AV) \u7b49\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u5b89\u5168\u95ee\u9898\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5272\u7f51\u7edc\uff0c\u53ef\u4ee5\u68c0\u6d4b\u7531\u5177\u6709\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u57df\u5f15\u8d77\u7684\u9519\u8bef\uff0c\u800c\u65e0\u9700\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u8fdb\u884c\u4efb\u4f55\u9644\u52a0\u6ce8\u91ca\u3002\u7531\u4e8e\u6ce8\u91ca\u6210\u672c\u9650\u5236\u4e86\u6807\u8bb0\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\uff0c\u56e0\u6b64\u6211\u4eec\u4f7f\u7528\u6613\u4e8e\u83b7\u53d6\u3001\u672a\u7ecf\u6574\u7406\u548c\u672a\u6807\u8bb0\u7684\u6570\u636e\u6765\u5b66\u4e60\u901a\u8fc7\u6709\u9009\u62e9\u5730\u5f3a\u5236\u6570\u636e\u589e\u5f3a\u7684\u4e00\u81f4\u6027\u6765\u6267\u884c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u4e3a\u6b64\uff0c\u4f7f\u7528\u4e86\u57fa\u4e8e SAX \u6570\u636e\u96c6\u7684\u65b0\u9896\u5206\u5272\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u62ec\u8de8\u8d8a\u4e09\u4e2a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u6807\u8bb0\u6d4b\u8bd5\u6570\u636e\uff0c\u8303\u56f4\u4ece\u5bc6\u96c6\u7684\u57ce\u5e02\u5230\u8d8a\u91ce\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u540d\u4e3a Gamma-SSL\uff0c\u5728\u8fd9\u4e00\u56f0\u96be\u7684\u57fa\u51c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u5206\u5e03\u5916 (OoD) \u6280\u672f - \u63a5\u6536\u8005\u64cd\u4f5c\u7279\u6027 (ROC) \u66f2\u7ebf\u4e0b\u9762\u79ef\u9ad8\u8fbe 10.7%\uff0c\u800c\u4e0b\u9762\u79ef\u9ad8\u8fbe 19.2%\u3002\u4e09\u79cd\u573a\u666f\u4e2d\u6700\u5177\u6311\u6218\u6027\u7684\u7cbe\u786e\u53ec\u56de\uff08PR\uff09\u66f2\u7ebf\u3002|[2402.17653v1](http://arxiv.org/pdf/2402.17653v1)|null|\n", "2402.17622": "|**2024-02-27**|**Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image Modeling**|Masked Gamma-SSL\uff1a\u901a\u8fc7\u63a9\u6a21\u56fe\u50cf\u5efa\u6a21\u5b66\u4e60\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1|David S. W. Williams, Matthew Gadd, Paul Newman, Daniele De Martini|This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass. We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques. For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network's limitations at run time and act accordingly. To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains. The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark.|\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u5206\u5272\u7f51\u7edc\uff0c\u53ef\u4ee5\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u6211\u4eec\u901a\u8fc7\u63a9\u6a21\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u65b9\u6cd5\u5229\u7528\u57fa\u7840\u6a21\u578b\u548c\u672a\u6807\u8bb0\u6570\u636e\u96c6\u7684\u4e00\u822c\u8868\u793a\uff0c\u8be5\u65b9\u6cd5\u5bf9\u589e\u5f3a\u8d85\u53c2\u6570\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u6bd4\u4ee5\u524d\u7684\u6280\u672f\u66f4\u7b80\u5355\u3002\u5bf9\u4e8e\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u4f7f\u7528\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u504f\u5dee\u53ef\u80fd\u4f1a\u5bfc\u81f4\u9519\u8bef\uff1b\u56e0\u6b64\uff0c\u4e86\u89e3\u7f51\u7edc\u5728\u8fd0\u884c\u65f6\u7684\u9650\u5236\u5e76\u91c7\u53d6\u76f8\u5e94\u7684\u884c\u52a8\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5728\u591a\u4e2a\u6d4b\u8bd5\u9886\u57df\u6d4b\u8bd5\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5305\u62ec SAX \u5206\u6bb5\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u62ec\u6765\u81ea\u5bc6\u96c6\u57ce\u5e02\u3001\u519c\u6751\u548c\u8d8a\u91ce\u9a7e\u9a76\u9886\u57df\u7684\u6807\u8bb0\u6d4b\u8bd5\u6570\u636e\u3002\u5728\u8fd9\u4e2a\u56f0\u96be\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u5206\u5e03\u5916\uff08OoD\uff09\u6280\u672f\u3002|[2402.17622v1](http://arxiv.org/pdf/2402.17622v1)|null|\n", "2402.17614": "|**2024-02-27**|**Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation**|\u5148\u9002\u5e94\u540e\u6bd4\u8f83\uff1a\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\u7684\u65b0\u89c6\u89d2|Jonas Herzog|Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases. To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged. Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains. Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network. We show test-time task-adaption is the key for successful CD-FSS instead. Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone. To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers. Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task.|\u5f53\u9762\u5bf9\u6765\u81ea\u4e0e\u8bad\u7ec3\u57df\u4e0d\u540c\u7684\u57df\u7684\u56fe\u50cf\u65f6\uff0c\u5c11\u955c\u5934\u5206\u5272\u6027\u80fd\u4f1a\u5927\u5e45\u4e0b\u964d\uff0c\u4ece\u800c\u6709\u6548\u5730\u9650\u5236\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u7528\u4f8b\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6700\u8fd1\u51fa\u73b0\u4e86\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\uff08CD-FSS\uff09\u3002\u89e3\u51b3\u6b64\u4efb\u52a1\u7684\u5de5\u4f5c\u4e3b\u8981\u5c1d\u8bd5\u4ee5\u8de8\u57df\u6cdb\u5316\u7684\u65b9\u5f0f\u5b66\u4e60\u6e90\u57df\u4e0a\u7684\u5206\u6bb5\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6211\u4eec\u53ef\u4ee5\u8d85\u8d8a\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u540c\u65f6\u6d88\u9664\u8bad\u7ec3\u9636\u6bb5\u5e76\u5220\u9664\u5b83\u4eec\u7684\u4e3b\u8981\u5206\u5272\u7f51\u7edc\u3002\u6211\u4eec\u8bc1\u660e\u6d4b\u8bd5\u65f6\u4efb\u52a1\u9002\u5e94\u662f CD-FSS \u6210\u529f\u7684\u5173\u952e\u3002\u4efb\u52a1\u9002\u5e94\u662f\u901a\u8fc7\u5c06\u5c0f\u578b\u7f51\u7edc\u9644\u52a0\u5230\u4f20\u7edf\u5206\u7c7b\u9884\u8bad\u7ec3\u4e3b\u5e72\u7684\u7279\u5f81\u91d1\u5b57\u5854\u6765\u5b9e\u73b0\u7684\u3002\u4e3a\u4e86\u907f\u514d\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u8fc7\u5ea6\u62df\u5408\u5c11\u6570\u6807\u8bb0\u6837\u672c\uff0c\u8f93\u5165\u56fe\u50cf\u7684\u589e\u5f3a\u89c6\u56fe\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u53ef\u4ee5\u5728\u5b66\u4e60\u9644\u52a0\u5c42\u7684\u53c2\u6570\u65f6\u8d77\u5230\u6307\u5bfc\u4f5c\u7528\u3002\u5c3d\u7ba1\u6211\u4eec\u81ea\u6211\u9650\u5236\u5728\u6d4b\u8bd5\u65f6\u4e0d\u4f7f\u7528\u9664\u5c11\u6570\u6807\u8bb0\u6837\u672c\u4e4b\u5916\u7684\u4efb\u4f55\u56fe\u50cf\uff0c\u4f46\u6211\u4eec\u5728 CD-FSS \u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8fd9\u8bc1\u660e\u9700\u8981\u91cd\u65b0\u8003\u8651\u8be5\u4efb\u52a1\u7684\u65b9\u6cd5\u3002|[2402.17614v1](http://arxiv.org/pdf/2402.17614v1)|null|\n", "2402.17611": "|**2024-02-27**|**A Large-scale Evaluation of Pretraining Paradigms for the Detection of Defects in Electroluminescence Solar Cell Images**|\u7528\u4e8e\u68c0\u6d4b\u7535\u81f4\u53d1\u5149\u592a\u9633\u80fd\u7535\u6c60\u56fe\u50cf\u7f3a\u9677\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\u7684\u5927\u89c4\u6a21\u8bc4\u4f30|David Torpey, Lawrence Pratt, Richard Klein|Pretraining has been shown to improve performance in many domains, including semantic segmentation, especially in domains with limited labelled data. In this work, we perform a large-scale evaluation and benchmarking of various pretraining methods for Solar Cell Defect Detection (SCDD) in electroluminescence images, a field with limited labelled datasets. We cover supervised training with semantic segmentation, semi-supervised learning, and two self-supervised techniques. We also experiment with both in-distribution and out-of-distribution (OOD) pretraining and observe how this affects downstream performance. The results suggest that supervised training on a large OOD dataset (COCO), self-supervised pretraining on a large OOD dataset (ImageNet), and semi-supervised pretraining (CCT) all yield statistically equivalent performance for mean Intersection over Union (mIoU). We achieve a new state-of-the-art for SCDD and demonstrate that certain pretraining schemes result in superior performance on underrepresented classes. Additionally, we provide a large-scale unlabelled EL image dataset of $22000$ images, and a $642$-image labelled semantic segmentation EL dataset, for further research in developing self- and semi-supervised training techniques in this domain.|\u9884\u8bad\u7ec3\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u63d0\u9ad8\u8bb8\u591a\u9886\u57df\u7684\u6027\u80fd\uff0c\u5305\u62ec\u8bed\u4e49\u5206\u5272\uff0c\u7279\u522b\u662f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u9886\u57df\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5bf9\u7535\u81f4\u53d1\u5149\u56fe\u50cf\u4e2d\u592a\u9633\u80fd\u7535\u6c60\u7f3a\u9677\u68c0\u6d4b\uff08SCDD\uff09\u7684\u5404\u79cd\u9884\u8bad\u7ec3\u65b9\u6cd5\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u6807\u8bb0\u6570\u636e\u96c6\u6709\u9650\u7684\u9886\u57df\u3002\u6211\u4eec\u6db5\u76d6\u4e86\u8bed\u4e49\u5206\u5272\u7684\u76d1\u7763\u8bad\u7ec3\u3001\u534a\u76d1\u7763\u5b66\u4e60\u548c\u4e24\u79cd\u81ea\u76d1\u7763\u6280\u672f\u3002\u6211\u4eec\u8fd8\u5c1d\u8bd5\u4e86\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916 (OOD) \u9884\u8bad\u7ec3\uff0c\u5e76\u89c2\u5bdf\u8fd9\u5982\u4f55\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b OOD \u6570\u636e\u96c6 (COCO) \u4e0a\u7684\u76d1\u7763\u8bad\u7ec3\u3001\u5927\u578b OOD \u6570\u636e\u96c6 (ImageNet) \u4e0a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u534a\u76d1\u7763\u9884\u8bad\u7ec3 (CCT) \u90fd\u5728\u5e73\u5747\u4ea4\u5e76\u96c6 (mIoU) \u65b9\u9762\u4ea7\u751f\u4e86\u7edf\u8ba1\u4e0a\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6211\u4eec\u5b9e\u73b0\u4e86 SCDD \u7684\u6700\u65b0\u6280\u672f\uff0c\u5e76\u8bc1\u660e\u67d0\u4e9b\u9884\u8bad\u7ec3\u65b9\u6848\u53ef\u4ee5\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u522b\u4e0a\u5e26\u6765\u4f18\u5f02\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b 22000 \u7f8e\u5143\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u672a\u6807\u8bb0 EL \u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5305\u542b 642 \u7f8e\u5143\u56fe\u50cf\u7684\u6807\u8bb0\u8bed\u4e49\u5206\u5272 EL \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u5f00\u53d1\u8be5\u9886\u57df\u7684\u81ea\u76d1\u7763\u548c\u534a\u76d1\u7763\u8bad\u7ec3\u6280\u672f\u3002|[2402.17611v1](http://arxiv.org/pdf/2402.17611v1)|null|\n", "2402.17555": "|**2024-02-27**|**Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label**|Scribble \u9690\u85cf\u7c7b\uff1a\u5229\u7528\u5176\u7c7b\u6807\u7b7e\u4fc3\u8fdb\u57fa\u4e8e Scribble \u7684\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272|Xinliang Zhang, Lei Zhu, Hangzhou He, Lujia Jin, Yanye Lu|Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.|\u4f7f\u7528\u7a00\u758f\u6d82\u9e26\u76d1\u7763\u7684\u57fa\u4e8e\u6d82\u9e26\u7684\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6b63\u5728\u83b7\u5f97\u5173\u6ce8\uff0c\u56e0\u4e3a\u4e0e\u5b8c\u5168\u6ce8\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u76f8\u6bd4\uff0c\u5b83\u964d\u4f4e\u4e86\u6ce8\u91ca\u6210\u672c\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u5c06\u6807\u8bb0\u50cf\u7d20\u6269\u6563\u5230\u5177\u6709\u5c40\u90e8\u76d1\u7763\u7ebf\u7d22\u7684\u672a\u6807\u8bb0\u50cf\u7d20\u6765\u751f\u6210\u4f2a\u6807\u7b7e\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u6269\u6563\u8fc7\u7a0b\u672a\u80fd\u5229\u7528\u5168\u5c40\u8bed\u4e49\u548c\u7279\u5b9a\u4e8e\u7c7b\u7684\u7ebf\u7d22\uff0c\u800c\u8fd9\u5bf9\u4e8e\u8bed\u4e49\u5206\u5272\u5f88\u91cd\u8981\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u9a71\u52a8\u7684\u6d82\u9e26\u63a8\u5e7f\u7f51\u7edc\uff0c\u5b83\u5229\u7528\u56fe\u50cf\u7ea7\u7c7b\u548c\u5168\u5c40\u8bed\u4e49\u63d0\u4f9b\u7684\u6d82\u9e26\u6ce8\u91ca\u548c\u4f2a\u6807\u7b7e\u8fdb\u884c\u76d1\u7763\u3002\u76f4\u63a5\u91c7\u7528\u4f2a\u6807\u7b7e\u53ef\u80fd\u4f1a\u8bef\u5bfc\u5206\u5272\u6a21\u578b\uff0c\u56e0\u6b64\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b9a\u4f4d\u6821\u6b63\u6a21\u5757\u6765\u7ea0\u6b63\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u524d\u666f\u8868\u793a\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u7ed3\u5408\u4e24\u79cd\u76d1\u7763\u7684\u4f18\u70b9\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u8ddd\u79bb\u71b5\u635f\u5931\u6765\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\uff0c\u5b83\u6839\u636e\u6d82\u9e26\u548c\u4f2a\u6807\u7b7e\u8fb9\u754c\u786e\u5b9a\u7684\u53ef\u9760\u533a\u57df\u6765\u8c03\u6574\u6bcf\u50cf\u7d20\u7684\u7f6e\u4fe1\u6743\u91cd\u3002\u5728\u5177\u6709\u4e0d\u540c\u8d28\u91cf\u7684\u6d82\u9e26\u6ce8\u91ca\u7684 ScribbleSup \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u4f18\u4e8e\u4e4b\u524d\u7684\u6240\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u9c81\u68d2\u6027\u3002\u4ee3\u7801\u4f4d\u4e8e https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network \u3002|[2402.17555v1](http://arxiv.org/pdf/2402.17555v1)|null|\n", "2402.17514": "|**2024-02-27**|**Robust Unsupervised Crowd Counting and Localization with Adaptive Resolution SAM**|\u5177\u6709\u81ea\u9002\u5e94\u5206\u8fa8\u7387 SAM \u7684\u7a33\u5065\u65e0\u76d1\u7763\u4eba\u7fa4\u8ba1\u6570\u548c\u5b9a\u4f4d|Jia Wan, Qiangqiang Wu, Wei Lin, Antoni B. Chan|The existing crowd counting models require extensive training data, which is time-consuming to annotate. To tackle this issue, we propose a simple yet effective crowd counting method by utilizing the Segment-Everything-Everywhere Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to generate pseudo-labels for training crowd counting models. However, our initial investigation reveals that SEEM's performance in dense crowd scenes is limited, primarily due to the omission of many persons in high-density areas. To overcome this limitation, we propose an adaptive resolution SEEM to handle the scale variations, occlusions, and overlapping of people within crowd scenes. Alongside this, we introduce a robust localization method, based on Gaussian Mixture Models, for predicting the head positions in the predicted people masks. Given the mask and point pseudo-labels, we propose a robust loss function, which is designed to exclude uncertain regions based on SEEM's predictions, thereby enhancing the training process of the counting networks. Finally, we propose an iterative method for generating pseudo-labels. This method aims at improving the quality of the segmentation masks by identifying more tiny persons in high-density regions, which are often missed in the first pseudo-labeling stage. Overall, our proposed method achieves the best unsupervised performance in crowd counting, while also being comparable results to some supervised methods. This makes it a highly effective and versatile tool for crowd counting, especially in situations where labeled data is not available.|\u73b0\u6709\u7684\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6ce8\u91ca\u8d77\u6765\u975e\u5e38\u8017\u65f6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u4eba\u7fa4\u8ba1\u6570\u65b9\u6cd5\uff0c\u5229\u7528\u5206\u6bb5\u4e00\u5207\u65e0\u5904\u4e0d\u5728\u6a21\u578b\uff08SEEM\uff09\uff08\u5206\u6bb5\u4efb\u4f55\u6a21\u578b\uff08SAM\uff09\u7684\u6539\u7f16\u7248\uff09\u6765\u751f\u6210\u7528\u4e8e\u8bad\u7ec3\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u7684\u4f2a\u6807\u7b7e\u3002\u7136\u800c\uff0c\u6211\u4eec\u7684\u521d\u6b65\u8c03\u67e5\u663e\u793a\uff0cSEEM \u5728\u5bc6\u96c6\u4eba\u7fa4\u573a\u666f\u4e2d\u7684\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u5728\u9ad8\u5bc6\u5ea6\u533a\u57df\u9057\u6f0f\u4e86\u8bb8\u591a\u4eba\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5206\u8fa8\u7387 SEEM \u6765\u5904\u7406\u4eba\u7fa4\u573a\u666f\u4e2d\u4eba\u5458\u7684\u5c3a\u5ea6\u53d8\u5316\u3001\u906e\u6321\u548c\u91cd\u53e0\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u9c81\u68d2\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u9884\u6d4b\u7684\u4eba\u7269\u9762\u5177\u4e2d\u7684\u5934\u90e8\u4f4d\u7f6e\u3002\u7ed9\u5b9a\u63a9\u6a21\u548c\u70b9\u4f2a\u6807\u7b7e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u635f\u5931\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u65e8\u5728\u6839\u636e SEEM \u7684\u9884\u6d4b\u6392\u9664\u4e0d\u786e\u5b9a\u533a\u57df\uff0c\u4ece\u800c\u589e\u5f3a\u8ba1\u6570\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u4f2a\u6807\u7b7e\u7684\u8fed\u4ee3\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u8bc6\u522b\u9ad8\u5bc6\u5ea6\u533a\u57df\u4e2d\u66f4\u591a\u7684\u5fae\u5c0f\u4eba\u7269\u6765\u63d0\u9ad8\u5206\u5272\u63a9\u6a21\u7684\u8d28\u91cf\uff0c\u800c\u8fd9\u4e9b\u4eba\u7269\u5728\u7b2c\u4e00\u4e2a\u4f2a\u6807\u8bb0\u9636\u6bb5\u7ecf\u5e38\u88ab\u9057\u6f0f\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4eba\u7fa4\u8ba1\u6570\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u65e0\u76d1\u7763\u6027\u80fd\uff0c\u540c\u65f6\u4e5f\u4e0e\u4e00\u4e9b\u76d1\u7763\u65b9\u6cd5\u7684\u7ed3\u679c\u76f8\u5f53\u3002\u8fd9\u4f7f\u5176\u6210\u4e3a\u4e00\u79cd\u9ad8\u6548\u4e14\u591a\u529f\u80fd\u7684\u4eba\u7fa4\u8ba1\u6570\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u65e0\u6cd5\u83b7\u5f97\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002|[2402.17514v1](http://arxiv.org/pdf/2402.17514v1)|null|\n", "2402.17502": "|**2024-02-27**|**FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation**|FedLPPA\uff1a\u5b66\u4e60\u8054\u5408\u5f31\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u4e2a\u6027\u5316\u63d0\u793a\u548c\u805a\u5408|Li Lin, Yixiang Liu, Jiewei Wu, Pujin Cheng, Zhiyuan Cai, Kenneth K. Y. Wong, Xiaoying Tang|Federated learning (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training. However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts. In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of weakly-supervised techniques which utilize sparse annotations such as points, scribbles, etc. A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated. In such context, we propose a novel personalized FL framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage heterogeneous weak supervision for medical image segmentation. In FedLPPA, a learnable universal knowledge prompt is maintained, complemented by multiple learnable personalized data distribution prompts and prompts representing the supervision sparsity. Integrated with sample features through a dual-attention mechanism, those prompts empower each local task decoder to adeptly adjust to both the local distribution and the supervision form. Concurrently, a dual-decoder strategy, predicated on prompt similarity, is introduced for enhancing the generation of pseudo-labels in weakly-supervised learning, alleviating overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis. Extensive experiments on three distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully supervised centralized training. Our code and data will be available.|\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u6709\u6548\u7f13\u89e3\u4e86\u653f\u7b56\u548c\u9690\u79c1\u95ee\u9898\u5e26\u6765\u7684\u6570\u636e\u5b64\u5c9b\u6311\u6218\uff0c\u9690\u5f0f\u5730\u5229\u7528\u66f4\u591a\u6570\u636e\u8fdb\u884c\u6df1\u5ea6\u6a21\u578b\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u96c6\u4e2d\u5f0f FL \u6a21\u578b\u9700\u8981\u5e94\u5bf9\u4e0d\u540c\u7684\u591a\u4e2d\u5fc3\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u663e\u7740\u7684\u6570\u636e\u5f02\u8d28\u6027\u65f6\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u73af\u5883\u4e2d\u3002\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\uff0c\u524a\u51cf\u6ce8\u91ca\u6210\u672c\u7684\u8feb\u5207\u9700\u8981\u653e\u5927\u4e86\u5229\u7528\u70b9\u3001\u6d82\u9e26\u7b49\u7a00\u758f\u6ce8\u91ca\u7684\u5f31\u76d1\u7763\u6280\u672f\u7684\u91cd\u8981\u6027\u3002\u5b9e\u7528\u7684 FL \u8303\u5f0f\u5e94\u9002\u5e94\u4e0d\u540c\u7ad9\u70b9\u7684\u4e0d\u540c\u6ce8\u91ca\u683c\u5f0f\uff0c\u8fd9\u7814\u7a76\u8bfe\u9898\u4ecd\u6709\u5f85\u7814\u7a76\u3002\u5728\u8fd9\u79cd\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u53ef\u5b66\u4e60\u63d0\u793a\u548c\u805a\u5408\u529f\u80fd\u7684\u65b0\u578b\u4e2a\u6027\u5316 FL \u6846\u67b6\uff08FedLPPA\uff09\uff0c\u4ee5\u7edf\u4e00\u5229\u7528\u5f02\u6784\u5f31\u76d1\u7763\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002\u5728FedLPPA\u4e2d\uff0c\u7ef4\u62a4\u4e86\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u901a\u7528\u77e5\u8bc6\u63d0\u793a\uff0c\u5e76\u8f85\u4ee5\u591a\u4e2a\u53ef\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u6570\u636e\u5206\u5e03\u63d0\u793a\u548c\u4ee3\u8868\u76d1\u7763\u7a00\u758f\u6027\u7684\u63d0\u793a\u3002\u901a\u8fc7\u53cc\u91cd\u6ce8\u610f\u673a\u5236\u4e0e\u6837\u672c\u7279\u5f81\u96c6\u6210\uff0c\u8fd9\u4e9b\u63d0\u793a\u4f7f\u6bcf\u4e2a\u672c\u5730\u4efb\u52a1\u89e3\u7801\u5668\u80fd\u591f\u719f\u7ec3\u5730\u9002\u5e94\u672c\u5730\u5206\u5e03\u548c\u76d1\u7763\u5f62\u5f0f\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5373\u65f6\u76f8\u4f3c\u6027\u7684\u53cc\u89e3\u7801\u5668\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u5f31\u76d1\u7763\u5b66\u4e60\u4e2d\u4f2a\u6807\u7b7e\u7684\u751f\u6210\uff0c\u51cf\u8f7b\u5c40\u90e8\u6570\u636e\u56fa\u6709\u7684\u8fc7\u5ea6\u62df\u5408\u548c\u566a\u58f0\u79ef\u7d2f\uff0c\u540c\u65f6\u91c7\u7528\u81ea\u9002\u5e94\u805a\u5408\u65b9\u6cd5\u6765\u5b9a\u5236\u4efb\u52a1\u57fa\u4e8e\u53c2\u6570\u7684\u89e3\u7801\u5668\u3002\u5bf9\u6d89\u53ca\u4e0d\u540c\u6a21\u5f0f\u7684\u4e09\u79cd\u4e0d\u540c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u5f3a\u8c03\u4e86 FedLPPA \u7684\u4f18\u8d8a\u6027\uff0c\u5176\u529f\u6548\u4e0e\u5b8c\u5168\u76d1\u7763\u7684\u96c6\u4e2d\u8bad\u7ec3\u7684\u529f\u6548\u975e\u5e38\u76f8\u4f3c\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u5c06\u53ef\u7528\u3002|[2402.17502v1](http://arxiv.org/pdf/2402.17502v1)|null|\n", "2402.17482": "|**2024-02-27**|**Automated Classification of Phonetic Segments in Child Speech Using Raw Ultrasound Imaging**|\u4f7f\u7528\u539f\u59cb\u8d85\u58f0\u6210\u50cf\u5bf9\u513f\u7ae5\u8bed\u97f3\u4e2d\u7684\u8bed\u97f3\u7247\u6bb5\u8fdb\u884c\u81ea\u52a8\u5206\u7c7b|Saja Al Ani, Joanne Cleland, Ahmed Zoha|Speech sound disorder (SSD) is defined as a persistent impairment in speech sound production leading to reduced speech intelligibility and hindered verbal communication. Early recognition and intervention of children with SSD and timely referral to speech and language therapists (SLTs) for treatment are crucial. Automated detection of speech impairment is regarded as an efficient method for examining and screening large populations. This study focuses on advancing the automatic diagnosis of SSD in early childhood by proposing a technical solution that integrates ultrasound tongue imaging (UTI) with deep-learning models. The introduced FusionNet model combines UTI data with the extracted texture features to classify UTI. The overarching aim is to elevate the accuracy and efficiency of UTI analysis, particularly for classifying speech sounds associated with SSD. This study compared the FusionNet approach with standard deep-learning methodologies, highlighting the excellent improvement results of the FusionNet model in UTI classification and the potential of multi-learning in improving UTI classification in speech therapy clinics.|\u8a00\u8bed\u969c\u788d\uff08SSD\uff09\u88ab\u5b9a\u4e49\u4e3a\u8a00\u8bed\u58f0\u97f3\u4ea7\u751f\u7684\u6301\u7eed\u969c\u788d\uff0c\u5bfc\u81f4\u8a00\u8bed\u6e05\u6670\u5ea6\u4e0b\u964d\u548c\u8a00\u8bed\u4ea4\u6d41\u969c\u788d\u3002\u65e9\u671f\u8bc6\u522b\u548c\u5e72\u9884 SSD \u513f\u7ae5\u5e76\u53ca\u65f6\u8f6c\u8bca\u81f3\u8a00\u8bed\u548c\u8bed\u8a00\u6cbb\u7597\u5e08 (SLT) \u8fdb\u884c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u8a00\u8bed\u969c\u788d\u7684\u81ea\u52a8\u68c0\u6d4b\u88ab\u8ba4\u4e3a\u662f\u68c0\u67e5\u548c\u7b5b\u67e5\u5927\u91cf\u4eba\u7fa4\u7684\u6709\u6548\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u7684\u91cd\u70b9\u662f\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u5c06\u8d85\u58f0\u820c\u6210\u50cf\uff08UTI\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u6765\u63a8\u8fdb\u5e7c\u513f\u671fSSD\u7684\u81ea\u52a8\u8bca\u65ad\u3002\u5f15\u5165\u7684 FusionNet \u6a21\u578b\u5c06 UTI \u6570\u636e\u4e0e\u63d0\u53d6\u7684\u7eb9\u7406\u7279\u5f81\u76f8\u7ed3\u5408\uff0c\u5bf9 UTI \u8fdb\u884c\u5206\u7c7b\u3002\u603b\u4f53\u76ee\u6807\u662f\u63d0\u9ad8 UTI \u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u7279\u522b\u662f\u5bf9\u4e0e SSD \u76f8\u5173\u7684\u8bed\u97f3\u8fdb\u884c\u5206\u7c7b\u3002\u672c\u7814\u7a76\u5c06 FusionNet \u65b9\u6cd5\u4e0e\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5f3a\u8c03\u4e86 FusionNet \u6a21\u578b\u5728 UTI \u5206\u7c7b\u65b9\u9762\u7684\u51fa\u8272\u6539\u8fdb\u7ed3\u679c\u4ee5\u53ca\u591a\u5143\u5b66\u4e60\u5728\u6539\u5584\u8a00\u8bed\u6cbb\u7597\u8bca\u6240\u4e2d UTI \u5206\u7c7b\u65b9\u9762\u7684\u6f5c\u529b\u3002|[2402.17482v1](http://arxiv.org/pdf/2402.17482v1)|null|\n", "2402.17465": "|**2024-02-27**|**Model X-ray:Detect Backdoored Models via Decision Boundary**|\u6a21\u578b X \u5c04\u7ebf\uff1a\u901a\u8fc7\u51b3\u7b56\u8fb9\u754c\u68c0\u6d4b\u540e\u95e8\u6a21\u578b|Yanghao Su, Jie Zhang, Ting Xu, Tianwei Zhang, Weiming Zhang, Nenghai Yu|Deep neural networks (DNNs) have revolutionized various industries, leading to the rise of Machine Learning as a Service (MLaaS). In this paradigm, well-trained models are typically deployed through APIs. However, DNNs are susceptible to backdoor attacks, which pose significant risks to their applications. This vulnerability necessitates a method for users to ascertain whether an API is compromised before usage. Although many backdoor detection methods have been developed, they often operate under the assumption that the defender has access to specific information such as details of the attack, soft predictions from the model API, and even the knowledge of the model parameters, limiting their practicality in MLaaS scenarios. To address it, in this paper, we begin by presenting an intriguing observation: the decision boundary of the backdoored model exhibits a greater degree of closeness than that of the clean model. Simultaneously, if only one single label is infected, a larger portion of the regions will be dominated by the attacked label. Building upon this observation, we propose Model X-ray, a novel backdoor detection approach for MLaaS through the analysis of decision boundaries. Model X-ray can not only identify whether the target API is infected by backdoor attacks but also determine the target attacked label under the all-to-one attack strategy. Importantly, it accomplishes this solely by the hard prediction of clean inputs, regardless of any assumptions about attacks and prior knowledge of the training details of the model. Extensive experiments demonstrated that Model X-ray can be effective for MLaaS across diverse backdoor attacks, datasets, and architectures.|\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u5f7b\u5e95\u6539\u53d8\u4e86\u5404\u4e2a\u884c\u4e1a\uff0c\u5bfc\u81f4\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1 (MLaaS) \u7684\u5174\u8d77\u3002\u5728\u8fd9\u79cd\u8303\u4f8b\u4e2d\uff0c\u8bad\u7ec3\u6709\u7d20\u7684\u6a21\u578b\u901a\u5e38\u901a\u8fc7 API \u8fdb\u884c\u90e8\u7f72\u3002\u7136\u800c\uff0cDNN \u5f88\u5bb9\u6613\u53d7\u5230\u540e\u95e8\u653b\u51fb\uff0c\u8fd9\u5bf9\u5176\u5e94\u7528\u7a0b\u5e8f\u6784\u6210\u91cd\u5927\u98ce\u9669\u3002\u6b64\u6f0f\u6d1e\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9\u7528\u6237\u5728\u4f7f\u7528\u4e4b\u524d\u786e\u5b9a API \u662f\u5426\u53d7\u5230\u635f\u5bb3\u3002\u5c3d\u7ba1\u5df2\u7ecf\u5f00\u53d1\u4e86\u8bb8\u591a\u540e\u95e8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5047\u8bbe\u9632\u5fa1\u8005\u53ef\u4ee5\u8bbf\u95ee\u7279\u5b9a\u4fe1\u606f\uff0c\u4f8b\u5982\u653b\u51fb\u7ec6\u8282\u3001\u6a21\u578b API \u7684\u8f6f\u9884\u6d4b\uff0c\u751a\u81f3\u6a21\u578b\u53c2\u6570\u7684\u77e5\u8bc6\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4ee5\u4e0b\u9886\u57df\u7684\u5b9e\u7528\u6027\uff1a MLaaS \u573a\u666f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e00\u4e2a\u6709\u8da3\u7684\u89c2\u5bdf\uff1a\u540e\u95e8\u6a21\u578b\u7684\u51b3\u7b56\u8fb9\u754c\u6bd4\u5e72\u51c0\u6a21\u578b\u7684\u51b3\u7b56\u8fb9\u754c\u8868\u73b0\u51fa\u66f4\u5927\u7a0b\u5ea6\u7684\u63a5\u8fd1\u5ea6\u3002\u540c\u65f6\uff0c\u5982\u679c\u53ea\u6709\u4e00\u4e2a\u6807\u7b7e\u88ab\u611f\u67d3\uff0c\u5219\u66f4\u5927\u4e00\u90e8\u5206\u533a\u57df\u5c06\u88ab\u53d7\u653b\u51fb\u7684\u6807\u7b7e\u6240\u63a7\u5236\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Model X-ray\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u51b3\u7b56\u8fb9\u754c\u8fdb\u884c MLaaS \u7684\u65b0\u578b\u540e\u95e8\u68c0\u6d4b\u65b9\u6cd5\u3002 Model X-ray\u4e0d\u4ec5\u53ef\u4ee5\u8bc6\u522b\u76ee\u6807API\u662f\u5426\u88ab\u540e\u95e8\u653b\u51fb\u611f\u67d3\uff0c\u8fd8\u53ef\u4ee5\u5728\u4e00\u5bf9\u4e00\u653b\u51fb\u7b56\u7565\u4e0b\u786e\u5b9a\u76ee\u6807\u88ab\u653b\u51fb\u6807\u7b7e\u3002\u91cd\u8981\u7684\u662f\uff0c\u5b83\u4ec5\u901a\u8fc7\u5bf9\u5e72\u51c0\u8f93\u5165\u7684\u786c\u9884\u6d4b\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u800c\u4e0d\u7ba1\u6709\u5173\u653b\u51fb\u7684\u4efb\u4f55\u5047\u8bbe\u548c\u6a21\u578b\u8bad\u7ec3\u7ec6\u8282\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cModel X-ray \u53ef\u4ee5\u6709\u6548\u5730\u5b9e\u73b0\u8de8\u4e0d\u540c\u540e\u95e8\u653b\u51fb\u3001\u6570\u636e\u96c6\u548c\u67b6\u6784\u7684 MLaaS\u3002|[2402.17465v1](http://arxiv.org/pdf/2402.17465v1)|null|\n", "2402.17454": "|**2024-02-27**|**Segment anything model for head and neck tumor segmentation with CT, PET and MRI multi-modality images**|\u4f7f\u7528 CT\u3001PET \u548c MRI \u591a\u6a21\u6001\u56fe\u50cf\u5206\u5272\u4efb\u4f55\u5934\u9888\u90e8\u80bf\u7624\u6a21\u578b|Jintao Ren, Mathis Rasmussen, Jasper Nijkamp, Jesper Grau Eriksen, Stine Korreman|Deep learning presents novel opportunities for the auto-segmentation of gross tumor volume (GTV) in head and neck cancer (HNC), yet fully automatic methods usually necessitate significant manual refinement. This study investigates the Segment Anything Model (SAM), recognized for requiring minimal human prompting and its zero-shot generalization ability across natural images. We specifically examine MedSAM, a version of SAM fine-tuned with large-scale public medical images. Despite its progress, the integration of multi-modality images (CT, PET, MRI) for effective GTV delineation remains a challenge. Focusing on SAM's application in HNC GTV segmentation, we assess its performance in both zero-shot and fine-tuned scenarios using single (CT-only) and fused multi-modality images. Our study demonstrates that fine-tuning SAM significantly enhances its segmentation accuracy, building upon the already effective zero-shot results achieved with bounding box prompts. These findings open a promising avenue for semi-automatic HNC GTV segmentation.|\u6df1\u5ea6\u5b66\u4e60\u4e3a\u5934\u9888\u764c (HNC) \u4e2d\u5927\u4f53\u80bf\u7624\u4f53\u79ef (GTV) \u7684\u81ea\u52a8\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u4f46\u5168\u81ea\u52a8\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u624b\u52a8\u7ec6\u5316\u3002\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u5206\u6bb5\u4efb\u610f\u6a21\u578b (SAM)\uff0c\u8be5\u6a21\u578b\u56e0\u9700\u8981\u6700\u5c11\u7684\u4eba\u7c7b\u63d0\u793a\u53ca\u5176\u5728\u81ea\u7136\u56fe\u50cf\u4e2d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u800c\u53d7\u5230\u8ba4\u53ef\u3002\u6211\u4eec\u4e13\u95e8\u7814\u7a76\u4e86 MedSAM\uff0c\u8fd9\u662f\u6839\u636e\u5927\u89c4\u6a21\u516c\u5171\u533b\u5b66\u56fe\u50cf\u8fdb\u884c\u5fae\u8c03\u7684 SAM \u7248\u672c\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u6574\u5408\u591a\u6a21\u6001\u56fe\u50cf\uff08CT\u3001PET\u3001MRI\uff09\u4ee5\u8fdb\u884c\u6709\u6548\u7684 GTV \u63cf\u7ed8\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u91cd\u70b9\u5173\u6ce8 SAM \u5728 HNC GTV \u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u6211\u4eec\u4f7f\u7528\u5355\u4e2a\uff08\u4ec5 CT\uff09\u548c\u878d\u5408\u7684\u591a\u6a21\u6001\u56fe\u50cf\u8bc4\u4f30\u5176\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u5728\u901a\u8fc7\u8fb9\u754c\u6846\u63d0\u793a\u5b9e\u73b0\u7684\u5df2\u7ecf\u6709\u6548\u7684\u96f6\u6837\u672c\u7ed3\u679c\u7684\u57fa\u7840\u4e0a\uff0c\u5fae\u8c03 SAM \u663e\u7740\u63d0\u9ad8\u4e86\u5176\u5206\u5272\u7cbe\u5ea6\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u534a\u81ea\u52a8 HNC GTV \u5206\u5272\u5f00\u8f9f\u4e86\u4e00\u6761\u5145\u6ee1\u5e0c\u671b\u7684\u9014\u5f84\u3002|[2402.17454v1](http://arxiv.org/pdf/2402.17454v1)|null|\n", "2402.17424": "|**2024-02-27**|**ViTaL: An Advanced Framework for Automated Plant Disease Identification in Leaf Images Using Vision Transformers and Linear Projection For Feature Reduction**|ViTaL\uff1a\u4f7f\u7528\u89c6\u89c9\u53d8\u6362\u5668\u548c\u7ebf\u6027\u6295\u5f71\u8fdb\u884c\u7279\u5f81\u7f29\u51cf\u7684\u53f6\u5b50\u56fe\u50cf\u4e2d\u81ea\u52a8\u690d\u7269\u75c5\u5bb3\u8bc6\u522b\u7684\u9ad8\u7ea7\u6846\u67b6|Abhishek Sebastian, Annis Fathima A, Pragna R, Madhan Kumar S, Yaswanth Kannan G, Vinay Murali|Our paper introduces a robust framework for the automated identification of diseases in plant leaf images. The framework incorporates several key stages to enhance disease recognition accuracy. In the pre-processing phase, a thumbnail resizing technique is employed to resize images, minimizing the loss of critical image details while ensuring computational efficiency. Normalization procedures are applied to standardize image data before feature extraction. Feature extraction is facilitated through a novel framework built upon Vision Transformers, a state-of-the-art approach in image analysis. Additionally, alternative versions of the framework with an added layer of linear projection and blockwise linear projections are explored. This comparative analysis allows for the evaluation of the impact of linear projection on feature extraction and overall model performance. To assess the effectiveness of the proposed framework, various Convolutional Neural Network (CNN) architectures are utilized, enabling a com- prehensive evaluation of linear projection's influence on key evaluation metrics. The findings demonstrate the efficacy of the proposed framework, with the top- performing model achieving a Hamming loss of 0.054. Furthermore, we propose a novel hardware design specifically tailored for scanning diseased leaves in an omnidirectional fashion. The hardware implementation utilizes a Raspberry Pi Compute Module to address low-memory configurations, ensuring practicality and affordability. This innovative hardware solution enhances the overall feasibility and accessibility of the proposed automated disease identification system. This research contributes to the field of agriculture by offering valuable insights and tools for the early detection and management of plant diseases, potentially leading to improved crop yields and enhanced food security.|\u6211\u4eec\u7684\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u690d\u7269\u53f6\u5b50\u56fe\u50cf\u4e2d\u7684\u75be\u75c5\u3002\u8be5\u6846\u67b6\u5305\u542b\u51e0\u4e2a\u5173\u952e\u9636\u6bb5\uff0c\u4ee5\u63d0\u9ad8\u75be\u75c5\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002\u5728\u9884\u5904\u7406\u9636\u6bb5\uff0c\u91c7\u7528\u7f29\u7565\u56fe\u8c03\u6574\u6280\u672f\u6765\u8c03\u6574\u56fe\u50cf\u5927\u5c0f\uff0c\u5728\u4fdd\u8bc1\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5173\u952e\u56fe\u50cf\u7ec6\u8282\u7684\u635f\u5931\u3002\u6807\u51c6\u5316\u7a0b\u5e8f\u7528\u4e8e\u5728\u7279\u5f81\u63d0\u53d6\u4e4b\u524d\u6807\u51c6\u5316\u56fe\u50cf\u6570\u636e\u3002\u901a\u8fc7\u57fa\u4e8e Vision Transformers\uff08\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u5206\u6790\u65b9\u6cd5\uff09\u6784\u5efa\u7684\u65b0\u9896\u6846\u67b6\u6765\u4fc3\u8fdb\u7279\u5f81\u63d0\u53d6\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u6dfb\u52a0\u4e86\u7ebf\u6027\u6295\u5f71\u548c\u5757\u7ebf\u6027\u6295\u5f71\u5c42\u7684\u6846\u67b6\u7684\u66ff\u4ee3\u7248\u672c\u3002\u8fd9\u79cd\u6bd4\u8f83\u5206\u6790\u53ef\u4ee5\u8bc4\u4f30\u7ebf\u6027\u6295\u5f71\u5bf9\u7279\u5f81\u63d0\u53d6\u548c\u6574\u4f53\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u8bc4\u4f30\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5229\u7528\u4e86\u5404\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u67b6\u6784\uff0c\u4ece\u800c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u7ebf\u6027\u6295\u5f71\u5bf9\u5173\u952e\u8bc4\u4f30\u6307\u6807\u7684\u5f71\u54cd\u3002\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5b9e\u73b0\u4e86 0.054 \u7684\u6c49\u660e\u635f\u5931\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u786c\u4ef6\u8bbe\u8ba1\uff0c\u4e13\u95e8\u7528\u4e8e\u4ee5\u5168\u65b9\u4f4d\u65b9\u5f0f\u626b\u63cf\u60a3\u75c5\u53f6\u5b50\u3002\u786c\u4ef6\u5b9e\u73b0\u5229\u7528 Raspberry Pi \u8ba1\u7b97\u6a21\u5757\u6765\u89e3\u51b3\u4f4e\u5185\u5b58\u914d\u7f6e\u95ee\u9898\uff0c\u786e\u4fdd\u5b9e\u7528\u6027\u548c\u7ecf\u6d4e\u6027\u3002\u8fd9\u79cd\u521b\u65b0\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u589e\u5f3a\u4e86\u6240\u63d0\u51fa\u7684\u81ea\u52a8\u75be\u75c5\u8bc6\u522b\u7cfb\u7edf\u7684\u6574\u4f53\u53ef\u884c\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002\u8fd9\u9879\u7814\u7a76\u4e3a\u690d\u7269\u75c5\u5bb3\u7684\u65e9\u671f\u53d1\u73b0\u548c\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u548c\u5de5\u5177\uff0c\u4e3a\u519c\u4e1a\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u6709\u53ef\u80fd\u63d0\u9ad8\u4f5c\u7269\u4ea7\u91cf\u5e76\u589e\u5f3a\u7cae\u98df\u5b89\u5168\u3002|[2402.17424v1](http://arxiv.org/pdf/2402.17424v1)|null|\n", "2402.17420": "|**2024-02-27**|**PANDAS: Prototype-based Novel Class Discovery and Detection**|PANDAS\uff1a\u57fa\u4e8e\u539f\u578b\u7684\u65b0\u7c7b\u53d1\u73b0\u548c\u68c0\u6d4b|Tyler L. Hayes, C\u00e9sar R. de Souza, Namil Kim, Jiwon Kim, Riccardo Volpi, Diane Larlus|Object detectors are typically trained once and for all on a fixed set of classes. However, this closed-world assumption is unrealistic in practice, as new classes will inevitably emerge after the detector is deployed in the wild. In this work, we look at ways to extend a detector trained for a set of base classes so it can i) spot the presence of novel classes, and ii) automatically enrich its repertoire to be able to detect those newly discovered classes together with the base ones. We propose PANDAS, a method for novel class discovery and detection. It discovers clusters representing novel classes from unlabeled data, and represents old and new classes with prototypes. During inference, a distance-based classifier uses these prototypes to assign a label to each detected object instance. The simplicity of our method makes it widely applicable. We experimentally demonstrate the effectiveness of PANDAS on the VOC 2012 and COCO-to-LVIS benchmarks. It performs favorably against the state of the art for this task while being computationally more affordable.|\u76ee\u6807\u68c0\u6d4b\u5668\u901a\u5e38\u5728\u4e00\u7ec4\u56fa\u5b9a\u7684\u7c7b\u522b\u4e0a\u8fdb\u884c\u4e00\u6b21\u6027\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5c01\u95ed\u4e16\u754c\u7684\u5047\u8bbe\u5728\u5b9e\u8df5\u4e2d\u662f\u4e0d\u73b0\u5b9e\u7684\uff0c\u56e0\u4e3a\u63a2\u6d4b\u5668\u5728\u91ce\u5916\u90e8\u7f72\u540e\u5c06\u4e0d\u53ef\u907f\u514d\u5730\u51fa\u73b0\u65b0\u7684\u7c7b\u522b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u6269\u5c55\u9488\u5bf9\u4e00\u7ec4\u57fa\u7c7b\u8bad\u7ec3\u7684\u68c0\u6d4b\u5668\u7684\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u5b83\u80fd\u591f i\uff09\u53d1\u73b0\u65b0\u7c7b\u7684\u5b58\u5728\uff0c\u4ee5\u53ca ii\uff09\u81ea\u52a8\u4e30\u5bcc\u5176\u529f\u80fd\uff0c\u4ee5\u4fbf\u80fd\u591f\u68c0\u6d4b\u8fd9\u4e9b\u65b0\u53d1\u73b0\u7684\u7c7b\u4ee5\u53ca\u57fa\u7840\u7684\u3002\u6211\u4eec\u63d0\u51fa\u4e86 PANDAS\uff0c\u4e00\u79cd\u65b0\u7c7b\u522b\u53d1\u73b0\u548c\u68c0\u6d4b\u7684\u65b9\u6cd5\u3002\u5b83\u4ece\u672a\u6807\u8bb0\u7684\u6570\u636e\u4e2d\u53d1\u73b0\u4ee3\u8868\u65b0\u7c7b\u7684\u7c07\uff0c\u5e76\u7528\u539f\u578b\u4ee3\u8868\u65b0\u65e7\u7c7b\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u57fa\u4e8e\u8ddd\u79bb\u7684\u5206\u7c7b\u5668\u4f7f\u7528\u8fd9\u4e9b\u539f\u578b\u4e3a\u6bcf\u4e2a\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u5b9e\u4f8b\u5206\u914d\u6807\u7b7e\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u7b80\u5355\u6027\u4f7f\u5176\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86 PANDAS \u5728 VOC 2012 \u548c COCO-to-LVIS \u57fa\u51c6\u4e0a\u7684\u6709\u6548\u6027\u3002\u5b83\u7684\u6027\u80fd\u4f18\u4e8e\u8fd9\u9879\u4efb\u52a1\u7684\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u4e5f\u66f4\u4fbf\u5b9c\u3002|[2402.17420v1](http://arxiv.org/pdf/2402.17420v1)|null|\n", "2402.17417": "|**2024-02-27**|**CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification**|CARZero\uff1a\u653e\u5c04\u5b66\u96f6\u6837\u672c\u5206\u7c7b\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u9f50|Haoran Lai, Qingsong Yao, Zihang Jiang, Rongsheng Wang, Zhiyang He, Xiaodong Tao, S. Kevin Zhou|The advancement of Zero-Shot Learning in the medical domain has been driven forward by using pre-trained models on large-scale image-text pairs, focusing on image-text alignment. However, existing methods primarily rely on cosine similarity for alignment, which may not fully capture the complex relationship between medical images and reports. To address this gap, we introduce a novel approach called Cross-Attention Alignment for Radiology Zero-Shot Classification (CARZero). Our approach innovatively leverages cross-attention mechanisms to process image and report features, creating a Similarity Representation that more accurately reflects the intricate relationships in medical semantics. This representation is then linearly projected to form an image-text similarity matrix for cross-modality alignment. Additionally, recognizing the pivotal role of prompt selection in zero-shot learning, CARZero incorporates a Large Language Model-based prompt alignment strategy. This strategy standardizes diverse diagnostic expressions into a unified format for both training and inference phases, overcoming the challenges of manual prompt design. Our approach is simple yet effective, demonstrating state-of-the-art performance in zero-shot classification on five official chest radiograph diagnostic test sets, including remarkable results on datasets with long-tail distributions of rare diseases. This achievement is attributed to our new image-text alignment strategy, which effectively addresses the complex relationship between medical images and reports.|\u901a\u8fc7\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u6587\u672c\u5bf9\u4e0a\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u56fe\u50cf\u6587\u672c\u5bf9\u9f50\uff0c\u63a8\u52a8\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u5728\u533b\u5b66\u9886\u57df\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u9760\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8fdb\u884c\u5bf9\u9f50\uff0c\u8fd9\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u533b\u5b66\u56fe\u50cf\u548c\u62a5\u544a\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u79f0\u4e3a\u653e\u5c04\u5b66\u96f6\u6837\u672c\u5206\u7c7b\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u9f50\uff08CARZero\uff09\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u521b\u65b0\u5730\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u6765\u5904\u7406\u56fe\u50cf\u548c\u62a5\u544a\u7279\u5f81\uff0c\u521b\u5efa\u76f8\u4f3c\u6027\u8868\u793a\uff0c\u66f4\u51c6\u786e\u5730\u53cd\u6620\u533b\u5b66\u8bed\u4e49\u4e2d\u590d\u6742\u7684\u5173\u7cfb\u3002\u7136\u540e\u7ebf\u6027\u6295\u5f71\u8be5\u8868\u793a\u4ee5\u5f62\u6210\u7528\u4e8e\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u56fe\u50cf\u6587\u672c\u76f8\u4f3c\u5ea6\u77e9\u9635\u3002\u6b64\u5916\uff0c\u8ba4\u8bc6\u5230\u63d0\u793a\u9009\u62e9\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0cCARZero \u7ed3\u5408\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u5bf9\u9f50\u7b56\u7565\u3002\u8be5\u7b56\u7565\u5c06\u4e0d\u540c\u7684\u8bca\u65ad\u8868\u8fbe\u5f0f\u6807\u51c6\u5316\u4e3a\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7684\u7edf\u4e00\u683c\u5f0f\uff0c\u514b\u670d\u4e86\u624b\u52a8\u63d0\u793a\u8bbe\u8ba1\u7684\u6311\u6218\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7b80\u5355\u800c\u6709\u6548\uff0c\u5728\u4e94\u4e2a\u5b98\u65b9\u80f8\u7247\u8bca\u65ad\u6d4b\u8bd5\u96c6\u4e0a\u5c55\u793a\u4e86\u96f6\u6837\u672c\u5206\u7c7b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5305\u62ec\u5728\u7f55\u89c1\u75be\u75c5\u957f\u5c3e\u5206\u5e03\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u7740\u7684\u7ed3\u679c\u3002\u8fd9\u4e00\u6210\u5c31\u5f52\u529f\u4e8e\u6211\u4eec\u65b0\u7684\u56fe\u6587\u5bf9\u9f50\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u6709\u6548\u5730\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u548c\u62a5\u544a\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002|[2402.17417v1](http://arxiv.org/pdf/2402.17417v1)|null|\n", "2402.17370": "|**2024-02-27**|**An Efficient MLP-based Point-guided Segmentation Network for Ore Images with Ambiguous Boundary**|\u4e00\u79cd\u57fa\u4e8eMLP\u7684\u9ad8\u6548\u7684\u8fb9\u754c\u6a21\u7cca\u77ff\u77f3\u56fe\u50cf\u70b9\u5f15\u5bfc\u5206\u5272\u7f51\u7edc|Guodong Sun, Yuting Peng, Le Cheng, Mengya Xu, An Wang, Bo Wu, Hongliang Ren, Yang Zhang|The precise segmentation of ore images is critical to the successful execution of the beneficiation process. Due to the homogeneous appearance of the ores, which leads to low contrast and unclear boundaries, accurate segmentation becomes challenging, and recognition becomes problematic. This paper proposes a lightweight framework based on Multi-Layer Perceptron (MLP), which focuses on solving the problem of edge burring. Specifically, we introduce a lightweight backbone better suited for efficiently extracting low-level features. Besides, we design a feature pyramid network consisting of two MLP structures that balance local and global information thus enhancing detection accuracy. Furthermore, we propose a novel loss function that guides the prediction points to match the instance edge points to achieve clear object boundaries. We have conducted extensive experiments to validate the efficacy of our proposed method. Our approach achieves a remarkable processing speed of over 27 frames per second (FPS) with a model size of only 73 MB. Moreover, our method delivers a consistently high level of accuracy, with impressive performance scores of 60.4 and 48.9 in~$AP_{50}^{box}$ and~$AP_{50}^{mask}$ respectively, as compared to the currently available state-of-the-art techniques, when tested on the ore image dataset. The source code will be released at \\url{https://github.com/MVME-HBUT/ORENEXT}.|\u77ff\u77f3\u56fe\u50cf\u7684\u7cbe\u786e\u5206\u5272\u5bf9\u4e8e\u9009\u77ff\u8fc7\u7a0b\u7684\u6210\u529f\u6267\u884c\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u77ff\u77f3\u5916\u89c2\u5747\u8d28\uff0c\u5bfc\u81f4\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u8fb9\u754c\u4e0d\u6e05\u6670\uff0c\u51c6\u786e\u5206\u5272\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\uff0c\u8bc6\u522b\u4e5f\u6210\u4e3a\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u91cd\u70b9\u89e3\u51b3\u8fb9\u7f18\u6bdb\u523a\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u66f4\u9002\u5408\u6709\u6548\u63d0\u53d6\u4f4e\u7ea7\u7279\u5f81\u7684\u8f7b\u91cf\u7ea7\u4e3b\u5e72\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7531\u4e24\u4e2a MLP \u7ed3\u6784\u7ec4\u6210\u7684\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff0c\u53ef\u4ee5\u5e73\u8861\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\uff0c\u5f15\u5bfc\u9884\u6d4b\u70b9\u5339\u914d\u5b9e\u4f8b\u8fb9\u7f18\u70b9\uff0c\u4ee5\u5b9e\u73b0\u6e05\u6670\u7684\u5bf9\u8c61\u8fb9\u754c\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6bcf\u79d2\u8d85\u8fc7 27 \u5e27 (FPS) \u7684\u5353\u8d8a\u5904\u7406\u901f\u5ea6\uff0c\u6a21\u578b\u5927\u5c0f\u4ec5\u4e3a 73 MB\u3002\u6b64\u5916\uff0c\u4e0e\u5728\u77ff\u77f3\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u65f6\uff0c\u91c7\u7528\u4e86\u5f53\u524d\u53ef\u7528\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002\u6e90\u4ee3\u7801\u5c06\u5728 \\url{https://github.com/MVME-HBUT/ORENEXT} \u53d1\u5e03\u3002|[2402.17370v1](http://arxiv.org/pdf/2402.17370v1)|null|\n", "2402.17323": "|**2024-02-27**|**SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection**|SDDGR\uff1a\u7528\u4e8e\u7c7b\u589e\u91cf\u5bf9\u8c61\u68c0\u6d4b\u7684\u7a33\u5b9a\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6df1\u5ea6\u751f\u6210\u91cd\u653e|Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, Seungryul Baek|In the field of class incremental learning (CIL), genera- tive replay has become increasingly prominent as a method to mitigate the catastrophic forgetting, alongside the con- tinuous improvements in generative models. However, its application in class incremental object detection (CIOD) has been significantly limited, primarily due to the com- plexities of scenes involving multiple labels. In this paper, we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a diffusion-based generative model with pre-trained text- to-diffusion networks to generate realistic and diverse syn- thetic images. SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes. Additionally, we adopt an L2 knowledge distilla- tion technique to improve the retention of prior knowledge in synthetic images. Furthermore, our approach includes pseudo-labeling for old objects within new task images, pre- venting misclassification as background elements. Exten- sive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms, achieving a new state-of-the-art in various CIOD scenarios. The source code will be made available to the public.|\u5728\u8bfe\u5802\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\u9886\u57df\uff0c\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u4e0d\u65ad\u6539\u8fdb\uff0c\u751f\u6210\u91cd\u64ad\u4f5c\u4e3a\u4e00\u79cd\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u7684\u65b9\u6cd5\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u5b83\u5728\u7c7b\u589e\u91cf\u5bf9\u8c61\u68c0\u6d4b\uff08CIOD\uff09\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u4e86\u6781\u5927\u7684\u9650\u5236\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u6d89\u53ca\u591a\u4e2a\u6807\u7b7e\u7684\u573a\u666f\u7684\u590d\u6742\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4e3a CIOD \u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u7a33\u5b9a\u6269\u6563\u6df1\u5ea6\u751f\u6210\u91cd\u653e\uff08SDDGR\uff09\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u548c\u9884\u5148\u8bad\u7ec3\u7684\u6587\u672c\u6269\u6563\u7f51\u7edc\u6765\u751f\u6210\u771f\u5b9e\u4e14\u591a\u6837\u5316\u7684\u5408\u6210\u56fe\u50cf\u3002 SDDGR \u91c7\u7528\u8fed\u4ee3\u7ec6\u5316\u7b56\u7565\u6765\u751f\u6210\u5305\u542b\u65e7\u7c7b\u522b\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528 L2 \u77e5\u8bc6\u84b8\u998f\u6280\u672f\u6765\u63d0\u9ad8\u5408\u6210\u56fe\u50cf\u4e2d\u5148\u9a8c\u77e5\u8bc6\u7684\u4fdd\u7559\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u5bf9\u65b0\u4efb\u52a1\u56fe\u50cf\u4e2d\u7684\u65e7\u5bf9\u8c61\u8fdb\u884c\u4f2a\u6807\u8bb0\uff0c\u9632\u6b62\u9519\u8bef\u5206\u7c7b\u4e3a\u80cc\u666f\u5143\u7d20\u3002\u5bf9 COCO 2017 \u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSDDGR \u663e\u7740\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u5728\u5404\u79cd CIOD \u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7b97\u6cd5\u3002\u6e90\u4ee3\u7801\u5c06\u5411\u516c\u4f17\u5f00\u653e\u3002|[2402.17323v1](http://arxiv.org/pdf/2402.17323v1)|null|\n", "2402.17319": "|**2024-02-27**|**A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to 1st VCL Challenge -- Multi-Task Robustness Track**|\u7528\u4e8e\u7b2c\u4e00\u5c4a VCL \u6311\u6218\u8d5b\u5bc6\u96c6\u89c6\u89c9\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u7684\u666e\u901a\u591a\u4efb\u52a1\u6846\u67b6\u2014\u2014\u591a\u4efb\u52a1\u9c81\u68d2\u6027\u8d5b\u9053|Zehui Chen, Qiuchen Wang, Zhenyu Li, Jiaming Liu, Shanghang Zhang, Feng Zhao|In this report, we present our solution to the multi-task robustness track of the 1st Visual Continual Learning (VCL) Challenge at ICCV 2023 Workshop. We propose a vanilla framework named UniNet that seamlessly combines various visual perception algorithms into a multi-task model. Specifically, we choose DETR3D, Mask2Former, and BinsFormer for 3D object detection, instance segmentation, and depth estimation tasks, respectively. The final submission is a single model with InternImage-L backbone, and achieves a 49.6 overall score (29.5 Det mAP, 80.3 mTPS, 46.4 Seg mAP, and 7.93 silog) on SHIFT validation set. Besides, we provide some interesting observations in our experiments which may facilitate the development of multi-task learning in dense visual prediction.|\u5728\u672c\u62a5\u544a\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 ICCV 2023 \u7814\u8ba8\u4f1a\u4e0a\u7b2c\u4e00\u5c4a\u89c6\u89c9\u6301\u7eed\u5b66\u4e60 (VCL) \u6311\u6218\u8d5b\u7684\u591a\u4efb\u52a1\u7a33\u5065\u6027\u8d5b\u9053\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a UniNet \u7684\u666e\u901a\u6846\u67b6\uff0c\u5b83\u5c06\u5404\u79cd\u89c6\u89c9\u611f\u77e5\u7b97\u6cd5\u65e0\u7f1d\u5730\u7ec4\u5408\u6210\u4e00\u4e2a\u591a\u4efb\u52a1\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9009\u62e9 DETR3D\u3001Mask2Former \u548c BinsFormer \u5206\u522b\u7528\u4e8e 3D \u5bf9\u8c61\u68c0\u6d4b\u3001\u5b9e\u4f8b\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u3002\u6700\u7ec8\u63d0\u4ea4\u7684\u662f\u5177\u6709 InternImage-L \u4e3b\u5e72\u7684\u5355\u4e00\u6a21\u578b\uff0c\u5728 SHIFT \u9a8c\u8bc1\u96c6\u4e0a\u83b7\u5f97\u4e86 49.6 \u7684\u603b\u5206\uff0829.5 Det mAP\u300180.3 mTPS\u300146.4 Seg mAP \u548c 7.93 silog\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u5b9e\u9a8c\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e9b\u6709\u8da3\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u5bc6\u96c6\u89c6\u89c9\u9884\u6d4b\u4e2d\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u53d1\u5c55\u3002|[2402.17319v1](http://arxiv.org/pdf/2402.17319v1)|null|\n", "2402.17318": "|**2024-02-27**|**Scaling Supervised Local Learning with Augmented Auxiliary Networks**|\u4f7f\u7528\u589e\u5f3a\u8f85\u52a9\u7f51\u7edc\u6269\u5c55\u76d1\u7763\u672c\u5730\u5b66\u4e60|Chenxiang Ma, Jibin Wu, Chenyang Si, Kay Chen Tan|Deep neural networks are typically trained using global error signals that backpropagate (BP) end-to-end, which is not only biologically implausible but also suffers from the update locking problem and requires huge memory consumption. Local learning, which updates each layer independently with a gradient-isolated auxiliary network, offers a promising alternative to address the above problems. However, existing local learning methods are confronted with a large accuracy gap with the BP counterpart, particularly for large-scale networks. This is due to the weak coupling between local layers and their subsequent network layers, as there is no gradient communication across layers. To tackle this issue, we put forward an augmented local learning method, dubbed AugLocal. AugLocal constructs each hidden layer's auxiliary network by uniformly selecting a small subset of layers from its subsequent network layers to enhance their synergy. We also propose to linearly reduce the depth of auxiliary networks as the hidden layer goes deeper, ensuring sufficient network capacity while reducing the computational cost of auxiliary networks. Our extensive experiments on four image classification datasets (i.e., CIFAR-10, SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up to tens of local layers with a comparable accuracy to BP-trained networks while reducing GPU memory usage by around 40%. The proposed AugLocal method, therefore, opens up a myriad of opportunities for training high-performance deep neural networks on resource-constrained platforms.Code is available at https://github.com/ChenxiangMA/AugLocal.|\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u4f7f\u7528\u7aef\u5230\u7aef\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u7684\u5168\u5c40\u8bef\u5dee\u4fe1\u53f7\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u4e0d\u4ec5\u5728\u751f\u7269\u5b66\u4e0a\u4e0d\u53ef\u4fe1\uff0c\u800c\u4e14\u8fd8\u5b58\u5728\u66f4\u65b0\u9501\u5b9a\u95ee\u9898\uff0c\u5e76\u4e14\u9700\u8981\u5de8\u5927\u7684\u5185\u5b58\u6d88\u8017\u3002\u5c40\u90e8\u5b66\u4e60\u901a\u8fc7\u68af\u5ea6\u9694\u79bb\u8f85\u52a9\u7f51\u7edc\u72ec\u7acb\u66f4\u65b0\u6bcf\u4e00\u5c42\uff0c\u4e3a\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5c40\u90e8\u5b66\u4e60\u65b9\u6cd5\u4e0e BP \u65b9\u6cd5\u76f8\u6bd4\uff0c\u9762\u4e34\u7740\u5de8\u5927\u7684\u7cbe\u5ea6\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\u3002\u8fd9\u662f\u7531\u4e8e\u672c\u5730\u5c42\u4e0e\u5176\u540e\u7eed\u7f51\u7edc\u5c42\u4e4b\u95f4\u7684\u8026\u5408\u8f83\u5f31\uff0c\u56e0\u4e3a\u8de8\u5c42\u6ca1\u6709\u68af\u5ea6\u901a\u4fe1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u672c\u5730\u5b66\u4e60\u65b9\u6cd5\uff0c\u79f0\u4e3a AugLocal\u3002 AugLocal\u901a\u8fc7\u4ece\u5176\u540e\u7eed\u7f51\u7edc\u5c42\u4e2d\u7edf\u4e00\u9009\u62e9\u4e00\u5c0f\u90e8\u5206\u5c42\u5b50\u96c6\u6765\u6784\u5efa\u6bcf\u4e2a\u9690\u85cf\u5c42\u7684\u8f85\u52a9\u7f51\u7edc\uff0c\u4ee5\u589e\u5f3a\u5b83\u4eec\u7684\u534f\u540c\u4f5c\u7528\u3002\u6211\u4eec\u8fd8\u5efa\u8bae\u968f\u7740\u9690\u85cf\u5c42\u7684\u52a0\u6df1\u800c\u7ebf\u6027\u51cf\u5c11\u8f85\u52a9\u7f51\u7edc\u7684\u6df1\u5ea6\uff0c\u4fdd\u8bc1\u8db3\u591f\u7684\u7f51\u7edc\u5bb9\u91cf\uff0c\u540c\u65f6\u964d\u4f4e\u8f85\u52a9\u7f51\u7edc\u7684\u8ba1\u7b97\u6210\u672c\u3002\u6211\u4eec\u5bf9\u56db\u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\uff08\u5373 CIFAR-10\u3001SVHN\u3001STL-10 \u548c ImageNet\uff09\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAugLocal \u53ef\u4ee5\u6709\u6548\u5730\u6269\u5c55\u5230\u6570\u5341\u4e2a\u5c40\u90e8\u5c42\uff0c\u5176\u7cbe\u5ea6\u4e0e BP \u8bad\u7ec3\u7684\u7f51\u7edc\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11 GPU \u5185\u5b58\u4f7f\u7528\u7ea6 40%\u3002\u56e0\u6b64\uff0c\u6240\u63d0\u51fa\u7684 AugLocal \u65b9\u6cd5\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5e73\u53f0\u4e0a\u8bad\u7ec3\u9ad8\u6027\u80fd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65e0\u6570\u7684\u673a\u4f1a\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/ChenyangMA/AugLocal \u83b7\u53d6\u3002|[2402.17318v1](http://arxiv.org/pdf/2402.17318v1)|null|\n", "2402.17317": "|**2024-02-27**|**How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation**|\u6211\u4eec\u5982\u4f55\u8d62\u5f97 BraTS 2023 \u6210\u4eba\u80f6\u8d28\u7624\u6311\u6218\u8d5b\uff1f\u53ea\u662f\u5047\u88c5\u800c\u5df2\uff01\u7528\u4e8e\u8111\u80bf\u7624\u5206\u5272\u7684\u589e\u5f3a\u578b\u5408\u6210\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u96c6\u6210|Andr\u00e9 Ferreira, Naida Solak, Jianning Li, Philipp Dammann, Jens Kleesiek, Victor Alves, Jan Egger|Deep Learning is the state-of-the-art technology for segmenting brain tumours. However, this requires a lot of high-quality data, which is difficult to obtain, especially in the medical field. Therefore, our solutions address this problem by using unconventional mechanisms for data augmentation. Generative adversarial networks and registration are used to massively increase the amount of available samples for training three different deep learning models for brain tumour segmentation, the first task of the BraTS2023 challenge. The first model is the standard nnU-Net, the second is the Swin UNETR and the third is the winning solution of the BraTS 2021 Challenge. The entire pipeline is built on the nnU-Net implementation, except for the generation of the synthetic data. The use of convolutional algorithms and transformers is able to fill each other's knowledge gaps. Using the new metric, our best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95 14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the validation set.|\u6df1\u5ea6\u5b66\u4e60\u662f\u5206\u5272\u8111\u80bf\u7624\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002\u7136\u800c\uff0c\u8fd9\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u7684\u6570\u636e\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u5f88\u96be\u83b7\u5f97\uff0c\u5c24\u5176\u662f\u5728\u533b\u5b66\u9886\u57df\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u4f7f\u7528\u975e\u5e38\u89c4\u7684\u6570\u636e\u589e\u5f3a\u673a\u5236\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u914d\u51c6\u7528\u4e8e\u5927\u91cf\u589e\u52a0\u53ef\u7528\u6837\u672c\u7684\u6570\u91cf\uff0c\u7528\u4e8e\u8bad\u7ec3\u4e09\u79cd\u4e0d\u540c\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ee5\u8fdb\u884c\u8111\u80bf\u7624\u5206\u5272\uff0c\u8fd9\u662f BraTS2023 \u6311\u6218\u8d5b\u7684\u7b2c\u4e00\u4e2a\u4efb\u52a1\u3002\u7b2c\u4e00\u4e2a\u6a21\u578b\u662f\u6807\u51c6 nnU-Net\uff0c\u7b2c\u4e8c\u4e2a\u662f Swin UNETR\uff0c\u7b2c\u4e09\u4e2a\u662f BraTS 2021 \u6311\u6218\u8d5b\u7684\u83b7\u80dc\u89e3\u51b3\u65b9\u6848\u3002\u9664\u4e86\u5408\u6210\u6570\u636e\u7684\u751f\u6210\u4e4b\u5916\uff0c\u6574\u4e2a\u6d41\u7a0b\u90fd\u5efa\u7acb\u5728 nnU-Net \u5b9e\u73b0\u4e0a\u3002\u5377\u79ef\u7b97\u6cd5\u548c\u8f6c\u6362\u5668\u7684\u4f7f\u7528\u80fd\u591f\u586b\u8865\u5f7c\u6b64\u7684\u77e5\u8bc6\u7a7a\u767d\u3002\u4f7f\u7528\u65b0\u7684\u6307\u6807\uff0c\u6211\u4eec\u7684\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u5728\u9a8c\u8bc1\u96c6\u4e2d\u5b9e\u73b0\u4e86\u9ab0\u5b50\u7ed3\u679c 0.9005\u30010.8673\u30010.8509 \u548c HD95 14.940\u300114.467\u300117.699\uff08\u6574\u4e2a\u80bf\u7624\u3001\u80bf\u7624\u6838\u5fc3\u548c\u589e\u5f3a\u80bf\u7624\uff09\u3002|[2402.17317v1](http://arxiv.org/pdf/2402.17317v1)|null|\n", "2402.17264": "|**2024-02-27**|**Explicit Interaction for Fusion-Based Place Recognition**|\u57fa\u4e8e\u878d\u5408\u7684\u5730\u70b9\u8bc6\u522b\u7684\u663e\u5f0f\u4ea4\u4e92|Jingyi Xu, Junyi Ma, Qi Wu, Zijie Zhou, Yue Wang, Xieyuanli Chen, Ling Pei|Fusion-based place recognition is an emerging technique jointly utilizing multi-modal perception data, to recognize previously visited places in GPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based place recognition methods combine multi-modal features in implicit manners. While achieving remarkable results, they do not explicitly consider what the individual modality affords in the fusion system. Therefore, the benefit of multi-modal feature fusion may not be fully explored. In this paper, we propose a novel fusion-based network, dubbed EINet, to achieve explicit interaction of the two modalities. EINet uses LiDAR ranges to supervise more robust vision features for long time spans, and simultaneously uses camera RGB data to improve the discrimination of LiDAR point clouds. In addition, we develop a new benchmark for the place recognition task based on the nuScenes dataset. To establish this benchmark for future research with comprehensive comparisons, we introduce both supervised and self-supervised training schemes alongside evaluation protocols. We conduct extensive experiments on the proposed benchmark, and the experimental results show that our EINet exhibits better recognition performance as well as solid generalization ability compared to the state-of-the-art fusion-based place recognition approaches. Our open-source code and benchmark are released at: https://github.com/BIT-XJY/EINet.|\u57fa\u4e8e\u878d\u5408\u7684\u5730\u70b9\u8bc6\u522b\u662f\u4e00\u79cd\u65b0\u5174\u6280\u672f\uff0c\u8054\u5408\u5229\u7528\u591a\u6a21\u6001\u611f\u77e5\u6570\u636e\uff0c\u5728 GPS \u65e0\u6cd5\u8bc6\u522b\u7684\u60c5\u51b5\u4e0b\u4e3a\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8bc6\u522b\u5148\u524d\u8bbf\u95ee\u8fc7\u7684\u5730\u70b9\u3002\u6700\u8fd1\u57fa\u4e8e\u878d\u5408\u7684\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u4ee5\u9690\u5f0f\u65b9\u5f0f\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u7279\u5f81\u3002\u867d\u7136\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u679c\uff0c\u4f46\u4ed6\u4eec\u6ca1\u6709\u660e\u786e\u8003\u8651\u4e2a\u4f53\u6a21\u6001\u5728\u878d\u5408\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u7684\u529f\u80fd\u3002\u56e0\u6b64\uff0c\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u7684\u597d\u5904\u53ef\u80fd\u6ca1\u6709\u5f97\u5230\u5145\u5206\u53d1\u6325\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u878d\u5408\u7684\u7f51\u7edc\uff0c\u79f0\u4e3a EINet\uff0c\u4ee5\u5b9e\u73b0\u4e24\u79cd\u6a21\u5f0f\u7684\u663e\u5f0f\u4ea4\u4e92\u3002 EINet \u4f7f\u7528 LiDAR \u8303\u56f4\u6765\u957f\u65f6\u95f4\u76d1\u63a7\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u540c\u65f6\u4f7f\u7528\u76f8\u673a RGB \u6570\u636e\u6765\u63d0\u9ad8 LiDAR \u70b9\u4e91\u7684\u8fa8\u522b\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u57fa\u4e8e nuScenes \u6570\u636e\u96c6\u4e3a\u5730\u70b9\u8bc6\u522b\u4efb\u52a1\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u3002\u4e3a\u4e86\u901a\u8fc7\u5168\u9762\u6bd4\u8f83\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5efa\u7acb\u8fd9\u4e00\u57fa\u51c6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u76d1\u7763\u548c\u81ea\u6211\u76d1\u7763\u7684\u57f9\u8bad\u65b9\u6848\u4ee5\u53ca\u8bc4\u4f30\u534f\u8bae\u3002\u6211\u4eec\u5bf9\u6240\u63d0\u51fa\u7684\u57fa\u51c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u878d\u5408\u7684\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684 EINet \u8868\u73b0\u51fa\u66f4\u597d\u7684\u8bc6\u522b\u6027\u80fd\u4ee5\u53ca\u624e\u5b9e\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u5f00\u6e90\u4ee3\u7801\u548c\u57fa\u51c6\u6d4b\u8bd5\u53d1\u5e03\u4e8e\uff1ahttps://github.com/BIT-XJY/EINet\u3002|[2402.17264v1](http://arxiv.org/pdf/2402.17264v1)|null|\n", "2402.17249": "|**2024-02-27**|**Deep Learning-Based Speech and Vision Synthesis to Improve Phishing Attack Detection through a Multi-layer Adaptive Framework**|\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8bed\u97f3\u548c\u89c6\u89c9\u5408\u6210\u901a\u8fc7\u591a\u5c42\u81ea\u9002\u5e94\u6846\u67b6\u6539\u8fdb\u7f51\u7edc\u9493\u9c7c\u653b\u51fb\u68c0\u6d4b|Tosin Ige, Christopher Kiekintveld, Aritran Piplai|The ever-evolving ways attacker continues to im prove their phishing techniques to bypass existing state-of-the-art phishing detection methods pose a mountain of challenges to researchers in both industry and academia research due to the inability of current approaches to detect complex phishing attack. Thus, current anti-phishing methods remain vulnerable to complex phishing because of the increasingly sophistication tactics adopted by attacker coupled with the rate at which new tactics are being developed to evade detection. In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize speech from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack detection.|\u653b\u51fb\u8005\u4e0d\u65ad\u6539\u8fdb\u5176\u7f51\u7edc\u9493\u9c7c\u6280\u672f\u4ee5\u7ed5\u8fc7\u73b0\u6709\u6700\u5148\u8fdb\u7684\u7f51\u7edc\u9493\u9c7c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8fd9\u7ed9\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u7814\u7a76\u4eba\u5458\u5e26\u6765\u4e86\u5de8\u5927\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u5f53\u524d\u7684\u65b9\u6cd5\u65e0\u6cd5\u68c0\u6d4b\u590d\u6742\u7684\u7f51\u7edc\u9493\u9c7c\u653b\u51fb\u3002\u56e0\u6b64\uff0c\u5f53\u524d\u7684\u53cd\u7f51\u7edc\u9493\u9c7c\u65b9\u6cd5\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u590d\u6742\u7684\u7f51\u7edc\u9493\u9c7c\u7684\u5f71\u54cd\uff0c\u56e0\u4e3a\u653b\u51fb\u8005\u91c7\u7528\u7684\u7b56\u7565\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u800c\u4e14\u5f00\u53d1\u65b0\u7b56\u7565\u4ee5\u9003\u907f\u68c0\u6d4b\u7684\u901f\u5ea6\u4e5f\u8d8a\u6765\u8d8a\u5feb\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u5e94\u6027\u6846\u67b6\uff0c\u5b83\u5c06\u6df1\u5ea6\u5b66\u4e60\u548c Randon Forest \u76f8\u7ed3\u5408\u6765\u8bfb\u53d6\u56fe\u50cf\u3001\u4ece\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u4e2d\u5408\u6210\u8bed\u97f3\u4ee5\u53ca\u5bf9\u5404\u79cd\u5206\u5c42\u9884\u6d4b\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u4ee5\u663e\u7740\u63d0\u9ad8\u7528\u4e8e\u7f51\u7edc\u9493\u9c7c\u653b\u51fb\u68c0\u6d4b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002|[2402.17249v1](http://arxiv.org/pdf/2402.17249v1)|null|\n", "2402.17246": "|**2024-02-27**|**SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging**|SDR-Former\uff1a\u4f7f\u7528 3D \u591a\u76f8\u6210\u50cf\u8fdb\u884c\u809d\u810f\u75c5\u53d8\u5206\u7c7b\u7684\u8fde\u4f53\u53cc\u5206\u8fa8\u7387\u53d8\u538b\u5668|Meng Lou, Hanning Ying, Xiaoqing Liu, Hong-Yu Zhou, Yuqing Zhang, Yizhou Yu|Automated classification of liver lesions in multi-phase CT and MR scans is of clinical significance but challenging. This study proposes a novel Siamese Dual-Resolution Transformer (SDR-Former) framework, specifically designed for liver lesion classification in 3D multi-phase CT and MR imaging with varying phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural Network (SNN) to process multi-phase imaging inputs, possessing robust feature representations while maintaining computational efficiency. The weight-sharing feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer (DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored 3D Transformer for processing high- and low-resolution images, respectively. This hybrid sub-architecture excels in capturing detailed local features and understanding global contextual information, thereby, boosting the SNN's feature extraction capabilities. Additionally, a novel Adaptive Phase Selection Module (APSM) is introduced, promoting phase-specific intercommunication and dynamically adjusting each phase's influence on the diagnostic outcome. The proposed SDR-Former framework has been validated through comprehensive experiments on two clinical datasets: a three-phase CT dataset and an eight-phase MR dataset. The experimental results affirm the efficacy of the proposed framework. To support the scientific community, we are releasing our extensive multi-phase MR dataset for liver lesion analysis to the public. This pioneering dataset, being the first publicly available multi-phase MR dataset in this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is accessible at:https://bit.ly/3IyYlgN.|\u591a\u671f CT \u548c MR \u626b\u63cf\u4e2d\u809d\u810f\u75c5\u53d8\u7684\u81ea\u52a8\u5206\u7c7b\u5177\u6709\u4e34\u5e8a\u610f\u4e49\uff0c\u4f46\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fde\u4f53\u53cc\u5206\u8fa8\u7387\u53d8\u538b\u5668 (SDR-Former) \u6846\u67b6\uff0c\u4e13\u4e3a\u5177\u6709\u4e0d\u540c\u76f8\u4f4d\u8ba1\u6570\u7684 3D \u591a\u76f8 CT \u548c MR \u6210\u50cf\u4e2d\u7684\u809d\u810f\u75c5\u53d8\u5206\u7c7b\u800c\u8bbe\u8ba1\u3002\u6240\u63d0\u51fa\u7684 SDR-Former \u5229\u7528\u7b80\u5316\u7684\u8fde\u4f53\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u6765\u5904\u7406\u591a\u76f8\u6210\u50cf\u8f93\u5165\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u62e5\u6709\u5f3a\u5927\u7684\u7279\u5f81\u8868\u793a\u3002\u6df7\u5408\u53cc\u5206\u8fa8\u7387\u53d8\u538b\u5668 (DR-Former) \u8fdb\u4e00\u6b65\u4e30\u5bcc\u4e86 SNN \u7684\u6743\u91cd\u5171\u4eab\u529f\u80fd\uff0c\u5176\u4e2d\u5305\u62ec 3D \u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u548c\u5206\u522b\u7528\u4e8e\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u548c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u5b9a\u5236 3D Transformer\u3002\u8fd9\u79cd\u6df7\u5408\u5b50\u67b6\u6784\u64c5\u957f\u6355\u83b7\u8be6\u7ec6\u7684\u5c40\u90e8\u7279\u5f81\u5e76\u7406\u89e3\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8 SNN \u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u76f8\u4f4d\u9009\u62e9\u6a21\u5757\uff08APSM\uff09\uff0c\u4fc3\u8fdb\u7279\u5b9a\u76f8\u4f4d\u7684\u76f8\u4e92\u901a\u4fe1\u5e76\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u76f8\u4f4d\u5bf9\u8bca\u65ad\u7ed3\u679c\u7684\u5f71\u54cd\u3002\u6240\u63d0\u51fa\u7684 SDR-Former \u6846\u67b6\u5df2\u901a\u8fc7\u5bf9\u4e24\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\uff08\u4e09\u76f8 CT \u6570\u636e\u96c6\u548c\u516b\u76f8 MR \u6570\u636e\u96c6\uff09\u7684\u7efc\u5408\u5b9e\u9a8c\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u652f\u6301\u79d1\u5b66\u754c\uff0c\u6211\u4eec\u5411\u516c\u4f17\u53d1\u5e03\u7528\u4e8e\u809d\u810f\u75c5\u53d8\u5206\u6790\u7684\u5e7f\u6cdb\u591a\u76f8 MR \u6570\u636e\u96c6\u3002\u8fd9\u4e00\u5f00\u521b\u6027\u7684\u6570\u636e\u96c6\u662f\u8be5\u9886\u57df\u7b2c\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u591a\u76f8 MR \u6570\u636e\u96c6\uff0c\u4e5f\u662f MICCAI LLD-MMRI \u6311\u6218\u8d5b\u7684\u57fa\u7840\u3002\u8be5\u6570\u636e\u96c6\u53ef\u901a\u8fc7\u4ee5\u4e0b\u7f51\u5740\u8bbf\u95ee\uff1ahttps://bit.ly/3IyYlgN\u3002|[2402.17246v1](http://arxiv.org/pdf/2402.17246v1)|null|\n", "2402.17229": "|**2024-02-27**|**Preserving Fairness Generalization in Deepfake Detection**|\u5728 Deepfake \u68c0\u6d4b\u4e2d\u4fdd\u6301\u516c\u5e73\u6cdb\u5316|Li Lin, Xinan He, Yan Ju, Xin Wang, Feng Ding, Shu Hu|Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them to encourage fair learning across a flattened loss landscape. Extensive experiments on prominent deepfake datasets demonstrate our method's effectiveness, surpassing state-of-the-art approaches in preserving fairness during cross-domain deepfake detection. The code is available at https://github.com/Purdue-M2/Fairness-Generalization|\u5c3d\u7ba1\u8fd1\u5e74\u6765\u5df2\u7ecf\u5f00\u53d1\u51fa\u6709\u6548\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u4f1a\u5bfc\u81f4\u79cd\u65cf\u548c\u6027\u522b\u7b49\u4eba\u53e3\u7fa4\u4f53\u4e4b\u95f4\u7684\u4e0d\u516c\u5e73\u8868\u73b0\u5dee\u5f02\u3002\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u7279\u5b9a\u7fa4\u4f53\u9762\u4e34\u4e0d\u516c\u5e73\u7684\u76ee\u6807\u6216\u88ab\u6392\u9664\u5728\u68c0\u6d4b\u4e4b\u5916\uff0c\u4ece\u800c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u5206\u7c7b\u7684\u6df1\u5ea6\u4f2a\u9020\u54c1\u64cd\u7eb5\u516c\u4f17\u8206\u8bba\u5e76\u7834\u574f\u5bf9\u6a21\u578b\u7684\u4fe1\u4efb\u3002\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u73b0\u6709\u65b9\u6cd5\u662f\u63d0\u4f9b\u516c\u5e73\u635f\u5931\u51fd\u6570\u3002\u5b83\u5728\u57df\u5185\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u516c\u5e73\u6027\uff0c\u4f46\u5728\u8de8\u57df\u6d4b\u8bd5\u65b9\u9762\u5374\u65e0\u6cd5\u4fdd\u6301\u516c\u5e73\u6027\u3002\u8fd9\u51f8\u663e\u4e86\u516c\u5e73\u6cdb\u5316\u5728\u6253\u51fb\u6df1\u5ea6\u9020\u5047\u4e2d\u7684\u91cd\u8981\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u79cd\u901a\u8fc7\u540c\u65f6\u8003\u8651\u7279\u5f81\u3001\u635f\u5931\u548c\u4f18\u5316\u65b9\u9762\u6765\u89e3\u51b3 Deepfake \u68c0\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u6cdb\u5316\u95ee\u9898\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u89e3\u7f20\u5b66\u4e60\u6765\u63d0\u53d6\u4eba\u53e3\u7edf\u8ba1\u548c\u4e0e\u9886\u57df\u65e0\u5173\u7684\u4f2a\u9020\u7279\u5f81\uff0c\u5c06\u5b83\u4eec\u878d\u5408\u8d77\u6765\u4ee5\u9f13\u52b1\u5728\u5e73\u5766\u7684\u635f\u5931\u73af\u5883\u4e2d\u8fdb\u884c\u516c\u5e73\u5b66\u4e60\u3002\u5bf9\u8457\u540d\u7684 Deepfake \u6570\u636e\u96c6\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u8de8\u57df Deepfake \u68c0\u6d4b\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u516c\u5e73\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/Purdue-M2/Fairness-Generalization \u83b7\u53d6|[2402.17229v1](http://arxiv.org/pdf/2402.17229v1)|null|\n", "2402.17207": "|**2024-02-27**|**Deployment Prior Injection for Run-time Calibratable Object Detection**|\u7528\u4e8e\u8fd0\u884c\u65f6\u53ef\u6821\u51c6\u5bf9\u8c61\u68c0\u6d4b\u7684\u90e8\u7f72\u9884\u6ce8\u5165|Mo Zhou, Yiding Yang, Haoxiang Li, Vishal M. Patel, Gang Hua|With a strong alignment between the training and test distributions, object relation as a context prior facilitates object detection. Yet, it turns into a harmful but inevitable training set bias upon test distributions that shift differently across space and time. Nevertheless, the existing detectors cannot incorporate deployment context prior during the test phase without parameter update. Such kind of capability requires the model to explicitly learn disentangled representations with respect to context prior. To achieve this, we introduce an additional graph input to the detector, where the graph represents the deployment context prior, and its edge values represent object relations. Then, the detector behavior is trained to bound to the graph with a modified training objective. As a result, during the test phase, any suitable deployment context prior can be injected into the detector via graph edits, hence calibrating, or \"re-biasing\" the detector towards the given prior at run-time without parameter update. Even if the deployment prior is unknown, the detector can self-calibrate using deployment prior approximated using its own predictions. Comprehensive experimental results on the COCO dataset, as well as cross-dataset testing on the Objects365 dataset, demonstrate the effectiveness of the run-time calibratable detector.|\u7531\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\u4e4b\u95f4\u5177\u6709\u5f88\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u5bf9\u8c61\u5173\u7cfb\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u5148\u9a8c\u6709\u5229\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u3002\u7136\u800c\uff0c\u5b83\u4f1a\u53d8\u6210\u4e00\u79cd\u6709\u5bb3\u4f46\u4e0d\u53ef\u907f\u514d\u7684\u8bad\u7ec3\u96c6\u504f\u5dee\uff0c\u56e0\u4e3a\u6d4b\u8bd5\u5206\u5e03\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u7684\u53d8\u5316\u4e0d\u540c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u68c0\u6d4b\u5668\u65e0\u6cd5\u5728\u6d4b\u8bd5\u9636\u6bb5\u4e4b\u524d\u5728\u6ca1\u6709\u53c2\u6570\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\u7eb3\u5165\u90e8\u7f72\u4e0a\u4e0b\u6587\u3002\u8fd9\u79cd\u80fd\u529b\u8981\u6c42\u6a21\u578b\u663e\u5f0f\u5730\u5b66\u4e60\u76f8\u5bf9\u4e8e\u4e0a\u4e0b\u6587\u5148\u9a8c\u7684\u89e3\u5f00\u8868\u793a\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u5411\u68c0\u6d4b\u5668\u5f15\u5165\u4e86\u4e00\u4e2a\u989d\u5916\u7684\u56fe\u8f93\u5165\uff0c\u5176\u4e2d\u8be5\u56fe\u8868\u793a\u5148\u9a8c\u7684\u90e8\u7f72\u4e0a\u4e0b\u6587\uff0c\u5176\u8fb9\u7f18\u503c\u8868\u793a\u5bf9\u8c61\u5173\u7cfb\u3002\u7136\u540e\uff0c\u4f7f\u7528\u4fee\u6539\u540e\u7684\u8bad\u7ec3\u76ee\u6807\u8bad\u7ec3\u68c0\u6d4b\u5668\u884c\u4e3a\u4ee5\u7ed1\u5b9a\u5230\u56fe\u3002\u56e0\u6b64\uff0c\u5728\u6d4b\u8bd5\u9636\u6bb5\uff0c\u4efb\u4f55\u5408\u9002\u7684\u90e8\u7f72\u4e0a\u4e0b\u6587\u5148\u9a8c\u90fd\u53ef\u4ee5\u901a\u8fc7\u56fe\u5f62\u7f16\u8f91\u6ce8\u5165\u5230\u68c0\u6d4b\u5668\u4e2d\uff0c\u4ece\u800c\u5728\u8fd0\u884c\u65f6\u5c06\u68c0\u6d4b\u5668\u6821\u51c6\u6216\u201c\u91cd\u65b0\u504f\u7f6e\u201d\u5230\u7ed9\u5b9a\u7684\u5148\u9a8c\uff0c\u800c\u65e0\u9700\u66f4\u65b0\u53c2\u6570\u3002\u5373\u4f7f\u90e8\u7f72\u5148\u9a8c\u672a\u77e5\uff0c\u63a2\u6d4b\u5668\u4e5f\u53ef\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u9884\u6d4b\u8fd1\u4f3c\u7684\u90e8\u7f72\u5148\u9a8c\u8fdb\u884c\u81ea\u6821\u51c6\u3002 COCO \u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u4ee5\u53ca Objects365 \u6570\u636e\u96c6\u4e0a\u7684\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u8bc1\u660e\u4e86\u8fd0\u884c\u65f6\u53ef\u6821\u51c6\u68c0\u6d4b\u5668\u7684\u6709\u6548\u6027\u3002|[2402.17207v1](http://arxiv.org/pdf/2402.17207v1)|null|\n", "2402.17187": "|**2024-02-27**|**PE-MVCNet: Multi-view and Cross-modal Fusion Network for Pulmonary Embolism Prediction**|PE-MVCNet\uff1a\u7528\u4e8e\u80ba\u6813\u585e\u9884\u6d4b\u7684\u591a\u89c6\u56fe\u548c\u8de8\u6a21\u6001\u878d\u5408\u7f51\u7edc|Zhaoxin Guo, Zhipeng Wang, Ruiquan Ge, Jianxun Yu, Feiwei Qin, Yuan Tian, Yuqing Peng, Yonghong Li, Changmiao Wang|The early detection of a pulmonary embolism (PE) is critical for enhancing patient survival rates. Both image-based and non-image-based features are of utmost importance in medical classification tasks. In a clinical setting, physicians tend to rely on the contextual information provided by Electronic Medical Records (EMR) to interpret medical imaging. However, very few models effectively integrate clinical information with imaging data. To address this shortcoming, we suggest a multimodal fusion methodology, termed PE-MVCNet, which capitalizes on Computed Tomography Pulmonary Angiography imaging and EMR data. This method comprises the Image-only module with an integrated multi-view block, the EMR-only module, and the Cross-modal Attention Fusion (CMAF) module. These modules cooperate to extract comprehensive features that subsequently generate predictions for PE. We conducted experiments using the publicly accessible Stanford University Medical Center dataset, achieving an AUROC of 94.1%, an accuracy rate of 90.2%, and an F1 score of 90.6%. Our proposed model outperforms existing methodologies, corroborating that our multimodal fusion model excels compared to models that use a single data modality.|\u80ba\u6813\u585e\uff08PE\uff09\u7684\u65e9\u671f\u53d1\u73b0\u5bf9\u4e8e\u63d0\u9ad8\u60a3\u8005\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\u3002\u57fa\u4e8e\u56fe\u50cf\u548c\u975e\u56fe\u50cf\u7684\u7279\u5f81\u5728\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\u4e2d\u90fd\u81f3\u5173\u91cd\u8981\u3002\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\uff0c\u533b\u751f\u503e\u5411\u4e8e\u4f9d\u8d56\u7535\u5b50\u75c5\u5386 (EMR) \u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u89e3\u91ca\u533b\u5b66\u6210\u50cf\u3002\u7136\u800c\uff0c\u5f88\u5c11\u6709\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5c06\u4e34\u5e8a\u4fe1\u606f\u4e0e\u5f71\u50cf\u6570\u636e\u6574\u5408\u8d77\u6765\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u7f3a\u70b9\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u4e00\u79cd\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u79f0\u4e3a PE-MVCNet\uff0c\u5b83\u5229\u7528\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u80ba\u8840\u7ba1\u9020\u5f71\u6210\u50cf\u548c EMR \u6570\u636e\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u5177\u6709\u96c6\u6210\u591a\u89c6\u56fe\u5757\u7684\u4ec5\u56fe\u50cf\u6a21\u5757\u3001\u4ec5EMR\u6a21\u5757\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u878d\u5408\uff08CMAF\uff09\u6a21\u5757\u3002\u8fd9\u4e9b\u6a21\u5757\u534f\u4f5c\u63d0\u53d6\u7efc\u5408\u7279\u5f81\uff0c\u968f\u540e\u751f\u6210 PE \u9884\u6d4b\u3002\u6211\u4eec\u4f7f\u7528\u53ef\u516c\u5f00\u8bbf\u95ee\u7684\u65af\u5766\u798f\u5927\u5b66\u533b\u5b66\u4e2d\u5fc3\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u83b7\u5f97\u4e86 94.1% \u7684 AUROC\u300190.2% \u7684\u51c6\u786e\u7387\u548c 90.6% \u7684 F1 \u5206\u6570\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u6bd4\u4f7f\u7528\u5355\u4e00\u6570\u636e\u6a21\u6001\u7684\u6a21\u578b\u66f4\u4f18\u79c0\u3002|[2402.17187v1](http://arxiv.org/pdf/2402.17187v1)|null|\n", "2402.17172": "|**2024-02-27**|**Lane2Seq: Towards Unified Lane Detection via Sequence Generation**|Lane2Seq\uff1a\u901a\u8fc7\u5e8f\u5217\u751f\u6210\u5b9e\u73b0\u7edf\u4e00\u8f66\u9053\u68c0\u6d4b|Kunyang Zhou|In this paper, we present a novel sequence generation-based framework for lane detection, called Lane2Seq. It unifies various lane detection formats by casting lane detection as a sequence generation task. This is different from previous lane detection methods, which depend on well-designed task-specific head networks and corresponding loss functions. Lane2Seq only adopts a plain transformer-based encoder-decoder architecture with a simple cross-entropy loss. Additionally, we propose a new multi-format model tuning based on reinforcement learning to incorporate the task-specific knowledge into Lane2Seq. Experimental results demonstrate that such a simple sequence generation paradigm not only unifies lane detection but also achieves competitive performance on benchmarks. For example, Lane2Seq gets 97.95\\% and 97.42\\% F1 score on Tusimple and LLAMAS datasets, establishing a new state-of-the-art result for two benchmarks.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5e8f\u5217\u751f\u6210\u7684\u8f66\u9053\u68c0\u6d4b\u6846\u67b6\uff0c\u79f0\u4e3a Lane2Seq\u3002\u5b83\u901a\u8fc7\u5c06\u8f66\u9053\u68c0\u6d4b\u4f5c\u4e3a\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u6765\u7edf\u4e00\u5404\u79cd\u8f66\u9053\u68c0\u6d4b\u683c\u5f0f\u3002\u8fd9\u4e0e\u4ee5\u524d\u7684\u8f66\u9053\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u540c\uff0c\u4ee5\u524d\u7684\u8f66\u9053\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5934\u90e8\u7f51\u7edc\u548c\u76f8\u5e94\u7684\u635f\u5931\u51fd\u6570\u3002 Lane2Seq\u4ec5\u91c7\u7528\u7b80\u5355\u7684\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5177\u6709\u7b80\u5355\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u7684\u591a\u683c\u5f0f\u6a21\u578b\u8c03\u6574\uff0c\u5c06\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u77e5\u8bc6\u7eb3\u5165 Lane2Seq \u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u7b80\u5355\u7684\u5e8f\u5217\u751f\u6210\u8303\u4f8b\u4e0d\u4ec5\u7edf\u4e00\u4e86\u8f66\u9053\u68c0\u6d4b\uff0c\u800c\u4e14\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0cLane2Seq \u5728 Tusimple \u548c LLAMAS \u6570\u636e\u96c6\u4e0a\u83b7\u5f97 97.95\\% \u548c 97.42\\% F1 \u5206\u6570\uff0c\u4e3a\u4e24\u4e2a\u57fa\u51c6\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002|[2402.17172v1](http://arxiv.org/pdf/2402.17172v1)|null|\n", "2402.17165": "|**2024-02-27**|**Few-shot adaptation for morphology-independent cell instance segmentation**|\u4e0e\u5f62\u6001\u65e0\u5173\u7684\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\u7684\u5c11\u6837\u672c\u9002\u5e94|Ram J. Zaveri, Voke Brume, Gianfranco Doretto|Microscopy data collections are becoming larger and more frequent. Accurate and precise quantitative analysis tools like cell instance segmentation are necessary to benefit from them. This is challenging due to the variability in the data, which requires retraining the segmentation model to maintain high accuracy on new collections. This is needed especially for segmenting cells with elongated and non-convex morphology like bacteria. We propose to reduce the amount of annotation and computing power needed for retraining the model by introducing a few-shot domain adaptation approach that requires annotating only one to five cells of the new data to process and that quickly adapts the model to maintain high accuracy. Our results show a significant boost in accuracy after adaptation to very challenging bacteria datasets.|\u663e\u5fae\u955c\u6570\u636e\u6536\u96c6\u53d8\u5f97\u8d8a\u6765\u8d8a\u5927\u3001\u8d8a\u6765\u8d8a\u9891\u7e41\u3002\u8981\u4ece\u4e2d\u53d7\u76ca\uff0c\u9700\u8981\u51c6\u786e\u3001\u7cbe\u786e\u7684\u5b9a\u91cf\u5206\u6790\u5de5\u5177\uff08\u4f8b\u5982\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\uff09\u3002\u7531\u4e8e\u6570\u636e\u7684\u53ef\u53d8\u6027\uff0c\u8fd9\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u4ee5\u4fdd\u6301\u65b0\u96c6\u5408\u7684\u9ad8\u7cbe\u5ea6\u3002\u8fd9\u5bf9\u4e8e\u5206\u5272\u5177\u6709\u7ec6\u957f\u548c\u975e\u51f8\u5f62\u6001\u7684\u7ec6\u80de\uff08\u5982\u7ec6\u83cc\uff09\u5c24\u5176\u9700\u8981\u3002\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u5f15\u5165\u5c11\u91cf\u57df\u9002\u5e94\u65b9\u6cd5\u6765\u51cf\u5c11\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u6240\u9700\u7684\u6ce8\u91ca\u91cf\u548c\u8ba1\u7b97\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u53ea\u9700\u8981\u6ce8\u91ca\u8981\u5904\u7406\u7684\u65b0\u6570\u636e\u7684\u4e00\u5230\u4e94\u4e2a\u5355\u5143\uff0c\u5e76\u5feb\u901f\u9002\u5e94\u6a21\u578b\u4ee5\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5728\u9002\u5e94\u6781\u5177\u6311\u6218\u6027\u7684\u7ec6\u83cc\u6570\u636e\u96c6\u540e\uff0c\u51c6\u786e\u6027\u663e\u7740\u63d0\u9ad8\u3002|[2402.17165v1](http://arxiv.org/pdf/2402.17165v1)|null|\n", "2402.17159": "|**2024-02-27**|**NocPlace: Nocturnal Visual Place Recognition Using Generative and Inherited Knowledge Transfer**|NocPlace\uff1a\u4f7f\u7528\u751f\u6210\u548c\u9057\u4f20\u77e5\u8bc6\u8f6c\u79fb\u8fdb\u884c\u591c\u95f4\u89c6\u89c9\u5730\u70b9\u8bc6\u522b|Bingxi Liu, Yiqun Wang, Huaqi Tao, Tingjun Huang, Fulin Tang, Yihong Wu, Jinqiang Cui, Hong Zhang|Visual Place Recognition (VPR) is crucial in computer vision, aiming to retrieve database images similar to a query image from an extensive collection of known images. However, like many vision-related tasks, learning-based VPR often experiences a decline in performance during nighttime due to the scarcity of nighttime images. Specifically, VPR needs to address the cross-domain problem of night-to-day rather than just the issue of a single nighttime domain. In response to these issues, we present NocPlace, which leverages a generated large-scale, multi-view, nighttime VPR dataset to embed resilience against dazzling lights and extreme darkness in the learned global descriptor. Firstly, we establish a day-night urban scene dataset called NightCities, capturing diverse nighttime scenarios and lighting variations across 60 cities globally. Following this, an unpaired image-to-image translation network is trained on this dataset. Using this trained translation network, we process an existing VPR dataset, thereby obtaining its nighttime version. The NocPlace is then fine-tuned using night-style images, the original labels, and descriptors inherited from the Daytime VPR model. Comprehensive experiments on various nighttime VPR test sets reveal that NocPlace considerably surpasses previous state-of-the-art methods.|\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b (VPR) \u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u65e8\u5728\u4ece\u5927\u91cf\u5df2\u77e5\u56fe\u50cf\u4e2d\u68c0\u7d22\u4e0e\u67e5\u8be2\u56fe\u50cf\u76f8\u4f3c\u7684\u6570\u636e\u5e93\u56fe\u50cf\u3002\u7136\u800c\uff0c\u4e0e\u8bb8\u591a\u4e0e\u89c6\u89c9\u76f8\u5173\u7684\u4efb\u52a1\u4e00\u6837\uff0c\u7531\u4e8e\u591c\u95f4\u56fe\u50cf\u7684\u7a00\u7f3a\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684 VPR \u5728\u591c\u95f4\u7684\u6027\u80fd\u901a\u5e38\u4f1a\u4e0b\u964d\u3002\u5177\u4f53\u6765\u8bf4\uff0cVPR\u9700\u8981\u89e3\u51b3\u591c\u95f4\u5230\u767d\u5929\u7684\u8de8\u57df\u95ee\u9898\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5355\u4e00\u591c\u95f4\u57df\u7684\u95ee\u9898\u3002\u9488\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 NocPlace\uff0c\u5b83\u5229\u7528\u751f\u6210\u7684\u5927\u89c4\u6a21\u3001\u591a\u89c6\u56fe\u3001\u591c\u95f4 VPR \u6570\u636e\u96c6\uff0c\u5728\u5b66\u4e60\u7684\u5168\u5c40\u63cf\u8ff0\u7b26\u4e2d\u5d4c\u5165\u62b5\u5fa1\u8000\u773c\u706f\u5149\u548c\u6781\u7aef\u9ed1\u6697\u7684\u80fd\u529b\u3002\u9996\u5148\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u540d\u4e3a NightCities \u7684\u663c\u591c\u57ce\u5e02\u573a\u666f\u6570\u636e\u96c6\uff0c\u6355\u6349\u5168\u7403 60 \u4e2a\u57ce\u5e02\u7684\u4e0d\u540c\u591c\u95f4\u573a\u666f\u548c\u7167\u660e\u53d8\u5316\u3002\u63a5\u4e0b\u6765\uff0c\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e0d\u914d\u5bf9\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u7f51\u7edc\u3002\u4f7f\u7528\u8fd9\u4e2a\u7ecf\u8fc7\u8bad\u7ec3\u7684\u7ffb\u8bd1\u7f51\u7edc\uff0c\u6211\u4eec\u5904\u7406\u73b0\u6709\u7684 VPR \u6570\u636e\u96c6\uff0c\u4ece\u800c\u83b7\u5f97\u5176\u591c\u95f4\u7248\u672c\u3002\u7136\u540e\u4f7f\u7528\u591c\u95f4\u98ce\u683c\u56fe\u50cf\u3001\u539f\u59cb\u6807\u7b7e\u548c\u4ece\u767d\u5929 VPR \u6a21\u578b\u7ee7\u627f\u7684\u63cf\u8ff0\u7b26\u5bf9 NocPlace \u8fdb\u884c\u5fae\u8c03\u3002\u5bf9\u5404\u79cd\u591c\u95f4 VPR \u6d4b\u8bd5\u96c6\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cNocPlace \u5927\u5927\u8d85\u8d8a\u4e86\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2402.17159v1](http://arxiv.org/pdf/2402.17159v1)|null|\n", "2402.17134": "|**2024-02-27**|**Efficiently Leveraging Linguistic Priors for Scene Text Spotting**|\u6709\u6548\u5229\u7528\u8bed\u8a00\u5148\u9a8c\u8fdb\u884c\u573a\u666f\u6587\u672c\u8bc6\u522b|Nguyen Nguyen, Yapeng Tian, Chenliang Xu|Incorporating linguistic knowledge can improve scene text recognition, but it is questionable whether the same holds for scene text spotting, which typically involves text detection and recognition. This paper proposes a method that leverages linguistic knowledge from a large text corpus to replace the traditional one-hot encoding used in auto-regressive scene text spotting and recognition models. This allows the model to capture the relationship between characters in the same word. Additionally, we introduce a technique to generate text distributions that align well with scene text datasets, removing the need for in-domain fine-tuning. As a result, the newly created text distributions are more informative than pure one-hot encoding, leading to improved spotting and recognition performance. Our method is simple and efficient, and it can easily be integrated into existing auto-regressive-based approaches. Experimental results show that our method not only improves recognition accuracy but also enables more accurate localization of words. It significantly improves both state-of-the-art scene text spotting and recognition pipelines, achieving state-of-the-art results on several benchmarks.|\u7ed3\u5408\u8bed\u8a00\u77e5\u8bc6\u53ef\u4ee5\u6539\u5584\u573a\u666f\u6587\u672c\u8bc6\u522b\uff0c\u4f46\u5bf9\u4e8e\u901a\u5e38\u6d89\u53ca\u6587\u672c\u68c0\u6d4b\u548c\u8bc6\u522b\u7684\u573a\u666f\u6587\u672c\u8bc6\u522b\u662f\u5426\u4e5f\u540c\u6837\u5982\u6b64\u503c\u5f97\u6000\u7591\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u6587\u672c\u8bed\u6599\u5e93\u4e2d\u7684\u8bed\u8a00\u77e5\u8bc6\u6765\u53d6\u4ee3\u81ea\u56de\u5f52\u573a\u666f\u6587\u672c\u8bc6\u522b\u548c\u8bc6\u522b\u6a21\u578b\u4e2d\u4f7f\u7528\u7684\u4f20\u7edf\u5355\u70ed\u7f16\u7801\u7684\u65b9\u6cd5\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u6355\u83b7\u540c\u4e00\u5355\u8bcd\u4e2d\u5b57\u7b26\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u751f\u6210\u4e0e\u573a\u666f\u6587\u672c\u6570\u636e\u96c6\u826f\u597d\u5339\u914d\u7684\u6587\u672c\u5206\u5e03\u7684\u6280\u672f\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u57df\u5185\u5fae\u8c03\u7684\u9700\u8981\u3002\u56e0\u6b64\uff0c\u65b0\u521b\u5efa\u7684\u6587\u672c\u5206\u5e03\u6bd4\u7eaf\u7cb9\u7684\u5355\u70ed\u7f16\u7801\u63d0\u4f9b\u66f4\u591a\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u53d1\u73b0\u548c\u8bc6\u522b\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7b80\u5355\u9ad8\u6548\uff0c\u5e76\u4e14\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7684\u57fa\u4e8e\u81ea\u56de\u5f52\u7684\u65b9\u6cd5\u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bc6\u522b\u7cbe\u5ea6\uff0c\u800c\u4e14\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5b9a\u4f4d\u5355\u8bcd\u3002\u5b83\u663e\u7740\u6539\u8fdb\u4e86\u6700\u5148\u8fdb\u7684\u573a\u666f\u6587\u672c\u8bc6\u522b\u548c\u8bc6\u522b\u7ba1\u9053\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002|[2402.17134v1](http://arxiv.org/pdf/2402.17134v1)|null|\n", "2402.17133": "|**2024-02-27**|**SAM-DiffSR: Structure-Modulated Diffusion Model for Image Super-Resolution**|SAM-DiffSR\uff1a\u7528\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u7ed3\u6784\u8c03\u5236\u6269\u6563\u6a21\u578b|Chengcheng Wang, Zhiwei Hao, Yehui Tang, Jianyuan Guo, Yujie Yang, Kai Han, Yunhe Wang|Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities. But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions. With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model. However, directly integrating SAM into SR models will result in much higher computational cost. In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The diffusion model is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset. The code and dataset are available at https://github.com/lose4578/SAM-DiffSR.|\u57fa\u4e8e\u6269\u6563\u7684\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u6a21\u578b\u6700\u8fd1\u56e0\u5176\u5f3a\u5927\u7684\u6062\u590d\u80fd\u529b\u800c\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u4f46\u4f20\u7edf\u7684\u6269\u6563\u6a21\u578b\u4ece\u5355\u4e00\u5206\u5e03\u4e2d\u6267\u884c\u566a\u58f0\u91c7\u6837\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5904\u7406\u8de8\u8bed\u4e49\u533a\u57df\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u548c\u590d\u6742\u7eb9\u7406\u7684\u80fd\u529b\u3002\u968f\u7740\u5206\u6bb5\u4efb\u4f55\u6a21\u578b\uff08SAM\uff09\u7684\u6210\u529f\uff0c\u751f\u6210\u8db3\u591f\u7ec6\u7c92\u5ea6\u7684\u533a\u57df\u63a9\u6a21\u53ef\u4ee5\u589e\u5f3a\u57fa\u4e8e\u6269\u6563\u7684SR\u6a21\u578b\u7684\u7ec6\u8282\u6062\u590d\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5c06 SAM \u96c6\u6210\u5230 SR \u6a21\u578b\u4e2d\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 SAM-DiffSR \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5229\u7528\u91c7\u6837\u566a\u58f0\u8fc7\u7a0b\u4e2d SAM \u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\u4fe1\u606f\u6765\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\uff0c\u800c\u65e0\u9700\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u7ed3\u6784\u4f4d\u7f6e\u4fe1\u606f\u7f16\u7801\u5230 SAM \u7684\u5206\u5272\u63a9\u6a21\u4e2d\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5c06\u7f16\u7801\u63a9\u6a21\u8c03\u5236\u4e3a\u91c7\u6837\u566a\u58f0\uff0c\u5c06\u5176\u96c6\u6210\u5230\u524d\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\u3002\u8fd9\u79cd\u8c03\u6574\u4f7f\u6211\u4eec\u80fd\u591f\u72ec\u7acb\u8c03\u6574\u6bcf\u4e2a\u76f8\u5e94\u5206\u5272\u533a\u57df\u5185\u7684\u566a\u58f0\u5e73\u5747\u503c\u3002\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6765\u4f30\u8ba1\u8fd9\u79cd\u8c03\u5236\u566a\u58f0\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u4e0d\u4f1a\u6539\u53d8\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u65f6\u4e0d\u9700\u8981 SAM\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u6291\u5236\u4f2a\u5f71\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u4e14\u5728 DIV2K \u6570\u636e\u96c6\u4e0a\u7684 PSNR \u65b9\u9762\u6700\u5927\u8d85\u8fc7\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5 0.74 dB\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u5728 https://github.com/lose4578/SAM-DiffSR \u83b7\u53d6\u3002|[2402.17133v1](http://arxiv.org/pdf/2402.17133v1)|**[link](https://github.com/lose4578/SAM-DiffSR)**|\n", "2402.17128": "|**2024-02-27**|**OSCaR: Object State Captioning and State Change Representation**|OSCaR\uff1a\u5bf9\u8c61\u72b6\u6001\u63cf\u8ff0\u548c\u72b6\u6001\u53d8\u5316\u8868\u793a|Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu|The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large language models (MLLMs). Our experiments demonstrate that while MLLMs show some skill, they lack a full understanding of object state changes. The benchmark includes a fine-tuned model that, despite initial capabilities, requires significant improvements in accuracy and generalization ability for effective understanding of these changes. Our code and dataset are available at https://github.com/nguyennm1024/OSCaR.|\u667a\u80fd\u6a21\u578b\u63a8\u65ad\u548c\u7406\u89e3\u7269\u4f53\u72b6\u6001\u53d8\u5316\u7684\u80fd\u529b\u662f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u4e00\u4e2a\u81f3\u5173\u91cd\u8981\u4f46\u8981\u6c42\u5f88\u9ad8\u7684\u65b9\u9762\uff0c\u7279\u522b\u662f\u901a\u8fc7\u73b0\u5b9e\u4e16\u754c\u4e2d\u4eba\u7c7b\u4ea4\u4e92\u7684\u89c6\u89d2\u3002\u8fd9\u9879\u4efb\u52a1\u6d89\u53ca\u63cf\u8ff0\u590d\u6742\u7684\u89c6\u89c9\u73af\u5883\uff0c\u8bc6\u522b\u6d3b\u52a8\u5bf9\u8c61\uff0c\u5e76\u89e3\u91ca\u901a\u8fc7\u8bed\u8a00\u4f20\u8fbe\u7684\u5b83\u4eec\u7684\u53d8\u5316\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u5bf9\u8c61\u63cf\u8ff0\u548c\u72b6\u6001\u53d8\u5316\u68c0\u6d4b\u9694\u79bb\u5f00\u6765\uff0c\u53ea\u80fd\u63d0\u4f9b\u6709\u9650\u7684\u52a8\u6001\u73af\u5883\u89c6\u56fe\u3002\u800c\u4e14\uff0c\u4f9d\u9760\u4e00\u5c0f\u7ec4\u7b26\u53f7\u8bcd\u6765\u8868\u793a\u53d8\u5316\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u5bf9\u8c61\u72b6\u6001\u63cf\u8ff0\u548c\u72b6\u6001\u53d8\u5316\u8868\u793a\uff08OSCaR\uff09\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002 OSCaR \u7531 14,084 \u4e2a\u5e26\u6ce8\u91ca\u7684\u89c6\u9891\u7247\u6bb5\u7ec4\u6210\uff0c\u5176\u4e2d\u5305\u542b\u6765\u81ea\u5404\u79cd\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u96c6\u5408\u4e2d\u7684\u8fd1 1,000 \u4e2a\u72ec\u7279\u5bf9\u8c61\u3002\u5b83\u4e3a\u8bc4\u4f30\u591a\u6a21\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8bbe\u7f6e\u4e86\u4e00\u4e2a\u65b0\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136 MLLM \u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u6280\u80fd\uff0c\u4f46\u5b83\u4eec\u7f3a\u4e4f\u5bf9\u5bf9\u8c61\u72b6\u6001\u53d8\u5316\u7684\u5145\u5206\u7406\u89e3\u3002\u8be5\u57fa\u51c6\u5305\u62ec\u4e00\u4e2a\u5fae\u8c03\u6a21\u578b\uff0c\u5c3d\u7ba1\u5177\u6709\u521d\u59cb\u529f\u80fd\uff0c\u4f46\u4ecd\u9700\u8981\u663e\u7740\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4ee5\u4fbf\u6709\u6548\u7406\u89e3\u8fd9\u4e9b\u53d8\u5316\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u5728 https://github.com/nguyennm1024/OSCaR \u83b7\u53d6\u3002|[2402.17128v1](http://arxiv.org/pdf/2402.17128v1)|null|\n", "2402.17098": "|**2024-02-27**|**In Defense and Revival of Bayesian Filtering for Thermal Infrared Object Tracking**|\u70ed\u7ea2\u5916\u7269\u4f53\u8ddf\u8e2a\u8d1d\u53f6\u65af\u8fc7\u6ee4\u7684\u9632\u5fa1\u548c\u590d\u5174|Peng Gao, Shi-Min Li, Feng Gao, Fei Wang, Ru-Yue Yuan, Hamido Fujita|Deep learning-based methods monopolize the latest research in the field of thermal infrared (TIR) object tracking. However, relying solely on deep learning models to obtain better tracking results requires carefully selecting feature information that is beneficial to representing the target object and designing a reasonable template update strategy, which undoubtedly increases the difficulty of model design. Thus, recent TIR tracking methods face many challenges in complex scenarios. This paper introduces a novel Deep Bayesian Filtering (DBF) method to enhance TIR tracking in these challenging situations. DBF is distinctive in its dual-model structure: the system and observation models. The system model leverages motion data to estimate the potential positions of the target object based on two-dimensional Brownian motion, thus generating a prior probability. Following this, the observation model comes into play upon capturing the TIR image. It serves as a classifier and employs infrared information to ascertain the likelihood of these estimated positions, creating a likelihood probability. According to the guidance of the two models, the position of the target object can be determined, and the template can be dynamically updated. Experimental analysis across several benchmark datasets reveals that DBF achieves competitive performance, surpassing most existing TIR tracking methods in complex scenarios.|\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5784\u65ad\u4e86\u70ed\u7ea2\u5916\uff08TIR\uff09\u76ee\u6807\u8ddf\u8e2a\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\u3002\u7136\u800c\uff0c\u5355\u7eaf\u4f9d\u9760\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6765\u83b7\u5f97\u66f4\u597d\u7684\u8ddf\u8e2a\u7ed3\u679c\u9700\u8981\u4ed4\u7ec6\u9009\u62e9\u6709\u5229\u4e8e\u8868\u793a\u76ee\u6807\u7269\u4f53\u7684\u7279\u5f81\u4fe1\u606f\u5e76\u8bbe\u8ba1\u5408\u7406\u7684\u6a21\u677f\u66f4\u65b0\u7b56\u7565\uff0c\u8fd9\u65e0\u7591\u589e\u52a0\u4e86\u6a21\u578b\u8bbe\u8ba1\u7684\u96be\u5ea6\u3002\u56e0\u6b64\uff0c\u6700\u8fd1\u7684 TIR \u8ddf\u8e2a\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u9762\u4e34\u7740\u8bb8\u591a\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u8d1d\u53f6\u65af\u8fc7\u6ee4 (DBF) \u65b9\u6cd5\uff0c\u53ef\u5728\u8fd9\u4e9b\u5177\u6709\u6311\u6218\u6027\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a TIR \u8ddf\u8e2a\u3002 DBF \u7684\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u5176\u53cc\u6a21\u578b\u7ed3\u6784\uff1a\u7cfb\u7edf\u6a21\u578b\u548c\u89c2\u6d4b\u6a21\u578b\u3002\u7cfb\u7edf\u6a21\u578b\u5229\u7528\u8fd0\u52a8\u6570\u636e\u57fa\u4e8e\u4e8c\u7ef4\u5e03\u6717\u8fd0\u52a8\u6765\u4f30\u8ba1\u76ee\u6807\u7269\u4f53\u7684\u6f5c\u5728\u4f4d\u7f6e\uff0c\u4ece\u800c\u751f\u6210\u5148\u9a8c\u6982\u7387\u3002\u6b64\u540e\uff0c\u89c2\u5bdf\u6a21\u578b\u5728\u6355\u83b7 TIR \u56fe\u50cf\u65f6\u53d1\u6325\u4f5c\u7528\u3002\u5b83\u5145\u5f53\u5206\u7c7b\u5668\u5e76\u5229\u7528\u7ea2\u5916\u4fe1\u606f\u6765\u786e\u5b9a\u8fd9\u4e9b\u4f30\u8ba1\u4f4d\u7f6e\u7684\u53ef\u80fd\u6027\uff0c\u4ece\u800c\u521b\u5efa\u4f3c\u7136\u6982\u7387\u3002\u6839\u636e\u4e24\u4e2a\u6a21\u578b\u7684\u5f15\u5bfc\uff0c\u53ef\u4ee5\u786e\u5b9a\u76ee\u6807\u7269\u4f53\u7684\u4f4d\u7f6e\uff0c\u5e76\u4e14\u53ef\u4ee5\u52a8\u6001\u66f4\u65b0\u6a21\u677f\u3002\u5bf9\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u5206\u6790\u8868\u660e\uff0cDBF \u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u8d85\u8d8a\u4e86\u5927\u591a\u6570\u73b0\u6709\u7684 TIR \u8ddf\u8e2a\u65b9\u6cd5\u3002|[2402.17098v1](http://arxiv.org/pdf/2402.17098v1)|null|\n"}, "OCR": {"2402.17204": "|**2024-02-27**|**Advancing Generative Model Evaluation: A Novel Algorithm for Realistic Image Synthesis and Comparison in OCR System**|\u63a8\u8fdb\u751f\u6210\u6a21\u578b\u8bc4\u4f30\uff1a\u4e00\u79cd\u7528\u4e8e OCR \u7cfb\u7edf\u4e2d\u771f\u5b9e\u56fe\u50cf\u5408\u6210\u548c\u6bd4\u8f83\u7684\u65b0\u7b97\u6cd5|Majid Memari, Khaled R. Ahmed, Shahram Rahimi, Noorbakhsh Amiri Golilarz|This research addresses a critical challenge in the field of generative models, particularly in the generation and evaluation of synthetic images. Given the inherent complexity of generative models and the absence of a standardized procedure for their comparison, our study introduces a pioneering algorithm to objectively assess the realism of synthetic images. This approach significantly enhances the evaluation methodology by refining the Fr\\'echet Inception Distance (FID) score, allowing for a more precise and subjective assessment of image quality. Our algorithm is particularly tailored to address the challenges in generating and evaluating realistic images of Arabic handwritten digits, a task that has traditionally been near-impossible due to the subjective nature of realism in image generation. By providing a systematic and objective framework, our method not only enables the comparison of different generative models but also paves the way for improvements in their design and output. This breakthrough in evaluation and comparison is crucial for advancing the field of OCR, especially for scripts that present unique complexities, and sets a new standard in the generation and assessment of high-quality synthetic images.|\u8fd9\u9879\u7814\u7a76\u89e3\u51b3\u4e86\u751f\u6210\u6a21\u578b\u9886\u57df\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5408\u6210\u56fe\u50cf\u7684\u751f\u6210\u548c\u8bc4\u4f30\u65b9\u9762\u3002\u8003\u8651\u5230\u751f\u6210\u6a21\u578b\u56fa\u6709\u7684\u590d\u6742\u6027\u4ee5\u53ca\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u6bd4\u8f83\u7a0b\u5e8f\uff0c\u6211\u4eec\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u5f00\u521b\u6027\u7684\u7b97\u6cd5\u6765\u5ba2\u89c2\u5730\u8bc4\u4f30\u5408\u6210\u56fe\u50cf\u7684\u771f\u5b9e\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ec6\u5316 Fr'echet \u8d77\u59cb\u8ddd\u79bb (FID) \u5206\u6570\u663e\u7740\u589e\u5f3a\u4e86\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ece\u800c\u53ef\u4ee5\u5bf9\u56fe\u50cf\u8d28\u91cf\u8fdb\u884c\u66f4\u7cbe\u786e\u548c\u4e3b\u89c2\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u7b97\u6cd5\u4e13\u95e8\u7528\u4e8e\u89e3\u51b3\u751f\u6210\u548c\u8bc4\u4f30\u963f\u62c9\u4f2f\u624b\u5199\u6570\u5b57\u7684\u771f\u5b9e\u56fe\u50cf\u7684\u6311\u6218\uff0c\u7531\u4e8e\u56fe\u50cf\u751f\u6210\u4e2d\u771f\u5b9e\u6027\u7684\u4e3b\u89c2\u672c\u8d28\uff0c\u8fd9\u9879\u4efb\u52a1\u5728\u4f20\u7edf\u4e0a\u51e0\u4e4e\u662f\u4e0d\u53ef\u80fd\u5b8c\u6210\u7684\u3002\u901a\u8fc7\u63d0\u4f9b\u7cfb\u7edf\u548c\u5ba2\u89c2\u7684\u6846\u67b6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u4ee5\u6bd4\u8f83\u4e0d\u540c\u7684\u751f\u6210\u6a21\u578b\uff0c\u800c\u4e14\u8fd8\u4e3a\u6539\u8fdb\u5176\u8bbe\u8ba1\u548c\u8f93\u51fa\u94fa\u5e73\u4e86\u9053\u8def\u3002\u8fd9\u4e00\u8bc4\u4f30\u548c\u6bd4\u8f83\u65b9\u9762\u7684\u7a81\u7834\u5bf9\u4e8e\u63a8\u8fdb OCR \u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5448\u73b0\u72ec\u7279\u590d\u6742\u6027\u7684\u811a\u672c\uff0c\u5e76\u4e3a\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\u7684\u751f\u6210\u548c\u8bc4\u4f30\u6811\u7acb\u4e86\u65b0\u6807\u51c6\u3002|[2402.17204v1](http://arxiv.org/pdf/2402.17204v1)|null|\n"}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "LLM": {}, "Transformer": {"2402.17678": "|**2024-02-27**|**CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise Sketch Instance Guided Attention**|CAD-SIGNet\uff1a\u4f7f\u7528\u5206\u5c42\u8349\u56fe\u5b9e\u4f8b\u5f15\u5bfc\u6ce8\u610f\u529b\u4ece\u70b9\u4e91\u8fdb\u884c CAD \u8bed\u8a00\u63a8\u7406|Mohammad Sadil Khan, Elona Dupont, Sk Aziz Ali, Kseniya Cherenkova, Anis Kacem, Djamila Aouada|Reverse engineering in the realm of Computer-Aided Design (CAD) has been a longstanding aspiration, though not yet entirely realized. Its primary aim is to uncover the CAD process behind a physical object given its 3D scan. We propose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to recover the design history of a CAD model represented as a sequence of sketch-and-extrusion from an input point cloud. Our model learns visual-language representations by layer-wise cross-attention between point cloud and CAD language embedding. In particular, a new Sketch instance Guided Attention (SGA) module is proposed in order to reconstruct the fine-grained details of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet not only reconstructs a unique full design history of the corresponding CAD model given an input point cloud but also provides multiple plausible design choices. This allows for an interactive reverse engineering scenario by providing designers with multiple next-step choices along with the design process. Extensive experiments on publicly available CAD datasets showcase the effectiveness of our approach against existing baseline models in two settings, namely, full design history recovery and conditional auto-completion from point clouds.|\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1 (CAD) \u9886\u57df\u7684\u9006\u5411\u5de5\u7a0b\u4e00\u76f4\u662f\u4eba\u4eec\u957f\u671f\u4ee5\u6765\u7684\u613f\u671b\uff0c\u4f46\u5c1a\u672a\u5b8c\u5168\u5b9e\u73b0\u3002\u5176\u4e3b\u8981\u76ee\u6807\u662f\u901a\u8fc7 3D \u626b\u63cf\u63ed\u793a\u7269\u7406\u5bf9\u8c61\u80cc\u540e\u7684 CAD \u6d41\u7a0b\u3002\u6211\u4eec\u63d0\u51fa\u4e86 CAD-SIGNet\uff0c\u8fd9\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u53ef\u8bad\u7ec3\u548c\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u7528\u4e8e\u6062\u590d CAD \u6a21\u578b\u7684\u8bbe\u8ba1\u5386\u53f2\uff0c\u8be5\u6a21\u578b\u8868\u793a\u4e3a\u6765\u81ea\u8f93\u5165\u70b9\u4e91\u7684\u4e00\u7cfb\u5217\u8349\u56fe\u548c\u6324\u538b\u3002\u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7\u70b9\u4e91\u548c CAD \u8bed\u8a00\u5d4c\u5165\u4e4b\u95f4\u7684\u5206\u5c42\u4ea4\u53c9\u6ce8\u610f\u529b\u6765\u5b66\u4e60\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u3002\u7279\u522b\u662f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 Sketch \u5b9e\u4f8b\u5f15\u5bfc\u6ce8\u610f\u529b\uff08SGA\uff09\u6a21\u5757\uff0c\u4ee5\u91cd\u5efa\u8349\u56fe\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u7531\u4e8e\u5176\u81ea\u56de\u5f52\u7279\u6027\uff0cCAD-SIGNet \u4e0d\u4ec5\u53ef\u4ee5\u5728\u7ed9\u5b9a\u8f93\u5165\u70b9\u4e91\u7684\u60c5\u51b5\u4e0b\u91cd\u5efa\u76f8\u5e94 CAD \u6a21\u578b\u7684\u72ec\u7279\u5b8c\u6574\u8bbe\u8ba1\u5386\u53f2\uff0c\u800c\u4e14\u8fd8\u63d0\u4f9b\u591a\u79cd\u5408\u7406\u7684\u8bbe\u8ba1\u9009\u62e9\u3002\u8fd9\u901a\u8fc7\u4e3a\u8bbe\u8ba1\u4eba\u5458\u63d0\u4f9b\u591a\u4e2a\u540e\u7eed\u9009\u62e9\u4ee5\u53ca\u8bbe\u8ba1\u8fc7\u7a0b\u6765\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u9006\u5411\u5de5\u7a0b\u573a\u666f\u3002\u5bf9\u516c\u5f00\u53ef\u7528\u7684 CAD \u6570\u636e\u96c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e2d\u9488\u5bf9\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5373\u5b8c\u6574\u7684\u8bbe\u8ba1\u5386\u53f2\u6062\u590d\u548c\u70b9\u4e91\u7684\u6761\u4ef6\u81ea\u52a8\u5b8c\u6210\u3002|[2402.17678v1](http://arxiv.org/pdf/2402.17678v1)|null|\n", "2402.17563": "|**2024-02-27**|**Structure-Guided Adversarial Training of Diffusion Models**|\u7ed3\u6784\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u5bf9\u6297\u8bad\u7ec3|Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui|Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively.|\u6269\u6563\u6a21\u578b\u5728\u5404\u79cd\u751f\u6210\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u529f\u6548\u3002\u867d\u7136\u73b0\u6709\u6a21\u578b\u4fa7\u91cd\u4e8e\u6700\u5c0f\u5316\u6570\u636e\u5206\u5e03\u5efa\u6a21\u7684\u53bb\u566a\u5206\u6570\u5339\u914d\u635f\u5931\u7684\u52a0\u6743\u548c\uff0c\u4f46\u5b83\u4eec\u7684\u8bad\u7ec3\u4e3b\u8981\u5f3a\u8c03\u5b9e\u4f8b\u7ea7\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u6bcf\u4e2a\u5c0f\u6279\u91cf\u5185\u6709\u4ef7\u503c\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u8868\u660e\u6837\u672c\u4e4b\u95f4\u7684\u6210\u5bf9\u5173\u7cfb\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u7ed3\u6784\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u5bf9\u6297\u8bad\u7ec3\uff08SADM\uff09\u3002\u5728\u8fd9\u79cd\u5f00\u521b\u6027\u7684\u65b9\u6cd5\u4e2d\uff0c\u6211\u4eec\u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u6bcf\u4e2a\u8bad\u7ec3\u6279\u6b21\u4e2d\u6837\u672c\u4e4b\u95f4\u7684\u6d41\u5f62\u7ed3\u6784\u3002\u4e3a\u4e86\u786e\u4fdd\u6a21\u578b\u6355\u83b7\u6570\u636e\u5206\u5e03\u4e2d\u771f\u5b9e\u7684\u6d41\u5f62\u7ed3\u6784\uff0c\u6211\u4eec\u63d0\u5021\u5728\u6781\u5c0f\u6781\u5927\u6e38\u620f\u4e2d\u9488\u5bf9\u65b0\u9896\u7684\u7ed3\u6784\u9274\u522b\u5668\u5bf9\u6269\u6563\u751f\u6210\u5668\u8fdb\u884c\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u5c06\u771f\u5b9e\u7684\u6d41\u5f62\u7ed3\u6784\u4e0e\u751f\u6210\u7684\u6d41\u5f62\u7ed3\u6784\u533a\u5206\u5f00\u6765\u3002 SADM \u6781\u5927\u5730\u6539\u8fdb\u4e86\u73b0\u6709\u7684\u6269\u6563\u53d8\u6362\u5668 (DiT)\uff0c\u5e76\u5728 12 \u4e2a\u6570\u636e\u96c6\u7684\u56fe\u50cf\u751f\u6210\u548c\u8de8\u57df\u5fae\u8c03\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728 ImageNet \u4e0a\u4e3a\u7c7b\u6761\u4ef6\u56fe\u50cf\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684 FID 1.58 \u548c 2.11\u5206\u522b\u4ee5 256x256 \u548c 512x512 \u7684\u5206\u8fa8\u7387\u751f\u6210\u3002|[2402.17563v1](http://arxiv.org/pdf/2402.17563v1)|null|\n", "2402.17562": "|**2024-02-27**|**An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains**|\u6fc0\u5149\u96f7\u8fbe 3D \u7269\u4f53\u63a2\u6d4b\u5668\u5bf9\u4e0d\u53ef\u89c1\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u7684\u5b9e\u8bc1\u7814\u7a76|George Eskandar, Chongzhe Zhang, Abhishek Kaushik, Karim Guirguis, Mohamed Sayed, Bin Yang|3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving. Including 3D information via Lidar sensors improves accuracy greatly. However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications. There exist methods to adapt 3D-ODs to these domains; however, these methods treat 3D-ODs as a black box, neglecting underlying architectural decisions and source-domain training strategies. Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation.   We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies. We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location.   Our main findings are: (1) transformer backbones with local point features are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data. We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs.|3D \u7269\u4f53\u68c0\u6d4b\u5668 (3D-OD) \u5bf9\u4e8e\u4e86\u89e3\u8bb8\u591a\u673a\u5668\u4eba\u4efb\u52a1\uff08\u5c24\u5176\u662f\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u7684\u73af\u5883\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u5305\u542b 3D \u4fe1\u606f\u53ef\u5927\u5927\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u63a2\u6d4b\u5668\u5728\u672a\u7ecf\u8bad\u7ec3\u7684\u9886\u57df\uff08\u5373\u4e0d\u540c\u4f4d\u7f6e\u3001\u4f20\u611f\u5668\u3001\u5929\u6c14\u7b49\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b89\u5168\u5173\u952e\u578b\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002\u5b58\u5728\u4f7f 3D-OD \u9002\u5e94\u8fd9\u4e9b\u9886\u57df\u7684\u65b9\u6cd5\uff1b\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5c06 3D-OD \u89c6\u4e3a\u9ed1\u5323\u5b50\uff0c\u5ffd\u7565\u4e86\u5e95\u5c42\u67b6\u6784\u51b3\u7b56\u548c\u6e90\u57df\u8bad\u7ec3\u7b56\u7565\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76 3D-OD \u7684\u7ec6\u8282\uff0c\u5c06\u6211\u4eec\u7684\u7cbe\u529b\u96c6\u4e2d\u5728\u5f71\u54cd\u57df\u9002\u5e94\u4e4b\u524d\u7684\u9c81\u68d2\u6027\u7684\u57fa\u672c\u56e0\u7d20\u4e0a\u3002\u6211\u4eec\u7cfb\u7edf\u5730\u7814\u7a76\u4e86 3D-OD \u9c81\u68d2\u6027\u548c\u9886\u57df\u9002\u5e94\u4e2d\u7ecf\u5e38\u88ab\u5ffd\u89c6\u7684\u56db\u79cd\u8bbe\u8ba1\u9009\u62e9\uff08\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff09\uff1a\u67b6\u6784\u3001\u4f53\u7d20\u7f16\u7801\u3001\u6570\u636e\u589e\u5f3a\u548c\u951a\u5b9a\u7b56\u7565\u3002\u6211\u4eec\u901a\u8fc7\u516d\u4e2a\u57fa\u51c6\u8bc4\u4f30\u4e86\u5b83\u4eec\u5bf9\u4e5d\u79cd\u6700\u5148\u8fdb\u7684 3D-OD \u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u8fd9\u4e9b\u57fa\u51c6\u6db5\u76d6\u4e09\u79cd\u7c7b\u578b\u7684\u9886\u57df\u5dee\u8ddd - \u4f20\u611f\u5668\u7c7b\u578b\u3001\u5929\u6c14\u548c\u4f4d\u7f6e\u3002\u6211\u4eec\u7684\u4e3b\u8981\u53d1\u73b0\u662f\uff1a(1) \u5177\u6709\u5c40\u90e8\u70b9\u7279\u5f81\u7684 Transformer \u4e3b\u5e72\u6bd4 3D CNN \u66f4\u7a33\u5065\uff0c(2) \u6d4b\u8bd5\u65f6\u951a\u70b9\u5927\u5c0f\u8c03\u6574\u5bf9\u4e8e\u8de8\u5730\u7406\u4f4d\u7f6e\u7684\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u663e\u7740\u63d0\u9ad8\u5206\u6570\uff0c(3) \u6e90\u57df\u589e\u5f3a\u529f\u80fd\u4f7f\u6a21\u578b\u80fd\u591f\u63a8\u5e7f\u5230\u4f4e\u5206\u8fa8\u7387\u4f20\u611f\u5668\uff0c\u5e76\u4e14\uff084\uff09\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u4e0e\u4f7f\u7528\u6076\u52a3\u5929\u6c14\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u76f8\u6bd4\uff0c\u76f4\u63a5\u4f7f\u7528\u66f4\u5e72\u51c0\u7684\u5929\u6c14\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u5bf9\u6076\u52a3\u5929\u6c14\u7684\u9c81\u68d2\u6027\u5f97\u5230\u4e86\u63d0\u9ad8\u3002\u6211\u4eec\u6982\u8ff0\u4e86\u6211\u4eec\u7684\u4e3b\u8981\u7ed3\u8bba\u548c\u53d1\u73b0\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684 3D-OD \u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002|[2402.17562v1](http://arxiv.org/pdf/2402.17562v1)|null|\n", "2402.17525": "|**2024-02-27**|**Diffusion Model-Based Image Editing: A Survey**|\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\uff1a\u8c03\u67e5|Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, Liangliang Cao|Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods.|\u53bb\u566a\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u5404\u79cd\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u4fc3\u8fdb\u4ee5\u65e0\u6761\u4ef6\u6216\u8f93\u5165\u6761\u4ef6\u65b9\u5f0f\u5408\u6210\u89c6\u89c9\u5185\u5bb9\u3002\u5b83\u4eec\u80cc\u540e\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5b66\u4e60\u9006\u8f6c\u9010\u6e10\u5411\u56fe\u50cf\u6dfb\u52a0\u566a\u58f0\u7684\u8fc7\u7a0b\uff0c\u4ece\u800c\u4f7f\u5b83\u4eec\u80fd\u591f\u4ece\u590d\u6742\u7684\u5206\u5e03\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6837\u672c\u3002\u5728\u672c\u6b21\u8c03\u67e5\u4e2d\uff0c\u6211\u4eec\u5bf9\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u7f16\u8f91\u7684\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u8be6\u5c3d\u7684\u6982\u8ff0\uff0c\u6db5\u76d6\u4e86\u8be5\u9886\u57df\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u65b9\u9762\u3002\u6211\u4eec\u4ece\u591a\u4e2a\u89d2\u5ea6\u5bf9\u8fd9\u4e9b\u4f5c\u54c1\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u5206\u6790\u548c\u5206\u7c7b\uff0c\u5305\u62ec\u5b66\u4e60\u7b56\u7565\u3001\u7528\u6237\u8f93\u5165\u6761\u4ef6\u4ee5\u53ca\u53ef\u4ee5\u5b8c\u6210\u7684\u4e00\u7cfb\u5217\u7279\u5b9a\u7f16\u8f91\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7279\u522b\u5173\u6ce8\u56fe\u50cf\u4fee\u590d\u548c\u4fee\u590d\uff0c\u63a2\u7d22\u65e9\u671f\u7684\u4f20\u7edf\u4e0a\u4e0b\u6587\u9a71\u52a8\u548c\u5f53\u524d\u7684\u591a\u6a21\u6001\u6761\u4ef6\u65b9\u6cd5\uff0c\u5bf9\u5176\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u5206\u6790\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u8bc4\u4f30\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u57fa\u51c6 EditEval\uff0c\u5176\u5177\u6709\u521b\u65b0\u6307\u6807 LMM Score\u3002\u6700\u540e\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u5f53\u524d\u7684\u5c40\u9650\u6027\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u7684\u4e00\u4e9b\u6f5c\u5728\u65b9\u5411\u3002\u968f\u9644\u7684\u5b58\u50a8\u5e93\u53d1\u5e03\u4e8e https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods\u3002|[2402.17525v1](http://arxiv.org/pdf/2402.17525v1)|null|\n", "2402.17507": "|**2024-02-27**|**Interactive Multi-Head Self-Attention with Linear Complexity**|\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u4ea4\u4e92\u5f0f\u591a\u5934\u81ea\u6ce8\u610f\u529b|Hankyul Kang, Ming-Hsuan Yang, Jongbin Ryu|We propose an efficient interactive method for multi-head self-attention via decomposition. For existing methods using multi-head self-attention, the attention operation of each head is computed independently. However, we show that the interactions between cross-heads of the attention matrix enhance the information flow of the attention operation. Considering that the attention matrix of each head can be seen as a feature of networks, it is beneficial to establish connectivity between them to capture interactions better. However, a straightforward approach to capture the interactions between the cross-heads is computationally prohibitive as the complexity grows substantially with the high dimension of an attention matrix. In this work, we propose an effective method to decompose the attention operation into query- and key-less components. This will result in a more manageable size for the attention matrix, specifically for the cross-head interactions. Expensive experimental results show that the proposed cross-head interaction approach performs favorably against existing efficient attention methods and state-of-the-art backbone models.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u89e3\u8fdb\u884c\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7684\u6709\u6548\u4ea4\u4e92\u65b9\u6cd5\u3002\u5bf9\u4e8e\u4f7f\u7528\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u5934\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u662f\u72ec\u7acb\u8ba1\u7b97\u7684\u3002\u7136\u800c\uff0c\u6211\u4eec\u8868\u660e\u6ce8\u610f\u529b\u77e9\u9635\u5341\u5b57\u5934\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u589e\u5f3a\u4e86\u6ce8\u610f\u529b\u64cd\u4f5c\u7684\u4fe1\u606f\u6d41\u3002\u8003\u8651\u5230\u6bcf\u4e2a\u5934\u7684\u6ce8\u610f\u529b\u77e9\u9635\u53ef\u4ee5\u88ab\u89c6\u4e3a\u7f51\u7edc\u7684\u4e00\u4e2a\u7279\u5f81\uff0c\u56e0\u6b64\u6709\u5229\u4e8e\u5728\u5b83\u4eec\u4e4b\u95f4\u5efa\u7acb\u8fde\u63a5\u4ee5\u66f4\u597d\u5730\u6355\u83b7\u4ea4\u4e92\u3002\u7136\u800c\uff0c\u6355\u83b7\u5341\u5b57\u5934\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u76f4\u63a5\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u662f\u4ee4\u4eba\u671b\u800c\u5374\u6b65\u7684\uff0c\u56e0\u4e3a\u590d\u6742\u6027\u968f\u7740\u6ce8\u610f\u529b\u77e9\u9635\u7684\u9ad8\u7ef4\u5ea6\u800c\u5927\u5e45\u589e\u957f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u5c06\u6ce8\u610f\u529b\u64cd\u4f5c\u5206\u89e3\u4e3a\u65e0\u67e5\u8be2\u548c\u65e0\u952e\u7ec4\u4ef6\u3002\u8fd9\u5c06\u4f7f\u6ce8\u610f\u529b\u77e9\u9635\u7684\u5927\u5c0f\u66f4\u6613\u4e8e\u7ba1\u7406\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5341\u5b57\u5934\u4ea4\u4e92\u3002\u6602\u8d35\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8de8\u5934\u4ea4\u4e92\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u73b0\u6709\u7684\u9ad8\u6548\u6ce8\u610f\u529b\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u9aa8\u5e72\u6a21\u578b\u8868\u73b0\u826f\u597d\u3002|[2402.17507v1](http://arxiv.org/pdf/2402.17507v1)|null|\n", "2402.17487": "|**2024-02-27**|**Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model**|JPEG-AI\u9a8c\u8bc1\u6a21\u578b\u4e2d\u7684\u6bd4\u7279\u7387\u5339\u914d\u7b97\u6cd5\u4f18\u5316|Panqi Jia, A. Burakhan Koyuncu, Jue Mao, Ze Cui, Yi Ma, Tiansheng Guo, Timofey Solovyev, Alexander Karabutov, Yin Zhao, Jing Wang, et.al.|The research on neural network (NN) based image compression has shown superior performance compared to classical compression frameworks. Unlike the hand-engineered transforms in the classical frameworks, NN-based models learn the non-linear transforms providing more compact bit representations, and achieve faster coding speed on parallel devices over their classical counterparts. Those properties evoked the attention of both scientific and industrial communities, resulting in the standardization activity JPEG-AI. The verification model for the standardization process of JPEG-AI is already in development and has surpassed the advanced VVC intra codec. To generate reconstructed images with the desired bits per pixel and assess the BD-rate performance of both the JPEG-AI verification model and VVC intra, bit rate matching is employed. However, the current state of the JPEG-AI verification model experiences significant slowdowns during bit rate matching, resulting in suboptimal performance due to an unsuitable model. The proposed methodology offers a gradual algorithmic optimization for matching bit rates, resulting in a fourfold acceleration and over 1% improvement in BD-rate at the base operation point. At the high operation point, the acceleration increases up to sixfold.|\u4e0e\u7ecf\u5178\u538b\u7f29\u6846\u67b6\u76f8\u6bd4\uff0c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u7684\u56fe\u50cf\u538b\u7f29\u7814\u7a76\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u4e0e\u7ecf\u5178\u6846\u67b6\u4e2d\u7684\u624b\u5de5\u8bbe\u8ba1\u53d8\u6362\u4e0d\u540c\uff0c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\u5b66\u4e60\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u63d0\u4f9b\u66f4\u7d27\u51d1\u7684\u4f4d\u8868\u793a\uff0c\u5e76\u5728\u5e76\u884c\u8bbe\u5907\u4e0a\u5b9e\u73b0\u6bd4\u7ecf\u5178\u6a21\u578b\u66f4\u5feb\u7684\u7f16\u7801\u901f\u5ea6\u3002\u8fd9\u4e9b\u7279\u6027\u5f15\u8d77\u4e86\u79d1\u5b66\u754c\u548c\u5de5\u4e1a\u754c\u7684\u5173\u6ce8\uff0c\u4ece\u800c\u50ac\u751f\u4e86 JPEG-AI \u6807\u51c6\u5316\u6d3b\u52a8\u3002 JPEG-AI\u6807\u51c6\u5316\u8fc7\u7a0b\u7684\u9a8c\u8bc1\u6a21\u578b\u5df2\u7ecf\u5728\u5f00\u53d1\u4e2d\uff0c\u5e76\u4e14\u5df2\u7ecf\u8d85\u8d8a\u4e86\u5148\u8fdb\u7684VVC\u5e27\u5185\u7f16\u89e3\u7801\u5668\u3002\u4e3a\u4e86\u751f\u6210\u5177\u6709\u6240\u9700\u6bcf\u50cf\u7d20\u4f4d\u6570\u7684\u91cd\u5efa\u56fe\u50cf\u5e76\u8bc4\u4f30 JPEG-AI \u9a8c\u8bc1\u6a21\u578b\u548c VVC \u5e27\u5185\u7684 BD \u901f\u7387\u6027\u80fd\uff0c\u91c7\u7528\u4e86\u6bd4\u7279\u7387\u5339\u914d\u3002\u7136\u800c\uff0cJPEG-AI \u9a8c\u8bc1\u6a21\u578b\u7684\u5f53\u524d\u72b6\u6001\u5728\u6bd4\u7279\u7387\u5339\u914d\u671f\u95f4\u7ecf\u5386\u4e86\u663e\u7740\u7684\u51cf\u901f\uff0c\u5bfc\u81f4\u7531\u4e8e\u6a21\u578b\u4e0d\u5408\u9002\u800c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7406\u60f3\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5339\u914d\u6bd4\u7279\u7387\u7684\u6e10\u8fdb\u7b97\u6cd5\u4f18\u5316\uff0c\u4ece\u800c\u5728\u57fa\u672c\u64cd\u4f5c\u70b9\u5b9e\u73b0\u56db\u500d\u52a0\u901f\u548c BD \u901f\u7387\u63d0\u9ad8\u8d85\u8fc7 1%\u3002\u5728\u9ad8\u64cd\u4f5c\u70b9\uff0c\u52a0\u901f\u5ea6\u589e\u52a0\u81f3\u516d\u500d\u3002|[2402.17487v1](http://arxiv.org/pdf/2402.17487v1)|null|\n", "2402.17430": "|**2024-02-27**|**Leveraging Enhanced Queries of Point Sets for Vectorized Map Construction**|\u5229\u7528\u589e\u5f3a\u7684\u70b9\u96c6\u67e5\u8be2\u8fdb\u884c\u77e2\u91cf\u5316\u5730\u56fe\u6784\u5efa|Zihao Liu, Xiaoyu Zhang, Guangwei Liu, Ji Zhao, Ningyi Xu|In autonomous driving, the high-definition (HD) map plays a crucial role in localization and planning. Recently, several methods have facilitated end-to-end online map construction in DETR-like frameworks. However, little attention has been paid to the potential capabilities of exploring the query mechanism. This paper introduces MapQR, an end-to-end method with an emphasis on enhancing query capabilities for constructing online vectorized maps. Although the map construction is essentially a point set prediction task, MapQR utilizes instance queries rather than point queries. These instance queries are scattered for the prediction of point sets and subsequently gathered for the final matching. This query design, called the scatter-and-gather query, shares content information in the same map element and avoids possible inconsistency of content information in point queries. We further exploit prior information to enhance an instance query by adding positional information embedded from their reference points. Together with a simple and effective improvement of a BEV encoder, the proposed MapQR achieves the best mean average precision (mAP) and maintains good efficiency on both nuScenes and Argoverse 2. In addition, integrating our query design into other models can boost their performance significantly. The code will be available at https://github.com/HXMap/MapQR.|\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u9ad8\u6e05\u5730\u56fe\u5728\u5b9a\u4f4d\u548c\u89c4\u5212\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u6700\u8fd1\uff0c\u6709\u51e0\u79cd\u65b9\u6cd5\u4fc3\u8fdb\u4e86\u7c7b\u4f3c DETR \u6846\u67b6\u4e2d\u7684\u7aef\u5230\u7aef\u5728\u7ebf\u5730\u56fe\u6784\u5efa\u3002\u7136\u800c\uff0c\u5f88\u5c11\u6709\u4eba\u5173\u6ce8\u63a2\u7d22\u67e5\u8be2\u673a\u5236\u7684\u6f5c\u5728\u80fd\u529b\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 MapQR\uff0c\u8fd9\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u91cd\u70b9\u662f\u589e\u5f3a\u6784\u5efa\u5728\u7ebf\u77e2\u91cf\u5316\u5730\u56fe\u7684\u67e5\u8be2\u80fd\u529b\u3002\u867d\u7136\u5730\u56fe\u6784\u5efa\u672c\u8d28\u4e0a\u662f\u70b9\u96c6\u9884\u6d4b\u4efb\u52a1\uff0c\u4f46 MapQR \u4f7f\u7528\u5b9e\u4f8b\u67e5\u8be2\u800c\u4e0d\u662f\u70b9\u67e5\u8be2\u3002\u8fd9\u4e9b\u5b9e\u4f8b\u67e5\u8be2\u88ab\u5206\u6563\u4ee5\u7528\u4e8e\u70b9\u96c6\u7684\u9884\u6d4b\uff0c\u5e76\u968f\u540e\u88ab\u805a\u96c6\u4ee5\u7528\u4e8e\u6700\u7ec8\u5339\u914d\u3002\u8fd9\u79cd\u67e5\u8be2\u8bbe\u8ba1\u79f0\u4e3a\u5206\u6563\u548c\u805a\u96c6\u67e5\u8be2\uff0c\u5728\u540c\u4e00\u5730\u56fe\u5143\u7d20\u4e2d\u5171\u4eab\u5185\u5bb9\u4fe1\u606f\uff0c\u5e76\u907f\u514d\u70b9\u67e5\u8be2\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u5185\u5bb9\u4fe1\u606f\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5229\u7528\u5148\u9a8c\u4fe1\u606f\u901a\u8fc7\u6dfb\u52a0\u4ece\u53c2\u8003\u70b9\u5d4c\u5165\u7684\u4f4d\u7f6e\u4fe1\u606f\u6765\u589e\u5f3a\u5b9e\u4f8b\u67e5\u8be2\u3002\u52a0\u4e0a\u5bf9 BEV \u7f16\u7801\u5668\u7684\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\uff0c\u6240\u63d0\u51fa\u7684 MapQR \u5b9e\u73b0\u4e86\u6700\u4f73\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\uff0c\u5e76\u5728 nuScenes \u548c Argoverse 2 \u4e0a\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6548\u7387\u3002\u6b64\u5916\uff0c\u5c06\u6211\u4eec\u7684\u67e5\u8be2\u8bbe\u8ba1\u96c6\u6210\u5230\u5176\u4ed6\u6a21\u578b\u4e2d\u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u5176\u6027\u80fd\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/HXMap/MapQR \u4e0a\u83b7\u53d6\u3002|[2402.17430v1](http://arxiv.org/pdf/2402.17430v1)|null|\n", "2402.17406": "|**2024-02-27**|**LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning**|LSPT\uff1a\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u957f\u671f\u7a7a\u95f4\u63d0\u793a\u8c03\u6574|Shentong Mo, Yansen Wang, Xufang Luo, Dongsheng Li|Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameters acquired from earlier blocks. Further enhancing its prowess, LSPT brings into play patch tokens, serving as spatial coding. This is strategically designed to perpetually amass class-conscious features, thereby fortifying the model's prowess in distinguishing and identifying visual categories. To validate the efficacy of our proposed method, we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks. Our empirical findings underscore the superiority of LSPT, showcasing its ability to set new benchmarks in visual prompt tuning performance.|\u89c6\u89c9\u63d0\u793a\u8c03\u6574\uff08VPT\uff09\u6280\u672f\u56e0\u5176\u4f7f\u7528\u79f0\u4e3a\u63d0\u793a\u7684\u4e13\u95e8\u53ef\u5b66\u4e60\u6807\u8bb0\u4f7f\u9884\u5148\u8bad\u7ec3\u7684\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u9002\u5e94\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u7684\u80fd\u529b\u800c\u53d7\u5230\u5173\u6ce8\u3002\u5f53\u4ee3 VPT \u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u4e0e\u81ea\u76d1\u7763\u89c6\u89c9\u8f6c\u6362\u5668\u4e00\u8d77\u4f7f\u7528\u65f6\uff0c\u901a\u5e38\u9ed8\u8ba4\u5f15\u5165\u4e3b\u8981\u6765\u81ea\u6a21\u578b\u5148\u524d\u5757\u7684\u65b0\u7684\u53ef\u5b66\u4e60\u63d0\u793a\u6216\u95e8\u63a7\u63d0\u793a\u6807\u8bb0\u3002\u8fd9\u79cd\u65b9\u6cd5\u7684\u4e00\u4e2a\u5173\u952e\u76d1\u7763\u662f\u5b83\u4eec\u672a\u80fd\u5229\u7528\u8fdc\u7a0b\u5148\u524d\u533a\u5757\u7684\u6f5c\u529b\u4f5c\u4e3a\u6bcf\u4e2a\u81ea\u6211\u76d1\u7763\u7684 ViT \u4e2d\u7684\u63d0\u793a\u6765\u6e90\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5173\u952e\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u957f\u671f\u7a7a\u95f4\u63d0\u793a\u8c03\u6574\uff08LSPT\uff09\u2014\u2014\u4e00\u79cd\u89c6\u89c9\u8868\u5f81\u5b66\u4e60\u7684\u9769\u547d\u6027\u65b9\u6cd5\u3002 LSPT \u4ece\u4eba\u7c7b\u5927\u8111\u7684\u590d\u6742\u6027\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u5de7\u5999\u5730\u878d\u5165\u4e86\u957f\u671f\u95e8\u63a7\u63d0\u793a\u3002\u6b64\u529f\u80fd\u7528\u4f5c\u65f6\u95f4\u7f16\u7801\uff0c\u6291\u5236\u5fd8\u8bb0\u4ece\u65e9\u671f\u5757\u83b7\u53d6\u7684\u53c2\u6570\u7684\u98ce\u9669\u3002 LSPT \u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5176\u80fd\u529b\uff0c\u5f15\u5165\u4e86\u8865\u4e01\u6807\u8bb0\uff0c\u5145\u5f53\u7a7a\u95f4\u7f16\u7801\u3002\u8fd9\u662f\u6218\u7565\u6027\u8bbe\u8ba1\u7684\uff0c\u65e8\u5728\u6c38\u4e45\u79ef\u7d2f\u9636\u7ea7\u610f\u8bc6\u7279\u5f81\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u533a\u5206\u548c\u8bc6\u522b\u89c6\u89c9\u7c7b\u522b\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728 5 \u4e2a FGVC \u548c 19 \u4e2a VTAB-1K \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u5b9e\u9a8c\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86 LSPT \u7684\u4f18\u8d8a\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u89c6\u89c9\u63d0\u793a\u8c03\u6574\u6027\u80fd\u65b9\u9762\u8bbe\u5b9a\u65b0\u57fa\u51c6\u7684\u80fd\u529b\u3002|[2402.17406v1](http://arxiv.org/pdf/2402.17406v1)|null|\n", "2402.17360": "|**2024-02-27**|**CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer**|CAPT\uff1a\u4f7f\u7528 Transformer \u4ece\u5355\u70b9\u4e91\u8fdb\u884c\u7c7b\u522b\u7ea7\u6e05\u6670\u5ea6\u4f30\u8ba1|Lian Fu, Ryoichi Ishikawa, Yoshihiro Sato, Takeshi Oishi|The ability to estimate joint parameters is essential for various applications in robotics and computer vision. In this paper, we propose CAPT: category-level articulation estimation from a point cloud using Transformer. CAPT uses an end-to-end transformer-based architecture for joint parameter and state estimation of articulated objects from a single point cloud. The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness. The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects. Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation. Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation. Our research provides a promising solution for applying Transformer-based architectures in articulated object analysis.|\u4f30\u8ba1\u5173\u8282\u53c2\u6570\u7684\u80fd\u529b\u5bf9\u4e8e\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5404\u79cd\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa CAPT\uff1a\u4f7f\u7528 Transformer \u4ece\u70b9\u4e91\u8fdb\u884c\u7c7b\u522b\u7ea7\u6e05\u6670\u5ea6\u4f30\u8ba1\u3002 CAPT \u4f7f\u7528\u57fa\u4e8e\u7aef\u5230\u7aef\u53d8\u538b\u5668\u7684\u67b6\u6784\uff0c\u4ece\u5355\u70b9\u4e91\u5bf9\u5173\u8282\u5bf9\u8c61\u8fdb\u884c\u5173\u8282\u53c2\u6570\u548c\u72b6\u6001\u4f30\u8ba1\u3002\u6240\u63d0\u51fa\u7684 CAPT \u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u5404\u79cd\u5173\u8282\u5bf9\u8c61\u7684\u5173\u8282\u53c2\u6570\u548c\u72b6\u6001\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002\u8be5\u8bba\u6587\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u79cd\u8fd0\u52a8\u635f\u5931\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f3a\u8c03\u5173\u8282\u5bf9\u8c61\u7684\u52a8\u6001\u7279\u5f81\u6765\u63d0\u9ad8\u5173\u8282\u4f30\u8ba1\u6027\u80fd\u3002\u6b64\u5916\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u53cc\u91cd\u6295\u7968\u7b56\u7565\uff0c\u4e3a\u6846\u67b6\u63d0\u4f9b\u4ece\u7c97\u5230\u7ec6\u7684\u53c2\u6570\u4f30\u8ba1\u3002\u51e0\u4e2a\u7c7b\u522b\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6e05\u6670\u5ea6\u4f30\u8ba1\u66ff\u4ee3\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u4e3a\u5728\u94f0\u63a5\u5f0f\u5bf9\u8c61\u5206\u6790\u4e2d\u5e94\u7528\u57fa\u4e8e Transformer \u7684\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002|[2402.17360v1](http://arxiv.org/pdf/2402.17360v1)|null|\n", "2402.17237": "|**2024-02-27**|**Image-Text Matching with Multi-View Attention**|\u5177\u6709\u591a\u89c6\u56fe\u6ce8\u610f\u529b\u7684\u56fe\u50cf\u6587\u672c\u5339\u914d|Rui Cheng, Wanqing Cui|Existing two-stream models for image-text matching show good performance while ensuring retrieval speed and have received extensive attention from industry and academia. These methods use a single representation to encode image and text separately and get a matching score with cosine similarity or the inner product of vectors. However, the performance of the two-stream model is often sub-optimal. On the one hand, a single representation is challenging to cover complex content comprehensively. On the other hand, in this framework of lack of interaction, it is challenging to match multiple meanings which leads to information being ignored. To address the problems mentioned above and facilitate the performance of the two-stream model, we propose a multi-view attention approach for two-stream image-text matching MVAM (\\textbf{M}ulti-\\textbf{V}iew \\textbf{A}ttention \\textbf{M}odel). It first learns multiple image and text representations by diverse attention heads with different view codes. And then concatenate these representations into one for matching. A diversity objective is also used to promote diversity between attention heads. With this method, models are able to encode images and text from different views and attend to more key points. So we can get representations that contain more information. When doing retrieval tasks, the matching scores between images and texts can be calculated from different aspects, leading to better matching performance. Experiment results on MSCOCO and Flickr30K show that our proposed model brings improvements over existing models. Further case studies show that different attention heads can focus on different contents and finally obtain a more comprehensive representation.|\u73b0\u6709\u7684\u56fe\u6587\u5339\u914d\u53cc\u6d41\u6a21\u578b\u5728\u4fdd\u8bc1\u68c0\u7d22\u901f\u5ea6\u7684\u540c\u65f6\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u53d7\u5230\u4e86\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u7684\u5e7f\u6cdb\u5173\u6ce8\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u8868\u793a\u5f62\u5f0f\u5206\u522b\u5bf9\u56fe\u50cf\u548c\u6587\u672c\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6216\u5411\u91cf\u7684\u5185\u79ef\u83b7\u5f97\u5339\u914d\u5206\u6570\u3002\u7136\u800c\uff0c\u53cc\u6d41\u6a21\u578b\u7684\u6027\u80fd\u901a\u5e38\u4e0d\u662f\u6700\u4f18\u7684\u3002\u4e00\u65b9\u9762\uff0c\u5355\u4e00\u7684\u8868\u793a\u5f88\u96be\u5168\u9762\u8986\u76d6\u590d\u6742\u7684\u5185\u5bb9\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5728\u8fd9\u79cd\u7f3a\u4e4f\u4ea4\u4e92\u7684\u6846\u67b6\u4e2d\uff0c\u5339\u914d\u591a\u4e2a\u542b\u4e49\u5177\u6709\u6311\u6218\u6027\uff0c\u4ece\u800c\u5bfc\u81f4\u4fe1\u606f\u88ab\u5ffd\u7565\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u5e76\u63d0\u9ad8\u53cc\u6d41\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53cc\u6d41\u56fe\u50cf\u6587\u672c\u5339\u914d MVAM \u7684\u591a\u89c6\u56fe\u6ce8\u610f\u65b9\u6cd5\uff08\\textbf{M}ulti-\\textbf{V}iew \\textbf {A}\u6ce8\u610f\\textbf{M}\u6a21\u578b\uff09\u3002\u5b83\u9996\u5148\u901a\u8fc7\u5177\u6709\u4e0d\u540c\u89c6\u56fe\u4ee3\u7801\u7684\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u5b66\u4e60\u591a\u4e2a\u56fe\u50cf\u548c\u6587\u672c\u8868\u793a\u3002\u7136\u540e\u5c06\u8fd9\u4e9b\u8868\u793a\u8fde\u63a5\u6210\u4e00\u4e2a\u4ee5\u8fdb\u884c\u5339\u914d\u3002\u591a\u6837\u6027\u76ee\u6807\u4e5f\u7528\u4e8e\u4fc3\u8fdb\u6ce8\u610f\u529b\u5934\u4e4b\u95f4\u7684\u591a\u6837\u6027\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u6a21\u578b\u80fd\u591f\u4ece\u4e0d\u540c\u7684\u89d2\u5ea6\u5bf9\u56fe\u50cf\u548c\u6587\u672c\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u5173\u6ce8\u66f4\u591a\u7684\u5173\u952e\u70b9\u3002\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97\u5305\u542b\u66f4\u591a\u4fe1\u606f\u7684\u8868\u793a\u3002\u5728\u8fdb\u884c\u68c0\u7d22\u4efb\u52a1\u65f6\uff0c\u53ef\u4ee5\u4ece\u4e0d\u540c\u65b9\u9762\u8ba1\u7b97\u56fe\u50cf\u548c\u6587\u672c\u4e4b\u95f4\u7684\u5339\u914d\u5206\u6570\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u597d\u7684\u5339\u914d\u6027\u80fd\u3002 MSCOCO \u548c Flickr30K \u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u6bd4\u73b0\u6709\u6a21\u578b\u5e26\u6765\u4e86\u6539\u8fdb\u3002\u8fdb\u4e00\u6b65\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u5934\u53ef\u4ee5\u5173\u6ce8\u4e0d\u540c\u7684\u5185\u5bb9\uff0c\u6700\u7ec8\u83b7\u5f97\u66f4\u5168\u9762\u7684\u8868\u5f81\u3002|[2402.17237v1](http://arxiv.org/pdf/2402.17237v1)|null|\n", "2402.17228": "|**2024-02-27**|**Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology**|\u7279\u5f81\u91cd\u65b0\u5d4c\u5165\uff1a\u8fc8\u5411\u8ba1\u7b97\u75c5\u7406\u5b66\u7684\u57fa\u7840\u6a21\u578b\u7ea7\u6027\u80fd|Wenhao Tang, Fengtao Zhou, Sheng Huang, Xiang Zhu, Yi Zhang, Bo Liu|Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more. However, the existing MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foundation model. This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance. To address this issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions. Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R$^2$T is tailored to re-embed instance features online. It serves as a portable module that can seamlessly integrate into mainstream MIL models. Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin. The code is available at:~\\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}.|\u591a\u5b9e\u4f8b\u5b66\u4e60 (MIL) \u662f\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u6846\u67b6\uff0c\u6db5\u76d6\u5206\u578b\u3001\u8bca\u65ad\u3001\u9884\u540e\u7b49\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684 MIL \u8303\u4f8b\u901a\u5e38\u9700\u8981\u79bb\u7ebf\u5b9e\u4f8b\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4f8b\u5982\u9884\u8bad\u7ec3\u7684 ResNet \u6216\u57fa\u7840\u6a21\u578b\u3002\u8fd9\u79cd\u65b9\u6cd5\u7f3a\u4e4f\u5728\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fdb\u884c\u7279\u5f81\u5fae\u8c03\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u65b0\u5d4c\u5165\u7684\u533a\u57df\u53d8\u538b\u5668\uff08R$^2$T\uff09\uff0c\u7528\u4e8e\u5728\u7ebf\u91cd\u65b0\u5d4c\u5165\u5b9e\u4f8b\u7279\u5f81\uff0c\u5b83\u6355\u83b7\u7ec6\u7c92\u5ea6\u7684\u5c40\u90e8\u7279\u5f81\u5e76\u5efa\u7acb\u8de8\u4e0d\u540c\u533a\u57df\u7684\u8fde\u63a5\u3002\u4e0e\u73b0\u6709\u7684\u4e13\u6ce8\u4e8e\u9884\u8bad\u7ec3\u5f3a\u5927\u7684\u7279\u5f81\u63d0\u53d6\u5668\u6216\u8bbe\u8ba1\u590d\u6742\u7684\u5b9e\u4f8b\u805a\u5408\u5668\u7684\u4f5c\u54c1\u4e0d\u540c\uff0cR$^2$T \u662f\u4e3a\u5728\u7ebf\u91cd\u65b0\u5d4c\u5165\u5b9e\u4f8b\u7279\u5f81\u800c\u5b9a\u5236\u7684\u3002\u5b83\u4f5c\u4e3a\u4e00\u4e2a\u4fbf\u643a\u5f0f\u6a21\u5757\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u4e3b\u6d41 MIL \u6a21\u578b\u4e2d\u3002\u5728\u5e38\u89c1\u8ba1\u7b97\u75c5\u7406\u5b66\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\uff1a1\uff09\u7279\u5f81\u91cd\u65b0\u5d4c\u5165\u5c06\u57fa\u4e8eResNet-50\u7279\u5f81\u7684MIL\u6a21\u578b\u7684\u6027\u80fd\u63d0\u9ad8\u5230\u57fa\u7840\u6a21\u578b\u7279\u5f81\u7684\u6c34\u5e73\uff0c\u5e76\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u57fa\u7840\u6a21\u578b\u7279\u5f81\u7684\u6027\u80fd\uff1b 2\uff09R$^2$T\u53ef\u4ee5\u4e3a\u5404\u79cdMIL\u6a21\u578b\u5e26\u6765\u66f4\u663e\u7740\u7684\u6027\u80fd\u6539\u8fdb\uff1b 3) R$^2$T-MIL \u4f5c\u4e3a R$^2$T \u589e\u5f3a\u578b AB-MIL\uff0c\u5927\u5927\u4f18\u4e8e\u5176\u4ed6\u6700\u65b0\u65b9\u6cd5\u3002\u4ee3\u7801\u4f4d\u4e8e\uff1a~\\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}\u3002|[2402.17228v1](http://arxiv.org/pdf/2402.17228v1)|null|\n", "2402.17214": "|**2024-02-27**|**CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization**|CharacterGen\uff1a\u901a\u8fc7\u591a\u89c6\u56fe\u59ff\u52bf\u89c4\u8303\u5316\u4ece\u5355\u5f20\u56fe\u50cf\u9ad8\u6548\u751f\u6210 3D \u89d2\u8272|Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu|In the field of digital content creation, generating high-quality 3D characters from single images is challenging, especially given the complexities of various body poses and the issues of self-occlusion and pose ambiguity. In this paper, we present CharacterGen, a framework developed to efficiently generate 3D characters. CharacterGen introduces a streamlined generation pipeline along with an image-conditioned multi-view diffusion model. This model effectively calibrates input poses to a canonical form while retaining key attributes of the input image, thereby addressing the challenges posed by diverse poses. A transformer-based, generalizable sparse-view reconstruction model is the other core component of our approach, facilitating the creation of detailed 3D models from multi-view images. We also adopt a texture-back-projection strategy to produce high-quality texture maps. Additionally, we have curated a dataset of anime characters, rendered in multiple poses and views, to train and evaluate our model. Our approach has been thoroughly evaluated through quantitative and qualitative experiments, showing its proficiency in generating 3D characters with high-quality shapes and textures, ready for downstream applications such as rigging and animation.|\u5728\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u9886\u57df\uff0c\u4ece\u5355\u4e2a\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u7684 3D \u89d2\u8272\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u8003\u8651\u5230\u5404\u79cd\u8eab\u4f53\u59ff\u52bf\u7684\u590d\u6742\u6027\u4ee5\u53ca\u81ea\u906e\u6321\u548c\u59ff\u52bf\u6a21\u7cca\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86CharacterGen\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e3a\u9ad8\u6548\u751f\u62103D \u89d2\u8272\u800c\u5f00\u53d1\u7684\u6846\u67b6\u3002 CharacterGen \u5f15\u5165\u4e86\u7b80\u5316\u7684\u751f\u6210\u6d41\u7a0b\u4ee5\u53ca\u56fe\u50cf\u8c03\u8282\u7684\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u3002\u8be5\u6a21\u578b\u6709\u6548\u5730\u5c06\u8f93\u5165\u59ff\u52bf\u6821\u51c6\u4e3a\u89c4\u8303\u5f62\u5f0f\uff0c\u540c\u65f6\u4fdd\u7559\u8f93\u5165\u56fe\u50cf\u7684\u5173\u952e\u5c5e\u6027\uff0c\u4ece\u800c\u89e3\u51b3\u4e0d\u540c\u59ff\u52bf\u5e26\u6765\u7684\u6311\u6218\u3002\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u3001\u53ef\u63a8\u5e7f\u7684\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u6a21\u578b\u662f\u6211\u4eec\u65b9\u6cd5\u7684\u53e6\u4e00\u4e2a\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u6709\u52a9\u4e8e\u4ece\u591a\u89c6\u56fe\u56fe\u50cf\u521b\u5efa\u8be6\u7ec6\u7684 3D \u6a21\u578b\u3002\u6211\u4eec\u8fd8\u91c7\u7528\u7eb9\u7406\u53cd\u6295\u5f71\u7b56\u7565\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7eb9\u7406\u56fe\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u6574\u7406\u4e86\u4e00\u4e2a\u52a8\u6f2b\u89d2\u8272\u6570\u636e\u96c6\uff0c\u4ee5\u591a\u79cd\u59ff\u52bf\u548c\u89c6\u56fe\u5448\u73b0\uff0c\u4ee5\u8bad\u7ec3\u548c\u8bc4\u4f30\u6211\u4eec\u7684\u6a21\u578b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5df2\u7ecf\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8fdb\u884c\u4e86\u5f7b\u5e95\u8bc4\u4f30\uff0c\u663e\u793a\u51fa\u5176\u5728\u751f\u6210\u5177\u6709\u9ad8\u8d28\u91cf\u5f62\u72b6\u548c\u7eb9\u7406\u7684 3D \u89d2\u8272\u65b9\u9762\u7684\u719f\u7ec3\u7a0b\u5ea6\uff0c\u4e3a\u4e0b\u6e38\u5e94\u7528\uff08\u4f8b\u5982\u7ed1\u5b9a\u548c\u52a8\u753b\uff09\u505a\u597d\u4e86\u51c6\u5907\u3002|[2402.17214v1](http://arxiv.org/pdf/2402.17214v1)|null|\n", "2402.17115": "|**2024-02-27**|**CharNeRF: 3D Character Generation from Concept Art**|CharNeRF\uff1a\u4ece\u6982\u5ff5\u827a\u672f\u751f\u6210 3D \u89d2\u8272|Eddy Chu, Yiyang Chen, Chedy Raissi, Anand Bhojan|3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications. However, the process is often time-consuming and demands a high level of skill. In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art. To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model. We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer. Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network. Our model is able to generate high-quality 360-degree views of characters. Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh. It is important to note that our model's inferencing capabilities are influenced by the training data's characteristics, primarily focusing on characters with a single head, two arms, and two legs. Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data.|3D \u5efa\u6a21\u5728 AR/VR \u548c\u6e38\u620f\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u53ef\u4ee5\u5b9e\u73b0\u827a\u672f\u521b\u9020\u529b\u548c\u5b9e\u9645\u5e94\u7528\u3002\u7136\u800c\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u901a\u5e38\u975e\u5e38\u8017\u65f6\u5e76\u4e14\u9700\u8981\u9ad8\u6c34\u5e73\u7684\u6280\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u4e00\u81f4\u7684\u5468\u8f6c\u6982\u5ff5\u827a\u672f\u521b\u5efa 3D \u89d2\u8272\u4f53\u79ef\u8868\u793a\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u8be5\u6982\u5ff5\u827a\u672f\u4f5c\u4e3a 3D \u5efa\u6a21\u884c\u4e1a\u7684\u6807\u51c6\u8f93\u5165\u3002\u867d\u7136\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u5df2\u7ecf\u6210\u4e3a\u57fa\u4e8e\u56fe\u50cf\u7684 3D \u91cd\u5efa\u9886\u57df\u7684\u6e38\u620f\u89c4\u5219\u6539\u53d8\u8005\uff0c\u4f46\u636e\u6211\u4eec\u6240\u77e5\uff0c\u5c1a\u65e0\u5df2\u77e5\u7684\u7814\u7a76\u53ef\u4ee5\u4f18\u5316\u6982\u5ff5\u827a\u672f\u7684\u6d41\u7a0b\u3002\u4e3a\u4e86\u5229\u7528\u6982\u5ff5\u827a\u672f\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5176\u5b9a\u4e49\u7684\u8eab\u4f53\u59ff\u52bf\u548c\u7279\u5b9a\u7684\u89c6\u89d2\uff0c\u6211\u4eec\u5efa\u8bae\u5c06\u5176\u7f16\u7801\u4e3a\u6211\u4eec\u6a21\u578b\u7684\u5148\u9a8c\u3002\u6211\u4eec\u8bad\u7ec3\u7f51\u7edc\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u89c6\u56fe\u65b9\u5411\u53c2\u4e0e\u7684\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u5c06\u8fd9\u4e9b\u5148\u9a8c\u7528\u4e8e\u5404\u79cd 3D \u70b9\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bc1\u660e\u4e86\u5c04\u7ebf\u91c7\u6837\u548c\u8868\u9762\u91c7\u6837\u7684\u7ed3\u5408\u53ef\u4ee5\u589e\u5f3a\u6211\u4eec\u7f51\u7edc\u7684\u63a8\u7406\u80fd\u529b\u3002\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684 360 \u5ea6\u89d2\u8272\u89c6\u56fe\u3002\u968f\u540e\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u6307\u5357\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u6211\u4eec\u7684\u6a21\u578b\u6765\u63d0\u53d6 3D \u7f51\u683c\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u53d7\u5230\u8bad\u7ec3\u6570\u636e\u7279\u5f81\u7684\u5f71\u54cd\uff0c\u4e3b\u8981\u5173\u6ce8\u5355\u5934\u3001\u4e24\u6761\u624b\u81c2\u548c\u4e24\u6761\u817f\u7684\u89d2\u8272\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ecd\u7136\u662f\u901a\u7528\u7684\uff0c\u5e76\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u4e3b\u9898\u7684\u6982\u5ff5\u827a\u672f\uff0c\u800c\u4e0d\u5bf9\u6570\u636e\u5f3a\u52a0\u4efb\u4f55\u7279\u5b9a\u7684\u5047\u8bbe\u3002|[2402.17115v1](http://arxiv.org/pdf/2402.17115v1)|null|\n"}, "3D/CG": {"2402.17745": "|**2024-02-27**|**LoDIP: Low light phase retrieval with deep image prior**|LoDIP\uff1a\u5177\u6709\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u7684\u4f4e\u5149\u76f8\u4f4d\u68c0\u7d22|Raunak Manekar, Elisa Negrini, Minh Pham, Daniel Jacobs, Jaideep Srivastava|Phase retrieval (PR) is a fundamental challenge in scientific imaging, enabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging at low radiation doses becomes important in applications where samples are susceptible to radiation damage. However, most PR methods struggle in low dose scenario due to the presence of very high shot noise. Advancements in the optical data acquisition setup, exemplified by in-situ CDI, have shown potential for low-dose imaging. But these depend on a time series of measurements, rendering them unsuitable for single-image applications. Similarly, on the computational front, data-driven phase retrieval techniques are not readily adaptable to the single-image context. Deep learning based single-image methods, such as deep image prior, have been effective for various imaging tasks but have exhibited limited success when applied to PR. In this work, we propose LoDIP which combines the in-situ CDI setup with the power of implicit neural priors to tackle the problem of single-image low-dose phase retrieval. Quantitative evaluations demonstrate the superior performance of LoDIP on this task as well as applicability to real experimental scenarios.|\u76f8\u4f4d\u68c0\u7d22 (PR) \u662f\u79d1\u5b66\u6210\u50cf\u4e2d\u7684\u4e00\u9879\u57fa\u672c\u6311\u6218\uff0c\u5b83\u4f7f\u76f8\u5e72\u884d\u5c04\u6210\u50cf (CDI) \u7b49\u7eb3\u7c73\u7ea7\u6280\u672f\u6210\u4e3a\u53ef\u80fd\u3002\u5728\u6837\u54c1\u6613\u53d7\u8f90\u5c04\u635f\u4f24\u7684\u5e94\u7528\u4e2d\uff0c\u4f4e\u8f90\u5c04\u5242\u91cf\u6210\u50cf\u53d8\u5f97\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5b58\u5728\u975e\u5e38\u9ad8\u7684\u6563\u7c92\u566a\u58f0\uff0c\u5927\u591a\u6570 PR \u65b9\u6cd5\u5728\u4f4e\u5242\u91cf\u60c5\u51b5\u4e0b\u90fd\u5f88\u56f0\u96be\u3002\u4ee5\u539f\u4f4d CDI \u4e3a\u4ee3\u8868\u7684\u5149\u5b66\u6570\u636e\u91c7\u96c6\u88c5\u7f6e\u7684\u8fdb\u6b65\u5df2\u663e\u793a\u51fa\u4f4e\u5242\u91cf\u6210\u50cf\u7684\u6f5c\u529b\u3002\u4f46\u8fd9\u4e9b\u4f9d\u8d56\u4e8e\u6d4b\u91cf\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u4f7f\u5f97\u5b83\u4eec\u4e0d\u9002\u5408\u5355\u56fe\u50cf\u5e94\u7528\u3002\u7c7b\u4f3c\u5730\uff0c\u5728\u8ba1\u7b97\u65b9\u9762\uff0c\u6570\u636e\u9a71\u52a8\u7684\u76f8\u4f4d\u68c0\u7d22\u6280\u672f\u4e0d\u5bb9\u6613\u9002\u5e94\u5355\u56fe\u50cf\u73af\u5883\u3002\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5355\u56fe\u50cf\u65b9\u6cd5\uff08\u4f8b\u5982\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\uff09\u5bf9\u4e8e\u5404\u79cd\u6210\u50cf\u4efb\u52a1\u90fd\u6709\u6548\uff0c\u4f46\u5728\u5e94\u7528\u4e8e PR \u65f6\u5374\u8868\u73b0\u6709\u9650\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 LoDIP\uff0c\u5b83\u5c06\u539f\u4f4d CDI \u8bbe\u7f6e\u4e0e\u9690\u5f0f\u795e\u7ecf\u5148\u9a8c\u7684\u529b\u91cf\u76f8\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u5355\u56fe\u50cf\u4f4e\u5242\u91cf\u76f8\u4f4d\u68c0\u7d22\u7684\u95ee\u9898\u3002\u5b9a\u91cf\u8bc4\u4f30\u8bc1\u660e\u4e86 LoDIP \u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u4ee5\u53ca\u5bf9\u771f\u5b9e\u5b9e\u9a8c\u573a\u666f\u7684\u9002\u7528\u6027\u3002|[2402.17745v1](http://arxiv.org/pdf/2402.17745v1)|null|\n", "2402.17744": "|**2024-02-27**|**Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using Contrastive Learning and Geometric Unfolding**|\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u51e0\u4f55\u5c55\u5f00\u5728 3D-PLI \u4e2d\u5206\u6790\u4eba\u7c7b\u6d77\u9a6c\u7684\u533a\u57df\u7ec4\u7ec7|Alexander Oberstrass, Jordan DeKraker, Nicola Palomero-Gallagher, Sascha E. A. Muenzing, Alan C. Evans, Markus Axer, Katrin Amunts, Timo Dickscheid|Understanding the cortical organization of the human brain requires interpretable descriptors for distinct structural and functional imaging data. 3D polarized light imaging (3D-PLI) is an imaging modality for visualizing fiber architecture in postmortem brains with high resolution that also captures the presence of cell bodies, for example, to identify hippocampal subfields. The rich texture in 3D-PLI images, however, makes this modality particularly difficult to analyze and best practices for characterizing architectonic patterns still need to be established. In this work, we demonstrate a novel method to analyze the regional organization of the human hippocampus in 3D-PLI by combining recent advances in unfolding methods with deep texture features obtained using a self-supervised contrastive learning approach. We identify clusters in the representations that correspond well with classical descriptions of hippocampal subfields, lending validity to the developed methodology.|\u4e86\u89e3\u4eba\u8111\u7684\u76ae\u8d28\u7ec4\u7ec7\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u7ed3\u6784\u548c\u529f\u80fd\u6210\u50cf\u6570\u636e\u7684\u53ef\u89e3\u91ca\u63cf\u8ff0\u7b26\u3002 3D \u504f\u632f\u5149\u6210\u50cf (3D-PLI) \u662f\u4e00\u79cd\u4ee5\u9ad8\u5206\u8fa8\u7387\u53ef\u89c6\u5316\u6b7b\u540e\u5927\u8111\u4e2d\u7ea4\u7ef4\u7ed3\u6784\u7684\u6210\u50cf\u65b9\u5f0f\uff0c\u8fd8\u53ef\u4ee5\u6355\u83b7\u7ec6\u80de\u4f53\u7684\u5b58\u5728\uff0c\u4f8b\u5982\u8bc6\u522b\u6d77\u9a6c\u4e9a\u533a\u3002\u7136\u800c\uff0c3D-PLI \u56fe\u50cf\u4e2d\u4e30\u5bcc\u7684\u7eb9\u7406\u4f7f\u5f97\u8fd9\u79cd\u6a21\u5f0f\u7279\u522b\u96be\u4ee5\u5206\u6790\uff0c\u5e76\u4e14\u4ecd\u7136\u9700\u8981\u5efa\u7acb\u8868\u5f81\u5efa\u7b51\u56fe\u6848\u7684\u6700\u4f73\u5b9e\u8df5\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5c55\u5f00\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\u4e0e\u4f7f\u7528\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u83b7\u5f97\u7684\u6df1\u5c42\u7eb9\u7406\u7279\u5f81\u76f8\u7ed3\u5408\uff0c\u6765\u5206\u6790 3D-PLI \u4e2d\u4eba\u7c7b\u6d77\u9a6c\u4f53\u7684\u533a\u57df\u7ec4\u7ec7\u3002\u6211\u4eec\u5728\u8868\u5f81\u4e2d\u8bc6\u522b\u51fa\u4e0e\u6d77\u9a6c\u5b50\u533a\u57df\u7684\u7ecf\u5178\u63cf\u8ff0\u975e\u5e38\u4e00\u81f4\u7684\u805a\u7c7b\uff0c\u4ece\u800c\u4e3a\u6240\u5f00\u53d1\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u6548\u6027\u3002|[2402.17744v1](http://arxiv.org/pdf/2402.17744v1)|null|\n", "2402.17561": "|**2024-02-27**|**PHNet: Patch-based Normalization for Portrait Harmonization**|PHNet\uff1a\u57fa\u4e8e\u8865\u4e01\u7684\u6807\u51c6\u5316\u8096\u50cf\u534f\u8c03|Karen Efremyan, Elizaveta Petrova, Evgeny Kaskov, Alexander Kapitanov|A common problem for composite images is the incompatibility of their foreground and background components. Image harmonization aims to solve this problem, making the whole image look more authentic and coherent. Most existing solutions predict lookup tables (LUTs) or reconstruct images, utilizing various attributes of composite images. Recent approaches have primarily focused on employing global transformations like normalization and color curve rendering to achieve visual consistency, and they often overlook the importance of local visual coherence. We present a patch-based harmonization network consisting of novel Patch-based normalization (PN) blocks and a feature extractor based on statistical color transfer. Extensive experiments demonstrate the network's high generalization capability for different domains. Our network achieves state-of-the-art results on the iHarmony4 dataset. Also, we created a new human portrait harmonization dataset based on FFHQ and checked the proposed method to show the generalization ability by achieving the best metrics on it. The benchmark experiments confirm that the suggested patch-based normalization block and feature extractor effectively improve the network's capability to harmonize portraits. Our code and model baselines are publicly available.|\u5408\u6210\u56fe\u50cf\u7684\u4e00\u4e2a\u5e38\u89c1\u95ee\u9898\u662f\u5176\u524d\u666f\u548c\u80cc\u666f\u7ec4\u4ef6\u7684\u4e0d\u517c\u5bb9\u3002\u56fe\u50cf\u534f\u8c03\u5c31\u662f\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8ba9\u6574\u4e2a\u56fe\u50cf\u770b\u8d77\u6765\u66f4\u52a0\u771f\u5b9e\u3001\u8fde\u8d2f\u3002\u5927\u591a\u6570\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5229\u7528\u5408\u6210\u56fe\u50cf\u7684\u5404\u79cd\u5c5e\u6027\u6765\u9884\u6d4b\u67e5\u627e\u8868 (LUT) \u6216\u91cd\u5efa\u56fe\u50cf\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u91c7\u7528\u5f52\u4e00\u5316\u548c\u989c\u8272\u66f2\u7ebf\u6e32\u67d3\u7b49\u5168\u5c40\u53d8\u6362\u6765\u5b9e\u73b0\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u5b83\u4eec\u7ecf\u5e38\u5ffd\u89c6\u5c40\u90e8\u89c6\u89c9\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8865\u4e01\u7684\u534f\u8c03\u7f51\u7edc\uff0c\u7531\u65b0\u9896\u7684\u57fa\u4e8e\u8865\u4e01\u7684\u5f52\u4e00\u5316\uff08PN\uff09\u5757\u548c\u57fa\u4e8e\u7edf\u8ba1\u989c\u8272\u4f20\u8f93\u7684\u7279\u5f81\u63d0\u53d6\u5668\u7ec4\u6210\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u7f51\u7edc\u5bf9\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u9ad8\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u7f51\u7edc\u5728 iHarmony4 \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u57fa\u4e8e FFHQ \u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4eba\u4f53\u8096\u50cf\u534f\u8c03\u6570\u636e\u96c6\uff0c\u5e76\u68c0\u67e5\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u4ee5\u901a\u8fc7\u5728\u5176\u4e0a\u5b9e\u73b0\u6700\u4f73\u6307\u6807\u6765\u663e\u793a\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u51c6\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u6240\u5efa\u8bae\u7684\u57fa\u4e8e\u8865\u4e01\u7684\u5f52\u4e00\u5316\u5757\u548c\u7279\u5f81\u63d0\u53d6\u5668\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u7f51\u7edc\u534f\u8c03\u8096\u50cf\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6a21\u578b\u57fa\u7ebf\u662f\u516c\u5f00\u7684\u3002|[2402.17561v1](http://arxiv.org/pdf/2402.17561v1)|null|\n", "2402.17521": "|**2024-02-27**|**AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud Analysis**|AVS-Net\uff1a\u91c7\u7528\u81ea\u9002\u5e94\u4f53\u7d20\u5927\u5c0f\u7684\u70b9\u91c7\u6837\u8fdb\u884c 3D \u70b9\u4e91\u5206\u6790|Hongcheng Yang, Dingkang Liang, Dingyuan Zhang, Xingyu Jiang, Zhe Liu, Zhikang Zou, Yingying Zhu|Efficient downsampling plays a crucial role in point cloud learning, particularly for large-scale 3D scenes. Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information. This paper presents an advanced sampler that achieves both high accuracy and efficiency. The proposed method utilizes voxel-based sampling as a foundation, but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues. Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio. This ensures the sampling results exhibit a favorable distribution for comprehending various 3D objects or scenes. Additionally, we introduce a network compatible with arbitrary voxel sizes for sampling and feature extraction while maintaining high efficiency. Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet benchmarks with promising efficiency. Code will be available at https://github.com/yhc2021/AVS-Net.|\u9ad8\u6548\u7684\u4e0b\u91c7\u6837\u5728\u70b9\u4e91\u5b66\u4e60\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21 3D \u573a\u666f\u3002\u73b0\u6709\u7684\u4e0b\u91c7\u6837\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u8981\u4e48\u727a\u7272\u7ec6\u7c92\u5ea6\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u91c7\u6837\u5668\uff0c\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5229\u7528\u57fa\u4e8e\u4f53\u7d20\u7684\u91c7\u6837\u4f5c\u4e3a\u57fa\u7840\uff0c\u4f46\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4f53\u7d20\u5927\u5c0f\u786e\u5b9a\u548c\u5173\u952e\u51e0\u4f55\u7ebf\u7d22\u4fdd\u5b58\u65b9\u9762\u7684\u6311\u6218\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f53\u7d20\u9002\u5e94\u6a21\u5757\uff0c\u5b83\u53c2\u8003\u57fa\u4e8e\u70b9\u7684\u4e0b\u91c7\u6837\u7387\u81ea\u9002\u5e94\u5730\u8c03\u6574\u4f53\u7d20\u5927\u5c0f\u3002\u8fd9\u786e\u4fdd\u91c7\u6837\u7ed3\u679c\u5448\u73b0\u51fa\u6709\u5229\u4e8e\u7406\u89e3\u5404\u79cd 3D \u5bf9\u8c61\u6216\u573a\u666f\u7684\u5206\u5e03\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u4e0e\u4efb\u610f\u4f53\u7d20\u5927\u5c0f\u517c\u5bb9\u7684\u7f51\u7edc\uff0c\u7528\u4e8e\u91c7\u6837\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7387\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 ShapeNetPart \u548c ScanNet \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u5e76\u4e14\u6548\u7387\u5f88\u9ad8\u3002\u4ee3\u7801\u5c06\u5728 https://github.com/yhc2021/AVS-Net \u4e0a\u63d0\u4f9b\u3002|[2402.17521v1](http://arxiv.org/pdf/2402.17521v1)|null|\n", "2402.17485": "|**2024-02-27**|**EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions**|EMO\uff1aEmote Portrait Alive - \u5728\u5f31\u6761\u4ef6\u4e0b\u4f7f\u7528\u97f3\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8096\u50cf\u89c6\u9891|Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo|In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5173\u6ce8\u97f3\u9891\u63d0\u793a\u548c\u9762\u90e8\u52a8\u4f5c\u4e4b\u95f4\u7684\u52a8\u6001\u548c\u5fae\u5999\u5173\u7cfb\u6765\u89e3\u51b3\u589e\u5f3a\u5934\u90e8\u8bf4\u8bdd\u89c6\u9891\u751f\u6210\u7684\u771f\u5b9e\u6027\u548c\u8868\u73b0\u529b\u7684\u6311\u6218\u3002\u6211\u4eec\u53d1\u73b0\u4f20\u7edf\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u6280\u672f\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u8868\u60c5\u7684\u5168\u90e8\u8303\u56f4\u548c\u4e2a\u4eba\u9762\u90e8\u98ce\u683c\u7684\u72ec\u7279\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 EMO\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u76f4\u63a5\u97f3\u9891\u5230\u89c6\u9891\u5408\u6210\u65b9\u6cd5\u7684\u65b0\u9896\u6846\u67b6\uff0c\u7ed5\u8fc7\u4e86\u5bf9\u4e2d\u95f4 3D \u6a21\u578b\u6216\u9762\u90e8\u6807\u5fd7\u7684\u9700\u6c42\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u786e\u4fdd\u6574\u4e2a\u89c6\u9891\u4e2d\u7684\u65e0\u7f1d\u5e27\u8fc7\u6e21\u548c\u4e00\u81f4\u7684\u8eab\u4efd\u4fdd\u7559\uff0c\u4ece\u800c\u4ea7\u751f\u9ad8\u5ea6\u8868\u73b0\u529b\u548c\u903c\u771f\u7684\u52a8\u753b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEMO\u4e0d\u4ec5\u80fd\u591f\u5236\u4f5c\u4ee4\u4eba\u4fe1\u670d\u7684\u53e3\u8bed\u89c6\u9891\uff0c\u8fd8\u80fd\u591f\u5236\u4f5c\u5404\u79cd\u98ce\u683c\u7684\u6b4c\u5531\u89c6\u9891\uff0c\u5728\u8868\u73b0\u529b\u548c\u771f\u5b9e\u611f\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2402.17485v1](http://arxiv.org/pdf/2402.17485v1)|null|\n", "2402.17464": "|**2024-02-27**|**Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing**|\u901a\u8fc7\u90e8\u5206-\u6574\u4f53-\u5c42\u6b21\u7ed3\u6784\u6d88\u606f\u4f20\u9012\u751f\u6210 3D \u96f6\u4ef6\u7ec4\u88c5|Bi'an Du, Xiang Gao, Wei Hu, Renjie Liao|Generative 3D part assembly involves understanding part relationships and predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work often focus on the geometry of individual parts, neglecting part-whole hierarchies of objects. Leveraging two key observations: 1) super-part poses provide strong hints about part poses, and 2) predicting super-part poses is easier due to fewer superparts, we propose a part-whole-hierarchy message passing network for efficient 3D part assembly. We first introduce super-parts by grouping geometrically similar parts without any semantic labels. Then we employ a part-whole hierarchical encoder, wherein a super-part encoder predicts latent super-part poses based on input parts. Subsequently, we transform the point cloud using the latent poses, feeding it to the part encoder for aggregating super-part information and reasoning about part relationships to predict all part poses. In training, only ground-truth part poses are required. During inference, the predicted latent poses of super-parts enhance interpretability. Experimental results on the PartNet dataset show that our method achieves state-of-the-art performance in part and connectivity accuracy and enables an interpretable hierarchical part assembly.|\u751f\u6210\u5f0f 3D \u96f6\u4ef6\u7ec4\u88c5\u6d89\u53ca\u4e86\u89e3\u96f6\u4ef6\u5173\u7cfb\u5e76\u9884\u6d4b\u5176 6-DoF \u59ff\u52bf\uff0c\u4ee5\u7ec4\u88c5\u771f\u5b9e\u7684 3D \u5f62\u72b6\u3002\u5148\u524d\u7684\u5de5\u4f5c\u901a\u5e38\u5173\u6ce8\u5404\u4e2a\u90e8\u5206\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u8c61\u7684\u90e8\u5206\u6574\u4f53\u5c42\u6b21\u7ed3\u6784\u3002\u5229\u7528\u4e24\u4e2a\u5173\u952e\u89c2\u5bdf\u7ed3\u679c\uff1a1\uff09\u8d85\u7ea7\u96f6\u4ef6\u59ff\u52bf\u63d0\u4f9b\u4e86\u6709\u5173\u96f6\u4ef6\u59ff\u52bf\u7684\u5f3a\u70c8\u63d0\u793a\uff0c2\uff09\u7531\u4e8e\u8d85\u7ea7\u96f6\u4ef6\u8f83\u5c11\uff0c\u9884\u6d4b\u8d85\u7ea7\u96f6\u4ef6\u59ff\u52bf\u66f4\u5bb9\u6613\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u6548 3D \u96f6\u4ef6\u7ec4\u88c5\u7684\u96f6\u4ef6-\u6574\u4f53-\u5c42\u6b21\u7ed3\u6784\u6d88\u606f\u4f20\u9012\u7f51\u7edc\u3002\u6211\u4eec\u9996\u5148\u901a\u8fc7\u5bf9\u6ca1\u6709\u4efb\u4f55\u8bed\u4e49\u6807\u7b7e\u7684\u51e0\u4f55\u76f8\u4f3c\u90e8\u4ef6\u8fdb\u884c\u5206\u7ec4\u6765\u5f15\u5165\u8d85\u7ea7\u90e8\u4ef6\u3002\u7136\u540e\u6211\u4eec\u91c7\u7528\u90e8\u5206-\u6574\u4f53\u5206\u5c42\u7f16\u7801\u5668\uff0c\u5176\u4e2d\u8d85\u7ea7\u90e8\u5206\u7f16\u7801\u5668\u6839\u636e\u8f93\u5165\u90e8\u5206\u9884\u6d4b\u6f5c\u5728\u7684\u8d85\u7ea7\u90e8\u5206\u59ff\u52bf\u3002\u968f\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u6f5c\u5728\u59ff\u52bf\u53d8\u6362\u70b9\u4e91\uff0c\u5c06\u5176\u9988\u9001\u5230\u96f6\u4ef6\u7f16\u7801\u5668\u4ee5\u805a\u5408\u8d85\u7ea7\u96f6\u4ef6\u4fe1\u606f\u5e76\u63a8\u7406\u96f6\u4ef6\u5173\u7cfb\u4ee5\u9884\u6d4b\u6240\u6709\u96f6\u4ef6\u59ff\u52bf\u3002\u5728\u8bad\u7ec3\u4e2d\uff0c\u53ea\u9700\u8981\u771f\u5b9e\u7684\u90e8\u5206\u59ff\u52bf\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u8d85\u7ea7\u90e8\u4ef6\u7684\u9884\u6d4b\u6f5c\u5728\u59ff\u52bf\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002 PartNet \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u96f6\u4ef6\u548c\u8fde\u63a5\u7cbe\u5ea6\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u5206\u5c42\u96f6\u4ef6\u88c5\u914d\u3002|[2402.17464v1](http://arxiv.org/pdf/2402.17464v1)|null|\n", "2402.17427": "|**2024-02-27**|**VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction**|VastGaussian\uff1a\u7528\u4e8e\u5927\u578b\u573a\u666f\u91cd\u5efa\u7684 Vast 3D \u9ad8\u65af|Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, et.al.|Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering.|\u73b0\u6709\u7684\u57fa\u4e8e NeRF \u7684\u5927\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6e32\u67d3\u901f\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u867d\u7136\u6700\u8fd1\u7684 3D \u9ad8\u65af\u6e85\u5c04\u5728\u5c0f\u89c4\u6a21\u548c\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u573a\u666f\u4e2d\u6548\u679c\u5f88\u597d\uff0c\u4f46\u7531\u4e8e\u89c6\u9891\u5185\u5b58\u6709\u9650\u3001\u4f18\u5316\u65f6\u95f4\u957f\u548c\u660e\u663e\u7684\u5916\u89c2\u53d8\u5316\uff0c\u5c06\u5176\u6269\u5c55\u5230\u5927\u578b\u573a\u666f\u4f1a\u5e26\u6765\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 VastGaussian\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e 3D \u9ad8\u65af\u5206\u5e03\u5728\u5927\u578b\u573a\u666f\u4e0a\u8fdb\u884c\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u5b9e\u65f6\u6e32\u67d3\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5206\u533a\u7b56\u7565\uff0c\u5c06\u5927\u573a\u666f\u5212\u5206\u4e3a\u591a\u4e2a\u5355\u5143\uff0c\u5176\u4e2d\u8bad\u7ec3\u6444\u50cf\u673a\u548c\u70b9\u4e91\u6309\u7167\u7a7a\u57df\u611f\u77e5\u53ef\u89c1\u6027\u6807\u51c6\u6b63\u786e\u5206\u5e03\u3002\u8fd9\u4e9b\u5355\u5143\u7ecf\u8fc7\u5e76\u884c\u4f18\u5316\u540e\u5408\u5e76\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u573a\u666f\u3002\u6211\u4eec\u8fd8\u5c06\u89e3\u8026\u5916\u89c2\u5efa\u6a21\u5f15\u5165\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u51cf\u5c11\u6e32\u67d3\u56fe\u50cf\u4e2d\u7684\u5916\u89c2\u53d8\u5316\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e NeRF \u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u5927\u578b\u573a\u666f\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5feb\u901f\u4f18\u5316\u548c\u9ad8\u4fdd\u771f\u5b9e\u65f6\u6e32\u67d3\u3002|[2402.17427v1](http://arxiv.org/pdf/2402.17427v1)|null|\n", "2402.17412": "|**2024-02-27**|**DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Model**|DiffuseKronA\uff1a\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5|Shyam Marjit, Harshit Singh, Nityanand Mathur, Sayak Paul, Chia-Mu Yu, Pin-Yu Chen|In the realm of subject-driven text-to-image (T2I) generative models, recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis. Addressing these constraints, we introduce \\textbf{\\textit{DiffuseKronA}}, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by 35\\% and 99.947\\% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis. Crucially, \\textit{DiffuseKronA} mitigates the issue of hyperparameter sensitivity, delivering consistent high-quality generations across a wide range of hyperparameters, thereby diminishing the necessity for extensive fine-tuning. Furthermore, a more controllable decomposition makes \\textit{DiffuseKronA} more interpretable and even can achieve up to a 50\\% reduction with results comparable to LoRA-Dreambooth. Evaluated against diverse and complex input images and text prompts, \\textit{DiffuseKronA} consistently outperforms existing models, producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects, all the while upholding exceptional parameter efficiency, thus presenting a substantial advancement in the field of T2I generative modeling. Our project page, consisting of links to the code, and pre-trained checkpoints, is available at \\href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}.|\u5728\u4e3b\u9898\u9a71\u52a8\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u6a21\u578b\u9886\u57df\uff0cDreamBooth \u548c BLIP-Diffusion \u7b49\u6700\u65b0\u53d1\u5c55\u5df2\u7ecf\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u7531\u4e8e\u5176\u5bc6\u96c6\u7684\u5fae\u8c03\u9700\u6c42\u548c\u5927\u91cf\u7684\u53c2\u6570\u8981\u6c42\u800c\u9047\u5230\u4e86\u9650\u5236\u3002\u867d\u7136 DreamBooth \u4e2d\u7684\u4f4e\u79e9\u9002\u5e94 (LoRA) \u6a21\u5757\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u4f46\u5b83\u5f15\u5165\u4e86\u5bf9\u8d85\u53c2\u6570\u7684\u660e\u663e\u654f\u611f\u6027\uff0c\u5bfc\u81f4\u53c2\u6570\u6548\u7387\u548c T2I \u4e2a\u6027\u5316\u56fe\u50cf\u5408\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u6298\u8877\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 \\textbf{\\textit{DiffuseKronA}}\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e Kronecker \u4ea7\u54c1\u7684\u81ea\u9002\u5e94\u6a21\u5757\uff0c\u4e0e LoRA-DreamBooth \u548c\u539f\u59cb DreamBooth \u76f8\u6bd4\uff0c\u5b83\u4e0d\u4ec5\u663e\u7740\u51cf\u5c11\u4e86 35\\% \u548c 99.947\\% \u7684\u53c2\u6570\u6570\u91cf\uff0c\u5206\u522b\uff0c\u800c\u4e14\u8fd8\u63d0\u9ad8\u4e86\u56fe\u50cf\u5408\u6210\u7684\u8d28\u91cf\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0c \\textit{DiffuseKronA} \u7f13\u89e3\u4e86\u8d85\u53c2\u6570\u654f\u611f\u6027\u95ee\u9898\uff0c\u5728\u5404\u79cd\u8d85\u53c2\u6570\u4e0a\u63d0\u4f9b\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u751f\u6210\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5e7f\u6cdb\u5fae\u8c03\u7684\u5fc5\u8981\u6027\u3002\u6b64\u5916\uff0c\u66f4\u53ef\u63a7\u7684\u5206\u89e3\u4f7f\u5f97 \\textit{DiffuseKronA} \u66f4\u5177\u53ef\u89e3\u91ca\u6027\uff0c\u751a\u81f3\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8fbe 50% \u7684\u51cf\u5c11\uff0c\u7ed3\u679c\u4e0e LoRA-Dreambooth \u76f8\u5f53\u3002\u6839\u636e\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u8f93\u5165\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u8fdb\u884c\u8bc4\u4f30\uff0c\\textit{DiffuseKronA} \u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u591a\u6837\u5316\u56fe\u50cf\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u548c\u66f4\u51c6\u786e\u7684\u5bf9\u8c61\u989c\u8272\u5206\u5e03\uff0c\u540c\u65f6\u4fdd\u6301\u5353\u8d8a\u7684\u53c2\u6570\u6548\u7387\uff0c\u4ece\u800c\u5448\u73b0\u51faT2I \u751f\u6210\u5efa\u6a21\u9886\u57df\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u8fdb\u5c55\u3002\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\u5305\u542b\u4ee3\u7801\u94fe\u63a5\u548c\u9884\u5148\u8bad\u7ec3\u7684\u68c0\u67e5\u70b9\uff0c\u53ef\u5728 \\href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/} \u83b7\u53d6\u3002|[2402.17412v1](http://arxiv.org/pdf/2402.17412v1)|null|\n", "2402.17403": "|**2024-02-27**|**Sora Generates Videos with Stunning Geometrical Consistency**|Sora \u751f\u6210\u5177\u6709\u4ee4\u4eba\u60ca\u53f9\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u89c6\u9891|Xuanyi Li, Daquan Zhou, Chenxu Zhang, Shaodong Wei, Qibin Hou, Ming-Ming Cheng|The recently developed Sora model [1] has exhibited remarkable capabilities in video generation, sparking intense discussions regarding its ability to simulate real-world phenomena. Despite its growing popularity, there is a lack of established metrics to evaluate its fidelity to real-world physics quantitatively. In this paper, we introduce a new benchmark that assesses the quality of the generated videos based on their adherence to real-world physics principles. We employ a method that transforms the generated videos into 3D models, leveraging the premise that the accuracy of 3D reconstruction is heavily contingent on the video quality. From the perspective of 3D reconstruction, we use the fidelity of the geometric constraints satisfied by the constructed 3D models as a proxy to gauge the extent to which the generated videos conform to real-world physics rules. Project page: https://sora-geometrical-consistency.github.io/|\u6700\u8fd1\u5f00\u53d1\u7684 Sora \u6a21\u578b [1] \u5728\u89c6\u9891\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u5f15\u53d1\u4e86\u5173\u4e8e\u5176\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u73b0\u8c61\u7684\u80fd\u529b\u7684\u6fc0\u70c8\u8ba8\u8bba\u3002\u5c3d\u7ba1\u5b83\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u4f46\u7f3a\u4e4f\u65e2\u5b9a\u7684\u6307\u6807\u6765\u5b9a\u91cf\u8bc4\u4f30\u5176\u5bf9\u73b0\u5b9e\u4e16\u754c\u7269\u7406\u7684\u4fdd\u771f\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u6839\u636e\u751f\u6210\u7684\u89c6\u9891\u662f\u5426\u7b26\u5408\u73b0\u5b9e\u4e16\u754c\u7684\u7269\u7406\u539f\u7406\u6765\u8bc4\u4f30\u5176\u8d28\u91cf\u3002\u6211\u4eec\u91c7\u7528\u4e00\u79cd\u5c06\u751f\u6210\u7684\u89c6\u9891\u8f6c\u6362\u4e3a 3D \u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5229\u7528 3D \u91cd\u5efa\u7684\u51c6\u786e\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u89c6\u9891\u8d28\u91cf\u7684\u524d\u63d0\u3002\u4ece 3D \u91cd\u5efa\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u6211\u4eec\u4f7f\u7528\u6784\u5efa\u7684 3D \u6a21\u578b\u6ee1\u8db3\u7684\u51e0\u4f55\u7ea6\u675f\u7684\u4fdd\u771f\u5ea6\u4f5c\u4e3a\u4ee3\u7406\u6765\u8861\u91cf\u751f\u6210\u7684\u89c6\u9891\u7b26\u5408\u73b0\u5b9e\u4e16\u754c\u7269\u7406\u89c4\u5219\u7684\u7a0b\u5ea6\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://sora-geometrical-consistency.github.io/|[2402.17403v1](http://arxiv.org/pdf/2402.17403v1)|null|\n", "2402.17372": "|**2024-02-27**|**Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching**|\u7528\u4e8e\u5c40\u90e8\u611f\u77e5 3D \u521a\u6027\u70b9\u4e91\u5339\u914d\u7684\u8026\u5408\u62c9\u666e\u62c9\u65af\u7279\u5f81\u56fe|Matteo Bastico, Etienne Decenci\u00e8re, Laurent Cort\u00e9, Yannick Tillier, David Ryckelynck|Point cloud matching, a crucial technique in computer vision, medical and robotics fields, is primarily concerned with finding correspondences between pairs of point clouds or voxels. In some practical scenarios, emphasizing local differences is crucial for accurately identifying a correct match, thereby enhancing the overall robustness and reliability of the matching process. Commonly used shape descriptors have several limitations and often fail to provide meaningful local insights on the paired geometries. In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account fine local structures. To deal with the order and sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called Coupled Laplacian, that allows to easily generate aligned eigenspaces for multiple rigidly-registered geometries. We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes. We initially evaluate the performance of the proposed technique in a point-wise manner, specifically focusing on the task of object anomaly localization using the MVTec 3D-AD dataset. Additionally, we define a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces. In order to test it, we propose a benchmark collecting bone surface structures from various public datasets. Our matching technique, based on Coupled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks. The code to reproduce our experiments is publicly available at https://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary Code.|\u70b9\u4e91\u5339\u914d\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u533b\u5b66\u548c\u673a\u5668\u4eba\u9886\u57df\u7684\u4e00\u9879\u5173\u952e\u6280\u672f\uff0c\u4e3b\u8981\u6d89\u53ca\u5bfb\u627e\u70b9\u4e91\u5bf9\u6216\u4f53\u7d20\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u5728\u4e00\u4e9b\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u5f3a\u8c03\u5c40\u90e8\u5dee\u5f02\u5bf9\u4e8e\u51c6\u786e\u8bc6\u522b\u6b63\u786e\u5339\u914d\u81f3\u5173\u91cd\u8981\uff0c\u4ece\u800c\u589e\u5f3a\u5339\u914d\u8fc7\u7a0b\u7684\u6574\u4f53\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002\u5e38\u7528\u7684\u5f62\u72b6\u63cf\u8ff0\u7b26\u6709\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u901a\u5e38\u65e0\u6cd5\u63d0\u4f9b\u6709\u5173\u914d\u5bf9\u51e0\u4f55\u5f62\u72b6\u7684\u6709\u610f\u4e49\u7684\u5c40\u90e8\u89c1\u89e3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u62c9\u666e\u62c9\u65af\u7279\u5f81\u56fe\u7684\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u8003\u8651\u7cbe\u7ec6\u7684\u5c40\u90e8\u7ed3\u6784\u6765\u5339\u914d\u70b9\u4e91\u3002\u4e3a\u4e86\u5904\u7406\u62c9\u666e\u62c9\u65af\u7279\u5f81\u56fe\u7684\u987a\u5e8f\u548c\u7b26\u53f7\u6a21\u7cca\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u5b50\uff0c\u79f0\u4e3a\u8026\u5408\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u5b83\u53ef\u4ee5\u8f7b\u677e\u5730\u4e3a\u591a\u4e2a\u4e25\u683c\u914d\u51c6\u7684\u51e0\u4f55\u56fe\u5f62\u751f\u6210\u5bf9\u9f50\u7684\u7279\u5f81\u7a7a\u95f4\u3002\u6211\u4eec\u8868\u660e\uff0c\u8fd9\u4e9b\u5bf9\u9f50\u7684\u9ad8\u7ef4\u7a7a\u95f4\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u4e3a\u5339\u914d\u5f62\u72b6\u63d0\u4f9b\u4e86\u5c40\u90e8\u6709\u610f\u4e49\u7684\u5206\u6570\u3002\u6211\u4eec\u6700\u521d\u4ee5\u9010\u70b9\u65b9\u5f0f\u8bc4\u4f30\u6240\u63d0\u51fa\u6280\u672f\u7684\u6027\u80fd\uff0c\u7279\u522b\u5173\u6ce8\u4f7f\u7528 MVTec 3D-AD \u6570\u636e\u96c6\u8fdb\u884c\u5bf9\u8c61\u5f02\u5e38\u5b9a\u4f4d\u7684\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u9879\u65b0\u7684\u533b\u7597\u4efb\u52a1\uff0c\u79f0\u4e3a\u81ea\u52a8\u9aa8\u4fa7\u4f30\u8ba1\uff08BSE\uff09\uff0c\u6211\u4eec\u901a\u8fc7\u4ece\u8026\u5408\u7279\u5f81\u7a7a\u95f4\u5bfc\u51fa\u7684\u5168\u5c40\u76f8\u4f3c\u6027\u5f97\u5206\u6765\u89e3\u51b3\u8be5\u4efb\u52a1\u3002\u4e3a\u4e86\u6d4b\u8bd5\u5b83\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u5404\u79cd\u516c\u5171\u6570\u636e\u96c6\u4e2d\u6536\u96c6\u9aa8\u8868\u9762\u7ed3\u6784\u7684\u57fa\u51c6\u3002\u6211\u4eec\u7684\u5339\u914d\u6280\u672f\u57fa\u4e8e\u8026\u5408\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u5728\u8fd9\u4e24\u9879\u4efb\u52a1\u4e0a\u90fd\u8fbe\u5230\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u91cd\u73b0\u6211\u4eec\u5b9e\u9a8c\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/matteo-bastico/CoupledLaplacian \u548c\u8865\u5145\u4ee3\u7801\u4e2d\u516c\u5f00\u83b7\u53d6\u3002|[2402.17372v1](http://arxiv.org/pdf/2402.17372v1)|null|\n", "2402.17364": "|**2024-02-27**|**Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis**|\u5b66\u4e60\u52a8\u6001\u56db\u9762\u4f53\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5934\u90e8\u8bf4\u8bdd\u5408\u6210|Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang|Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications.|\u6700\u8fd1\u5728\u9690\u5f0f\u8868\u793a\u65b9\u9762\u7684\u5de5\u4f5c\uff0c\u4f8b\u5982\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\uff0c\u5df2\u7ecf\u4fc3\u8fdb\u4e86\u4ece\u89c6\u9891\u5e8f\u5217\u4e2d\u751f\u6210\u903c\u771f\u4e14\u53ef\u52a8\u753b\u5316\u7684\u5934\u90e8\u5316\u8eab\u3002\u8fd9\u4e9b\u9690\u5f0f\u65b9\u6cd5\u4ecd\u7136\u9762\u4e34\u89c6\u89c9\u4f2a\u5f71\u548c\u6296\u52a8\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u663e\u5f0f\u51e0\u4f55\u7ea6\u675f\u5bf9\u7cbe\u786e\u5efa\u6a21\u590d\u6742\u7684\u9762\u90e8\u53d8\u5f62\u63d0\u51fa\u4e86\u6839\u672c\u6027\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u52a8\u6001\u56db\u9762\u4f53\uff08DynTet\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u8868\u793a\uff0c\u5b83\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5bf9\u663e\u5f0f\u52a8\u6001\u7f51\u683c\u8fdb\u884c\u7f16\u7801\uff0c\u4ee5\u786e\u4fdd\u5404\u79cd\u8fd0\u52a8\u548c\u89c6\u70b9\u4e4b\u95f4\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002 DynTet \u901a\u8fc7\u57fa\u4e8e\u5750\u6807\u7684\u7f51\u7edc\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u8be5\u7f51\u7edc\u5b66\u4e60\u7b26\u53f7\u8ddd\u79bb\u3001\u53d8\u5f62\u548c\u6750\u6599\u7eb9\u7406\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u951a\u5b9a\u5230\u9884\u5b9a\u4e49\u7684\u56db\u9762\u4f53\u7f51\u683c\u4e2d\u3002\u5229\u7528 Marching Tetrahedra\uff0cDynTet \u53ef\u4ee5\u901a\u8fc7\u4e00\u81f4\u7684\u62d3\u6251\u6709\u6548\u89e3\u7801\u7eb9\u7406\u7f51\u683c\uff0c\u4ece\u800c\u901a\u8fc7\u53ef\u5fae\u5206\u5149\u6805\u5668\u5b9e\u73b0\u5feb\u901f\u6e32\u67d3\uff0c\u5e76\u901a\u8fc7\u50cf\u7d20\u635f\u5931\u8fdb\u884c\u76d1\u7763\u3002\u4e3a\u4e86\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u7ecf\u5178\u7684 3D Morphable \u6a21\u578b\u6765\u4fc3\u8fdb\u51e0\u4f55\u5b66\u4e60\uff0c\u5e76\u5b9a\u4e49\u4e00\u4e2a\u89c4\u8303\u7a7a\u95f4\u6765\u7b80\u5316\u7eb9\u7406\u5b66\u4e60\u3002\u7531\u4e8e DynTet \u4e2d\u91c7\u7528\u4e86\u6709\u6548\u7684\u51e0\u4f55\u8868\u793a\uff0c\u8fd9\u4e9b\u4f18\u70b9\u5f88\u5bb9\u6613\u5b9e\u73b0\u3002\u4e0e\u4e4b\u524d\u7684\u4f5c\u54c1\u76f8\u6bd4\uff0c\u6839\u636e\u5404\u79cd\u6307\u6807\uff0cDynTet \u5728\u4fdd\u771f\u5ea6\u3001\u5507\u5f62\u540c\u6b65\u548c\u5b9e\u65f6\u6027\u80fd\u65b9\u9762\u90fd\u6709\u663e\u7740\u6539\u8fdb\u3002\u9664\u4e86\u751f\u6210\u7a33\u5b9a\u4e14\u5177\u6709\u89c6\u89c9\u5438\u5f15\u529b\u7684\u5408\u6210\u89c6\u9891\u4e4b\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fd8\u8f93\u51fa\u52a8\u6001\u7f51\u683c\uff0c\u8fd9\u6709\u671b\u5b9e\u73b0\u8bb8\u591a\u65b0\u5174\u5e94\u7528\u3002|[2402.17364v1](http://arxiv.org/pdf/2402.17364v1)|null|\n", "2402.17351": "|**2024-02-27**|**ICP-Flow: LiDAR Scene Flow Estimation with ICP**|ICP-Flow\uff1a\u5229\u7528 ICP \u8fdb\u884c LiDAR \u573a\u666f\u6d41\u91cf\u4f30\u8ba1|Yancong Lin, Holger Caesar|Scene flow characterizes the 3D motion between two LiDAR scans captured by an autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow as point-wise unconstrained flow vectors that can be learned by either large-scale training beforehand or time-consuming optimization at inference. However, these methods do not take into account that objects in autonomous driving often move rigidly. We incorporate this rigid-motion assumption into our design, where the goal is to associate objects over scans and then estimate the locally rigid transformations. We propose ICP-Flow, a learning-free flow estimator. The core of our design is the conventional Iterative Closest Point (ICP) algorithm, which aligns the objects over time and outputs the corresponding rigid transformations. Crucially, to aid ICP, we propose a histogram-based initialization that discovers the most likely translation, thus providing a good starting point for ICP. The complete scene flow is then recovered from the rigid transformations. We outperform state-of-the-art baselines, including supervised models, on the Waymo dataset and perform competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward neural network, supervised by the pseudo labels from our model, and achieve top performance among all models capable of real-time inference. We validate the advantage of our model on scene flow estimation with longer temporal gaps, up to 0.5 seconds where other models fail to deliver meaningful results.|\u573a\u666f\u6d41\u8868\u5f81\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u9644\u8fd1\u65f6\u95f4\u6b65\u6355\u83b7\u7684\u4e24\u6b21 LiDAR \u626b\u63cf\u4e4b\u95f4\u7684 3D \u8fd0\u52a8\u3002\u6d41\u884c\u7684\u65b9\u6cd5\u5c06\u573a\u666f\u6d41\u89c6\u4e3a\u9010\u70b9\u65e0\u7ea6\u675f\u6d41\u5411\u91cf\uff0c\u53ef\u4ee5\u901a\u8fc7\u9884\u5148\u5927\u89c4\u6a21\u8bad\u7ec3\u6216\u63a8\u7406\u65f6\u8017\u65f6\u7684\u4f18\u5316\u6765\u5b66\u4e60\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6ca1\u6709\u8003\u8651\u5230\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u7269\u4f53\u901a\u5e38\u662f\u521a\u6027\u8fd0\u52a8\u7684\u3002\u6211\u4eec\u5c06\u8fd9\u79cd\u521a\u6027\u8fd0\u52a8\u5047\u8bbe\u7eb3\u5165\u6211\u4eec\u7684\u8bbe\u8ba1\u4e2d\uff0c\u5176\u76ee\u6807\u662f\u901a\u8fc7\u626b\u63cf\u5173\u8054\u5bf9\u8c61\uff0c\u7136\u540e\u4f30\u8ba1\u5c40\u90e8\u521a\u6027\u53d8\u6362\u3002\u6211\u4eec\u63d0\u51fa\u4e86 ICP-Flow\uff0c\u4e00\u79cd\u514d\u5b66\u4e60\u7684\u6d41\u91cf\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u8bbe\u8ba1\u7684\u6838\u5fc3\u662f\u4f20\u7edf\u7684\u8fed\u4ee3\u6700\u8fd1\u70b9\uff08ICP\uff09\u7b97\u6cd5\uff0c\u5b83\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u5bf9\u9f50\u5bf9\u8c61\u5e76\u8f93\u51fa\u76f8\u5e94\u7684\u521a\u6027\u53d8\u6362\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0c\u4e3a\u4e86\u5e2e\u52a9 ICP\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u521d\u59cb\u5316\uff0c\u53ef\u4ee5\u53d1\u73b0\u6700\u53ef\u80fd\u7684\u7ffb\u8bd1\uff0c\u4ece\u800c\u4e3a ICP \u63d0\u4f9b\u4e00\u4e2a\u826f\u597d\u7684\u8d77\u70b9\u3002\u7136\u540e\u4ece\u4e25\u683c\u7684\u53d8\u6362\u4e2d\u6062\u590d\u5b8c\u6574\u7684\u573a\u666f\u6d41\u3002\u6211\u4eec\u5728 Waymo \u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff08\u5305\u62ec\u76d1\u7763\u6a21\u578b\uff09\uff0c\u5e76\u5728 Argoverse-v2 \u548c nuScenes \u4e0a\u5177\u6709\u7ade\u4e89\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u7531\u6a21\u578b\u4e2d\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u76d1\u7763\uff0c\u5e76\u5728\u6240\u6709\u80fd\u591f\u5b9e\u65f6\u63a8\u7406\u7684\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u3002\u6211\u4eec\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5728\u5177\u6709\u8f83\u957f\u65f6\u95f4\u95f4\u9699\uff08\u957f\u8fbe 0.5 \u79d2\uff09\u7684\u573a\u666f\u6d41\u4f30\u8ba1\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u65e0\u6cd5\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u7ed3\u679c\u3002|[2402.17351v1](http://arxiv.org/pdf/2402.17351v1)|null|\n", "2402.17307": "|**2024-02-27**|**Denoising Diffusion Models for Inpainting of Healthy Brain Tissue**|\u7528\u4e8e\u4fee\u590d\u5065\u5eb7\u8111\u7ec4\u7ec7\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b|Alicia Durrer, Philippe C. Cattin, Julia Wolleb|This paper is a contribution to the \"BraTS 2023 Local Synthesis of Healthy Brain Tissue via Inpainting Challenge\". The task of this challenge is to transform tumor tissue into healthy tissue in brain magnetic resonance (MR) images. This idea originates from the problem that MR images can be evaluated using automatic processing tools, however, many of these tools are optimized for the analysis of healthy tissue. By solving the given inpainting task, we enable the automatic analysis of images featuring lesions, and further downstream tasks. Our approach builds on denoising diffusion probabilistic models. We use a 2D model that is trained using slices in which healthy tissue was cropped out and is learned to be inpainted again. This allows us to use the ground truth healthy tissue during training. In the sampling stage, we replace the slices containing diseased tissue in the original 3D volume with the slices containing the healthy tissue inpainting. With our approach, we achieve comparable results to the competing methods. On the validation set our model achieves a mean SSIM of 0.7804, a PSNR of 20.3525 and a MSE of 0.0113. In future we plan to extend our 2D model to a 3D model, allowing to inpaint the region of interest as a whole without losing context information of neighboring slices.|\u672c\u6587\u662f\u5bf9\u201cBraTS 2023 \u901a\u8fc7\u4fee\u590d\u6311\u6218\u5c40\u90e8\u5408\u6210\u5065\u5eb7\u8111\u7ec4\u7ec7\u201d\u7684\u8d21\u732e\u3002\u672c\u6b21\u6311\u6218\u7684\u4efb\u52a1\u662f\u5728\u8111\u78c1\u5171\u632f\uff08MR\uff09\u56fe\u50cf\u4e2d\u5c06\u80bf\u7624\u7ec4\u7ec7\u8f6c\u5316\u4e3a\u5065\u5eb7\u7ec4\u7ec7\u3002\u8fd9\u4e2a\u60f3\u6cd5\u6e90\u4e8e\u8fd9\u6837\u4e00\u4e2a\u95ee\u9898\uff1aMR \u56fe\u50cf\u53ef\u4ee5\u4f7f\u7528\u81ea\u52a8\u5904\u7406\u5de5\u5177\u8fdb\u884c\u8bc4\u4f30\uff0c\u7136\u800c\uff0c\u8fd9\u4e9b\u5de5\u5177\u4e2d\u7684\u8bb8\u591a\u5de5\u5177\u90fd\u9488\u5bf9\u5065\u5eb7\u7ec4\u7ec7\u7684\u5206\u6790\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u901a\u8fc7\u89e3\u51b3\u7ed9\u5b9a\u7684\u4fee\u590d\u4efb\u52a1\uff0c\u6211\u4eec\u53ef\u4ee5\u81ea\u52a8\u5206\u6790\u5177\u6709\u75c5\u53d8\u7279\u5f81\u7684\u56fe\u50cf\u4ee5\u53ca\u8fdb\u4e00\u6b65\u7684\u4e0b\u6e38\u4efb\u52a1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5efa\u7acb\u5728\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u7684\u57fa\u7840\u4e0a\u3002\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a 2D \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u5207\u7247\u8fdb\u884c\u8bad\u7ec3\uff0c\u5176\u4e2d\u5065\u5eb7\u7ec4\u7ec7\u88ab\u526a\u6389\u5e76\u5b66\u4f1a\u518d\u6b21\u4fee\u590d\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u5728\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u771f\u5b9e\u7684\u5065\u5eb7\u7ec4\u7ec7\u3002\u5728\u91c7\u6837\u9636\u6bb5\uff0c\u6211\u4eec\u7528\u5305\u542b\u5065\u5eb7\u7ec4\u7ec7\u4fee\u590d\u7684\u5207\u7247\u66ff\u6362\u539f\u59cb3D\u4f53\u79ef\u4e2d\u5305\u542b\u60a3\u75c5\u7ec4\u7ec7\u7684\u5207\u7247\u3002\u901a\u8fc7\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u53d6\u5f97\u4e86\u4e0e\u7ade\u4e89\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u5728\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5b9e\u73b0\u4e86 0.7804 \u7684\u5e73\u5747 SSIM\u300120.3525 \u7684 PSNR \u548c 0.0113 \u7684 MSE\u3002\u5c06\u6765\uff0c\u6211\u4eec\u8ba1\u5212\u5c06 2D \u6a21\u578b\u6269\u5c55\u5230 3D \u6a21\u578b\uff0c\u4ece\u800c\u53ef\u4ee5\u5bf9\u6574\u4e2a\u611f\u5174\u8da3\u533a\u57df\u8fdb\u884c\u4fee\u590d\uff0c\u800c\u4e0d\u4f1a\u4e22\u5931\u76f8\u90bb\u5207\u7247\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002|[2402.17307v1](http://arxiv.org/pdf/2402.17307v1)|null|\n", "2402.17292": "|**2024-02-27**|**DivAvatar: Diverse 3D Avatar Generation with a Single Prompt**|DivAvatar\uff1a\u901a\u8fc7\u5355\u4e00\u63d0\u793a\u751f\u6210\u591a\u6837\u5316\u7684 3D \u5934\u50cf|Weijing Tao, Biwen Lei, Kunhao Liu, Shijian Lu, Miaomiao Cui, Xuansong Xie, Chunyan Miao|Text-to-Avatar generation has recently made significant strides due to advancements in diffusion models. However, most existing work remains constrained by limited diversity, producing avatars with subtle differences in appearance for a given text prompt. We design DivAvatar, a novel framework that generates diverse avatars, empowering 3D creatives with a multitude of distinct and richly varied 3D avatars from a single text prompt. Different from most existing work that exploits scene-specific 3D representations such as NeRF, DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse avatar generation from simply noise sampling in inference time. DivAvatar has two key designs that help achieve generation diversity and visual quality. The first is a noise sampling technique during training phase which is critical in generating diverse appearances. The second is a semantic-aware zoom mechanism and a novel depth loss, the former producing appearances of high textual fidelity by separate fine-tuning of specific body parts and the latter improving geometry quality greatly by smoothing the generated mesh in the features space. Extensive experiments show that DivAvatar is highly versatile in generating avatars of diverse appearances.|\u7531\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u6587\u672c\u5230\u5934\u50cf\u7684\u751f\u6210\u6700\u8fd1\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u5de5\u4f5c\u4ecd\u7136\u53d7\u5230\u6709\u9650\u7684\u591a\u6837\u6027\u7684\u9650\u5236\uff0c\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u6587\u672c\u63d0\u793a\u751f\u6210\u7684\u5934\u50cf\u5728\u5916\u89c2\u4e0a\u5b58\u5728\u7ec6\u5fae\u7684\u5dee\u5f02\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86 DivAvatar\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u4ee5\u751f\u6210\u591a\u6837\u5316\u5934\u50cf\u7684\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e2a\u6587\u672c\u63d0\u793a\u4e3a 3D \u521b\u610f\u4eba\u5458\u63d0\u4f9b\u5927\u91cf\u72ec\u7279\u4e14\u4e30\u5bcc\u591a\u6837\u7684 3D \u5934\u50cf\u3002\u4e0e\u5927\u591a\u6570\u5229\u7528\u7279\u5b9a\u4e8e\u573a\u666f\u7684 3D \u8868\u793a\uff08\u4f8b\u5982 NeRF\uff09\u7684\u73b0\u6709\u5de5\u4f5c\u4e0d\u540c\uff0cDivAvatar \u5bf9 3D \u751f\u6210\u6a21\u578b\uff08\u5373 EVA3D\uff09\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5141\u8bb8\u5728\u63a8\u7406\u65f6\u95f4\u5185\u901a\u8fc7\u7b80\u5355\u7684\u566a\u58f0\u91c7\u6837\u751f\u6210\u4e0d\u540c\u7684\u5316\u8eab\u3002 DivAvatar \u6709\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\u6709\u52a9\u4e8e\u5b9e\u73b0\u4e16\u4ee3\u591a\u6837\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002\u7b2c\u4e00\u4e2a\u662f\u8bad\u7ec3\u9636\u6bb5\u7684\u566a\u58f0\u91c7\u6837\u6280\u672f\uff0c\u8fd9\u5bf9\u4e8e\u751f\u6210\u4e0d\u540c\u7684\u5916\u89c2\u81f3\u5173\u91cd\u8981\u3002\u7b2c\u4e8c\u4e2a\u662f\u8bed\u4e49\u611f\u77e5\u7f29\u653e\u673a\u5236\u548c\u65b0\u9896\u7684\u6df1\u5ea6\u635f\u5931\uff0c\u524d\u8005\u901a\u8fc7\u5bf9\u7279\u5b9a\u8eab\u4f53\u90e8\u4f4d\u8fdb\u884c\u5355\u72ec\u5fae\u8c03\u6765\u4ea7\u751f\u9ad8\u6587\u672c\u4fdd\u771f\u5ea6\u7684\u5916\u89c2\uff0c\u540e\u8005\u901a\u8fc7\u5e73\u6ed1\u7279\u5f81\u7a7a\u95f4\u4e2d\u751f\u6210\u7684\u7f51\u683c\u6765\u6781\u5927\u5730\u63d0\u9ad8\u51e0\u4f55\u8d28\u91cf\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDivAvatar \u5728\u751f\u6210\u4e0d\u540c\u5916\u89c2\u7684\u5934\u50cf\u65b9\u9762\u5177\u6709\u5f88\u5f3a\u7684\u901a\u7528\u6027\u3002|[2402.17292v1](http://arxiv.org/pdf/2402.17292v1)|null|\n", "2402.17285": "|**2024-02-27**|**Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder Super-resolution Network**|\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u7ec4\u81ea\u52a8\u7f16\u7801\u5668\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\u589e\u5f3a\u9ad8\u5149\u8c31\u56fe\u50cf|Zhaoyang Wang, Dongyang Li, Mingyang Zhang, Hao Luo, Maoguo Gong|Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time. Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically.|\u73b0\u6709\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u65b9\u6cd5\u5f88\u96be\u6709\u6548\u5730\u6355\u83b7\u590d\u6742\u7684\u5149\u8c31\u7a7a\u95f4\u5173\u7cfb\u548c\u4f4e\u7ea7\u7ec6\u8282\uff0c\u800c\u6269\u6563\u6a21\u578b\u4ee3\u8868\u4e86\u4e00\u79cd\u6709\u524d\u9014\u7684\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u5176\u5728\u590d\u6742\u5173\u7cfb\u5efa\u6a21\u548c\u5b66\u4e60\u9ad8\u7ea7\u548c\u4f4e\u7ea7\u7ec6\u8282\u65b9\u9762\u7684\u5353\u8d8a\u6027\u80fd\u800c\u95fb\u540d\u3002\u4f4e\u7ea7\u89c6\u89c9\u7279\u5f81\u3002\u6269\u6563\u6a21\u578b\u5728 HSI SR \u4e2d\u7684\u76f4\u63a5\u5e94\u7528\u53d7\u5230\u6a21\u578b\u6536\u655b\u56f0\u96be\u548c\u63a8\u7406\u65f6\u95f4\u5ef6\u957f\u7b49\u6311\u6218\u7684\u963b\u788d\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ec4\u81ea\u52a8\u7f16\u7801\u5668\uff08GAE\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0e\u6269\u6563\u6a21\u578b\u534f\u540c\u7ed3\u5408\uff0c\u6784\u5efa\u4e86\u9ad8\u6548\u7684 HSI SR \u6a21\u578b\uff08DMGASR\uff09\u3002\u6211\u4eec\u63d0\u51fa\u7684 GAE \u6846\u67b6\u5c06\u9ad8\u7ef4 HSI \u6570\u636e\u7f16\u7801\u5230\u6269\u6563\u6a21\u578b\u5de5\u4f5c\u7684\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u51cf\u8f7b\u4e86\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u96be\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5e26\u76f8\u5173\u6027\u5e76\u5927\u5927\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u3002\u81ea\u7136\u548c\u9065\u611f\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u548c\u6d4b\u91cf\u4e0a\u90fd\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2402.17285v1](http://arxiv.org/pdf/2402.17285v1)|null|\n", "2402.17192": "|**2024-02-27**|**Differentiable Biomechanics Unlocks Opportunities for Markerless Motion Capture**|\u53ef\u5fae\u5206\u751f\u7269\u529b\u5b66\u4e3a\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u5e26\u6765\u673a\u9047|R. James Cotton|Recent developments have created differentiable physics simulators designed for machine learning pipelines that can be accelerated on a GPU. While these can simulate biomechanical models, these opportunities have not been exploited for biomechanics research or markerless motion capture. We show that these simulators can be used to fit inverse kinematics to markerless motion capture data, including scaling the model to fit the anthropomorphic measurements of an individual. This is performed end-to-end with an implicit representation of the movement trajectory, which is propagated through the forward kinematic model to minimize the error from the 3D markers reprojected into the images. The differential optimizer yields other opportunities, such as adding bundle adjustment during trajectory optimization to refine the extrinsic camera parameters or meta-optimization to improve the base model jointly over trajectories from multiple participants. This approach improves the reprojection error from markerless motion capture over prior methods and produces accurate spatial step parameters compared to an instrumented walkway for control and clinical populations.|\u6700\u8fd1\u7684\u53d1\u5c55\u521b\u5efa\u4e86\u4e13\u4e3a\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u8bbe\u8ba1\u7684\u53ef\u5fae\u5206\u7269\u7406\u6a21\u62df\u5668\uff0c\u53ef\u4ee5\u5728 GPU \u4e0a\u52a0\u901f\u3002\u867d\u7136\u8fd9\u4e9b\u53ef\u4ee5\u6a21\u62df\u751f\u7269\u529b\u5b66\u6a21\u578b\uff0c\u4f46\u8fd9\u4e9b\u673a\u4f1a\u5c1a\u672a\u88ab\u7528\u4e8e\u751f\u7269\u529b\u5b66\u7814\u7a76\u6216\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u3002\u6211\u4eec\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u62df\u5668\u53ef\u7528\u4e8e\u5c06\u9006\u8fd0\u52a8\u5b66\u62df\u5408\u5230\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u6570\u636e\uff0c\u5305\u62ec\u7f29\u653e\u6a21\u578b\u4ee5\u9002\u5e94\u4e2a\u4f53\u7684\u62df\u4eba\u5316\u6d4b\u91cf\u3002\u8fd9\u662f\u901a\u8fc7\u8fd0\u52a8\u8f68\u8ff9\u7684\u9690\u5f0f\u8868\u793a\u8fdb\u884c\u7aef\u5230\u7aef\u7684\uff0c\u8be5\u8868\u793a\u901a\u8fc7\u6b63\u5411\u8fd0\u52a8\u5b66\u6a21\u578b\u4f20\u64ad\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u91cd\u65b0\u6295\u5f71\u5230\u56fe\u50cf\u4e2d\u7684 3D \u6807\u8bb0\u7684\u8bef\u5dee\u3002\u5dee\u5206\u4f18\u5316\u5668\u4ea7\u751f\u4e86\u5176\u4ed6\u673a\u4f1a\uff0c\u4f8b\u5982\u5728\u8f68\u8ff9\u4f18\u5316\u671f\u95f4\u6dfb\u52a0\u6346\u7ed1\u8c03\u6574\u4ee5\u7ec6\u5316\u5916\u90e8\u76f8\u673a\u53c2\u6570\u6216\u5143\u4f18\u5316\u4ee5\u5171\u540c\u6539\u8fdb\u6765\u81ea\u591a\u4e2a\u53c2\u4e0e\u8005\u7684\u8f68\u8ff9\u7684\u57fa\u672c\u6a21\u578b\u3002\u4e0e\u4e4b\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6539\u5584\u4e86\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u7684\u91cd\u6295\u5f71\u8bef\u5dee\uff0c\u5e76\u4e14\u4e0e\u7528\u4e8e\u63a7\u5236\u548c\u4e34\u5e8a\u4eba\u7fa4\u7684\u4eea\u8868\u8d70\u9053\u76f8\u6bd4\uff0c\u4ea7\u751f\u4e86\u51c6\u786e\u7684\u7a7a\u95f4\u6b65\u957f\u53c2\u6570\u3002|[2402.17192v1](http://arxiv.org/pdf/2402.17192v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.17767": "|**2024-02-27**|**Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator**|\u4f7f\u7528\u5546\u54c1\u79fb\u52a8\u673a\u68b0\u624b\u6253\u5f00\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6a71\u67dc\u548c\u62bd\u5c49|Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta|Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and build upon our system.|\u62c9\u5f00\u67dc\u5b50\u548c\u62bd\u5c49\u5728\u611f\u77e5\uff08\u4ece\u673a\u8f7d\u4f20\u611f\u5668\u63a8\u65ad\u7269\u4f53\u7684\u5173\u8282\u53c2\u6570\uff09\u3001\u89c4\u5212\uff08\u751f\u6210\u7b26\u5408\u4e25\u683c\u4efb\u52a1\u9650\u5236\u7684\u8fd0\u52a8\u8ba1\u5212\uff09\u548c\u63a7\u5236\uff08\u5728\u5bf9\u73af\u5883\u65bd\u52a0\u529b\u7684\u540c\u65f6\u5efa\u7acb\u548c\u4fdd\u6301\u63a5\u89e6\uff09\u65b9\u9762\u63d0\u51fa\u4e86\u8bb8\u591a\u56f0\u96be\u7684\u6280\u672f\u6311\u6218\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u4f7f\u5546\u54c1\u79fb\u52a8\u673a\u68b0\u624b\uff08Stretch RE2\uff09\u80fd\u591f\u5728\u5404\u79cd\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u62c9\u5f00\u6a71\u67dc\u548c\u62bd\u5c49\u3002\u6211\u4eec\u5bf9\u8be5\u7cfb\u7edf\u8fdb\u884c\u4e86\u4e3a\u671f 4 \u5929\u7684\u73b0\u5b9e\u4e16\u754c\u6d4b\u8bd5\uff0c\u6db5\u76d6 13 \u4e2a\u4e0d\u540c\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684 31 \u4e2a\u4e0d\u540c\u5bf9\u8c61\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u672a\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\u96f6\u6b21\u6253\u5f00\u65b0\u9896\u7684\u6a71\u67dc\u548c\u62bd\u5c49\u7684\u6210\u529f\u7387\u8fbe\u5230 61%\u3002\u5bf9\u6545\u969c\u6a21\u5f0f\u7684\u5206\u6790\u8868\u660e\uff0c\u611f\u77e5\u9519\u8bef\u662f\u6211\u4eec\u7cfb\u7edf\u9762\u4e34\u7684\u6700\u91cd\u5927\u6311\u6218\u3002\u6211\u4eec\u5c06\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u578b\uff0c\u4f9b\u5176\u4ed6\u4eba\u590d\u5236\u548c\u6784\u5efa\u6211\u4eec\u7684\u7cfb\u7edf\u3002|[2402.17767v1](http://arxiv.org/pdf/2402.17767v1)|null|\n", "2402.17589": "|**2024-02-27**|**PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning**|PLReMix\uff1a\u7528\u4f2a\u6807\u7b7e\u677e\u5f1b\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u5bf9\u6297\u566a\u58f0\u6807\u7b7e|Xiaoyu Liu, Beitong Zhou, Cheng Cheng|Recently, the application of Contrastive Representation Learning (CRL) in learning with noisy labels (LNL) has shown promising advancements due to its remarkable ability to learn well-distributed representations for better distinguishing noisy labels. However, CRL is mainly used as a pre-training technique, leading to a complicated multi-stage training pipeline. We also observed that trivially combining CRL with supervised LNL methods decreases performance. Using different images from the same class as negative pairs in CRL creates optimization conflicts between CRL and the supervised loss. To address these two issues, we propose an end-to-end PLReMix framework that avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR) contrastive loss to alleviate the conflicts between losses. This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs that overlap at the top k indices of prediction probabilities, leading to more compact semantic clusters than vanilla CRL. Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simultaneously, which is expanded on the previously widely used one-dimensional form. The PLR loss and a semi-supervised loss are simultaneously applied to train on the GMM divided clean and noisy samples. Experiments on multiple benchmark datasets demonstrate the effectiveness of the proposed method. Our proposed PLR loss is scalable, which can be easily integrated into other LNL methods and boost their performance. Codes will be available.|\u6700\u8fd1\uff0c\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\uff08CRL\uff09\u5728\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\uff08LNL\uff09\u4e2d\u7684\u5e94\u7528\u663e\u793a\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u8fdb\u6b65\uff0c\u56e0\u4e3a\u5b83\u5177\u6709\u5b66\u4e60\u5747\u5300\u5206\u5e03\u8868\u793a\u4ee5\u66f4\u597d\u5730\u533a\u5206\u566a\u58f0\u6807\u7b7e\u7684\u5353\u8d8a\u80fd\u529b\u3002\u7136\u800c\uff0cCRL \u4e3b\u8981\u7528\u4f5c\u9884\u8bad\u7ec3\u6280\u672f\uff0c\u5bfc\u81f4\u590d\u6742\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\uff0c\u5c06 CRL \u4e0e\u76d1\u7763 LNL \u65b9\u6cd5\u7ed3\u5408\u8d77\u6765\u4f1a\u964d\u4f4e\u6027\u80fd\u3002\u5728 CRL \u4e2d\u4f7f\u7528\u540c\u4e00\u7c7b\u7684\u4e0d\u540c\u56fe\u50cf\u4f5c\u4e3a\u8d1f\u5bf9\u4f1a\u9020\u6210 CRL \u548c\u76d1\u7763\u635f\u5931\u4e4b\u95f4\u7684\u4f18\u5316\u51b2\u7a81\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684 PLReMix \u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4f2a\u6807\u7b7e\u677e\u5f1b\uff08PLR\uff09\u5bf9\u6bd4\u635f\u5931\u6765\u7f13\u89e3\u635f\u5931\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4ece\u800c\u907f\u514d\u590d\u6742\u7684\u6d41\u7a0b\u3002\u8fd9\u79cd PLR \u635f\u5931\u901a\u8fc7\u8fc7\u6ee4\u6389\u5728\u9884\u6d4b\u6982\u7387\u7684\u524d k \u4e2a\u7d22\u5f15\u5904\u91cd\u53e0\u7684\u4e0d\u9002\u5f53\u7684\u8d1f\u5bf9\uff0c\u6784\u5efa\u6bcf\u4e2a\u6837\u672c\u7684\u53ef\u9760\u8d1f\u96c6\uff0c\u4ece\u800c\u4ea7\u751f\u6bd4\u666e\u901a CRL \u66f4\u7d27\u51d1\u7684\u8bed\u4e49\u7c07\u3002\u6b64\u5916\uff0c\u91c7\u7528\u4e8c\u7ef4\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\uff0c\u901a\u8fc7\u540c\u65f6\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u548c\u6a21\u578b\u8f93\u51fa\u6765\u533a\u5206\u5e72\u51c0\u6837\u672c\u548c\u566a\u58f0\u6837\u672c\uff0c\u8be5\u6a21\u578b\u662f\u5bf9\u5148\u524d\u5e7f\u6cdb\u4f7f\u7528\u7684\u4e00\u7ef4\u5f62\u5f0f\u7684\u6269\u5c55\u3002 PLR \u635f\u5931\u548c\u534a\u76d1\u7763\u635f\u5931\u540c\u65f6\u5e94\u7528\u4e8e GMM \u5212\u5206\u7684\u5e72\u51c0\u6837\u672c\u548c\u566a\u58f0\u6837\u672c\u4e0a\u7684\u8bad\u7ec3\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u63d0\u51fa\u7684 PLR \u635f\u5931\u662f\u53ef\u6269\u5c55\u7684\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u5176\u4ed6 LNL \u65b9\u6cd5\u4e2d\u5e76\u63d0\u9ad8\u5176\u6027\u80fd\u3002\u4ee3\u7801\u5c06\u53ef\u7528\u3002|[2402.17589v1](http://arxiv.org/pdf/2402.17589v1)|null|\n", "2402.17510": "|**2024-02-27**|**Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning**|\u5c55\u793a\u548c\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u8868\u5f81\u5b66\u4e60\u7684\u6377\u5f84|Maurits Bleeker, Mariya Hendriksen, Andrew Yates, Maarten de Rijke|Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcut. Hence, contrastive losses are not sufficient to learn task-optimal representations, i.e., representations that contain all task-relevant information shared between the image and associated captions. We examine two methods to reduce shortcut learning in our training and evaluation framework: (i) latent target decoding and (ii) implicit feature modification. We show empirically that both methods improve performance on the evaluation task, but only partly reduce shortcut learning when training and evaluating with our shortcut learning framework. Hence, we show the difficulty and challenge of our shortcut learning framework for contrastive vision-language representation learning.|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e3b\u8981\u4f9d\u9760\u5bf9\u6bd4\u8bad\u7ec3\u6765\u5b66\u4e60\u56fe\u50cf\u548c\u6807\u9898\u7684\u901a\u7528\u8868\u793a\u3002\u6211\u4eec\u5173\u6ce8\u7684\u662f\u5f53\u4e00\u5f20\u56fe\u50cf\u4e0e\u591a\u4e2a\u6807\u9898\u76f8\u5173\u8054\u65f6\u7684\u60c5\u51b5\uff0c\u6bcf\u4e2a\u6807\u9898\u5305\u542b\u6240\u6709\u6807\u9898\u4e4b\u95f4\u5171\u4eab\u7684\u4fe1\u606f\u4ee5\u53ca\u6bcf\u4e2a\u6807\u9898\u5173\u4e8e\u56fe\u50cf\u4e2d\u63cf\u7ed8\u7684\u573a\u666f\u7684\u552f\u4e00\u4fe1\u606f\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5c1a\u4e0d\u6e05\u695a\u5bf9\u6bd4\u635f\u5931\u662f\u5426\u8db3\u4ee5\u5b66\u4e60\u5305\u542b\u5b57\u5e55\u63d0\u4f9b\u7684\u6240\u6709\u4fe1\u606f\u7684\u4efb\u52a1\u6700\u4f73\u8868\u793a\uff0c\u6216\u8005\u5bf9\u6bd4\u5b66\u4e60\u8bbe\u7f6e\u662f\u5426\u9f13\u52b1\u5b66\u4e60\u6700\u5c0f\u5316\u5bf9\u6bd4\u635f\u5931\u7684\u7b80\u5355\u6377\u5f84\u3002\u6211\u4eec\u5f15\u5165\u4e86\u89c6\u89c9\u8bed\u8a00\u7684\u5408\u6210\u5feb\u6377\u65b9\u5f0f\uff1a\u4e00\u4e2a\u8bad\u7ec3\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u6211\u4eec\u5c06\u5408\u6210\u5feb\u6377\u65b9\u5f0f\u6ce8\u5165\u5230\u56fe\u50cf\u6587\u672c\u6570\u636e\u4e2d\u3002\u6211\u4eec\u8868\u660e\uff0c\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6216\u4f7f\u7528\u5305\u542b\u8fd9\u4e9b\u5408\u6210\u5feb\u6377\u65b9\u5f0f\u7684\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u7684\u5bf9\u6bd4 VLM \u4e3b\u8981\u5b66\u4e60\u4ee3\u8868\u5feb\u6377\u65b9\u5f0f\u7684\u7279\u5f81\u3002\u56e0\u6b64\uff0c\u5bf9\u6bd4\u635f\u5931\u4e0d\u8db3\u4ee5\u5b66\u4e60\u4efb\u52a1\u6700\u4f73\u8868\u793a\uff0c\u5373\u5305\u542b\u56fe\u50cf\u548c\u76f8\u5173\u6807\u9898\u4e4b\u95f4\u5171\u4eab\u7684\u6240\u6709\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u8868\u793a\u3002\u6211\u4eec\u7814\u7a76\u4e86\u4e24\u79cd\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u6846\u67b6\u4e2d\u51cf\u5c11\u5feb\u6377\u5b66\u4e60\u7684\u65b9\u6cd5\uff1a\uff08i\uff09\u6f5c\u5728\u76ee\u6807\u89e3\u7801\u548c\uff08ii\uff09\u9690\u5f0f\u7279\u5f81\u4fee\u6539\u3002\u6211\u4eec\u7684\u7ecf\u9a8c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u63d0\u9ad8\u4e86\u8bc4\u4f30\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4f46\u5728\u4f7f\u7528\u6211\u4eec\u7684\u5feb\u6377\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u65f6\u4ec5\u90e8\u5206\u51cf\u5c11\u4e86\u5feb\u6377\u5b66\u4e60\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u7684\u5feb\u6377\u5b66\u4e60\u6846\u67b6\u7684\u56f0\u96be\u548c\u6311\u6218\u3002|[2402.17510v1](http://arxiv.org/pdf/2402.17510v1)|null|\n", "2402.17486": "|**2024-02-27**|**MGE: A Training-Free and Efficient Model Generation and Enhancement Scheme**|MGE\uff1a\u514d\u8bad\u7ec3\u7684\u9ad8\u6548\u6a21\u578b\u751f\u6210\u548c\u589e\u5f3a\u65b9\u6848|Xuan Wang, Zeshan Pang, Yuliang Lu, Xuehu Yan|To provide a foundation for the research of deep learning models, the construction of model pool is an essential step. This paper proposes a Training-Free and Efficient Model Generation and Enhancement Scheme (MGE). This scheme primarily considers two aspects during the model generation process: the distribution of model parameters and model performance. Experiments result shows that generated models are comparable to models obtained through normal training, and even superior in some cases. Moreover, the time consumed in generating models accounts for only 1\\% of the time required for normal model training. More importantly, with the enhancement of Evolution-MGE, generated models exhibits competitive generalization ability in few-shot tasks. And the behavioral dissimilarity of generated models has the potential of adversarial defense.|\u4e3a\u4e86\u7ed9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\uff0c\u6a21\u578b\u6c60\u7684\u6784\u5efa\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u4e00\u6b65\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u514d\u8bad\u7ec3\u4e14\u9ad8\u6548\u7684\u6a21\u578b\u751f\u6210\u548c\u589e\u5f3a\u65b9\u6848\uff08MGE\uff09\u3002\u8be5\u65b9\u6848\u5728\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e3b\u8981\u8003\u8651\u4e24\u4e2a\u65b9\u9762\uff1a\u6a21\u578b\u53c2\u6570\u7684\u5206\u5e03\u548c\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u7684\u6a21\u578b\u4e0e\u6b63\u5e38\u8bad\u7ec3\u83b7\u5f97\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u66f4\u4f18\u8d8a\u3002\u800c\u4e14\uff0c\u751f\u6210\u6a21\u578b\u6240\u6d88\u8017\u7684\u65f6\u95f4\u4ec5\u5360\u6b63\u5e38\u6a21\u578b\u8bad\u7ec3\u6240\u9700\u65f6\u95f4\u76841\\%\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u968f\u7740 Evolution-MGE \u7684\u589e\u5f3a\uff0c\u751f\u6210\u7684\u6a21\u578b\u5728\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u751f\u6210\u6a21\u578b\u7684\u884c\u4e3a\u5dee\u5f02\u5177\u6709\u5bf9\u6297\u6027\u9632\u5fa1\u7684\u6f5c\u529b\u3002|[2402.17486v1](http://arxiv.org/pdf/2402.17486v1)|null|\n", "2402.17298": "|**2024-02-27**|**ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks**|ArcSin\uff1a\u7528\u4e8e\u8bed\u8a00\u9a71\u52a8\u89c6\u89c9\u4efb\u52a1\u7684\u81ea\u9002\u5e94\u8303\u56f4\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6ce8\u5165\u566a\u58f0|Yang Liu, Xiaomin Yu, Gongyu Zhang, Christos Bergeles, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin|In this study, we address the challenging task of bridging the modality gap between learning from language and inference for visual tasks, including Visual Question Answering (VQA), Image Captioning (IC) and Visual Entailment (VE). We train models for these tasks in a zero-shot cross-modal transfer setting, a domain where the previous state-of-the-art method relied on the fixed scale noise injection, often compromising the semantic content of the original modality embedding. To combat it, we propose a novel method called Adaptive ranged cosine Similarity injected noise (ArcSin). First, we introduce an innovative adaptive noise scale that effectively generates the textual elements with more variability while preserving the original text feature's integrity. Second, a similarity pool strategy is employed, expanding the domain generalization potential by broadening the overall noise scale. This dual strategy effectively widens the scope of the original domain while safeguarding content integrity. Our empirical results demonstrate that these models closely rival those trained on images in terms of performance. Specifically, our method exhibits substantial improvements over the previous state-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively. Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries of what is achievable within the constraints of image-trained model benchmarks. The code will be released.|\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u5f25\u5408\u8bed\u8a00\u5b66\u4e60\u548c\u89c6\u89c9\u4efb\u52a1\u63a8\u7406\u4e4b\u95f4\u6a21\u6001\u5dee\u8ddd\u7684\u6311\u6218\u6027\u4efb\u52a1\uff0c\u5305\u62ec\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u3001\u56fe\u50cf\u5b57\u5e55\uff08IC\uff09\u548c\u89c6\u89c9\u8574\u6db5\uff08VE\uff09\u3002\u6211\u4eec\u5728\u96f6\u6837\u672c\u8de8\u6a21\u6001\u4f20\u8f93\u8bbe\u7f6e\u4e2d\u8bad\u7ec3\u8fd9\u4e9b\u4efb\u52a1\u7684\u6a21\u578b\uff0c\u5728\u8be5\u9886\u57df\u4e2d\uff0c\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u5c3a\u5ea6\u7684\u566a\u58f0\u6ce8\u5165\uff0c\u901a\u5e38\u4f1a\u635f\u5bb3\u539f\u59cb\u6a21\u6001\u5d4c\u5165\u7684\u8bed\u4e49\u5185\u5bb9\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u81ea\u9002\u5e94\u8303\u56f4\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6ce8\u5165\u566a\u58f0\uff08ArcSin\uff09\u7684\u65b0\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u81ea\u9002\u5e94\u566a\u58f0\u5c3a\u5ea6\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u751f\u6210\u5177\u6709\u66f4\u591a\u53ef\u53d8\u6027\u7684\u6587\u672c\u5143\u7d20\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u6587\u672c\u7279\u5f81\u7684\u5b8c\u6574\u6027\u3002\u5176\u6b21\uff0c\u91c7\u7528\u76f8\u4f3c\u6c60\u7b56\u7565\uff0c\u901a\u8fc7\u6269\u5927\u6574\u4f53\u566a\u58f0\u89c4\u6a21\u6765\u6269\u5927\u9886\u57df\u6cdb\u5316\u6f5c\u529b\u3002\u8fd9\u79cd\u53cc\u91cd\u7b56\u7565\u6709\u6548\u5730\u6269\u5927\u4e86\u539f\u59cb\u57df\u7684\u8303\u56f4\uff0c\u540c\u65f6\u7ef4\u62a4\u4e86\u5185\u5bb9\u7684\u5b8c\u6574\u6027\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u6027\u80fd\u65b9\u9762\u4e0e\u56fe\u50cf\u8bad\u7ec3\u7684\u6a21\u578b\u975e\u5e38\u63a5\u8fd1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u6709\u4e86\u663e\u7740\u7684\u6539\u8fdb\uff0c\u5728 S-Cap \u548c M-Cap \u4e2d\u5206\u522b\u5b9e\u73b0\u4e86 1.9 \u548c 1.1 CIDEr \u70b9\u7684\u589e\u76ca\u3002\u6b64\u5916\uff0c\u6211\u4eec\u89c2\u5bdf\u5230 VQA\u3001VQA-E \u548c VE \u7684\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e86 1.5 \u4e2a\u767e\u5206\u70b9 (pp)\u30011.4 \u4e2a\u767e\u5206\u70b9\u548c 1.4 \u4e2a\u767e\u5206\u70b9\uff0c\u7a81\u7834\u4e86\u56fe\u50cf\u8bad\u7ec3\u6a21\u578b\u57fa\u51c6\u7684\u9650\u5236\u5185\u53ef\u5b9e\u73b0\u7684\u754c\u9650\u3002\u4ee3\u7801\u5c06\u88ab\u53d1\u5e03\u3002|[2402.17298v1](http://arxiv.org/pdf/2402.17298v1)|null|\n", "2402.17251": "|**2024-02-27**|**Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning**|\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u57fa\u4e8e\u4e0a\u4e0b\u6587\u548c\u591a\u6837\u6027\u9a71\u52a8\u7684\u7279\u5f02\u6027|Yun Li, Zhe Liu, Hang Chen, Lina Yao|Compositional Zero-Shot Learning (CZSL) aims to recognize unseen attribute-object pairs based on a limited set of observed examples. Current CZSL methodologies, despite their advancements, tend to neglect the distinct specificity levels present in attributes. For instance, given images of sliced strawberries, they may fail to prioritize `Sliced-Strawberry' over a generic `Red-Strawberry', despite the former being more informative. They also suffer from ballooning search space when shifting from Close-World (CW) to Open-World (OW) CZSL. To address the issues, we introduce the Context-based and Diversity-driven Specificity learning framework for CZSL (CDS-CZSL). Our framework evaluates the specificity of attributes by considering the diversity of objects they apply to and their related context. This novel approach allows for more accurate predictions by emphasizing specific attribute-object pairs and improves composition filtering in OW-CZSL. We conduct experiments in both CW and OW scenarios, and our model achieves state-of-the-art results across three datasets.|\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\uff08CZSL\uff09\u65e8\u5728\u57fa\u4e8e\u4e00\u7ec4\u6709\u9650\u7684\u89c2\u5bdf\u5230\u7684\u793a\u4f8b\u6765\u8bc6\u522b\u770b\u4e0d\u89c1\u7684\u5c5e\u6027-\u5bf9\u8c61\u5bf9\u3002\u5f53\u524d\u7684 CZSL \u65b9\u6cd5\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u5c5e\u6027\u4e2d\u5b58\u5728\u7684\u4e0d\u540c\u7279\u5f02\u6027\u6c34\u5e73\u3002\u4f8b\u5982\uff0c\u7ed9\u5b9a\u5207\u7247\u8349\u8393\u7684\u56fe\u50cf\uff0c\u4ed6\u4eec\u53ef\u80fd\u65e0\u6cd5\u5c06\u201c\u5207\u7247\u8349\u8393\u201d\u4f18\u5148\u4e8e\u901a\u7528\u7684\u201c\u7ea2\u8349\u8393\u201d\uff0c\u5c3d\u7ba1\u524d\u8005\u4fe1\u606f\u66f4\u4e30\u5bcc\u3002\u5f53\u4ece\u5c01\u95ed\u4e16\u754c (CW) \u8f6c\u79fb\u5230\u5f00\u653e\u4e16\u754c (OW) CZSL \u65f6\uff0c\u5b83\u4eec\u8fd8\u4f1a\u906d\u53d7\u641c\u7d22\u7a7a\u95f4\u81a8\u80c0\u7684\u56f0\u6270\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u57fa\u4e8e\u60c5\u5883\u548c\u591a\u6837\u6027\u9a71\u52a8\u7684 CZSL \u7279\u5f02\u6027\u5b66\u4e60\u6846\u67b6\uff08CDS-CZSL\uff09\u3002\u6211\u4eec\u7684\u6846\u67b6\u901a\u8fc7\u8003\u8651\u5c5e\u6027\u6240\u5e94\u7528\u7684\u5bf9\u8c61\u7684\u591a\u6837\u6027\u53ca\u5176\u76f8\u5173\u4e0a\u4e0b\u6587\u6765\u8bc4\u4f30\u5c5e\u6027\u7684\u7279\u6b8a\u6027\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u901a\u8fc7\u5f3a\u8c03\u7279\u5b9a\u7684\u5c5e\u6027-\u5bf9\u8c61\u5bf9\u6765\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u9884\u6d4b\uff0c\u5e76\u6539\u8fdb OW-CZSL \u4e2d\u7684\u7ec4\u5408\u8fc7\u6ee4\u3002\u6211\u4eec\u5728 CW \u548c OW \u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002|[2402.17251v1](http://arxiv.org/pdf/2402.17251v1)|null|\n"}, "\u5176\u4ed6": {"2402.17768": "|**2024-02-27**|**Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning**|Diffusion \u9047\u4e0a DAgger\uff1a\u589e\u5f3a\u624b\u773c\u6a21\u4eff\u5b66\u4e60|Xiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh Gupta|A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models. This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%. DMD also outperform competing NeRF-based augmentation schemes by 50%.|\u901a\u8fc7\u6a21\u4eff\u8bad\u7ec3\u7684\u7b56\u7565\u7684\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u662f\u5728\u6d4b\u8bd5\u65f6\u590d\u5408\u6267\u884c\u9519\u8bef\u3002\u5f53\u5b66\u4e60\u5230\u7684\u7b56\u7565\u9047\u5230\u4e13\u5bb6\u6f14\u793a\u4e2d\u672a\u51fa\u73b0\u7684\u72b6\u6001\u65f6\uff0c\u8be5\u7b56\u7565\u5c31\u4f1a\u5931\u8d25\uff0c\u4ece\u800c\u5bfc\u81f4\u9000\u5316\u884c\u4e3a\u3002\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u6570\u636e\u96c6\u805a\u5408\u6216 DAgger \u65b9\u6cd5\u53ea\u662f\u6536\u96c6\u66f4\u591a\u6570\u636e\u6765\u8986\u76d6\u8fd9\u4e9b\u6545\u969c\u72b6\u6001\u3002\u7136\u800c\uff0c\u5728\u5b9e\u8df5\u4e2d\uff0c\u8fd9\u901a\u5e38\u975e\u5e38\u6602\u8d35\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Diffusion Meets DAgger (DMD)\uff0c\u8fd9\u662f\u4e00\u79cd\u83b7\u5f97 DAgger \u4f18\u52bf\u7684\u65b9\u6cd5\uff0c\u800c\u65e0\u9700\u4ed8\u51fa\u624b\u773c\u6a21\u4eff\u5b66\u4e60\u95ee\u9898\u7684\u4ee3\u4ef7\u3002 DMD \u4e0d\u662f\u6536\u96c6\u65b0\u6837\u672c\u6765\u8986\u76d6\u5206\u5e03\u5916\u72b6\u6001\uff0c\u800c\u662f\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u6765\u901a\u8fc7\u6269\u6563\u6a21\u578b\u521b\u5efa\u8fd9\u4e9b\u6837\u672c\u3002\u8fd9\u4f7f\u5f97\u901a\u8fc7\u5f88\u5c11\u7684\u6f14\u793a\u5c31\u80fd\u83b7\u5f97\u5f3a\u5927\u7684\u6027\u80fd\u3002\u5728 Franka Research 3 \u4e0a\u8fdb\u884c\u7684\u975e\u6293\u63e1\u5f0f\u63a8\u52a8\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u8868\u660e\uff0cDMD \u53ea\u9700 8 \u540d\u4e13\u5bb6\u6f14\u793a\u5373\u53ef\u5b9e\u73b0 80% \u7684\u6210\u529f\u7387\uff0c\u800c\u5e7c\u7a1a\u884c\u4e3a\u514b\u9686\u4ec5\u8fbe\u5230 20%\u3002 DMD \u7684\u6027\u80fd\u4e5f\u6bd4\u57fa\u4e8e NeRF \u7684\u7ade\u4e89\u589e\u5f3a\u65b9\u6848\u9ad8\u51fa 50%\u3002|[2402.17768v1](http://arxiv.org/pdf/2402.17768v1)|null|\n", "2402.17729": "|**2024-02-27**|**Towards Fairness-Aware Adversarial Learning**|\u8fc8\u5411\u516c\u5e73\u610f\u8bc6\u7684\u5bf9\u6297\u6027\u5b66\u4e60|Yanghao Zhang, Tianle Zhang, Ronghui Mu, Xiaowei Huang, Wenjie Ruan|Although adversarial training (AT) has proven effective in enhancing the model's robustness, the recently revealed issue of fairness in robustness has not been well addressed, i.e. the robust accuracy varies significantly among different categories. In this paper, instead of uniformly evaluating the model's average class performance, we delve into the issue of robust fairness, by considering the worst-case distribution across various classes. We propose a novel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As a generalization of conventional AT, we re-define the problem of adversarial training as a min-max-max framework, to ensure both robustness and fairness of the trained model. Specifically, by taking advantage of distributional robust optimization, our method aims to find the worst distribution among different categories, and the solution is guaranteed to obtain the upper bound performance with high probability. In particular, FAAL can fine-tune an unfair robust model to be fair within only two epochs, without compromising the overall clean and robust accuracies. Extensive experiments on various image datasets validate the superior performance and efficiency of the proposed FAAL compared to other state-of-the-art methods.|\u5c3d\u7ba1\u5bf9\u6297\u6027\u8bad\u7ec3\uff08AT\uff09\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u6700\u8fd1\u66b4\u9732\u7684\u9c81\u68d2\u6027\u516c\u5e73\u6027\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5f88\u597d\u7684\u89e3\u51b3\uff0c\u5373\u4e0d\u540c\u7c7b\u522b\u4e4b\u95f4\u7684\u9c81\u68d2\u51c6\u786e\u6027\u5b58\u5728\u663e\u7740\u5dee\u5f02\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6ca1\u6709\u7edf\u4e00\u8bc4\u4f30\u6a21\u578b\u7684\u5e73\u5747\u7c7b\u522b\u8868\u73b0\uff0c\u800c\u662f\u901a\u8fc7\u8003\u8651\u5404\u4e2a\u7c7b\u522b\u7684\u6700\u574f\u60c5\u51b5\u5206\u5e03\u6765\u6df1\u5165\u7814\u7a76\u7a33\u5065\u516c\u5e73\u6027\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u79f0\u4e3a\u516c\u5e73\u611f\u77e5\u5bf9\u6297\u5b66\u4e60\uff08FAAL\uff09\u3002\u4f5c\u4e3a\u4f20\u7edf AT \u7684\u63a8\u5e7f\uff0c\u6211\u4eec\u5c06\u5bf9\u6297\u8bad\u7ec3\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6700\u5c0f-\u6700\u5927-\u6700\u5927\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u8bad\u7ec3\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u901a\u8fc7\u5229\u7528\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u65e8\u5728\u627e\u5230\u4e0d\u540c\u7c7b\u522b\u4e4b\u95f4\u7684\u6700\u5dee\u5206\u5e03\uff0c\u5e76\u4e14\u4fdd\u8bc1\u89e3\u51b3\u65b9\u6848\u4ee5\u9ad8\u6982\u7387\u83b7\u5f97\u4e0a\u9650\u6027\u80fd\u3002\u7279\u522b\u662f\uff0cFAAL \u53ef\u4ee5\u5728\u4ec5\u4e24\u4e2a epoch \u5185\u5c06\u4e0d\u516c\u5e73\u7684\u9c81\u68d2\u6a21\u578b\u5fae\u8c03\u4e3a\u516c\u5e73\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u6574\u4f53\u7684\u5e72\u51c0\u548c\u9c81\u68d2\u7cbe\u5ea6\u3002\u5bf9\u5404\u79cd\u56fe\u50cf\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684 FAAL \u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u6548\u7387\u3002|[2402.17729v1](http://arxiv.org/pdf/2402.17729v1)|**[link](https://github.com/TrustAI/FAAL)**|\n", "2402.17723": "|**2024-02-27**|**Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners**|\u89c6\u89c9\u548c\u542c\u89c9\uff1a\u5177\u6709\u6269\u6563\u6f5c\u5728\u5bf9\u51c6\u5668\u7684\u5f00\u653e\u57df\u89c6\u97f3\u9891\u751f\u6210|Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, Qifeng Chen|Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation. We observe the powerful generation ability of off-the-shelf video or audio generation models. Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space. Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model. Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time. Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks. The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/|\u89c6\u97f3\u9891\u5185\u5bb9\u521b\u4f5c\u662f\u7535\u5f71\u884c\u4e1a\u548c\u4e13\u4e1a\u7528\u6237\u7684\u6838\u5fc3\u6280\u672f\u3002\u6700\u8fd1\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5206\u522b\u5904\u7406\u89c6\u9891\u548c\u97f3\u9891\u7684\u751f\u6210\uff0c\u8fd9\u963b\u788d\u4e86\u4ece\u5b66\u672f\u754c\u5230\u5de5\u4e1a\u754c\u7684\u6280\u672f\u8f6c\u79fb\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u8de8\u89c6\u97f3\u9891\u548c\u8054\u5408\u89c6\u97f3\u9891\u751f\u6210\u6846\u67b6\u6765\u586b\u8865\u7a7a\u767d\u3002\u6211\u4eec\u89c2\u5bdf\u5230\u73b0\u6210\u7684\u89c6\u9891\u6216\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u5f3a\u5927\u751f\u6210\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5efa\u8bae\u5c06\u73b0\u6709\u7684\u5f3a\u6a21\u578b\u4e0e\u5171\u4eab\u7684\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u8fde\u63a5\u8d77\u6765\uff0c\u800c\u4e0d\u662f\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u5de8\u578b\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u9884\u8bad\u7ec3 ImageBind \u6a21\u578b\u7684\u591a\u6a21\u6001\u6f5c\u5728\u5bf9\u51c6\u5668\u3002\u6211\u4eec\u7684\u6f5c\u5728\u5bf9\u9f50\u5668\u4e0e\u5206\u7c7b\u5668\u6307\u5bfc\u5171\u4eab\u76f8\u4f3c\u7684\u6838\u5fc3\uff0c\u5206\u7c7b\u5668\u6307\u5bfc\u5728\u63a8\u7406\u65f6\u95f4\u5185\u6307\u5bfc\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4f18\u5316\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8054\u5408\u89c6\u9891\u97f3\u9891\u751f\u6210\u3001\u89c6\u89c9\u5f15\u5bfc\u97f3\u9891\u751f\u6210\u548c\u97f3\u9891\u5f15\u5bfc\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5353\u8d8a\u6027\u80fd\u3002\u9879\u76ee\u7f51\u7ad9\u53ef\u4ee5\u5728https://yzxing87.github.io/Seeing-and-Hearing/\u627e\u5230|[2402.17723v1](http://arxiv.org/pdf/2402.17723v1)|null|\n", "2402.17664": "|**2024-02-27**|**Bayesian Differentiable Physics for Cloth Digitalization**|\u7528\u4e8e\u5e03\u6599\u6570\u5b57\u5316\u7684\u8d1d\u53f6\u65af\u5fae\u5206\u7269\u7406|Deshan Gong, Ningtao Mao, He Wang|We propose a new method for cloth digitalization. Deviating from existing methods which learn from data captured under relatively casual settings, we propose to learn from data captured in strictly tested measuring protocols, and find plausible physical parameters of the cloths. However, such data is currently absent, so we first propose a new dataset with accurate cloth measurements. Further, the data size is considerably smaller than the ones in current deep learning, due to the nature of the data capture process. To learn from small data, we propose a new Bayesian differentiable cloth model to estimate the complex material heterogeneity of real cloths. It can provide highly accurate digitalization from very limited data samples. Through exhaustive evaluation and comparison, we show our method is accurate in cloth digitalization, efficient in learning from limited data samples, and general in capturing material variations. Code and data are available https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5e03\u6599\u6570\u5b57\u5316\u7684\u65b0\u65b9\u6cd5\u3002\u4e0e\u4ece\u76f8\u5bf9\u968f\u610f\u7684\u8bbe\u7f6e\u4e0b\u6355\u83b7\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u5efa\u8bae\u4ece\u4e25\u683c\u6d4b\u8bd5\u7684\u6d4b\u91cf\u534f\u8bae\u4e2d\u6355\u83b7\u7684\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5e76\u627e\u5230\u8863\u670d\u7684\u5408\u7406\u7269\u7406\u53c2\u6570\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u6b64\u7c7b\u6570\u636e\uff0c\u56e0\u6b64\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e00\u4e2a\u5177\u6709\u7cbe\u786e\u5e03\u6599\u6d4b\u91cf\u7684\u65b0\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u6570\u636e\u6355\u83b7\u8fc7\u7a0b\u7684\u6027\u8d28\uff0c\u6570\u636e\u5927\u5c0f\u6bd4\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5c0f\u5f97\u591a\u3002\u4e3a\u4e86\u4ece\u5c0f\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u53ef\u5fae\u5e03\u6599\u6a21\u578b\u6765\u4f30\u8ba1\u771f\u5b9e\u5e03\u6599\u7684\u590d\u6742\u6750\u6599\u5f02\u8d28\u6027\u3002\u5b83\u53ef\u4ee5\u4ece\u975e\u5e38\u6709\u9650\u7684\u6570\u636e\u6837\u672c\u4e2d\u63d0\u4f9b\u9ad8\u5ea6\u51c6\u786e\u7684\u6570\u5b57\u5316\u3002\u901a\u8fc7\u8be6\u5c3d\u7684\u8bc4\u4f30\u548c\u6bd4\u8f83\uff0c\u6211\u4eec\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5e03\u6599\u6570\u5b57\u5316\u65b9\u9762\u662f\u51c6\u786e\u7684\uff0c\u4ece\u6709\u9650\u7684\u6570\u636e\u6837\u672c\u4e2d\u5b66\u4e60\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5728\u6355\u6349\u6750\u6599\u53d8\u5316\u65b9\u9762\u5177\u6709\u666e\u904d\u6027\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u7528 https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization|[2402.17664v1](http://arxiv.org/pdf/2402.17664v1)|null|\n", "2402.17624": "|**2024-02-27**|**CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing**|CustomSketching\uff1a\u57fa\u4e8e\u8349\u56fe\u7684\u56fe\u50cf\u5408\u6210\u548c\u7f16\u8f91\u7684\u8349\u56fe\u6982\u5ff5\u63d0\u53d6|Chufeng Xiao, Hongbo Fu|Personalization techniques for large text-to-image (T2I) models allow users to incorporate new concepts from reference images. However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details). In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture. This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level. To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts. Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction. We employ a shape loss and a regularization loss to balance fidelity and editability during optimization. Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines.|\u5927\u578b\u6587\u672c\u5230\u56fe\u50cf (T2I) \u6a21\u578b\u7684\u4e2a\u6027\u5316\u6280\u672f\u5141\u8bb8\u7528\u6237\u6574\u5408\u53c2\u8003\u56fe\u50cf\u4e2d\u7684\u65b0\u6982\u5ff5\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6587\u672c\u63cf\u8ff0\uff0c\u5bfc\u81f4\u5bf9\u5b9a\u5236\u56fe\u50cf\u7684\u63a7\u5236\u6709\u9650\uff0c\u5e76\u4e14\u65e0\u6cd5\u652f\u6301\u7ec6\u7c92\u5ea6\u548c\u672c\u5730\u7f16\u8f91\uff08\u4f8b\u5982\u5f62\u72b6\u3001\u59ff\u52bf\u548c\u7ec6\u8282\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u8349\u56fe\u89c6\u4e3a\u4e00\u79cd\u76f4\u89c2\u4e14\u901a\u7528\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u53ef\u4ee5\u4fc3\u8fdb\u8fd9\u79cd\u63a7\u5236\uff0c\u4f8b\u5982\u6355\u83b7\u5f62\u72b6\u4fe1\u606f\u7684\u8f6e\u5ed3\u7ebf\u548c\u8868\u793a\u7eb9\u7406\u7684\u6d41\u7ebf\u3002\u8fd9\u4fc3\u4f7f\u6211\u4eec\u63a2\u7d22\u8349\u56fe\u6982\u5ff5\u63d0\u53d6\u7684\u4e00\u9879\u65b0\u4efb\u52a1\uff1a\u7ed9\u5b9a\u4e00\u4e2a\u6216\u591a\u4e2a\u8349\u56fe\u56fe\u50cf\u5bf9\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u63d0\u53d6\u4e00\u4e2a\u7279\u6b8a\u7684\u8349\u56fe\u6982\u5ff5\uff0c\u8be5\u6982\u5ff5\u8fde\u63a5\u56fe\u50cf\u548c\u8349\u56fe\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ece\u800c\u5b9e\u73b0\u57fa\u4e8e\u8349\u56fe\u7684\u56fe\u50cf\u5408\u6210\u548c\u7f16\u8f91\u7ec6\u7c92\u5ea6\u7684\u7ea7\u522b\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u5f15\u5165\u4e86 CustomSketching\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u63d0\u53d6\u65b0\u9896\u8349\u56fe\u6982\u5ff5\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u3002\u8003\u8651\u5230\u4e00\u4e2a\u5bf9\u8c61\u901a\u5e38\u53ef\u4ee5\u901a\u8fc7\u4e00\u822c\u5f62\u72b6\u7684\u8f6e\u5ed3\u548c\u5185\u90e8\u7ec6\u8282\u7684\u9644\u52a0\u7b14\u753b\u6765\u63cf\u7ed8\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u53cc\u8349\u56fe\u8868\u793a\u6765\u51cf\u5c11\u8349\u56fe\u63cf\u7ed8\u4e2d\u56fa\u6709\u7684\u6a21\u7cca\u6027\u3002\u6211\u4eec\u91c7\u7528\u5f62\u72b6\u635f\u5931\u548c\u6b63\u5219\u5316\u635f\u5931\u6765\u5e73\u8861\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u4fdd\u771f\u5ea6\u548c\u53ef\u7f16\u8f91\u6027\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3001\u7528\u6237\u7814\u7a76\u548c\u591a\u6b21\u5e94\u7528\uff0c\u6211\u4eec\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u6709\u6548\u7684\u5e76\u4e14\u4f18\u4e8e\u9002\u5e94\u7684\u57fa\u7ebf\u3002|[2402.17624v1](http://arxiv.org/pdf/2402.17624v1)|null|\n", "2402.17553": "|**2024-02-27**|**OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web**|OmniACT\uff1a\u4e3a\u684c\u9762\u548c Web \u542f\u7528\u591a\u6a21\u5f0f\u901a\u624d\u81ea\u6cbb\u4ee3\u7406\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6|Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, Ruslan Salakhutdinov|For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as \"Play the next song\", as well as longer horizon tasks such as \"Send an email to John Doe mentioning the time and place to meet\". Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.|\u51e0\u5341\u5e74\u6765\uff0c\u4eba\u673a\u4ea4\u4e92\u57fa\u672c\u4e0a\u90fd\u662f\u624b\u52a8\u7684\u3002\u5373\u4f7f\u5728\u4eca\u5929\uff0c\u51e0\u4e4e\u6240\u6709\u5728\u8ba1\u7b97\u673a\u4e0a\u5b8c\u6210\u7684\u751f\u4ea7\u6027\u5de5\u4f5c\u90fd\u9700\u8981\u4eba\u5de5\u5728\u6bcf\u4e00\u6b65\u8fdb\u884c\u8f93\u5165\u3002\u81ea\u4e3b\u865a\u62df\u4ee3\u7406\u4ee3\u8868\u4e86\u8bb8\u591a\u8fd9\u4e9b\u7410\u788e\u4efb\u52a1\u81ea\u52a8\u5316\u7684\u4ee4\u4eba\u5174\u594b\u7684\u4e00\u6b65\u3002\u865a\u62df\u4ee3\u7406\u5c06\u4f7f\u6280\u672f\u80fd\u529b\u6709\u9650\u7684\u7528\u6237\u80fd\u591f\u5145\u5206\u5229\u7528\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u53ef\u80fd\u6027\u3002\u5b83\u4eec\u8fd8\u53ef\u4ee5\u6709\u6548\u5730\u7b80\u5316\u5927\u91cf\u8ba1\u7b97\u673a\u4efb\u52a1\uff0c\u4ece\u65e5\u5386\u7ba1\u7406\u5230\u590d\u6742\u7684\u65c5\u884c\u9884\u8ba2\uff0c\u800c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 OmniACT\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4ee3\u7406\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\u4ee5\u5b8c\u6210\u8ba1\u7b97\u673a\u4efb\u52a1\u7684\u80fd\u529b\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002\u6211\u4eec\u7684\u8303\u56f4\u8d85\u51fa\u4e86\u4f20\u7edf\u7684\u7f51\u7edc\u81ea\u52a8\u5316\uff0c\u6db5\u76d6\u4e86\u5404\u79cd\u684c\u9762\u5e94\u7528\u7a0b\u5e8f\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u201c\u64ad\u653e\u4e0b\u4e00\u9996\u6b4c\u66f2\u201d\u7b49\u57fa\u672c\u4efb\u52a1\uff0c\u4ee5\u53ca\u201c\u5411 John Doe \u53d1\u9001\u7535\u5b50\u90ae\u4ef6\uff0c\u63d0\u53ca\u89c1\u9762\u7684\u65f6\u95f4\u548c\u5730\u70b9\u201d\u7b49\u957f\u671f\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7ed9\u5b9a\u4e00\u5bf9\u5c4f\u5e55\u56fe\u50cf\u548c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\uff0c\u76ee\u6807\u662f\u751f\u6210\u80fd\u591f\u5b8c\u5168\u6267\u884c\u8be5\u4efb\u52a1\u7684\u811a\u672c\u3002\u6211\u4eec\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fd0\u884c\u4e86\u51e0\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u3002\u6700\u5f3a\u7684\u57fa\u7ebf GPT-4 \u5728\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u597d\uff0c\u4f46\u662f\uff0c\u5728\u751f\u6210\u80fd\u591f\u5b8c\u6210\u4efb\u52a1\u7684\u53ef\u6267\u884c\u811a\u672c\u65b9\u9762\uff0c\u5176\u6027\u80fd\u6c34\u5e73\u4ecd\u7136\u4ec5\u8fbe\u5230\u4eba\u7c7b\u719f\u7ec3\u7a0b\u5ea6\u7684 15%\uff0c\u8fd9\u8868\u660e\u6211\u4eec\u7684\u4efb\u52a1\u5bf9\u4f20\u7edf Web \u4ee3\u7406\u7684\u6311\u6218\u3002\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e73\u53f0\u6765\u8861\u91cf\u548c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u8ba1\u7b97\u673a\u4efb\u52a1\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u5e76\u6fc0\u52b1\u672a\u6765\u7684\u5de5\u4f5c\u6784\u5efa\u8fde\u63a5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u5c4f\u5e55\u89c6\u89c9\u57fa\u7840\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002|[2402.17553v1](http://arxiv.org/pdf/2402.17553v1)|null|\n", "2402.17544": "|**2024-02-27**|**Adapting Learned Image Codecs to Screen Content via Adjustable Transformations**|\u901a\u8fc7\u53ef\u8c03\u6574\u7684\u8f6c\u6362\u4f7f\u5b66\u4e60\u7684\u56fe\u50cf\u7f16\u89e3\u7801\u5668\u9002\u5e94\u5c4f\u5e55\u5185\u5bb9|H. Burak Dogaroglu, A. Burakhan Koyuncu, Atanas Boev, Elena Alshina, Eckehard Steinbach|As learned image codecs (LICs) become more prevalent, their low coding efficiency for out-of-distribution data becomes a bottleneck for some applications. To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec's operation flow. We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts. Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters.|\u968f\u7740\u5b66\u4e60\u56fe\u50cf\u7f16\u89e3\u7801\u5668\uff08LIC\uff09\u53d8\u5f97\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u5b83\u4eec\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u4f4e\u7f16\u7801\u6548\u7387\u6210\u4e3a\u67d0\u4e9b\u5e94\u7528\u7a0b\u5e8f\u7684\u74f6\u9888\u3002\u4e3a\u4e86\u5728\u4e0d\u7834\u574f\u5411\u540e\u517c\u5bb9\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5c4f\u5e55\u5185\u5bb9\uff08SC\uff09\u56fe\u50cf\u7684LIC\u6027\u80fd\uff0c\u6211\u4eec\u5efa\u8bae\u5728\u7f16\u7801\u7ba1\u9053\u4e2d\u5f15\u5165\u53c2\u6570\u5316\u548c\u53ef\u9006\u7ebf\u6027\u53d8\u6362\uff0c\u800c\u4e0d\u6539\u53d8\u5e95\u5c42\u57fa\u7ebf\u7f16\u89e3\u7801\u5668\u7684\u64cd\u4f5c\u6d41\u7a0b\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\u5728\u6211\u4eec\u7684\u8bbe\u7f6e\u4e2d\u5145\u5f53\u9884\u8fc7\u6ee4\u5668\u548c\u540e\u8fc7\u6ee4\u5668\uff0c\u4ee5\u63d0\u9ad8\u7f16\u7801\u6548\u7387\u5e76\u5e2e\u52a9\u4ece\u7f16\u7801\u4f2a\u5f71\u4e2d\u6062\u590d\u3002\u4e0e\u57fa\u7ebf LIC \u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u53ef\u5728 SC \u538b\u7f29\u4e0a\u8282\u7701\u9ad8\u8fbe 10% \u7684\u6bd4\u7279\u7387\uff0c\u540c\u65f6\u4ec5\u5f15\u5165 1% \u7684\u989d\u5916\u53c2\u6570\u3002|[2402.17544v1](http://arxiv.org/pdf/2402.17544v1)|null|\n", "2402.17535": "|**2024-02-27**|**Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control**|\u5177\u6709\u6982\u7387\u6269\u5c55\u63a7\u5236\u7684\u591a\u6a21\u6001\u5b66\u4e60\u7a00\u758f\u68c0\u7d22|Thong Nguyen, Mariya Hendriksen, Andrew Yates, Maarten de Rijke|Learned sparse retrieval (LSR) is a family of neural methods that encode queries and documents into sparse lexical vectors that can be indexed and retrieved efficiently with an inverted index. We explore the application of LSR to the multi-modal domain, with a focus on text-image retrieval. While LSR has seen success in text retrieval, its application in multimodal retrieval remains underexplored. Current approaches like LexLIP and STAIR require complex multi-step training on massive datasets. Our proposed approach efficiently transforms dense vectors from a frozen dense model into sparse lexical vectors. We address issues of high dimension co-activation and semantic deviation through a new training algorithm, using Bernoulli random variables to control query expansion. Experiments with two dense models (BLIP, ALBEF) and two datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively reduces co-activation and semantic deviation. Our best-performing sparsified model outperforms state-of-the-art text-image LSR models with a shorter training time and lower GPU memory requirements. Our approach offers an effective solution for training LSR retrieval models in multimodal settings. Our code and model checkpoints are available at github.com/thongnt99/lsr-multimodal|\u5b66\u4e60\u7a00\u758f\u68c0\u7d22 (LSR) \u662f\u4e00\u7cfb\u5217\u795e\u7ecf\u65b9\u6cd5\uff0c\u5b83\u5c06\u67e5\u8be2\u548c\u6587\u6863\u7f16\u7801\u4e3a\u7a00\u758f\u8bcd\u6c47\u5411\u91cf\uff0c\u53ef\u4ee5\u901a\u8fc7\u5012\u6392\u7d22\u5f15\u9ad8\u6548\u5730\u8fdb\u884c\u7d22\u5f15\u548c\u68c0\u7d22\u3002\u6211\u4eec\u63a2\u7d22 LSR \u5728\u591a\u6a21\u6001\u9886\u57df\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u662f\u6587\u672c\u56fe\u50cf\u68c0\u7d22\u3002\u867d\u7136 LSR \u5728\u6587\u672c\u68c0\u7d22\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u5728\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u76ee\u524d\u7684\u65b9\u6cd5\uff08\u4f8b\u5982 LexLIP \u548c STAIR\uff09\u9700\u8981\u5bf9\u5927\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u590d\u6742\u7684\u591a\u6b65\u9aa4\u8bad\u7ec3\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5730\u5c06\u5bc6\u96c6\u5411\u91cf\u4ece\u51bb\u7ed3\u5bc6\u96c6\u6a21\u578b\u8f6c\u6362\u4e3a\u7a00\u758f\u8bcd\u6c47\u5411\u91cf\u3002\u6211\u4eec\u901a\u8fc7\u65b0\u7684\u8bad\u7ec3\u7b97\u6cd5\u89e3\u51b3\u9ad8\u7ef4\u534f\u540c\u6fc0\u6d3b\u548c\u8bed\u4e49\u504f\u5dee\u7684\u95ee\u9898\uff0c\u4f7f\u7528\u4f2f\u52aa\u5229\u968f\u673a\u53d8\u91cf\u6765\u63a7\u5236\u67e5\u8be2\u6269\u5c55\u3002\u4e24\u4e2a\u5bc6\u96c6\u6a21\u578b\uff08BLIP\uff0cALBEF\uff09\u548c\u4e24\u4e2a\u6570\u636e\u96c6\uff08MSCOCO\uff0cFlickr30k\uff09\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\u5730\u51cf\u5c11\u4e86\u5171\u540c\u6fc0\u6d3b\u548c\u8bed\u4e49\u504f\u5dee\u3002\u6211\u4eec\u6027\u80fd\u6700\u4f73\u7684\u7a00\u758f\u6a21\u578b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6587\u672c\u56fe\u50cf LSR \u6a21\u578b\uff0c\u8bad\u7ec3\u65f6\u95f4\u66f4\u77ed\uff0cGPU \u5185\u5b58\u8981\u6c42\u66f4\u4f4e\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u8bad\u7ec3 LSR \u68c0\u7d22\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u4f4d\u4e8e github.com/thongnt99/lsr-multimodal|[2402.17535v1](http://arxiv.org/pdf/2402.17535v1)|null|\n", "2402.17533": "|**2024-02-27**|**Black-box Adversarial Attacks Against Image Quality Assessment Models**|\u9488\u5bf9\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u7684\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb|Yu Ran, Ao-Xiang Zhang, Mingjie Li, Weixuan Tang, Yuan-Gen Wang|The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the perceptual quality of an image in line with its subjective evaluation. To put the NR-IQA models into practice, it is essential to study their potential loopholes for model refinement. This paper makes the first attempt to explore the black-box adversarial attacks on NR-IQA models. Specifically, we first formulate the attack problem as maximizing the deviation between the estimated quality scores of original and perturbed images, while restricting the perturbed image distortions for visual quality preservation. Under such formulation, we then design a Bi-directional loss function to mislead the estimated quality scores of adversarial examples towards an opposite direction with maximum deviation. On this basis, we finally develop an efficient and effective black-box attack method against NR-IQA models. Extensive experiments reveal that all the evaluated NR-IQA models are vulnerable to the proposed attack method. And the generated perturbations are not transferable, enabling them to serve the investigation of specialities of disparate IQA models.|\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30 (NR-IQA) \u7684\u76ee\u6807\u662f\u6839\u636e\u5176\u4e3b\u89c2\u8bc4\u4ef7\u6765\u9884\u6d4b\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u3002\u4e3a\u4e86\u5c06 NR-IQA \u6a21\u578b\u4ed8\u8bf8\u5b9e\u8df5\uff0c\u6709\u5fc5\u8981\u7814\u7a76\u5176\u6a21\u578b\u7ec6\u5316\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002\u672c\u6587\u9996\u6b21\u5c1d\u8bd5\u63a2\u7d22 NR-IQA \u6a21\u578b\u7684\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u5c06\u653b\u51fb\u95ee\u9898\u8868\u8ff0\u4e3a\u6700\u5927\u5316\u539f\u59cb\u56fe\u50cf\u548c\u6270\u52a8\u56fe\u50cf\u7684\u4f30\u8ba1\u8d28\u91cf\u5f97\u5206\u4e4b\u95f4\u7684\u504f\u5dee\uff0c\u540c\u65f6\u9650\u5236\u6270\u52a8\u56fe\u50cf\u5931\u771f\u4ee5\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002\u5728\u8fd9\u6837\u7684\u516c\u5f0f\u4e0b\uff0c\u6211\u4eec\u7136\u540e\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53cc\u5411\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u5c06\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u4f30\u8ba1\u8d28\u91cf\u5206\u6570\u8bef\u5bfc\u5230\u5177\u6709\u6700\u5927\u504f\u5dee\u7684\u76f8\u53cd\u65b9\u5411\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u6700\u7ec8\u5f00\u53d1\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u9488\u5bf9NR-IQA\u6a21\u578b\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u6709\u8bc4\u4f30\u7684 NR-IQA \u6a21\u578b\u90fd\u5bb9\u6613\u53d7\u5230\u6240\u63d0\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u7684\u5f71\u54cd\u3002\u800c\u4e14\u751f\u6210\u7684\u6270\u52a8\u662f\u4e0d\u53ef\u8f6c\u79fb\u7684\uff0c\u4f7f\u5b83\u4eec\u80fd\u591f\u670d\u52a1\u4e8e\u4e0d\u540c IQA \u6a21\u578b\u7684\u4e13\u4e1a\u7814\u7a76\u3002|[2402.17533v1](http://arxiv.org/pdf/2402.17533v1)|null|\n", "2402.17483": "|**2024-02-27**|**AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera Joint Synthesis**|AlignMiF\uff1a\u7528\u4e8e LiDAR-\u76f8\u673a\u8054\u5408\u5408\u6210\u7684\u51e0\u4f55\u5bf9\u9f50\u591a\u6a21\u6001\u9690\u5f0f\u573a|Tao Tang, Guangrun Wang, Yixing Lao, Peng Chen, Jie Liu, Liang Lin, Kaicheng Yu, Xiaodan Liang|Neural implicit fields have been a de facto standard in novel view synthesis. Recently, there exist some methods exploring fusing multiple modalities within a single field, aiming to share implicit features from different modalities to enhance reconstruction performance. However, these modalities often exhibit misaligned behaviors: optimizing for one modality, such as LiDAR, can adversely affect another, like camera performance, and vice versa. In this work, we conduct comprehensive analyses on the multimodal implicit field of LiDAR-camera joint synthesis, revealing the underlying issue lies in the misalignment of different sensors. Furthermore, we introduce AlignMiF, a geometrically aligned multimodal implicit field with two proposed modules: Geometry-Aware Alignment (GAA) and Shared Geometry Initialization (SGI). These modules effectively align the coarse geometry across different modalities, significantly enhancing the fusion process between LiDAR and camera data. Through extensive experiments across various datasets and scenes, we demonstrate the effectiveness of our approach in facilitating better interaction between LiDAR and camera modalities within a unified neural field. Specifically, our proposed AlignMiF, achieves remarkable improvement over recent implicit fusion methods (+2.01 and +3.11 image PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses single modality performance (13.8% and 14.2% reduction in LiDAR Chamfer Distance on the respective datasets).|\u795e\u7ecf\u9690\u5f0f\u573a\u5df2\u7ecf\u6210\u4e3a\u65b0\u9896\u89c6\u56fe\u5408\u6210\u7684\u4e8b\u5b9e\u200b\u200b\u4e0a\u7684\u6807\u51c6\u3002\u6700\u8fd1\uff0c\u5b58\u5728\u4e00\u4e9b\u63a2\u7d22\u5728\u5355\u4e2a\u5b57\u6bb5\u4e2d\u878d\u5408\u591a\u79cd\u6a21\u6001\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u5171\u4eab\u6765\u81ea\u4e0d\u540c\u6a21\u6001\u7684\u9690\u5f0f\u7279\u5f81\u4ee5\u63d0\u9ad8\u91cd\u5efa\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u7ecf\u5e38\u8868\u73b0\u51fa\u4e0d\u4e00\u81f4\u7684\u884c\u4e3a\uff1a\u4f18\u5316\u4e00\u79cd\u6a21\u5f0f\uff08\u4f8b\u5982\u6fc0\u5149\u96f7\u8fbe\uff09\u53ef\u80fd\u4f1a\u5bf9\u53e6\u4e00\u79cd\u6a21\u5f0f\uff08\u4f8b\u5982\u76f8\u673a\u6027\u80fd\uff09\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5bf9\u6fc0\u5149\u96f7\u8fbe-\u76f8\u673a\u8054\u5408\u5408\u6210\u7684\u591a\u6a21\u6001\u9690\u573a\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6839\u672c\u95ee\u9898\u5728\u4e8e\u4e0d\u540c\u4f20\u611f\u5668\u7684\u672a\u5bf9\u51c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86 AlignMiF\uff0c\u4e00\u79cd\u51e0\u4f55\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u9690\u5f0f\u573a\uff0c\u5177\u6709\u4e24\u4e2a\u5efa\u8bae\u7684\u6a21\u5757\uff1a\u51e0\u4f55\u611f\u77e5\u5bf9\u9f50\uff08GAA\uff09\u548c\u5171\u4eab\u51e0\u4f55\u521d\u59cb\u5316\uff08SGI\uff09\u3002\u8fd9\u4e9b\u6a21\u5757\u6709\u6548\u5730\u5bf9\u9f50\u4e0d\u540c\u6a21\u6001\u7684\u7c97\u7565\u51e0\u4f55\u5f62\u72b6\uff0c\u663e\u7740\u589e\u5f3a\u6fc0\u5149\u96f7\u8fbe\u548c\u76f8\u673a\u6570\u636e\u4e4b\u95f4\u7684\u878d\u5408\u8fc7\u7a0b\u3002\u901a\u8fc7\u5bf9\u5404\u79cd\u6570\u636e\u96c6\u548c\u573a\u666f\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fc3\u8fdb\u7edf\u4e00\u795e\u7ecf\u573a\u5185\u6fc0\u5149\u96f7\u8fbe\u548c\u76f8\u673a\u6a21\u5f0f\u4e4b\u95f4\u66f4\u597d\u4ea4\u4e92\u7684\u6709\u6548\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u7684 AlignMiF \u6bd4\u6700\u8fd1\u7684\u9690\u5f0f\u878d\u5408\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6539\u8fdb\uff08KITTI-360 \u548c Waymo \u6570\u636e\u96c6\u4e0a\u7684\u56fe\u50cf PSNR +2.01 \u548c +3.11\uff09\uff0c\u5e76\u4e14\u59cb\u7ec8\u8d85\u8d8a\u5355\u4e00\u6a21\u6001\u6027\u80fd\uff08LiDAR \u5012\u89d2\u8ddd\u79bb\u5728\u5404\u81ea\u7684\u6570\u636e\u96c6\uff09\u3002|[2402.17483v1](http://arxiv.org/pdf/2402.17483v1)|null|\n", "2402.17470": "|**2024-02-27**|**Bit Distribution Study and Implementation of Spatial Quality Map in the JPEG-AI Standardization**|JPEG-AI\u6807\u51c6\u5316\u4e2d\u7a7a\u95f4\u8d28\u91cf\u56fe\u7684\u6bd4\u7279\u5206\u5e03\u7814\u7a76\u4e0e\u5b9e\u73b0|Panqi Jia, Jue Mao, Esin Koyuncu, A. Burakhan Koyuncu, Timofey Solovyev, Alexander Karabutov, Yin Zhao, Elena Alshina, Andre Kaup|Currently, there is a high demand for neural network-based image compression codecs. These codecs employ non-linear transforms to create compact bit representations and facilitate faster coding speeds on devices compared to the hand-crafted transforms used in classical frameworks. The scientific and industrial communities are highly interested in these properties, leading to the standardization effort of JPEG-AI. The JPEG-AI verification model has been released and is currently under development for standardization. Utilizing neural networks, it can outperform the classic codec VVC intra by over 10% BD-rate operating at base operation point. Researchers attribute this success to the flexible bit distribution in the spatial domain, in contrast to VVC intra's anchor that is generated with a constant quality point. However, our study reveals that VVC intra displays a more adaptable bit distribution structure through the implementation of various block sizes. As a result of our observations, we have proposed a spatial bit allocation method to optimize the JPEG-AI verification model's bit distribution and enhance the visual quality. Furthermore, by applying the VVC bit distribution strategy, the objective performance of JPEG-AI verification mode can be further improved, resulting in a maximum gain of 0.45 dB in PSNR-Y.|\u76ee\u524d\uff0c\u5bf9\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u50cf\u538b\u7f29\u7f16\u89e3\u7801\u5668\u7684\u9700\u6c42\u5f88\u9ad8\u3002\u4e0e\u7ecf\u5178\u6846\u67b6\u4e2d\u4f7f\u7528\u7684\u624b\u5de5\u53d8\u6362\u76f8\u6bd4\uff0c\u8fd9\u4e9b\u7f16\u89e3\u7801\u5668\u91c7\u7528\u975e\u7ebf\u6027\u53d8\u6362\u6765\u521b\u5efa\u7d27\u51d1\u7684\u4f4d\u8868\u793a\uff0c\u5e76\u4fc3\u8fdb\u8bbe\u5907\u4e0a\u66f4\u5feb\u7684\u7f16\u7801\u901f\u5ea6\u3002\u79d1\u5b66\u754c\u548c\u5de5\u4e1a\u754c\u5bf9\u8fd9\u4e9b\u7279\u6027\u975e\u5e38\u611f\u5174\u8da3\uff0c\u4ece\u800c\u4fc3\u6210\u4e86 JPEG-AI \u7684\u6807\u51c6\u5316\u5de5\u4f5c\u3002 JPEG-AI\u9a8c\u8bc1\u6a21\u578b\u5df2\u53d1\u5e03\uff0c\u76ee\u524d\u6b63\u5728\u6807\u51c6\u5316\u5f00\u53d1\u4e2d\u3002\u5229\u7528\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5728\u57fa\u672c\u64cd\u4f5c\u70b9\u4e0a\u7684 BD \u901f\u7387\u6bd4\u7ecf\u5178\u7f16\u89e3\u7801\u5668 VVC intra \u7684\u6027\u80fd\u9ad8\u51fa 10% \u4ee5\u4e0a\u3002\u7814\u7a76\u4eba\u5458\u5c06\u8fd9\u4e00\u6210\u529f\u5f52\u56e0\u4e8e\u7a7a\u95f4\u57df\u4e2d\u7075\u6d3b\u7684\u6bd4\u7279\u5206\u5e03\uff0c\u8fd9\u4e0e\u4f7f\u7528\u6052\u5b9a\u8d28\u91cf\u70b9\u751f\u6210\u7684 VVC \u5185\u90e8\u951a\u70b9\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002\u7136\u800c\uff0c\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0cVVC \u5185\u90e8\u901a\u8fc7\u5b9e\u73b0\u5404\u79cd\u5757\u5927\u5c0f\u663e\u793a\u51fa\u66f4\u5177\u9002\u5e94\u6027\u7684\u4f4d\u5206\u5e03\u7ed3\u6784\u3002\u6839\u636e\u6211\u4eec\u7684\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u6bd4\u7279\u5206\u914d\u65b9\u6cd5\u6765\u4f18\u5316 JPEG-AI \u9a8c\u8bc1\u6a21\u578b\u7684\u6bd4\u7279\u5206\u5e03\u5e76\u63d0\u9ad8\u89c6\u89c9\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5e94\u7528VVC\u6bd4\u7279\u5206\u914d\u7b56\u7565\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8JPEG-AI\u9a8c\u8bc1\u6a21\u5f0f\u7684\u5ba2\u89c2\u6027\u80fd\uff0c\u4f7f\u5f97PSNR-Y\u6700\u5927\u589e\u76ca\u8fbe\u52300.45 dB\u3002|[2402.17470v1](http://arxiv.org/pdf/2402.17470v1)|null|\n", "2402.17438": "|**2024-02-27**|**V2C-Long: Longitudinal Cortex Reconstruction with Spatiotemporal Correspondence**|V2C-Long\uff1a\u5177\u6709\u65f6\u7a7a\u5bf9\u5e94\u6027\u7684\u7eb5\u5411\u76ae\u5c42\u91cd\u5efa|Fabian Bongratz, Jan Fecht, Anne-Marie Rickmann, Christian Wachinger|Reconstructing the cortex from longitudinal MRI is indispensable for analyzing morphological changes in the human brain. Despite the recent disruption of cortical surface reconstruction with deep learning, challenges arising from longitudinal data are still persistent. Especially the lack of strong spatiotemporal point correspondence hinders downstream analyses due to the introduced noise. To address this issue, we present V2C-Long, the first dedicated deep learning-based cortex reconstruction method for longitudinal MRI. In contrast to existing methods, V2C-Long surfaces are directly comparable in a cross-sectional and longitudinal manner. We establish strong inherent spatiotemporal correspondences via a novel composition of two deep mesh deformation networks and fast aggregation of feature-enhanced within-subject templates. The results on internal and external test data demonstrate that V2C-Long yields cortical surfaces with improved accuracy and consistency compared to previous methods. Finally, this improvement manifests in higher sensitivity to regional cortical atrophy in Alzheimer's disease.|\u4ece\u7eb5\u5411 MRI \u91cd\u5efa\u76ae\u8d28\u5bf9\u4e8e\u5206\u6790\u4eba\u8111\u7684\u5f62\u6001\u53d8\u5316\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6700\u8fd1\u7834\u574f\u4e86\u76ae\u8d28\u8868\u9762\u91cd\u5efa\uff0c\u4f46\u7eb5\u5411\u6570\u636e\u5e26\u6765\u7684\u6311\u6218\u4ecd\u7136\u6301\u7eed\u5b58\u5728\u3002\u7279\u522b\u662f\u7f3a\u4e4f\u5f3a\u65f6\u7a7a\u70b9\u5bf9\u5e94\u6027\uff0c\u7531\u4e8e\u5f15\u5165\u7684\u566a\u58f0\u800c\u963b\u788d\u4e86\u4e0b\u6e38\u5206\u6790\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 V2C-Long\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u7eb5\u5411 MRI \u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u76ae\u5c42\u91cd\u5efa\u65b9\u6cd5\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cV2C-Long \u8868\u9762\u53ef\u4ee5\u5728\u6a2a\u622a\u9762\u548c\u7eb5\u5411\u4e0a\u76f4\u63a5\u8fdb\u884c\u6bd4\u8f83\u3002\u6211\u4eec\u901a\u8fc7\u4e24\u4e2a\u6df1\u5ea6\u7f51\u683c\u53d8\u5f62\u7f51\u7edc\u7684\u65b0\u9896\u7ec4\u5408\u548c\u7279\u5f81\u589e\u5f3a\u7684\u4e3b\u9898\u5185\u6a21\u677f\u7684\u5feb\u901f\u805a\u5408\u6765\u5efa\u7acb\u5f3a\u5927\u7684\u56fa\u6709\u65f6\u7a7a\u5bf9\u5e94\u5173\u7cfb\u3002\u5185\u90e8\u548c\u5916\u90e8\u6d4b\u8bd5\u6570\u636e\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cV2C-Long \u4ea7\u751f\u7684\u76ae\u8d28\u8868\u9762\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002\u6700\u540e\uff0c\u8fd9\u79cd\u6539\u5584\u4f53\u73b0\u5728\u5bf9\u963f\u5c14\u8328\u6d77\u9ed8\u6c0f\u75c5\u5c40\u90e8\u76ae\u8d28\u840e\u7f29\u7684\u654f\u611f\u6027\u66f4\u9ad8\u3002|[2402.17438v1](http://arxiv.org/pdf/2402.17438v1)|null|\n", "2402.17410": "|**2024-02-27**|**A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis**|\u7528\u4e8e\u566a\u58f0\u4f20\u64ad\u5206\u6790\u7684\u5085\u91cc\u53f6\u57df\u63d2\u503c\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u9896\u56fe\u50cf\u7a7a\u95f4\u5f62\u5f0f|Peter Dawood, Felix Breuer, Istvan Homolya, Jannik Stebani, Maximilian Gram, Peter M. Jakob, Moritz Zaiss, Martin Blaimer|Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference. Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks. This operation is transformed into a convolution in the image space. After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space. This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics. Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for validation. The framework was tested on retrospectively undersampled invivo brain images. Results: Inferences conducted in the image domain are quasi-identical to inferences in the k-space, underlined by corresponding quantitative metrics. Noise variance maps obtained from the analytical expression correspond with those obtained via Monte-Carlo simulations, as well as via an auto-differentiation approach. The noise resilience is well characterized, as in the case of classical Parallel Imaging. Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes in variance maps obtained via Monte-Carlo simulations. Conclusion: The quasi-equivalent image space formalism for neural networks for k-space interpolation enables fast and accurate description of the noise characteristics during CNN inference, analogous to geometry-factor maps in traditional parallel imaging methods.|\u76ee\u7684\uff1a\u5f00\u53d1\u591a\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7684\u56fe\u50cf\u7a7a\u95f4\u5f62\u5f0f\uff0c\u7528\u4e8e MRI \u91cd\u5efa\u4e2d\u7684\u5085\u7acb\u53f6\u57df\u63d2\u503c\uff0c\u5e76\u5206\u6790\u4f30\u8ba1 CNN \u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u4f20\u64ad\u3002\u7406\u8bba\u548c\u65b9\u6cd5\uff1a\u4f7f\u7528\u590d\u503c\u6574\u6d41\u5668\u7ebf\u6027\u5355\u5143\u7684\u5085\u7acb\u53f6\u57df\uff08\u4e5f\u79f0\u4e3a k \u7a7a\u95f4\uff09\u4e2d\u7684\u975e\u7ebf\u6027\u6fc0\u6d3b\u8868\u793a\u4e3a\u4e0e\u6fc0\u6d3b\u63a9\u7801\u7684\u5143\u7d20\u76f8\u4e58\u3002\u8be5\u64cd\u4f5c\u88ab\u8f6c\u5316\u4e3a\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u5377\u79ef\u3002\u5728 k \u7a7a\u95f4\u4e2d\u8fdb\u884c\u7f51\u7edc\u8bad\u7ec3\u540e\uff0c\u8be5\u65b9\u6cd5\u4e3a\u91cd\u5efa\u56fe\u50cf\u76f8\u5bf9\u4e8e\u6df7\u53e0\u7ebf\u5708\u56fe\u50cf\u7684\u5bfc\u6570\u63d0\u4f9b\u4e86\u4ee3\u6570\u8868\u8fbe\u5f0f\uff0c\u6df7\u53e0\u7ebf\u5708\u56fe\u50cf\u5145\u5f53\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7f51\u7edc\u7684\u8f93\u5165\u5f20\u91cf\u3002\u8fd9\u5141\u8bb8\u901a\u8fc7\u5206\u6790\u65b9\u5f0f\u4f30\u8ba1\u7f51\u7edc\u63a8\u7406\u4e2d\u7684\u65b9\u5dee\u5e76\u7528\u4e8e\u63cf\u8ff0\u566a\u58f0\u7279\u5f81\u3002\u4f7f\u7528\u8499\u7279\u5361\u7f57\u6a21\u62df\u548c\u57fa\u4e8e\u81ea\u5fae\u5206\u7684\u6570\u503c\u65b9\u6cd5\u8fdb\u884c\u9a8c\u8bc1\u3002\u8be5\u6846\u67b6\u5728\u56de\u987e\u6027\u6b20\u91c7\u6837\u4f53\u5185\u5927\u8111\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u7ed3\u679c\uff1a\u5728\u56fe\u50cf\u57df\u4e2d\u8fdb\u884c\u7684\u63a8\u7406\u4e0e k \u7a7a\u95f4\u4e2d\u7684\u63a8\u7406\u51e0\u4e4e\u76f8\u540c\uff0c\u5e76\u901a\u8fc7\u76f8\u5e94\u7684\u5b9a\u91cf\u6307\u6807\u8fdb\u884c\u4e86\u5f3a\u8c03\u3002\u4ece\u89e3\u6790\u8868\u8fbe\u5f0f\u83b7\u5f97\u7684\u566a\u58f0\u65b9\u5dee\u56fe\u4e0e\u901a\u8fc7\u8499\u7279\u5361\u7f57\u6a21\u62df\u4ee5\u53ca\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u65b9\u6cd5\u83b7\u5f97\u7684\u566a\u58f0\u65b9\u5dee\u56fe\u76f8\u5bf9\u5e94\u3002\u4e0e\u7ecf\u5178\u5e76\u884c\u6210\u50cf\u7684\u60c5\u51b5\u4e00\u6837\uff0c\u566a\u58f0\u6062\u590d\u80fd\u529b\u5f97\u5230\u4e86\u5f88\u597d\u7684\u8868\u5f81\u3002 Komolgorov-Smirnov \u68c0\u9a8c\u8bc1\u660e\u4e86\u901a\u8fc7\u8499\u7279\u5361\u7f57\u6a21\u62df\u83b7\u5f97\u7684\u65b9\u5dee\u56fe\u4e2d\u4f53\u7d20\u5927\u5c0f\u7684\u9ad8\u65af\u5206\u5e03\u3002\u7ed3\u8bba\uff1a\u7528\u4e8e k \u7a7a\u95f4\u63d2\u503c\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u51c6\u7b49\u6548\u56fe\u50cf\u7a7a\u95f4\u5f62\u5f0f\u80fd\u591f\u5728 CNN \u63a8\u7406\u8fc7\u7a0b\u4e2d\u5feb\u901f\u51c6\u786e\u5730\u63cf\u8ff0\u566a\u58f0\u7279\u5f81\uff0c\u7c7b\u4f3c\u4e8e\u4f20\u7edf\u5e76\u884c\u6210\u50cf\u65b9\u6cd5\u4e2d\u7684\u51e0\u4f55\u56e0\u5b50\u56fe\u3002|[2402.17410v1](http://arxiv.org/pdf/2402.17410v1)|null|\n", "2402.17376": "|**2024-02-27**|**Accelerating Diffusion Sampling with Optimized Time Steps**|\u901a\u8fc7\u4f18\u5316\u65f6\u95f4\u6b65\u957f\u52a0\u901f\u6269\u6563\u91c7\u6837|Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, Zhenguo Li|Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than $15$ seconds. Our extensive experiments on both unconditional and conditional sampling using pixel- and latent-space DPMs demonstrate that, when combined with the state-of-the-art sampling method UniPC, our optimized time steps significantly improve image generation performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet, compared to using uniform time steps.|\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DPM\uff09\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u901a\u5e38\u9700\u8981\u5927\u91cf\u91c7\u6837\u6b65\u9aa4\uff0c\u56e0\u6b64\u5176\u91c7\u6837\u6548\u7387\u4ecd\u7136\u4e0d\u591f\u7406\u60f3\u3002 DPM \u9ad8\u9636\u6570\u503c ODE \u6c42\u89e3\u5668\u7684\u6700\u65b0\u8fdb\u5c55\u4f7f\u5f97\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u91c7\u6837\u6b65\u9aa4\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u867d\u7136\u8fd9\u662f\u4e00\u4e2a\u91cd\u5927\u7684\u8fdb\u6b65\uff0c\u4f46\u5927\u591a\u6570\u91c7\u6837\u65b9\u6cd5\u4ecd\u7136\u91c7\u7528\u7edf\u4e00\u7684\u65f6\u95f4\u6b65\u957f\uff0c\u8fd9\u5728\u4f7f\u7528\u5c11\u91cf\u6b65\u957f\u65f6\u5e76\u4e0d\u662f\u6700\u4f73\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bbe\u8ba1\u4f18\u5316\u95ee\u9898\u7684\u901a\u7528\u6846\u67b6\uff0c\u4e3a DPM \u7684\u7279\u5b9a\u6570\u503c ODE \u6c42\u89e3\u5668\u5bfb\u6c42\u66f4\u5408\u9002\u7684\u65f6\u95f4\u6b65\u957f\u3002\u8be5\u4f18\u5316\u95ee\u9898\u65e8\u5728\u6700\u5c0f\u5316 ODE \u7684\u771f\u5b9e\u89e3\u4e0e\u6570\u503c\u6c42\u89e3\u5668\u5bf9\u5e94\u7684\u8fd1\u4f3c\u89e3\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u53ef\u4ee5\u4f7f\u7528\u7ea6\u675f\u4fe1\u4efb\u57df\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u8017\u65f6\u4e0d\u5230 15 \u79d2\u3002\u6211\u4eec\u4f7f\u7528\u50cf\u7d20\u548c\u6f5c\u5728\u7a7a\u95f4 DPM \u5bf9\u65e0\u6761\u4ef6\u548c\u6761\u4ef6\u91c7\u6837\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u4e0e\u6700\u5148\u8fdb\u7684\u91c7\u6837\u65b9\u6cd5 UniPC \u76f8\u7ed3\u5408\u65f6\uff0c\u6211\u4eec\u4f18\u5316\u7684\u65f6\u95f4\u6b65\u663e\u7740\u63d0\u9ad8\u4e86 FID \u5206\u6570\u65b9\u9762\u7684\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002\u4e0e\u4f7f\u7528\u7edf\u4e00\u65f6\u95f4\u6b65\u957f\u7684\u6570\u636e\u96c6\uff08\u4f8b\u5982 CIFAR-10 \u548c ImageNet\uff09\u8fdb\u884c\u6bd4\u8f83\u3002|[2402.17376v1](http://arxiv.org/pdf/2402.17376v1)|null|\n", "2402.17339": "|**2024-02-27**|**SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents**|SocialCVAE\uff1a\u901a\u8fc7\u4ea4\u4e92\u6761\u4ef6\u6f5c\u4f0f\u9884\u6d4b\u884c\u4eba\u8f68\u8ff9|Wei Xiang, Haoteng Yin, He Wang, Xiaogang Jin|Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions. Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with a deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability. However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance. To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE's condition, which illustrates the future occupancy of each pedestrian's local neighborhood area. The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians' interactions with neighbors. Experimental results on two public benchmarks including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE).|\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u662f\u8bb8\u591a\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6280\u672f\uff0c\u53ef\u63d0\u4f9b\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7684\u6d1e\u5bdf\u5e76\u9884\u6d4b\u4eba\u7c7b\u672a\u6765\u7684\u8fd0\u52a8\u3002\u5927\u591a\u6570\u73b0\u6709\u7684\u7ecf\u9a8c\u6a21\u578b\u90fd\u662f\u901a\u8fc7\u4f7f\u7528\u5177\u6709\u786e\u5b9a\u6027\u6027\u8d28\u7684\u53ef\u89e3\u91ca\u7684\u6570\u5b66\u672f\u8bed\u901a\u8fc7\u89c2\u5bdf\u5230\u7684\u4eba\u7c7b\u884c\u4e3a\u660e\u786e\u5236\u5b9a\u7684\uff0c\u800c\u6700\u8fd1\u7684\u5de5\u4f5c\u91cd\u70b9\u662f\u5f00\u53d1\u4e0e\u57fa\u4e8e\u5b66\u4e60\u7684\u6280\u672f\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002\u7136\u800c\uff0c\u4ece\u7ecf\u9a8c\u6a21\u578b\u4e2d\u5b66\u4e60\u5230\u7684\u8f6c\u5411\u884c\u4e3a\u7684\u786e\u5b9a\u6027\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u7528\u4e8e\u9884\u6d4b\u884c\u4eba\u8f68\u8ff9\u7684\u793e\u4f1a\u6761\u4ef6\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\uff08SocialCVAE\uff09\uff0c\u5b83\u91c7\u7528 CVAE \u6765\u63a2\u7d22\u4eba\u7c7b\u8fd0\u52a8\u51b3\u7b56\u4e2d\u7684\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u3002 SocialCVAE \u901a\u8fc7\u5229\u7528\u793e\u4f1a\u53ef\u89e3\u91ca\u7684\u4ea4\u4e92\u80fd\u91cf\u56fe\u4f5c\u4e3a CVAE \u7684\u6761\u4ef6\u6765\u5b66\u4e60\u793e\u4f1a\u5408\u7406\u7684\u8fd0\u52a8\u968f\u673a\u6027\uff0c\u8be5\u6761\u4ef6\u8bf4\u660e\u4e86\u6bcf\u4e2a\u884c\u4eba\u5f53\u5730\u90bb\u91cc\u533a\u57df\u7684\u672a\u6765\u5360\u7528\u60c5\u51b5\u3002\u80fd\u91cf\u56fe\u662f\u4f7f\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u4ea4\u4e92\u6a21\u578b\u751f\u6210\u7684\uff0c\u8be5\u6a21\u578b\u9884\u6d4b\u884c\u4eba\u4e0e\u90bb\u5c45\u4ea4\u4e92\u7684\u80fd\u91cf\u6210\u672c\uff08\u5373\u6392\u65a5\u5f3a\u5ea6\uff09\u3002\u5305\u62ec 25 \u4e2a\u573a\u666f\u7684\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cSocialCVAE \u663e\u7740\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee (ADE) \u63d0\u9ad8\u4e86 16.85%\uff0c\u6700\u7ec8\u4f4d\u79fb\u8bef\u5dee\u63d0\u9ad8\u4e86 69.18%\uff08 FDE\uff09\u3002|[2402.17339v1](http://arxiv.org/pdf/2402.17339v1)|null|\n", "2402.17310": "|**2024-02-27**|**Method of Tracking and Analysis of Fluorescent-Labeled Cells Using Automatic Thresholding and Labeling**|\u4f7f\u7528\u81ea\u52a8\u9608\u503c\u548c\u6807\u8bb0\u8ddf\u8e2a\u548c\u5206\u6790\u8367\u5149\u6807\u8bb0\u7ec6\u80de\u7684\u65b9\u6cd5|Mizuki Fukasawa, Tomokazu Fukuda, Takuya Akashi|High-throughput screening using cell images is an efficient method for screening new candidates for pharmaceutical drugs. To complete the screening process, it is essential to have an efficient process for analyzing cell images. This paper presents a new method for efficiently tracking cells and quantitatively detecting the signal ratio between cytoplasm and nuclei. Existing methods include those that use image processing techniques and those that utilize artificial intelligence (AI). However, these methods do not consider the correspondence of cells between images, or require a significant amount of new learning data to train AI. Therefore, our method uses automatic thresholding and labeling algorithms to compare the position of each cell between images, and continuously measure and analyze the signal ratio of cells. This paper describes the algorithm of our method. Using the method, we experimented to investigate the effect of the number of opening and closing operations during the binarization process on the tracking of the cells. Through the experiment, we determined the appropriate number of opening and closing processes.|\u4f7f\u7528\u7ec6\u80de\u56fe\u50cf\u8fdb\u884c\u9ad8\u901a\u91cf\u7b5b\u9009\u662f\u7b5b\u9009\u65b0\u5019\u9009\u836f\u7269\u7684\u6709\u6548\u65b9\u6cd5\u3002\u4e3a\u4e86\u5b8c\u6210\u7b5b\u9009\u8fc7\u7a0b\uff0c\u5fc5\u987b\u6709\u4e00\u4e2a\u6709\u6548\u7684\u7ec6\u80de\u56fe\u50cf\u5206\u6790\u6d41\u7a0b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u8ddf\u8e2a\u7ec6\u80de\u5e76\u5b9a\u91cf\u68c0\u6d4b\u7ec6\u80de\u8d28\u4e0e\u7ec6\u80de\u6838\u4e4b\u95f4\u4fe1\u53f7\u6bd4\u7684\u65b0\u65b9\u6cd5\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u56fe\u50cf\u5904\u7406\u6280\u672f\u7684\u65b9\u6cd5\u548c\u5229\u7528\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6ca1\u6709\u8003\u8651\u56fe\u50cf\u4e4b\u95f4\u7ec6\u80de\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u6216\u8005\u9700\u8981\u5927\u91cf\u65b0\u7684\u5b66\u4e60\u6570\u636e\u6765\u8bad\u7ec3\u4eba\u5de5\u667a\u80fd\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u81ea\u52a8\u9608\u503c\u548c\u6807\u8bb0\u7b97\u6cd5\u6765\u6bd4\u8f83\u56fe\u50cf\u4e4b\u95f4\u6bcf\u4e2a\u7ec6\u80de\u7684\u4f4d\u7f6e\uff0c\u5e76\u8fde\u7eed\u6d4b\u91cf\u548c\u5206\u6790\u7ec6\u80de\u7684\u4fe1\u53f7\u6bd4\u3002\u672c\u6587\u63cf\u8ff0\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u7b97\u6cd5\u3002\u4f7f\u7528\u8be5\u65b9\u6cd5\uff0c\u6211\u4eec\u5b9e\u9a8c\u7814\u7a76\u4e86\u4e8c\u503c\u5316\u8fc7\u7a0b\u4e2d\u6253\u5f00\u548c\u5173\u95ed\u64cd\u4f5c\u7684\u6b21\u6570\u5bf9\u7ec6\u80de\u8ddf\u8e2a\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u5408\u9002\u7684\u6253\u5f00\u548c\u5173\u95ed\u8fdb\u7a0b\u7684\u6570\u91cf\u3002|[2402.17310v1](http://arxiv.org/pdf/2402.17310v1)|null|\n", "2402.17296": "|**2024-02-27**|**Learning Exposure Correction in Dynamic Scenes**|\u5b66\u4e60\u52a8\u6001\u573a\u666f\u4e2d\u7684\u66dd\u5149\u6821\u6b63|Jin Liu, Bo Wang, Chuanming Wang, Huiyuan Fu, Huadong Ma|Capturing videos with wrong exposure usually produces unsatisfactory visual effects. While image exposure correction is a popular topic, the video counterpart is less explored in the literature. Directly applying prior image-based methods to input videos often results in temporal incoherence with low visual quality. Existing research in this area is also limited by the lack of high-quality benchmark datasets. To address these issues, we construct the first real-world paired video dataset, including both underexposure and overexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR cameras and a beam splitter to simultaneously capture improper and normal exposure videos. In addition, we propose a Video Exposure Correction Network (VECNet) based on Retinex theory, which incorporates a two-stream illumination learning mechanism to enhance the overexposure and underexposure factors, respectively. The estimated multi-frame reflectance and dual-path illumination components are fused at both feature and image levels, leading to visually appealing results. Experimental results demonstrate that the proposed method outperforms existing image exposure correction and underexposed video enhancement methods. The code and dataset will be available soon.|\u4ee5\u9519\u8bef\u7684\u66dd\u5149\u62cd\u6444\u89c6\u9891\u901a\u5e38\u4f1a\u4ea7\u751f\u4ee4\u4eba\u4e0d\u6ee1\u610f\u7684\u89c6\u89c9\u6548\u679c\u3002\u867d\u7136\u56fe\u50cf\u66dd\u5149\u6821\u6b63\u662f\u4e00\u4e2a\u70ed\u95e8\u8bdd\u9898\uff0c\u4f46\u6587\u732e\u4e2d\u5bf9\u89c6\u9891\u66dd\u5149\u6821\u6b63\u7684\u63a2\u8ba8\u8f83\u5c11\u3002\u76f4\u63a5\u5c06\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u8f93\u5165\u89c6\u9891\u901a\u5e38\u4f1a\u5bfc\u81f4\u65f6\u95f4\u4e0d\u8fde\u8d2f\u4e14\u89c6\u89c9\u8d28\u91cf\u4f4e\u4e0b\u3002\u8be5\u9886\u57df\u7684\u73b0\u6709\u7814\u7a76\u4e5f\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\u6570\u636e\u96c6\u800c\u53d7\u5230\u9650\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u914d\u5bf9\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5305\u62ec\u66dd\u5149\u4e0d\u8db3\u548c\u66dd\u5149\u8fc7\u5ea6\u7684\u52a8\u6001\u573a\u666f\u3002\u4e3a\u4e86\u5b9e\u73b0\u7a7a\u95f4\u5bf9\u9f50\uff0c\u6211\u4eec\u5229\u7528\u4e24\u4e2a\u6570\u7801\u5355\u53cd\u76f8\u673a\u548c\u4e00\u4e2a\u5206\u675f\u5668\u540c\u65f6\u6355\u83b7\u4e0d\u6b63\u786e\u548c\u6b63\u5e38\u66dd\u5149\u7684\u89c6\u9891\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRetinex\u7406\u8bba\u7684\u89c6\u9891\u66dd\u5149\u6821\u6b63\u7f51\u7edc\uff08VECNet\uff09\uff0c\u5b83\u7ed3\u5408\u4e86\u53cc\u6d41\u7167\u660e\u5b66\u4e60\u673a\u5236\u6765\u5206\u522b\u589e\u5f3a\u8fc7\u5ea6\u66dd\u5149\u548c\u66dd\u5149\u4e0d\u8db3\u7684\u56e0\u7d20\u3002\u4f30\u8ba1\u7684\u591a\u5e27\u53cd\u5c04\u7387\u548c\u53cc\u8def\u7167\u660e\u5206\u91cf\u5728\u7279\u5f81\u548c\u56fe\u50cf\u7ea7\u522b\u4e0a\u878d\u5408\uff0c\u4ece\u800c\u4ea7\u751f\u89c6\u89c9\u4e0a\u5438\u5f15\u4eba\u7684\u7ed3\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u56fe\u50cf\u66dd\u5149\u6821\u6b63\u548c\u66dd\u5149\u4e0d\u8db3\u7684\u89c6\u9891\u589e\u5f3a\u65b9\u6cd5\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5373\u5c06\u63a8\u51fa\u3002|[2402.17296v1](http://arxiv.org/pdf/2402.17296v1)|null|\n", "2402.17287": "|**2024-02-27**|**An Interpretable Evaluation of Entropy-based Novelty of Generative Models**|\u57fa\u4e8e\u71b5\u7684\u751f\u6210\u6a21\u578b\u65b0\u9896\u6027\u7684\u53ef\u89e3\u91ca\u8bc4\u4f30|Jingwei Zhang, Cheuk Ting Li, Farzan Farnia|The massive developments of generative model frameworks and architectures require principled methods for the evaluation of a model's novelty compared to a reference dataset or baseline generative models. While the recent literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a baseline model has not been adequately studied in the machine learning community. In this work, we focus on the novelty assessment under multi-modal generative models and attempt to answer the following question: Given the samples of a generative model $\\mathcal{G}$ and a reference dataset $\\mathcal{S}$, how can we discover and count the modes expressed by $\\mathcal{G}$ more frequently than in $\\mathcal{S}$. We introduce a spectral approach to the described task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty of distribution $P_\\mathcal{G}$ with respect to distribution $P_\\mathcal{S}$. We analytically interpret the behavior of the KEN score under mixture distributions with sub-Gaussian components. Next, we develop a method based on Cholesky decomposition to compute the KEN score from observed samples. We support the KEN-based quantification of novelty by presenting several numerical results on synthetic and real image distributions. Our numerical results indicate the success of the proposed approach in detecting the novel modes and the comparison of state-of-the-art generative models.|\u751f\u6210\u6a21\u578b\u6846\u67b6\u548c\u67b6\u6784\u7684\u5927\u89c4\u6a21\u53d1\u5c55\u9700\u8981\u6709\u539f\u5219\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u6a21\u578b\u4e0e\u53c2\u8003\u6570\u636e\u96c6\u6216\u57fa\u7ebf\u751f\u6210\u6a21\u578b\u76f8\u6bd4\u7684\u65b0\u9896\u6027\u3002\u867d\u7136\u6700\u8fd1\u7684\u6587\u732e\u5e7f\u6cdb\u7814\u7a76\u4e86\u751f\u6210\u6a21\u578b\u7684\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u6cdb\u5316\u6027\u7684\u8bc4\u4f30\uff0c\u4f46\u673a\u5668\u5b66\u4e60\u793e\u533a\u5c1a\u672a\u5145\u5206\u7814\u7a76\u6a21\u578b\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u65b0\u9896\u6027\u8bc4\u4f30\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5173\u6ce8\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u4e0b\u7684\u65b0\u9896\u6027\u8bc4\u4f30\uff0c\u5e76\u5c1d\u8bd5\u56de\u7b54\u4ee5\u4e0b\u95ee\u9898\uff1a\u7ed9\u5b9a\u751f\u6210\u6a21\u578b $\\mathcal{G}$ \u7684\u6837\u672c\u548c\u53c2\u8003\u6570\u636e\u96c6 $\\mathcal{S}$\uff0c\u6211\u4eec\u5982\u4f55\u624d\u80fd\u6bd4 $\\mathcal{S}$ \u66f4\u9891\u7e41\u5730\u53d1\u73b0\u548c\u8ba1\u7b97 $\\mathcal{G}$ \u8868\u8fbe\u7684\u6a21\u5f0f\u3002\u6211\u4eec\u4e3a\u6240\u63cf\u8ff0\u7684\u4efb\u52a1\u5f15\u5165\u4e86\u8c31\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5185\u6838\u7684\u71b5\u65b0\u9896\u6027\uff08KEN\uff09\u5206\u6570\u6765\u91cf\u5316\u5206\u5e03 $P_\\mathcal{G}$ \u76f8\u5bf9\u4e8e\u5206\u5e03 $P_\\mathcal{S}$ \u7684\u57fa\u4e8e\u6a21\u5f0f\u7684\u65b0\u9896\u6027\u3002\u6211\u4eec\u5206\u6790\u89e3\u91ca\u4e86\u5177\u6709\u4e9a\u9ad8\u65af\u5206\u91cf\u7684\u6df7\u5408\u5206\u5e03\u4e0b KEN \u5f97\u5206\u7684\u884c\u4e3a\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e Cholesky \u5206\u89e3\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u89c2\u5bdf\u6837\u672c\u7684 KEN \u5206\u6570\u3002\u6211\u4eec\u901a\u8fc7\u5c55\u793a\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u5206\u5e03\u7684\u51e0\u4e2a\u6570\u503c\u7ed3\u679c\u6765\u652f\u6301\u57fa\u4e8e KEN \u7684\u65b0\u9896\u6027\u91cf\u5316\u3002\u6211\u4eec\u7684\u6570\u503c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u68c0\u6d4b\u65b0\u9896\u6a21\u5f0f\u548c\u6bd4\u8f83\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\u3002|[2402.17287v1](http://arxiv.org/pdf/2402.17287v1)|null|\n", "2402.17275": "|**2024-02-27**|**One-Shot Structure-Aware Stylized Image Synthesis**|\u4e00\u6b21\u6027\u7ed3\u6784\u611f\u77e5\u98ce\u683c\u5316\u56fe\u50cf\u5408\u6210|Hansam Cho, Jonghyun Lee, Seunggyu Chang, Yonghyun Jeong|While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with out-of-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models.|\u867d\u7136\u57fa\u4e8e GAN \u7684\u6a21\u578b\u5728\u56fe\u50cf\u98ce\u683c\u5316\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u5728\u5bf9\u5404\u79cd\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u98ce\u683c\u5316\u65f6\u5e38\u5e38\u96be\u4ee5\u4fdd\u6301\u7ed3\u6784\u3002\u6700\u8fd1\uff0c\u6269\u6563\u6a21\u578b\u5df2\u88ab\u7528\u4e8e\u56fe\u50cf\u98ce\u683c\u5316\uff0c\u4f46\u4ecd\u7136\u7f3a\u4e4f\u4fdd\u6301\u8f93\u5165\u56fe\u50cf\u7684\u539f\u59cb\u8d28\u91cf\u7684\u80fd\u529b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 OSASIS\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u4e00\u6b21\u6027\u98ce\u683c\u5316\u65b9\u6cd5\uff0c\u5728\u7ed3\u6784\u4fdd\u5b58\u65b9\u9762\u5177\u6709\u9c81\u68d2\u6027\u3002\u6211\u4eec\u8bc1\u660e OSASIS \u80fd\u591f\u6709\u6548\u5730\u5c06\u8bed\u4e49\u4e0e\u56fe\u50cf\u7ed3\u6784\u5206\u5f00\uff0c\u4ece\u800c\u4f7f\u5176\u80fd\u591f\u63a7\u5236\u7ed9\u5b9a\u8f93\u5165\u6240\u5b9e\u73b0\u7684\u5185\u5bb9\u548c\u98ce\u683c\u7684\u7ea7\u522b\u3002\u6211\u4eec\u5c06 OSASIS \u5e94\u7528\u4e8e\u5404\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u5305\u62ec\u57df\u5916\u53c2\u8003\u56fe\u50cf\u7684\u98ce\u683c\u5316\u548c\u6587\u672c\u9a71\u52a8\u64cd\u4f5c\u7684\u98ce\u683c\u5316\u3002\u7ed3\u679c\u8868\u660e\uff0cOSASIS \u4f18\u4e8e\u5176\u4ed6\u98ce\u683c\u5316\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u8bad\u7ec3\u671f\u95f4\u5f88\u5c11\u9047\u5230\u7684\u8f93\u5165\u56fe\u50cf\uff0c\u4e3a\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fdb\u884c\u98ce\u683c\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002|[2402.17275v1](http://arxiv.org/pdf/2402.17275v1)|null|\n", "2402.17245": "|**2024-02-27**|**Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation**|Playground v2.5\uff1a\u589e\u5f3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7f8e\u5b66\u8d28\u91cf\u7684\u4e09\u4e2a\u89c1\u89e3|Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, Suhail Doshi|In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2. Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of diffusion-based image generation models.|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5206\u4eab\u4e86\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u7f8e\u5b66\u8d28\u91cf\u7684\u4e09\u4e2a\u89c1\u89e3\u3002\u6211\u4eec\u4e13\u6ce8\u4e8e\u6a21\u578b\u6539\u8fdb\u7684\u4e09\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u589e\u5f3a\u989c\u8272\u548c\u5bf9\u6bd4\u5ea6\u3001\u6539\u8fdb\u591a\u4e2a\u7eb5\u6a2a\u6bd4\u7684\u751f\u6210\u4ee5\u53ca\u6539\u8fdb\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u7cbe\u7ec6\u7ec6\u8282\u3002\u9996\u5148\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u566a\u58f0\u8868\u5728\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5c55\u793a\u5176\u5bf9\u771f\u5b9e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u6df1\u8fdc\u5f71\u54cd\u3002\u5176\u6b21\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u9002\u5e94\u5404\u79cd\u7eb5\u6a2a\u6bd4\u7684\u6311\u6218\uff0c\u5f3a\u8c03\u51c6\u5907\u5e73\u8861\u7684\u5206\u6876\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5c06\u6a21\u578b\u8f93\u51fa\u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u7684\u5173\u952e\u4f5c\u7528\uff0c\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u4eba\u7c7b\u7684\u611f\u77e5\u671f\u671b\u4ea7\u751f\u5171\u9e23\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5206\u6790\u548c\u5b9e\u9a8c\uff0cPlayground v2.5 \u5728\u5404\u79cd\u6761\u4ef6\u548c\u5bbd\u9ad8\u6bd4\u4e0b\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u7f8e\u5b66\u8d28\u91cf\u6027\u80fd\uff0c\u4f18\u4e8e SDXL \u548c Playground v2 \u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u6e90\u6a21\u578b\u4ee5\u53ca\u95ed\u6e90\u6a21\u578b\u5546\u4e1a\u7cfb\u7edf\uff0c\u4f8b\u5982 DALLE 3 \u548c Midjourney v5.2\u3002\u6211\u4eec\u7684\u6a21\u578b\u662f\u5f00\u6e90\u7684\uff0c\u6211\u4eec\u5e0c\u671b Playground v2.5 \u7684\u5f00\u53d1\u4e3a\u65e8\u5728\u63d0\u9ad8\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7f8e\u5b66\u8d28\u91cf\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002|[2402.17245v1](http://arxiv.org/pdf/2402.17245v1)|null|\n", "2402.17213": "|**2024-02-27**|**VCD: Knowledge Base Guided Visual Commonsense Discovery in Images**|VCD\uff1a\u77e5\u8bc6\u5e93\u5f15\u5bfc\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u5e38\u8bc6\u53d1\u73b0|Xiangqing Shen, Yurun Song, Siwei Wu, Rui Xia|Visual commonsense contains knowledge about object properties, relationships, and behaviors in visual data. Discovering visual commonsense can provide a more comprehensive and richer understanding of images, and enhance the reasoning and decision-making capabilities of computer vision systems. However, the visual commonsense defined in existing visual commonsense discovery studies is coarse-grained and incomplete. In this work, we draw inspiration from a commonsense knowledge base ConceptNet in natural language processing, and systematically define the types of visual commonsense. Based on this, we introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract fine-grained commonsense of different types contained within different objects in the image. We accordingly construct a dataset (VCDD) from Visual Genome and ConceptNet for VCD, featuring over 100,000 images and 14 million object-commonsense pairs. We furthermore propose a generative model (VCDM) that integrates a vision-language model with instruction tuning to tackle VCD. Automatic and human evaluations demonstrate VCDM's proficiency in VCD, particularly outperforming GPT-4V in implicit commonsense discovery. The value of VCD is further demonstrated by its application to two downstream tasks, including visual commonsense evaluation and visual question answering. The data and code will be made available on GitHub.|\u89c6\u89c9\u5e38\u8bc6\u5305\u542b\u6709\u5173\u89c6\u89c9\u6570\u636e\u4e2d\u7684\u5bf9\u8c61\u5c5e\u6027\u3001\u5173\u7cfb\u548c\u884c\u4e3a\u7684\u77e5\u8bc6\u3002\u53d1\u73b0\u89c6\u89c9\u5e38\u8bc6\u53ef\u4ee5\u63d0\u4f9b\u5bf9\u56fe\u50cf\u66f4\u5168\u9762\u3001\u66f4\u4e30\u5bcc\u7684\u7406\u89e3\uff0c\u589e\u5f3a\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u7684\u63a8\u7406\u548c\u51b3\u7b56\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u89c6\u89c9\u5e38\u8bc6\u53d1\u73b0\u7814\u7a76\u4e2d\u5b9a\u4e49\u7684\u89c6\u89c9\u5e38\u8bc6\u662f\u7c97\u7c92\u5ea6\u4e14\u4e0d\u5b8c\u6574\u7684\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ece\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u5e38\u8bc6\u77e5\u8bc6\u5e93ConceptNet\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u7cfb\u7edf\u5730\u5b9a\u4e49\u4e86\u89c6\u89c9\u5e38\u8bc6\u7684\u7c7b\u578b\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u4efb\u52a1\uff0c\u89c6\u89c9\u5e38\u8bc6\u53d1\u73b0\uff08VCD\uff09\uff0c\u65e8\u5728\u63d0\u53d6\u56fe\u50cf\u4e2d\u4e0d\u540c\u5bf9\u8c61\u4e2d\u5305\u542b\u7684\u4e0d\u540c\u7c7b\u578b\u7684\u7ec6\u7c92\u5ea6\u5e38\u8bc6\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4ece Visual Genome \u548c ConceptNet for VCD \u6784\u5efa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6 (VCDD)\uff0c\u5176\u4e2d\u5305\u542b\u8d85\u8fc7 100,000 \u5f20\u56fe\u50cf\u548c 1400 \u4e07\u4e2a\u5bf9\u8c61\u5e38\u8bc6\u5bf9\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u6a21\u578b\uff08VCDM\uff09\uff0c\u5b83\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u6307\u4ee4\u8c03\u6574\u76f8\u7ed3\u5408\u6765\u5904\u7406 VCD\u3002\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\u8bc1\u660e\u4e86 VCDM \u5728 VCD \u65b9\u9762\u7684\u719f\u7ec3\u7a0b\u5ea6\uff0c\u7279\u522b\u662f\u5728\u9690\u5f0f\u5e38\u8bc6\u53d1\u73b0\u65b9\u9762\u4f18\u4e8e GPT-4V\u3002 VCD\u7684\u4ef7\u503c\u901a\u8fc7\u5176\u5728\u89c6\u89c9\u5e38\u8bc6\u8bc4\u4f30\u548c\u89c6\u89c9\u95ee\u7b54\u8fd9\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f97\u5230\u4e86\u8fdb\u4e00\u6b65\u8bc1\u660e\u3002\u6570\u636e\u548c\u4ee3\u7801\u5c06\u5728 GitHub \u4e0a\u63d0\u4f9b\u3002|[2402.17213v1](http://arxiv.org/pdf/2402.17213v1)|null|\n", "2402.17210": "|**2024-02-27**|**Purified and Unified Steganographic Network**|\u7eaf\u51c0\u7edf\u4e00\u7684\u9690\u5199\u7f51\u7edc|Guobiao Li, Sheng Li, Zicong Luo, Zhenxing Qian, Xinpeng Zhang|Steganography is the art of hiding secret data into the cover media for covert communication. In recent years, more and more deep neural network (DNN)-based steganographic schemes are proposed to train steganographic networks for secret embedding and recovery, which are shown to be promising. Compared with the handcrafted steganographic tools, steganographic networks tend to be large in size. It raises concerns on how to imperceptibly and effectively transmit these networks to the sender and receiver to facilitate the covert communication. To address this issue, we propose in this paper a Purified and Unified Steganographic Network (PUSNet). It performs an ordinary machine learning task in a purified network, which could be triggered into steganographic networks for secret embedding or recovery using different keys. We formulate the construction of the PUSNet into a sparse weight filling problem to flexibly switch between the purified and steganographic networks. We further instantiate our PUSNet as an image denoising network with two steganographic networks concealed for secret image embedding and recovery. Comprehensive experiments demonstrate that our PUSNet achieves good performance on secret image embedding, secret image recovery, and image denoising in a single architecture. It is also shown to be capable of imperceptibly carrying the steganographic networks in a purified network. Code is available at \\url{https://github.com/albblgb/PUSNet}|\u9690\u5199\u672f\u662f\u5c06\u79d8\u5bc6\u6570\u636e\u9690\u85cf\u5230\u5c01\u9762\u5a92\u4f53\u4e2d\u4ee5\u8fdb\u884c\u79d8\u5bc6\u901a\u4fe1\u7684\u827a\u672f\u3002\u8fd1\u5e74\u6765\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u9690\u5199\u65b9\u6848\u88ab\u63d0\u51fa\u6765\u8bad\u7ec3\u9690\u5199\u7f51\u7edc\u8fdb\u884c\u79d8\u5bc6\u5d4c\u5165\u548c\u6062\u590d\uff0c\u8fd9\u88ab\u8bc1\u660e\u662f\u6709\u524d\u9014\u7684\u3002\u4e0e\u624b\u5de5\u5236\u4f5c\u7684\u9690\u5199\u5de5\u5177\u76f8\u6bd4\uff0c\u9690\u5199\u7f51\u7edc\u7684\u89c4\u6a21\u5f80\u5f80\u5f88\u5927\u3002\u8fd9\u5c31\u5f15\u53d1\u4e86\u4eba\u4eec\u5bf9\u5982\u4f55\u5728\u4e0d\u77e5\u4e0d\u89c9\u4e2d\u6709\u6548\u5730\u5c06\u8fd9\u4e9b\u7f51\u7edc\u4f20\u8f93\u7ed9\u53d1\u9001\u8005\u548c\u63a5\u6536\u8005\u4ee5\u4fc3\u8fdb\u79d8\u5bc6\u901a\u4fe1\u7684\u62c5\u5fe7\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5728\u672c\u6587\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u7eaf\u5316\u7edf\u4e00\u7684\u9690\u5199\u7f51\u7edc\uff08PUSNet\uff09\u3002\u5b83\u5728\u7eaf\u5316\u7684\u7f51\u7edc\u4e2d\u6267\u884c\u666e\u901a\u7684\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u53ef\u4ee5\u88ab\u89e6\u53d1\u5230\u9690\u5199\u7f51\u7edc\u4e2d\uff0c\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684\u5bc6\u94a5\u8fdb\u884c\u79d8\u5bc6\u5d4c\u5165\u6216\u6062\u590d\u3002\u6211\u4eec\u5c06 PUSNet \u7684\u6784\u5efa\u516c\u5f0f\u5316\u4e3a\u7a00\u758f\u6743\u91cd\u586b\u5145\u95ee\u9898\uff0c\u4ee5\u4fbf\u5728\u7eaf\u5316\u7f51\u7edc\u548c\u9690\u5199\u7f51\u7edc\u4e4b\u95f4\u7075\u6d3b\u5207\u6362\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06 PUSNet \u5b9e\u4f8b\u5316\u4e3a\u56fe\u50cf\u53bb\u566a\u7f51\u7edc\uff0c\u5176\u4e2d\u9690\u85cf\u4e86\u4e24\u4e2a\u9690\u5199\u7f51\u7edc\uff0c\u7528\u4e8e\u79d8\u5bc6\u56fe\u50cf\u5d4c\u5165\u548c\u6062\u590d\u3002\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 PUSNet \u5728\u5355\u4e00\u67b6\u6784\u4e2d\u7684\u79d8\u5bc6\u56fe\u50cf\u5d4c\u5165\u3001\u79d8\u5bc6\u56fe\u50cf\u6062\u590d\u548c\u56fe\u50cf\u53bb\u566a\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002\u5b83\u8fd8\u88ab\u8bc1\u660e\u80fd\u591f\u5728\u7eaf\u5316\u7f51\u7edc\u4e2d\u4e0d\u77e5\u4e0d\u89c9\u5730\u643a\u5e26\u9690\u5199\u7f51\u7edc\u3002\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/albblgb/PUSNet} \u83b7\u53d6|[2402.17210v1](http://arxiv.org/pdf/2402.17210v1)|null|\n", "2402.17200": "|**2024-02-27**|**Enhancing Quality of Compressed Images by Mitigating Enhancement Bias Towards Compression Domain**|\u901a\u8fc7\u51cf\u8f7b\u538b\u7f29\u57df\u7684\u589e\u5f3a\u504f\u5dee\u6765\u589e\u5f3a\u538b\u7f29\u56fe\u50cf\u7684\u8d28\u91cf|Qunliang Xing, Mai Xu, Shengxi Li, Xin Deng, Meisong Zheng, Huaida Liu, Ying Chen|Existing quality enhancement methods for compressed images focus on aligning the enhancement domain with the raw domain to yield realistic images. However, these methods exhibit a pervasive enhancement bias towards the compression domain, inadvertently regarding it as more realistic than the raw domain. This bias makes enhanced images closely resemble their compressed counterparts, thus degrading their perceptual quality. In this paper, we propose a simple yet effective method to mitigate this bias and enhance the quality of compressed images. Our method employs a conditional discriminator with the compressed image as a key condition, and then incorporates a domain-divergence regularization to actively distance the enhancement domain from the compression domain. Through this dual strategy, our method enables the discrimination against the compression domain, and brings the enhancement domain closer to the raw domain. Comprehensive quality evaluations confirm the superiority of our method over other state-of-the-art methods without incurring inference overheads.|\u73b0\u6709\u7684\u538b\u7f29\u56fe\u50cf\u8d28\u91cf\u589e\u5f3a\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u5c06\u589e\u5f3a\u57df\u4e0e\u539f\u59cb\u57df\u5bf9\u9f50\u4ee5\u4ea7\u751f\u903c\u771f\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8868\u73b0\u51fa\u5bf9\u538b\u7f29\u57df\u7684\u666e\u904d\u589e\u5f3a\u504f\u89c1\uff0c\u65e0\u610f\u4e2d\u8ba4\u4e3a\u5b83\u6bd4\u539f\u59cb\u57df\u66f4\u73b0\u5b9e\u3002\u8fd9\u79cd\u504f\u5dee\u4f7f\u5f97\u589e\u5f3a\u56fe\u50cf\u4e0e\u538b\u7f29\u56fe\u50cf\u975e\u5e38\u76f8\u4f3c\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u5b83\u4eec\u7684\u611f\u77e5\u8d28\u91cf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\u6765\u51cf\u8f7b\u8fd9\u79cd\u504f\u5dee\u5e76\u63d0\u9ad8\u538b\u7f29\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4ee5\u538b\u7f29\u56fe\u50cf\u4e3a\u5173\u952e\u6761\u4ef6\u7684\u6761\u4ef6\u9274\u522b\u5668\uff0c\u7136\u540e\u7ed3\u5408\u57df\u6563\u5ea6\u6b63\u5219\u5316\u6765\u4e3b\u52a8\u5c06\u589e\u5f3a\u57df\u4e0e\u538b\u7f29\u57df\u5206\u5f00\u3002\u901a\u8fc7\u8fd9\u79cd\u53cc\u91cd\u7b56\u7565\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u533a\u5206\u538b\u7f29\u57df\uff0c\u5e76\u4f7f\u589e\u5f3a\u57df\u66f4\u63a5\u8fd1\u539f\u59cb\u57df\u3002\u5168\u9762\u7684\u8d28\u91cf\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u4e14\u4e0d\u4f1a\u4ea7\u751f\u63a8\u7406\u5f00\u9500\u3002|[2402.17200v1](http://arxiv.org/pdf/2402.17200v1)|null|\n", "2402.17177": "|**2024-02-27**|**Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models**|Sora\uff1a\u5927\u89c6\u89c9\u6a21\u578b\u7684\u80cc\u666f\u3001\u6280\u672f\u3001\u5c40\u9650\u6027\u548c\u673a\u9047\u56de\u987e|Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et.al.|Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.|Sora \u662f\u4e00\u79cd\u6587\u672c\u5230\u89c6\u9891\u751f\u6210 AI \u6a21\u578b\uff0c\u7531 OpenAI \u4e8e 2024 \u5e74 2 \u6708\u53d1\u5e03\u3002\u8be5\u6a21\u578b\u7ecf\u8fc7\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6839\u636e\u6587\u672c\u6307\u4ee4\u751f\u6210\u73b0\u5b9e\u6216\u60f3\u8c61\u573a\u666f\u7684\u89c6\u9891\uff0c\u5e76\u663e\u793a\u51fa\u6a21\u62df\u7269\u7406\u4e16\u754c\u7684\u6f5c\u529b\u3002\u672c\u6587\u57fa\u4e8e\u516c\u5f00\u6280\u672f\u62a5\u544a\u548c\u9006\u5411\u5de5\u7a0b\uff0c\u5bf9\u6587\u672c\u8f6c\u89c6\u9891\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u6a21\u578b\u80cc\u666f\u3001\u76f8\u5173\u6280\u672f\u3001\u5e94\u7528\u3001\u5269\u4f59\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\u3002\u6211\u4eec\u9996\u5148\u8ffd\u8e2a Sora \u7684\u5f00\u53d1\u5e76\u7814\u7a76\u7528\u4e8e\u6784\u5efa\u8fd9\u4e2a\u201c\u4e16\u754c\u6a21\u62df\u5668\u201d\u7684\u5e95\u5c42\u6280\u672f\u3002\u7136\u540e\uff0c\u6211\u4eec\u8be6\u7ec6\u63cf\u8ff0\u4e86Sora\u5728\u7535\u5f71\u5236\u4f5c\u3001\u6559\u80b2\u3001\u8425\u9500\u7b49\u591a\u4e2a\u884c\u4e1a\u4e2d\u7684\u5e94\u7528\u548c\u6f5c\u5728\u5f71\u54cd\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u5e7f\u6cdb\u90e8\u7f72 Sora \u9700\u8981\u89e3\u51b3\u7684\u4e3b\u8981\u6311\u6218\u548c\u9650\u5236\uff0c\u4f8b\u5982\u786e\u4fdd\u5b89\u5168\u548c\u516c\u6b63\u7684\u89c6\u9891\u751f\u6210\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86 Sora \u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\uff0c\u4ee5\u53ca\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u5982\u4f55\u5b9e\u73b0\u4eba\u673a\u4ea4\u4e92\u7684\u65b0\u65b9\u5f0f\uff0c\u4ece\u800c\u63d0\u9ad8\u89c6\u9891\u751f\u6210\u7684\u751f\u4ea7\u529b\u548c\u521b\u9020\u529b\u3002|[2402.17177v1](http://arxiv.org/pdf/2402.17177v1)|null|\n", "2402.17169": "|**2024-02-27**|**Deep Umbra: A Generative Approach for Sunlight Access Computation in Urban Spaces**|Deep Umbra\uff1a\u57ce\u5e02\u7a7a\u95f4\u4e2d\u9633\u5149\u7167\u5c04\u8ba1\u7b97\u7684\u751f\u6210\u65b9\u6cd5|Kazi Shahrukh Omar, Gustavo Moreira, Daniel Hodczak, Maryam Hosseini, Nicola Colaninno, Marcos Lage, Fabio Miranda|Sunlight and shadow play critical roles in how urban spaces are utilized, thrive, and grow. While access to sunlight is essential to the success of urban environments, shadows can provide shaded places to stay during the hot seasons, mitigate heat island effect, and increase pedestrian comfort levels. Properly quantifying sunlight access and shadows in large urban environments is key in tackling some of the important challenges facing cities today. In this paper, we propose Deep Umbra, a novel computational framework that enables the quantification of sunlight access and shadows at a global scale. Our framework is based on a conditional generative adversarial network that considers the physical form of cities to compute high-resolution spatial information of accumulated sunlight access for the different seasons of the year. We use data from seven different cities to train our model, and show, through an extensive set of experiments, its low overall RMSE (below 0.1) as well as its extensibility to cities that were not part of the training set. Additionally, we contribute a set of case studies and a comprehensive dataset with sunlight access information for more than 100 cities across six continents of the world. Deep Umbra is available at https://urbantk.org/shadows.|\u9633\u5149\u548c\u9634\u5f71\u5728\u57ce\u5e02\u7a7a\u95f4\u7684\u5229\u7528\u3001\u7e41\u8363\u548c\u53d1\u5c55\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u867d\u7136\u83b7\u5f97\u9633\u5149\u5bf9\u4e8e\u57ce\u5e02\u73af\u5883\u7684\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9634\u5f71\u53ef\u4ee5\u5728\u708e\u70ed\u5b63\u8282\u63d0\u4f9b\u906e\u836b\u7684\u5730\u65b9\uff0c\u51cf\u8f7b\u70ed\u5c9b\u6548\u5e94\u5e76\u63d0\u9ad8\u884c\u4eba\u8212\u9002\u5ea6\u3002\u6b63\u786e\u91cf\u5316\u5927\u578b\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u9633\u5149\u7167\u5c04\u548c\u9634\u5f71\u662f\u89e3\u51b3\u5f53\u4eca\u57ce\u5e02\u9762\u4e34\u7684\u4e00\u4e9b\u91cd\u8981\u6311\u6218\u7684\u5173\u952e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Deep Umbra\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u5168\u7403\u8303\u56f4\u5185\u91cf\u5316\u9633\u5149\u7684\u8fdb\u5165\u548c\u9634\u5f71\u3002\u6211\u4eec\u7684\u6846\u67b6\u57fa\u4e8e\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u8003\u8651\u57ce\u5e02\u7684\u7269\u7406\u5f62\u6001\u6765\u8ba1\u7b97\u4e00\u5e74\u4e2d\u4e0d\u540c\u5b63\u8282\u7d2f\u79ef\u9633\u5149\u7167\u5c04\u7684\u9ad8\u5206\u8fa8\u7387\u7a7a\u95f4\u4fe1\u606f\u3002\u6211\u4eec\u4f7f\u7528\u6765\u81ea\u4e03\u4e2a\u4e0d\u540c\u57ce\u5e02\u7684\u6570\u636e\u6765\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u5c55\u793a\u5176\u8f83\u4f4e\u7684\u603b\u4f53 RMSE\uff08\u4f4e\u4e8e 0.1\uff09\u4ee5\u53ca\u5176\u5bf9\u4e0d\u5c5e\u4e8e\u8bad\u7ec3\u96c6\u7684\u57ce\u5e02\u7684\u53ef\u6269\u5c55\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u7ec4\u6848\u4f8b\u7814\u7a76\u548c\u4e00\u4e2a\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4e16\u754c\u516d\u5927\u6d32 100 \u591a\u4e2a\u57ce\u5e02\u7684\u9633\u5149\u83b7\u53d6\u4fe1\u606f\u3002 Deep Umbra \u53ef\u4ee5\u5728 https://urbantk.org/shadows \u4e0a\u627e\u5230\u3002|[2402.17169v1](http://arxiv.org/pdf/2402.17169v1)|null|\n", "2402.17139": "|**2024-02-27**|**Video as the New Language for Real-World Decision Making**|\u89c6\u9891\u4f5c\u4e3a\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u7684\u65b0\u8bed\u8a00|Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, Dale Schuurmans|Both text and video data are abundant on the internet and support large-scale self-supervised learning through next token or frame prediction. However, they have not been equally leveraged: language models have had significant real-world impact, whereas video generation has remained largely limited to media entertainment. Yet video data captures important information about the physical world that is difficult to express in language. To address this gap, we discuss an under-appreciated opportunity to extend video generation to solve tasks in the real world. We observe how, akin to language, video can serve as a unified interface that can absorb internet knowledge and represent diverse tasks. Moreover, we demonstrate how, like language models, video generation can serve as planners, agents, compute engines, and environment simulators through techniques such as in-context learning, planning and reinforcement learning. We identify major impact opportunities in domains such as robotics, self-driving, and science, supported by recent work that demonstrates how such advanced capabilities in video generation are plausibly within reach. Lastly, we identify key challenges in video generation that mitigate progress. Addressing these challenges will enable video generation models to demonstrate unique value alongside language models in a wider array of AI applications.|\u4e92\u8054\u7f51\u4e0a\u7684\u6587\u672c\u548c\u89c6\u9891\u6570\u636e\u90fd\u5f88\u4e30\u5bcc\uff0c\u5e76\u4e14\u901a\u8fc7\u4e0b\u4e00\u4e2a\u6807\u8bb0\u6216\u5e27\u9884\u6d4b\u652f\u6301\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u5b66\u4e60\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5e76\u6ca1\u6709\u5f97\u5230\u540c\u7b49\u7684\u5229\u7528\uff1a\u8bed\u8a00\u6a21\u578b\u5bf9\u73b0\u5b9e\u4e16\u754c\u4ea7\u751f\u4e86\u91cd\u5927\u5f71\u54cd\uff0c\u800c\u89c6\u9891\u751f\u6210\u4ecd\u7136\u4e3b\u8981\u5c40\u9650\u4e8e\u5a92\u4f53\u5a31\u4e50\u3002\u7136\u800c\uff0c\u89c6\u9891\u6570\u636e\u6355\u83b7\u4e86\u96be\u4ee5\u7528\u8bed\u8a00\u8868\u8fbe\u7684\u6709\u5173\u7269\u7406\u4e16\u754c\u7684\u91cd\u8981\u4fe1\u606f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u91cd\u89c6\u7684\u673a\u4f1a\uff0c\u5373\u6269\u5c55\u89c6\u9891\u751f\u6210\u4ee5\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4efb\u52a1\u3002\u6211\u4eec\u89c2\u5bdf\u89c6\u9891\u5982\u4f55\u50cf\u8bed\u8a00\u4e00\u6837\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u754c\u9762\u6765\u5438\u6536\u4e92\u8054\u7f51\u77e5\u8bc6\u5e76\u4ee3\u8868\u4e0d\u540c\u7684\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u6f14\u793a\u4e86\u5982\u4f55\u50cf\u8bed\u8a00\u6a21\u578b\u4e00\u6837\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u89c6\u9891\u751f\u6210\u53ef\u4ee5\u5145\u5f53\u89c4\u5212\u5668\u3001\u4ee3\u7406\u3001\u8ba1\u7b97\u5f15\u64ce\u548c\u73af\u5883\u6a21\u62df\u5668\u3002\u6211\u4eec\u786e\u5b9a\u4e86\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u79d1\u5b66\u7b49\u9886\u57df\u7684\u91cd\u5927\u5f71\u54cd\u673a\u4f1a\uff0c\u6700\u8fd1\u7684\u5de5\u4f5c\u8bc1\u660e\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8fd9\u79cd\u5148\u8fdb\u529f\u80fd\u662f\u5982\u4f55\u89e6\u624b\u53ef\u53ca\u7684\u3002\u6700\u540e\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u963b\u788d\u8fdb\u5c55\u7684\u5173\u952e\u6311\u6218\u3002\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u5c06\u4f7f\u89c6\u9891\u751f\u6210\u6a21\u578b\u80fd\u591f\u4e0e\u8bed\u8a00\u6a21\u578b\u4e00\u8d77\u5728\u66f4\u5e7f\u6cdb\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u4e2d\u5c55\u793a\u72ec\u7279\u7684\u4ef7\u503c\u3002|[2402.17139v1](http://arxiv.org/pdf/2402.17139v1)|null|\n", "2402.17113": "|**2024-02-27**|**Transparent Image Layer Diffusion using Latent Transparency**|\u4f7f\u7528\u6f5c\u5728\u900f\u660e\u5ea6\u7684\u900f\u660e\u56fe\u50cf\u5c42\u6269\u6563|Lvmin Zhang, Maneesh Agrawala|We present LayerDiffusion, an approach enabling large-scale pretrained latent diffusion models to generate transparent images. The method allows generation of single transparent images or of multiple transparent layers. The method learns a \"latent transparency\" that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model. It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model. In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space. We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme. We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc. A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting. Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock.|\u6211\u4eec\u63d0\u51fa\u4e86 LayerDiffusion\uff0c\u4e00\u79cd\u4f7f\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u80fd\u591f\u751f\u6210\u900f\u660e\u56fe\u50cf\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u751f\u6210\u5355\u4e2a\u900f\u660e\u56fe\u50cf\u6216\u591a\u4e2a\u900f\u660e\u5c42\u3002\u8be5\u65b9\u6cd5\u5b66\u4e60\u201c\u6f5c\u5728\u900f\u660e\u5ea6\u201d\uff0c\u5c06 alpha \u901a\u9053\u900f\u660e\u5ea6\u7f16\u7801\u5230\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u6d41\u5f62\u4e2d\u3002\u5b83\u901a\u8fc7\u5c06\u589e\u52a0\u7684\u900f\u660e\u5ea6\u8c03\u8282\u4e3a\u6f5c\u5728\u504f\u79fb\uff0c\u5e76\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u539f\u59cb\u6f5c\u5728\u5206\u5e03\u8fdb\u884c\u6700\u5c0f\u7a0b\u5ea6\u7684\u66f4\u6539\uff0c\u4ece\u800c\u4fdd\u7559\u4e86\u5927\u578b\u6269\u6563\u6a21\u578b\u7684\u751f\u4ea7\u5c31\u7eea\u8d28\u91cf\u3002\u8fd9\u6837\uff0c\u4efb\u4f55\u6f5c\u5728\u6269\u6563\u6a21\u578b\u90fd\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u8c03\u6574\u540e\u7684\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u5fae\u8c03\u6765\u8f6c\u6362\u4e3a\u900f\u660e\u56fe\u50cf\u751f\u6210\u5668\u3002\u6211\u4eec\u4f7f\u7528\u4eba\u673a\u5faa\u73af\u6536\u96c6\u65b9\u6848\u6536\u96c6\u7684 1M \u900f\u660e\u56fe\u50cf\u5c42\u5bf9\u6765\u8bad\u7ec3\u6a21\u578b\u3002\u6211\u4eec\u8868\u660e\uff0c\u6f5c\u5728\u900f\u660e\u5ea6\u53ef\u4ee5\u5e94\u7528\u4e8e\u4e0d\u540c\u7684\u5f00\u6e90\u56fe\u50cf\u751f\u6210\u5668\uff0c\u6216\u8005\u9002\u5e94\u5404\u79cd\u6761\u4ef6\u63a7\u5236\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0\u524d\u666f/\u80cc\u666f\u6761\u4ef6\u5c42\u751f\u6210\u3001\u8054\u5408\u5c42\u751f\u6210\u3001\u5c42\u5185\u5bb9\u7684\u7ed3\u6784\u63a7\u5236\u7b49\u5e94\u7528\u3002 \u7528\u6237\u7814\u7a76\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b (97%)\uff0c\u7528\u6237\u66f4\u559c\u6b22\u6211\u4eec\u539f\u751f\u751f\u6210\u7684\u900f\u660e\u5185\u5bb9\uff0c\u800c\u4e0d\u662f\u4ee5\u524d\u7684\u4e34\u65f6\u89e3\u51b3\u65b9\u6848\uff08\u4f8b\u5982\u751f\u6210\u7136\u540e\u62a0\u56fe\uff09\u3002\u7528\u6237\u8fd8\u62a5\u544a\u8bf4\uff0c\u6211\u4eec\u751f\u6210\u7684\u900f\u660e\u56fe\u50cf\u7684\u8d28\u91cf\u53ef\u4e0e Adob\u200b\u200be Stock \u7b49\u771f\u6b63\u7684\u5546\u4e1a\u900f\u660e\u8d44\u6e90\u76f8\u5ab2\u7f8e\u3002|[2402.17113v1](http://arxiv.org/pdf/2402.17113v1)|null|\n", "2402.17101": "|**2024-02-27**|**T-HITL Effectively Addresses Problematic Associations in Image Generation and Maintains Overall Visual Quality**|T-HITL \u6709\u6548\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u95ee\u9898\u5173\u8054\u5e76\u4fdd\u6301\u6574\u4f53\u89c6\u89c9\u8d28\u91cf|Susan Epstein, Li Chen, Alessandro Vecchiato, Ankit Jain|Generative AI image models may inadvertently generate problematic representations of people. Past research has noted that millions of users engage daily across the world with these models and that the models, including through problematic representations of people, have the potential to compound and accelerate real-world discrimination and other harms (Bianchi et al, 2023). In this paper, we focus on addressing the generation of problematic associations between demographic groups and semantic concepts that may reflect and reinforce negative narratives embedded in social data. Building on sociological literature (Blumer, 1958) and mapping representations to model behaviors, we have developed a taxonomy to study problematic associations in image generation models. We explore the effectiveness of fine tuning at the model level as a method to address these associations, identifying a potential reduction in visual quality as a limitation of traditional fine tuning. We also propose a new methodology with twice-human-in-the-loop (T-HITL) that promises improvements in both reducing problematic associations and also maintaining visual quality. We demonstrate the effectiveness of T-HITL by providing evidence of three problematic associations addressed by T-HITL at the model level. Our contributions to scholarship are two-fold. By defining problematic associations in the context of machine learning models and generative AI, we introduce a conceptual and technical taxonomy for addressing some of these associations. Finally, we provide a method, T-HITL, that addresses these associations and simultaneously maintains visual quality of image model generations. This mitigation need not be a tradeoff, but rather an enhancement.|\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u56fe\u50cf\u6a21\u578b\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u751f\u6210\u6709\u95ee\u9898\u7684\u4eba\u7269\u8868\u793a\u3002\u8fc7\u53bb\u7684\u7814\u7a76\u6307\u51fa\uff0c\u4e16\u754c\u5404\u5730\u6709\u6570\u4ee5\u767e\u4e07\u8ba1\u7684\u7528\u6237\u6bcf\u5929\u90fd\u5728\u4f7f\u7528\u8fd9\u4e9b\u6a21\u578b\uff0c\u800c\u8fd9\u4e9b\u6a21\u578b\uff0c\u5305\u62ec\u901a\u8fc7\u5bf9\u4eba\u8fdb\u884c\u6709\u95ee\u9898\u7684\u8868\u8ff0\uff0c\u6709\u53ef\u80fd\u52a0\u5267\u548c\u52a0\u901f\u73b0\u5b9e\u4e16\u754c\u7684\u6b67\u89c6\u548c\u5176\u4ed6\u5371\u5bb3\uff08Bianchi \u7b49\u4eba\uff0c2023\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91cd\u70b9\u89e3\u51b3\u4eba\u53e3\u7fa4\u4f53\u548c\u8bed\u4e49\u6982\u5ff5\u4e4b\u95f4\u4ea7\u751f\u7684\u6709\u95ee\u9898\u7684\u5173\u8054\uff0c\u8fd9\u4e9b\u5173\u8054\u53ef\u80fd\u53cd\u6620\u548c\u5f3a\u5316\u793e\u4f1a\u6570\u636e\u4e2d\u5d4c\u5165\u7684\u8d1f\u9762\u53d9\u8ff0\u3002\u57fa\u4e8e\u793e\u4f1a\u5b66\u6587\u732e\uff08Blumer\uff0c1958\uff09\u5e76\u5c06\u8868\u5f81\u6620\u5c04\u5230\u6a21\u578b\u884c\u4e3a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u5206\u7c7b\u6cd5\u6765\u7814\u7a76\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u5b58\u5728\u95ee\u9898\u7684\u5173\u8054\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u6a21\u578b\u7ea7\u522b\u5fae\u8c03\u4f5c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u5173\u8054\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c06\u89c6\u89c9\u8d28\u91cf\u7684\u6f5c\u5728\u964d\u4f4e\u786e\u5b9a\u4e3a\u4f20\u7edf\u5fae\u8c03\u7684\u9650\u5236\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u7528\u4e24\u6b21\u4eba\u673a\u4ea4\u4e92 (T-HITL) \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6709\u671b\u5728\u51cf\u5c11\u6709\u95ee\u9898\u7684\u5173\u8054\u548c\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u6709\u6240\u6539\u8fdb\u3002\u6211\u4eec\u901a\u8fc7\u63d0\u4f9b T-HITL \u5728\u6a21\u578b\u7ea7\u522b\u89e3\u51b3\u7684\u4e09\u4e2a\u6709\u95ee\u9898\u7684\u5173\u8054\u7684\u8bc1\u636e\u6765\u8bc1\u660e T-HITL \u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5bf9\u5b66\u672f\u7684\u8d21\u732e\u6709\u4e24\u4e2a\u65b9\u9762\u3002\u901a\u8fc7\u5728\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u751f\u6210\u4eba\u5de5\u667a\u80fd\u7684\u80cc\u666f\u4e0b\u5b9a\u4e49\u6709\u95ee\u9898\u7684\u5173\u8054\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6982\u5ff5\u548c\u6280\u672f\u5206\u7c7b\u6cd5\u6765\u89e3\u51b3\u5176\u4e2d\u4e00\u4e9b\u5173\u8054\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u6cd5 T-HITL\uff0c\u5b83\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u5173\u8054\u5e76\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u6a21\u578b\u751f\u6210\u7684\u89c6\u89c9\u8d28\u91cf\u3002\u8fd9\u79cd\u7f13\u89e3\u4e0d\u4e00\u5b9a\u662f\u4e00\u79cd\u6743\u8861\uff0c\u800c\u662f\u4e00\u79cd\u589e\u5f3a\u3002|[2402.17101v1](http://arxiv.org/pdf/2402.17101v1)|null|\n"}}