{"\u751f\u6210\u6a21\u578b": {"2402.13152": "|**2024-02-20**|**AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies**|AnnoTheia\uff1a\u7528\u4e8e\u89c6\u542c\u8bed\u97f3\u6280\u672f\u7684\u534a\u81ea\u52a8\u6ce8\u91ca\u5de5\u5177\u5305|Jos\u00e9-M. Acosta-Triana, David Gimeno-G\u00f3mez, Carlos-D. Mart\u00ednez-Hinarejos|More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. The AnnoTheia toolkit, tutorials, and pre-trained models are available on GitHub.|\u4e16\u754c\u5404\u5730\u4f7f\u7528\u8d85\u8fc7 7,000 \u79cd\u5df2\u77e5\u8bed\u8a00\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6807\u6ce8\u8d44\u6e90\uff0c\u76ee\u524d\u8bed\u97f3\u6280\u672f\u53ea\u8986\u76d6\u4e86\u5176\u4e2d\u7684\u4e00\u5c0f\u90e8\u5206\u3002\u5c3d\u7ba1\u81ea\u6211\u76d1\u7763\u7684\u8bed\u97f3\u8868\u793a\u3001\u6700\u8fd1\u7684\u5927\u91cf\u8bed\u97f3\u8bed\u6599\u5e93\u6536\u96c6\u4ee5\u53ca\u6311\u6218\u7684\u7ec4\u7ec7\u5df2\u7ecf\u7f13\u89e3\u4e86\u8fd9\u79cd\u4e0d\u5e73\u7b49\uff0c\u4f46\u5927\u591a\u6570\u7814\u7a76\u4e3b\u8981\u4ee5\u82f1\u8bed\u4e3a\u57fa\u51c6\u3002\u5f53\u6d89\u53ca\u542c\u89c9\u548c\u89c6\u89c9\u8bed\u97f3\u6a21\u5f0f\u7684\u4efb\u52a1\u5f97\u5230\u89e3\u51b3\u65f6\uff0c\u8fd9\u79cd\u60c5\u51b5\u4f1a\u66f4\u52a0\u4e25\u91cd\u3002\u4e3a\u4e86\u4fc3\u8fdb\u89c6\u542c\u8bed\u97f3\u6280\u672f\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7814\u7a76\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 AnnoTheia\uff0c\u4e00\u4e2a\u534a\u81ea\u52a8\u6ce8\u91ca\u5de5\u5177\u5305\uff0c\u53ef\u4ee5\u68c0\u6d4b\u4e00\u4e2a\u4eba\u4f55\u65f6\u5728\u573a\u666f\u4e2d\u8bf4\u8bdd\u4ee5\u53ca\u76f8\u5e94\u7684\u8f6c\u5f55\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5c55\u793a\u4e3a\u611f\u5174\u8da3\u7684\u8bed\u8a00\u51c6\u5907 AnnoTheia \u7684\u5b8c\u6574\u8fc7\u7a0b\uff0c\u6211\u4eec\u8fd8\u63cf\u8ff0\u4e86\u5c06\u7528\u4e8e\u4e3b\u52a8\u8bf4\u8bdd\u8005\u68c0\u6d4b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8c03\u6574\u4e3a\u897f\u73ed\u7259\u8bed\uff0c\u4f7f\u7528\u7684\u6570\u636e\u5e93\u6700\u521d\u4e0d\u662f\u4e3a\u6b64\u7c7b\u4efb\u52a1\u8bbe\u60f3\u7684\u3002 AnnoTheia \u5de5\u5177\u5305\u3001\u6559\u7a0b\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728 GitHub \u4e0a\u83b7\u53d6\u3002|[2402.13152v1](http://arxiv.org/pdf/2402.13152v1)|**[link](https://github.com/joactr/annotheia)**|\n", "2402.13144": "|**2024-02-20**|**Neural Network Diffusion**|\u795e\u7ecf\u7f51\u7edc\u6269\u6563|Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, Yang You|Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \\textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results encourage more exploration on the versatile use of diffusion models.|\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u529f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8bc1\u660e\u6269\u6563\u6a21\u578b\u8fd8\u53ef\u4ee5 \\textit{\u751f\u6210\u9ad8\u6027\u80fd\u7684\u795e\u7ecf\u7f51\u7edc\u53c2\u6570}\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f88\u7b80\u5355\uff0c\u5229\u7528\u81ea\u52a8\u7f16\u7801\u5668\u548c\u6807\u51c6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002\u81ea\u52a8\u7f16\u7801\u5668\u63d0\u53d6\u7ecf\u8fc7\u8bad\u7ec3\u7684\u7f51\u7edc\u53c2\u6570\u5b50\u96c6\u7684\u6f5c\u5728\u8868\u793a\u3002\u7136\u540e\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4ee5\u4ece\u968f\u673a\u566a\u58f0\u4e2d\u5408\u6210\u8fd9\u4e9b\u6f5c\u5728\u53c2\u6570\u8868\u793a\u3002\u7136\u540e\uff0c\u5b83\u751f\u6210\u65b0\u7684\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u7f16\u7801\u5668\u7684\u89e3\u7801\u5668\u4f20\u9012\uff0c\u5176\u8f93\u51fa\u53ef\u7528\u4f5c\u7f51\u7edc\u53c2\u6570\u7684\u65b0\u5b50\u96c6\u3002\u5728\u5404\u79cd\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e2d\uff0c\u6211\u4eec\u7684\u6269\u6563\u8fc7\u7a0b\u59cb\u7ec8\u4ee5\u6700\u5c0f\u7684\u989d\u5916\u6210\u672c\u751f\u6210\u4e0e\u7ecf\u8fc7\u8bad\u7ec3\u7684\u7f51\u7edc\u76f8\u6bd4\u5177\u6709\u53ef\u6bd4\u8f83\u6216\u6539\u8fdb\u6027\u80fd\u7684\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u51ed\u7ecf\u9a8c\u53d1\u73b0\u751f\u6210\u7684\u6a21\u578b\u4e0e\u7ecf\u8fc7\u8bad\u7ec3\u7684\u7f51\u7edc\u7684\u8868\u73b0\u4e0d\u540c\u3002\u6211\u4eec\u7684\u7ed3\u679c\u9f13\u52b1\u5bf9\u6269\u6563\u6a21\u578b\u7684\u591a\u529f\u80fd\u4f7f\u7528\u8fdb\u884c\u66f4\u591a\u63a2\u7d22\u3002|[2402.13144v1](http://arxiv.org/pdf/2402.13144v1)|**[link](https://github.com/nus-hpc-ai-lab/neural-network-diffusion)**|\n", "2402.13126": "|**2024-02-20**|**VGMShield: Mitigating Misuse of Video Generative Models**|VGMShield\uff1a\u51cf\u5c11\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6ee5\u7528|Yan Pang, Yang Zhang, Tianhao Wang|With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \\textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \\textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that current models still cannot perfectly handle spatial-temporal relationships, and thus, we can accomplish detection and tracing with nearly perfect accuracy.   Furthermore, anticipating future generative model improvements, we propose a {\\it prevention} method that adds invisible perturbations to images to make the generated videos look unreal. Together with fake video detection and tracing, our multi-faceted set of solutions can effectively mitigate misuse of video generative models.|\u968f\u7740\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4eba\u4eec\u53ef\u4ee5\u65b9\u4fbf\u5730\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u6765\u521b\u5efa\u9002\u5408\u81ea\u5df1\u7279\u5b9a\u9700\u6c42\u7684\u89c6\u9891\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u4eba\u4eec\u4e5f\u8d8a\u6765\u8d8a\u62c5\u5fc3\u5b83\u4eec\u53ef\u80fd\u88ab\u6ee5\u7528\u6765\u5236\u9020\u548c\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 VGMShield\uff1a\u4e00\u7ec4\u4e09\u4e2a\u7b80\u5355\u4f46\u5f00\u521b\u6027\u7684\u7f13\u89e3\u63aa\u65bd\uff0c\u8d2f\u7a7f\u865a\u5047\u89c6\u9891\u751f\u6210\u7684\u751f\u547d\u5468\u671f\u3002\u6211\u4eec\u4ece\\textit{\u5047\u89c6\u9891\u68c0\u6d4b}\u5f00\u59cb\u5c1d\u8bd5\u4e86\u89e3\u751f\u6210\u7684\u89c6\u9891\u662f\u5426\u5b58\u5728\u552f\u4e00\u6027\u4ee5\u53ca\u6211\u4eec\u662f\u5426\u53ef\u4ee5\u5c06\u5b83\u4eec\u4e0e\u771f\u5b9e\u89c6\u9891\u533a\u5206\u5f00\u6765\uff1b\u7136\u540e\uff0c\u6211\u4eec\u7814\u7a76 \\textit{tracing} \u95ee\u9898\uff0c\u8be5\u95ee\u9898\u5c06\u5047\u89c6\u9891\u6620\u5c04\u56de\u751f\u6210\u5b83\u7684\u6a21\u578b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528\u4e13\u6ce8\u4e8e{\\it\u65f6\u7a7a\u52a8\u6001}\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\u6765\u8bc6\u522b\u89c6\u9891\u4e2d\u7684\u4e0d\u4e00\u81f4\u4e4b\u5904\u3002\u901a\u8fc7\u5bf9\u4e03\u4e2a\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u6a21\u578b\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u5f53\u524d\u6a21\u578b\u4ecd\u7136\u65e0\u6cd5\u5b8c\u7f8e\u5904\u7406\u65f6\u7a7a\u5173\u7cfb\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u4ee5\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u7cbe\u5ea6\u5b8c\u6210\u68c0\u6d4b\u548c\u8ffd\u8e2a\u3002\u6b64\u5916\uff0c\u9884\u6d4b\u672a\u6765\u751f\u6210\u6a21\u578b\u7684\u6539\u8fdb\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd{\\it Prevention}\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5411\u56fe\u50cf\u6dfb\u52a0\u4e0d\u53ef\u89c1\u7684\u6270\u52a8\uff0c\u4f7f\u751f\u6210\u7684\u89c6\u9891\u770b\u8d77\u6765\u4e0d\u771f\u5b9e\u3002\u4e0e\u865a\u5047\u89c6\u9891\u68c0\u6d4b\u548c\u8ddf\u8e2a\u76f8\u7ed3\u5408\uff0c\u6211\u4eec\u7684\u591a\u65b9\u9762\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6ee5\u7528\u3002|[2402.13126v1](http://arxiv.org/pdf/2402.13126v1)|null|\n", "2402.12974": "|**2024-02-20**|**Visual Style Prompting with Swapping Self-Attention**|\u901a\u8fc7\u4ea4\u6362\u81ea\u6211\u6ce8\u610f\u529b\u6765\u63d0\u793a\u89c6\u89c9\u98ce\u683c|Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, Youngjung Uh|In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation. Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, \\ours, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers. This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately. Our project page is available \\href{https://curryjung.github.io/VisualStylePrompt/}{here}.|\u5728\u4e0d\u65ad\u53d1\u5c55\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u9886\u57df\uff0c\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u5185\u5bb9\u521b\u5efa\u7684\u5f3a\u5927\u5de5\u5177\u3002\u5c3d\u7ba1\u5176\u80fd\u529b\u975e\u51e1\uff0c\u73b0\u6709\u6a21\u578b\u5728\u5b9e\u73b0\u98ce\u683c\u4e00\u81f4\u7684\u53d7\u63a7\u751f\u6210\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\uff0c\u6216\u8005\u7531\u4e8e\u5185\u5bb9\u6cc4\u6f0f\u800c\u5e38\u5e38\u65e0\u6cd5\u5145\u5206\u4f20\u8f93\u89c6\u89c9\u5143\u7d20\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5373\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u7279\u5b9a\u98ce\u683c\u5143\u7d20\u548c\u7ec6\u5fae\u5dee\u522b\u7684\u540c\u65f6\u751f\u6210\u5404\u79cd\u56fe\u50cf\u3002\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4fdd\u7559\u539f\u59cb\u7279\u5f81\u7684\u67e5\u8be2\uff0c\u540c\u65f6\u5c06\u952e\u548c\u503c\u4e0e\u540e\u671f\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u7684\u53c2\u8003\u7279\u5f81\u4ea4\u6362\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u89c6\u89c9\u98ce\u683c\u63d0\u793a\uff0c\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\uff0c\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u4fdd\u6301\u5fe0\u5b9e\u7684\u98ce\u683c\u3002\u901a\u8fc7\u5bf9\u5404\u79cd\u6837\u5f0f\u548c\u6587\u672c\u63d0\u793a\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u6700\u597d\u5730\u53cd\u6620\u4e86\u53c2\u8003\u6587\u732e\u7684\u98ce\u683c\uff0c\u5e76\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u6700\u51c6\u786e\u5730\u5339\u914d\u6587\u672c\u63d0\u793a\u3002\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\u4f4d\u4e8e \\href{https://curryjung.github.io/VisualStylePrompt/}{here}\u3002|[2402.12974v1](http://arxiv.org/pdf/2402.12974v1)|null|\n", "2402.12928": "|**2024-02-20**|**A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence**|\u6a21\u5f0f\u5206\u6790\u4e0e\u673a\u5668\u667a\u80fd\u6587\u732e\u7efc\u8ff0|Penghai Zhao, Xin Zhang, Ming-Ming Cheng, Jian Yang, Xiang Li|By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, the study presents comparative analyses of different reviews, unveiling the characteristics of publications across various fields, periods, and journals. The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in several aspects. Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews. This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews. Finally, this Analysis offers insights into the current challenges of literature reviews and envisions future directions for their development.|\u901a\u8fc7\u5de9\u56fa\u5206\u6563\u7684\u77e5\u8bc6\uff0c\u6587\u732e\u7efc\u8ff0\u63d0\u4f9b\u4e86\u5bf9\u6240\u7814\u7a76\u4e3b\u9898\u7684\u5168\u9762\u7406\u89e3\u3002\u7136\u800c\uff0c\u8fc7\u591a\u7684\u8bc4\u8bba\uff0c\u5c24\u5176\u662f\u5728\u84ec\u52c3\u53d1\u5c55\u7684\u6a21\u5f0f\u5206\u6790\u548c\u673a\u5668\u667a\u80fd\uff08PAMI\uff09\u9886\u57df\uff0c\u5f15\u8d77\u4e86\u7814\u7a76\u4eba\u5458\u548c\u8bc4\u8bba\u8005\u7684\u62c5\u5fe7\u3002\u9488\u5bf9\u8fd9\u4e9b\u62c5\u5fe7\uff0c\u672c\u5206\u6790\u65e8\u5728\u4ece\u4e0d\u540c\u89d2\u5ea6\u5bf9 PAMI \u9886\u57df\u7684\u8bc4\u8bba\u8fdb\u884c\u5168\u9762\u56de\u987e\u3002\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u732e\u8ba1\u91cf\u6307\u6807\u6765\u81ea\u52a8\u8bc4\u4f30\u6587\u732e\u8bc4\u8bba\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a RiPAMI \u7684\u5143\u6570\u636e\u6570\u636e\u5e93\u548c\u4e00\u4e2a\u4e3b\u9898\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u83b7\u53d6 PAMI \u8bc4\u8bba\u7684\u7edf\u8ba1\u7279\u5f81\u3002\u4e0e\u4f20\u7edf\u7684\u6587\u732e\u8ba1\u91cf\u6d4b\u91cf\u4e0d\u540c\uff0c\u6240\u63d0\u51fa\u7684\u6587\u7ae0\u7ea7\u6307\u6807\u63d0\u4f9b\u4e86\u5b9e\u65f6\u548c\u73b0\u573a\u5f52\u4e00\u5316\u7684\u8bc4\u8bba\u91cf\u5316\u8bc4\u4f30\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u7528\u6237\u5b9a\u4e49\u7684\u5173\u952e\u8bcd\u3002\u5176\u6b21\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\uff0c\u7814\u7a76\u5bf9\u4e0d\u540c\u8bc4\u8bba\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\uff0c\u63ed\u793a\u4e0d\u540c\u9886\u57df\u3001\u4e0d\u540c\u65f6\u671f\u3001\u4e0d\u540c\u671f\u520a\u7684\u51fa\u7248\u7269\u7279\u5f81\u3002\u65b0\u51fa\u73b0\u7684\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u6587\u732e\u8bc4\u8bba\u4e5f\u53d7\u5230\u4e86\u8bc4\u4f30\uff0c\u89c2\u5bdf\u5230\u7684\u5dee\u5f02\u8868\u660e\uff0c\u5927\u591a\u6570\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u8bc4\u8bba\u5728\u51e0\u4e2a\u65b9\u9762\u4ecd\u7136\u843d\u540e\u4e8e\u4eba\u7c7b\u64b0\u5199\u7684\u8bc4\u8bba\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u7b80\u8981\u63d0\u4f9b\u4e86\u5bf9\u4ee3\u8868\u6027 PAMI \u8bc4\u8bba\u7684\u4e3b\u89c2\u8bc4\u4ef7\uff0c\u5e76\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u8bba\u6587\u7ed3\u6784\u7684\u6587\u732e\u8bc4\u8bba\u7c7b\u578b\u5b66\u3002\u8fd9\u79cd\u7c7b\u578b\u53ef\u4ee5\u63d0\u9ad8\u5b66\u8005\u9605\u8bfb\u548c\u64b0\u5199\u8bc4\u8bba\u7684\u6e05\u6670\u5ea6\u548c\u6709\u6548\u6027\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u751f\u6210\u7ec4\u7ec7\u826f\u597d\u7684\u8bc4\u8bba\u7684\u6307\u5357\u3002\u6700\u540e\uff0c\u672c\u5206\u6790\u6df1\u5165\u63a2\u8ba8\u4e86\u6587\u732e\u7efc\u8ff0\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u5176\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u3002|[2402.12928v1](http://arxiv.org/pdf/2402.12928v1)|null|\n", "2402.12927": "|**2024-02-20**|**CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection**|\u6d88\u9664\u6b3a\u9a97\uff1a\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u901a\u7528 Deepfake \u68c0\u6d4b|Sohail Ahmed Khan, Duc-Tien Dang-Nguyen|The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools.|\u751f\u6210\u5bf9\u6297\u7f51\u7edc (GAN) \u7684\u6700\u65b0\u8fdb\u5c55\u548c\u6269\u6563\u6a21\u578b\u7684\u51fa\u73b0\u6781\u5927\u5730\u7b80\u5316\u4e86\u9ad8\u5ea6\u771f\u5b9e\u4e14\u53ef\u5e7f\u6cdb\u8bbf\u95ee\u7684\u5408\u6210\u5185\u5bb9\u7684\u5236\u4f5c\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u6709\u6548\u7684\u901a\u7528\u68c0\u6d4b\u673a\u5236\u6765\u51cf\u8f7b\u6df1\u5ea6\u9020\u5047\u5e26\u6765\u7684\u6f5c\u5728\u98ce\u9669\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u6700\u65b0\u7684\u901a\u7528\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u81ea\u9002\u5e94\u65b9\u6cd5\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002\u7ee7\u8be5\u9886\u57df\u4e4b\u524d\u7684\u7814\u7a76\u4e4b\u540e\uff0c\u6211\u4eec\u4ec5\u4f7f\u7528\u5355\u4e2a\u6570\u636e\u96c6 (ProGAN) \u6765\u8c03\u6574 CLIP \u4ee5\u8fdb\u884c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u3002\u7136\u800c\uff0c\u4e0e\u4e4b\u524d\u7684\u7814\u7a76\u4ec5\u4f9d\u8d56 CLIP \u7684\u89c6\u89c9\u90e8\u5206\u800c\u5ffd\u7565\u5176\u6587\u672c\u90e8\u5206\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\u4fdd\u7559\u6587\u672c\u90e8\u5206\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u91c7\u7528\u7684\u7b80\u5355\u4e14\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e Prompt Tuning \u7684\u81ea\u9002\u5e94\u7b56\u7565\u6bd4\u4e4b\u524d\u7684 SOTA \u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u9ad8\u4e86 5.01% mAP \u548c 6.61% \u51c6\u786e\u5ea6\uff0c\u540c\u65f6\u5229\u7528\u4e86\u4e0d\u5230\u4e09\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\uff08200k \u56fe\u50cf\u4e0e 720k \u56fe\u50cf\u76f8\u6bd4\uff09\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u7684\u5b9e\u9645\u9002\u7528\u6027\uff0c\u6211\u4eec\u5bf9\u5404\u79cd\u573a\u666f\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002\u8fd9\u6d89\u53ca\u5bf9\u6765\u81ea 21 \u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u7684\u56fe\u50cf\u8fdb\u884c\u4e25\u683c\u6d4b\u8bd5\uff0c\u5305\u62ec\u7531\u57fa\u4e8e GAN\u3001\u57fa\u4e8e\u6269\u6563\u548c\u5546\u4e1a\u5de5\u5177\u751f\u6210\u7684\u56fe\u50cf\u3002|[2402.12927v1](http://arxiv.org/pdf/2402.12927v1)|null|\n", "2402.12908": "|**2024-02-20**|**RealCompo: Dynamic Equilibrium between Realism and Compositionality Improves Text-to-Image Diffusion Models**|RealCompo\uff1a\u73b0\u5b9e\u4e3b\u4e49\u548c\u7ec4\u5408\u6027\u4e4b\u95f4\u7684\u52a8\u6001\u5e73\u8861\u6539\u8fdb\u4e86\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b|Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui|Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose a new training-free and transferred-friendly text-to-image generation framework, namely RealCompo, which aims to leverage the advantages of text-to-image and layout-to-image models to enhance both realism and compositionality of the generated images. An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Code is available at https://github.com/YangLing0818/RealCompo|\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u73b0\u6709\u6a21\u578b\u5728\u9762\u5bf9\u591a\u5bf9\u8c61\u7ec4\u5408\u751f\u6210\u65f6\u4ecd\u7136\u5b58\u5728\u8bb8\u591a\u56f0\u96be\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u514d\u8bad\u7ec3\u4e14\u8fc1\u79fb\u53cb\u597d\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u5373 RealCompo\uff0c\u65e8\u5728\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u548c\u5e03\u5c40\u5230\u56fe\u50cf\u6a21\u578b\u7684\u4f18\u52bf\u6765\u589e\u5f3a\u771f\u5b9e\u611f\u548c\u751f\u6210\u56fe\u50cf\u7684\u7ec4\u5408\u6027\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u89c2\u4e14\u65b0\u9896\u7684\u5e73\u8861\u5668\u6765\u52a8\u6001\u5e73\u8861\u4e24\u4e2a\u6a21\u578b\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u4f18\u52bf\uff0c\u5141\u8bb8\u5373\u63d2\u5373\u7528\u4f7f\u7528\u4efb\u4f55\u6a21\u578b\u800c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 RealCompo \u5728\u591a\u5bf9\u8c61\u5408\u6210\u751f\u6210\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u548c\u5e03\u5c40\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u56fe\u50cf\u7684\u4ee4\u4eba\u6ee1\u610f\u7684\u771f\u5b9e\u6027\u548c\u5408\u6210\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/YangLing0818/RealCompo \u83b7\u53d6|[2402.12908v1](http://arxiv.org/pdf/2402.12908v1)|**[link](https://github.com/yangling0818/realcompo)**|\n", "2402.12797": "|**2024-02-20**|**A Geometric Algorithm for Tubular Shape Reconstruction from Skeletal Representation**|\u4e00\u79cd\u4ece\u9aa8\u9abc\u8868\u793a\u91cd\u5efa\u7ba1\u72b6\u5f62\u72b6\u7684\u51e0\u4f55\u7b97\u6cd5|Guoqing Zhang, Songzi Cat, Juzi Cat|We introduce a novel approach for the reconstruction of tubular shapes from skeletal representations. Our method processes all skeletal points as a whole, eliminating the need for splitting input structure into multiple segments. We represent the tubular shape as a truncated signed distance function (TSDF) in a voxel hashing manner, in which the signed distance between a voxel center and the object is computed through a simple geometric algorithm. Our method does not involve any surface sampling scheme or solving large matrix equations, and therefore is a faster and more elegant solution for tubular shape reconstruction compared to other approaches. Experiments demonstrate the efficiency and effectiveness of the proposed method. Code is avaliable at https://github.com/wlsdzyzl/Dragon.|\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4ece\u9aa8\u9abc\u8868\u793a\u91cd\u5efa\u7ba1\u72b6\u5f62\u72b6\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u6240\u6709\u9aa8\u67b6\u70b9\u4f5c\u4e3a\u4e00\u4e2a\u6574\u4f53\u8fdb\u884c\u5904\u7406\uff0c\u65e0\u9700\u5c06\u8f93\u5165\u7ed3\u6784\u62c6\u5206\u4e3a\u591a\u4e2a\u7247\u6bb5\u3002\u6211\u4eec\u4ee5\u4f53\u7d20\u6563\u5217\u65b9\u5f0f\u5c06\u7ba1\u72b6\u5f62\u72b6\u8868\u793a\u4e3a\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff08TSDF\uff09\uff0c\u5176\u4e2d\u4f53\u7d20\u4e2d\u5fc3\u4e0e\u7269\u4f53\u4e4b\u95f4\u7684\u7b26\u53f7\u8ddd\u79bb\u662f\u901a\u8fc7\u7b80\u5355\u7684\u51e0\u4f55\u7b97\u6cd5\u8ba1\u7b97\u7684\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u6d89\u53ca\u4efb\u4f55\u8868\u9762\u91c7\u6837\u65b9\u6848\u6216\u6c42\u89e3\u5927\u578b\u77e9\u9635\u65b9\u7a0b\uff0c\u56e0\u6b64\u4e0e\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8fd9\u662f\u4e00\u79cd\u66f4\u5feb\u3001\u66f4\u4f18\u96c5\u7684\u7ba1\u72b6\u5f62\u72b6\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/wlsdzyzl/Dragon \u83b7\u53d6\u3002|[2402.12797v1](http://arxiv.org/pdf/2402.12797v1)|**[link](https://github.com/wlsdzyzl/dragon)**|\n", "2402.12779": "|**2024-02-20**|**Two-stage Rainfall-Forecasting Diffusion Model**|\u4e24\u9636\u6bb5\u964d\u96e8\u91cf\u9884\u62a5\u6269\u6563\u6a21\u578b|XuDong Ling, ChaoRong Li, FengQing Qin, LiHong Zhu, Yuanyuan Huang|Deep neural networks have made great achievements in rainfall prediction.However, the current forecasting methods have certain limitations, such as with blurry generated images and incorrect spatial positions. To overcome these challenges, we propose a Two-stage Rainfall-Forecasting Diffusion Model (TRDM) aimed at improving the accuracy of long-term rainfall forecasts and addressing the imbalance in performance between temporal and spatial modeling. TRDM is a two-stage method for rainfall prediction tasks. The task of the first stage is to capture robust temporal information while preserving spatial information under low-resolution conditions. The task of the second stage is to reconstruct the low-resolution images generated in the first stage into high-resolution images. We demonstrate state-of-the-art results on the MRMS and Swedish radar datasets. Our project is open source and available on GitHub at: \\href{https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}.|\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u964d\u96e8\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u6210\u5c31\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e00\u5b9a\u7684\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u751f\u6210\u7684\u56fe\u50cf\u6a21\u7cca\u548c\u7a7a\u95f4\u4f4d\u7f6e\u4e0d\u6b63\u786e\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u964d\u96e8\u9884\u62a5\u6269\u6563\u6a21\u578b\uff08TRDM\uff09\uff0c\u65e8\u5728\u63d0\u9ad8\u957f\u671f\u964d\u96e8\u9884\u62a5\u7684\u51c6\u786e\u6027\u5e76\u89e3\u51b3\u65f6\u7a7a\u5efa\u6a21\u4e4b\u95f4\u7684\u6027\u80fd\u4e0d\u5e73\u8861\u95ee\u9898\u3002 TRDM \u662f\u4e00\u79cd\u7528\u4e8e\u964d\u96e8\u9884\u6d4b\u4efb\u52a1\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u3002\u7b2c\u4e00\u9636\u6bb5\u7684\u4efb\u52a1\u662f\u6355\u83b7\u9c81\u68d2\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u540c\u65f6\u5728\u4f4e\u5206\u8fa8\u7387\u6761\u4ef6\u4e0b\u4fdd\u7559\u7a7a\u95f4\u4fe1\u606f\u3002\u7b2c\u4e8c\u9636\u6bb5\u7684\u4efb\u52a1\u662f\u5c06\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7684\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u91cd\u5efa\u4e3a\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002\u6211\u4eec\u5728 MRMS \u548c\u745e\u5178\u96f7\u8fbe\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6211\u4eec\u7684\u9879\u76ee\u662f\u5f00\u6e90\u7684\uff0c\u53ef\u5728 GitHub \u4e0a\u83b7\u53d6\uff1a\\href{https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}\u3002|[2402.12779v1](http://arxiv.org/pdf/2402.12779v1)|**[link](https://github.com/clearlyzerolxd/trdm)**|\n", "2402.12741": "|**2024-02-20**|**MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion**|MuLan\uff1a\u7528\u4e8e\u6e10\u8fdb\u5f0f\u591a\u5bf9\u8c61\u6269\u6563\u7684\u591a\u6a21\u6001 LLM \u4ee3\u7406|Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, Tianyi Zhou|Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Multimodal-LLM agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on https://github.com/measure-infinity/mulan-code.|\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4ecd\u7136\u96be\u4ee5\u751f\u6210\u591a\u4e2a\u5bf9\u8c61\u7684\u56fe\u50cf\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5b83\u4eec\u7684\u7a7a\u95f4\u4f4d\u7f6e\u3001\u76f8\u5bf9\u5927\u5c0f\u3001\u91cd\u53e0\u548c\u5c5e\u6027\u7ed1\u5b9a\u65b9\u9762\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u514d\u8bad\u7ec3\u7684 Multimodal-LLM \u4ee3\u7406\uff08MuLan\uff09\uff0c\u901a\u8fc7\u5177\u6709\u89c4\u5212\u548c\u53cd\u9988\u63a7\u5236\u7684\u6e10\u8fdb\u5f0f\u591a\u5bf9\u8c61\u751f\u6210\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u5c31\u50cf\u4eba\u7c7b\u753b\u5bb6\u4e00\u6837\u3002 MuLan \u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u5b50\u4efb\u52a1\u4ec5\u751f\u6210\u4e00\u4e2a\u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u901a\u8fc7\u7a33\u5b9a\u6269\u6563\u4ee5\u5148\u524d\u751f\u6210\u7684\u5bf9\u8c61\u4e3a\u6761\u4ef6\u3002\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e LLM \u7684\u65b9\u6cd5\u4e0d\u540c\uff0cMuLan \u4ec5\u5728\u5f00\u59cb\u65f6\u751f\u6210\u4e00\u4e2a\u9ad8\u7ea7\u8ba1\u5212\uff0c\u800c\u6bcf\u4e2a\u5bf9\u8c61\u7684\u786e\u5207\u5927\u5c0f\u548c\u4f4d\u7f6e\u7531 LLM \u548c\u6bcf\u4e2a\u5b50\u4efb\u52a1\u7684\u6ce8\u610f\u529b\u6307\u5bfc\u786e\u5b9a\u3002\u6b64\u5916\uff0c\u6728\u5170\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e3a\u6bcf\u4e2a\u5b50\u4efb\u52a1\u4e2d\u751f\u6210\u7684\u56fe\u50cf\u63d0\u4f9b\u53cd\u9988\uff0c\u5e76\u5728\u8fdd\u53cd\u539f\u59cb\u63d0\u793a\u65f6\u63a7\u5236\u6269\u6563\u6a21\u578b\u91cd\u65b0\u751f\u6210\u56fe\u50cf\u3002\u56e0\u6b64\uff0c\u6728\u5170\u6bcf\u4e00\u6b65\u4e2d\u7684\u6bcf\u4e2a\u6a21\u578b\u53ea\u9700\u8981\u89e3\u51b3\u5b83\u4e13\u95e8\u5904\u7406\u7684\u4e00\u4e2a\u7b80\u5355\u7684\u5b50\u4efb\u52a1\u3002\u6211\u4eec\u4ece\u4e0d\u540c\u7684\u57fa\u51c6\u4e2d\u6536\u96c6\u4e86 200 \u4e2a\u5305\u542b\u5177\u6709\u7a7a\u95f4\u5173\u7cfb\u548c\u5c5e\u6027\u7ed1\u5b9a\u7684\u591a\u5bf9\u8c61\u7684\u63d0\u793a\u6765\u8bc4\u4f30 MuLan\u3002\u7ed3\u679c\u8bc1\u660e\u4e86 MuLan \u5728\u751f\u6210\u591a\u4e2a\u5bf9\u8c61\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/measure-infinity/mulan-code \u4e0a\u83b7\u53d6\u3002|[2402.12741v1](http://arxiv.org/pdf/2402.12741v1)|**[link](https://github.com/measure-infinity/mulan-code)**|\n", "2402.12712": "|**2024-02-20**|**MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction**|MVDiffusion++\uff1a\u7528\u4e8e\u5355\u89c6\u56fe\u6216\u7a00\u758f\u89c6\u56fe 3D \u5bf9\u8c61\u91cd\u5efa\u7684\u5bc6\u96c6\u9ad8\u5206\u8fa8\u7387\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b|Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, Rakesh Ranjan|This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A ``pose-free architecture'' where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A ``view dropout strategy'' that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a text-to-image generative model.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e 3D \u5bf9\u8c61\u91cd\u5efa\u7684\u795e\u7ecf\u67b6\u6784 MVDiffusion++\uff0c\u5728\u7ed9\u5b9a\u4e00\u5f20\u6216\u51e0\u5f20\u6ca1\u6709\u76f8\u673a\u59ff\u52bf\u7684\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u5408\u6210\u5bf9\u8c61\u7684\u5bc6\u96c6\u4e14\u9ad8\u5206\u8fa8\u7387\u7684\u89c6\u56fe\u3002 MVDiffusion++ \u901a\u8fc7\u4e24\u4e2a\u4ee4\u4eba\u60ca\u8bb6\u7684\u7b80\u5355\u60f3\u6cd5\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff1a1\uff09\u201c\u65e0\u59ff\u52bf\u67b6\u6784\u201d\uff0c\u5176\u4e2d 2D \u6f5c\u5728\u7279\u5f81\u4e4b\u95f4\u7684\u6807\u51c6\u81ea\u6ce8\u610f\u529b\u5b66\u4e60\u8de8\u4efb\u610f\u6570\u91cf\u7684\u6761\u4ef6\u548c\u751f\u6210\u89c6\u56fe\u7684 3D \u4e00\u81f4\u6027\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u4f7f\u7528\u76f8\u673a\u59ff\u52bf\u4fe1\u606f; 2\uff09\u201c\u89c6\u56fe\u4e22\u5f03\u7b56\u7565\u201d\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u4e22\u5f03\u5927\u91cf\u8f93\u51fa\u89c6\u56fe\uff0c\u4ece\u800c\u51cf\u5c11\u8bad\u7ec3\u65f6\u7684\u5185\u5b58\u5360\u7528\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u5b9e\u73b0\u5bc6\u96c6\u4e14\u9ad8\u5206\u8fa8\u7387\u7684\u89c6\u56fe\u5408\u6210\u3002\u6211\u4eec\u4f7f\u7528 Objaverse \u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u7528 Google Scanned Objects \u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u7684\u65b0\u9896\u89c6\u56fe\u5408\u6210\u548c 3D \u91cd\u5efa\u6307\u6807\uff0c\u5176\u4e2d MVDiffusion++ \u663e\u7740\u4f18\u4e8e\u5f53\u524d\u7684\u6280\u672f\u6c34\u5e73\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5c06 MVDiffusion++ \u4e0e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u76f8\u7ed3\u5408\u6765\u6f14\u793a\u6587\u672c\u5230 3D \u5e94\u7528\u793a\u4f8b\u3002|[2402.12712v1](http://arxiv.org/pdf/2402.12712v1)|null|\n", "2402.12647": "|**2024-02-20**|**DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation**|DiffusionNOCS\uff1a\u7ba1\u7406 Sim2Real \u591a\u6a21\u6001\u7c7b\u522b\u7ea7\u59ff\u52bf\u4f30\u8ba1\u4e2d\u7684\u5bf9\u79f0\u6027\u548c\u4e0d\u786e\u5b9a\u6027|Takuya Ikeda, Sergey Zakharov, Tianyi Ko, Muhammad Zubair Irshad, Robert Lee, Katherine Liu, Rares Ambrus, Koichi Nishiwaki|This paper addresses the challenging problem of category-level pose estimation. Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training. In this work, we address these challenges by proposing a probabilistic model that relies on diffusion to estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation. Furthermore, we introduce critical components to enhance performance by leveraging the strength of the diffusion models with multi-modal input representations. We demonstrate the effectiveness of our method by testing it on a range of real datasets. Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain.|\u672c\u6587\u89e3\u51b3\u4e86\u7c7b\u522b\u7ea7\u59ff\u6001\u4f30\u8ba1\u7684\u6311\u6218\u6027\u95ee\u9898\u3002\u5f53\u524d\u7528\u4e8e\u6b64\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u5904\u7406\u5bf9\u79f0\u5bf9\u8c61\u4ee5\u53ca\u5c1d\u8bd5\u4ec5\u901a\u8fc7\u5408\u6210\u6570\u636e\u8bad\u7ec3\u63a8\u5e7f\u5230\u65b0\u73af\u5883\u65f6\u9762\u4e34\u6311\u6218\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u6982\u7387\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u8be5\u6a21\u578b\u4f9d\u8d56\u4e8e\u6269\u6563\u6765\u4f30\u8ba1\u5bf9\u4e8e\u6062\u590d\u90e8\u5206\u5bf9\u8c61\u5f62\u72b6\u4ee5\u53ca\u5efa\u7acb\u59ff\u6001\u4f30\u8ba1\u6240\u5fc5\u9700\u7684\u5bf9\u5e94\u5173\u7cfb\u81f3\u5173\u91cd\u8981\u7684\u5bc6\u96c6\u89c4\u8303\u56fe\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5173\u952e\u7ec4\u4ef6\uff0c\u901a\u8fc7\u5229\u7528\u5177\u6709\u591a\u6a21\u6001\u8f93\u5165\u8868\u793a\u7684\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5ea6\u6765\u63d0\u9ad8\u6027\u80fd\u3002\u6211\u4eec\u901a\u8fc7\u5728\u4e00\u7cfb\u5217\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u6765\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5c3d\u7ba1\u4ec5\u6839\u636e\u6211\u4eec\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u524d\u6240\u672a\u6709\u7684\u6cdb\u5316\u8d28\u91cf\uff0c\u8d85\u8d8a\u4e86\u57fa\u7ebf\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u90a3\u4e9b\u5728\u76ee\u6807\u9886\u57df\u4e13\u95e8\u8bad\u7ec3\u8fc7\u7684\u57fa\u7ebf\u3002|[2402.12647v1](http://arxiv.org/pdf/2402.12647v1)|null|\n"}, "\u591a\u6a21\u6001": {"2402.13254": "|**2024-02-20**|**CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples**|CounterCurate\uff1a\u901a\u8fc7\u53cd\u4e8b\u5b9e\u793a\u4f8b\u589e\u5f3a\u7269\u7406\u548c\u8bed\u4e49\u89c6\u89c9\u8bed\u8a00\u7ec4\u5408\u63a8\u7406|Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee|We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V.|\u6211\u4eec\u63d0\u51fa\u4e86 CounterCurate\uff0c\u4e00\u4e2a\u5168\u9762\u63d0\u9ad8\u5bf9\u6bd4\u548c\u751f\u6210\u591a\u6a21\u6001\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u4e24\u4e2a\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u5173\u952e\u95ee\u9898\uff1a\u5ffd\u89c6\u7269\u7406\u63a8\u7406\uff08\u8ba1\u6570\u548c\u4f4d\u7f6e\u7406\u89e3\uff09\u4ee5\u53ca\u4f7f\u7528\u9ad8\u6027\u80fd\u6587\u672c\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u53cd\u4e8b\u5b9e\u5fae\u8c03\u7684\u6f5c\u529b\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5f00\u521b\u4e86\u4e00\u79cd\u89e3\u51b3\u8fd9\u4e9b\u5dee\u8ddd\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u9996\u5148\u5173\u6ce8 CLIP \u548c LLaVA \u7b49\u591a\u6a21\u6001\u6a21\u578b\u5728\u57fa\u4e8e\u7269\u7406\u7684\u7ec4\u5408\u63a8\u7406\u4e2d\u7684\u8fd1\u673a\u6027\u80fd\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u57fa\u7840\u56fe\u50cf\u751f\u6210\u6a21\u578b GLIGEN \u5e94\u7528\u7b80\u5355\u7684\u6570\u636e\u589e\u5f3a\u6765\u751f\u6210\u5fae\u8c03\u6570\u636e\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u6027\u80fd\uff1a\u5728\u6211\u4eec\u65b0\u7b56\u5212\u7684 Flickr30k-Positions \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLIP \u548c LLaVA \u5206\u522b +33% \u548c +37%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u9ad8\u6027\u80fd\u6587\u672c\u751f\u6210\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08\u7279\u522b\u662f GPT-4V \u548c DALLE-3\uff09\u7684\u529f\u80fd\u6765\u7b56\u5212\u5177\u6709\u6311\u6218\u6027\u7684\u8bed\u4e49\u53cd\u4e8b\u5b9e\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u589e\u5f3a\u5728 SugarCrepe \u7b49\u57fa\u51c6\u4e0a\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0c\u5176\u4e2d CounterCurate \u4f18\u4e8e GPT-4V \u3002|[2402.13254v1](http://arxiv.org/pdf/2402.13254v1)|null|\n", "2402.13232": "|**2024-02-20**|**A Touch, Vision, and Language Dataset for Multimodal Alignment**|\u7528\u4e8e\u591a\u6a21\u5f0f\u5bf9\u9f50\u7684\u89e6\u6478\u3001\u89c6\u89c9\u548c\u8bed\u8a00\u6570\u636e\u96c6|Letian Fu, Gaurav Datta, Huang Huang, William Chung-Ho Panitch, Jaimyn Drake, Joseph Ortiz, Mustafa Mukadam, Mike Lambeta, Roberto Calandra, Ken Goldberg|Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild vision-touch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-vision-language (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) touch-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human-labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark. Code and data: https://tactile-vlm.github.io.|\u89e6\u6478\u662f\u4eba\u7c7b\u91cd\u8981\u7684\u611f\u77e5\u65b9\u5f0f\uff0c\u4f46\u5c1a\u672a\u88ab\u7eb3\u5165\u591a\u6a21\u6001\u751f\u6210\u8bed\u8a00\u6a21\u578b\u4e2d\u3002\u8fd9\u90e8\u5206\u662f\u7531\u4e8e\u83b7\u5f97\u89e6\u89c9\u6570\u636e\u7684\u81ea\u7136\u8bed\u8a00\u6807\u7b7e\u7684\u56f0\u96be\u4ee5\u53ca\u5c06\u89e6\u89c9\u8bfb\u6570\u4e0e\u89c6\u89c9\u89c2\u5bdf\u548c\u8bed\u8a00\u63cf\u8ff0\u5bf9\u9f50\u7684\u590d\u6742\u6027\u3002\u4f5c\u4e3a\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u7684\u4e00\u6b65\uff0c\u8fd9\u9879\u5de5\u4f5c\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b 44K \u4e2a\u91ce\u5916\u89c6\u89c9-\u89e6\u6478\u5bf9\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u7531\u4eba\u7c7b\u6ce8\u91ca\u7684\u82f1\u8bed\u6807\u7b7e (10%) \u548c\u6765\u81ea GPT-4V \u7684\u6587\u672c\u4f2a\u6807\u7b7e (90%) \u3002\u6211\u4eec\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u7684\u89e6\u89c9\u7f16\u7801\u5668\u4ee5\u8fdb\u884c\u5f00\u653e\u8bcd\u6c47\u5206\u7c7b\uff0c\u5e76\u8bad\u7ec3\u89e6\u6478\u89c6\u89c9\u8bed\u8a00\uff08TVL\uff09\u6a21\u578b\u4ee5\u4f7f\u7528\u7ecf\u8fc7\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u751f\u6210\u6587\u672c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u7ed3\u5408\u89e6\u6478\uff0cTVL \u6a21\u578b\u6bd4\u5728\u4efb\u4f55\u4e00\u5bf9\u6a21\u6001\u4e0a\u8bad\u7ec3\u7684\u73b0\u6709\u6a21\u578b\u63d0\u9ad8\u4e86\uff08+29% \u5206\u7c7b\u51c6\u786e\u7387\uff09\u89e6\u6478-\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u3002\u5c3d\u7ba1\u6570\u636e\u96c6\u53ea\u6709\u4e00\u5c0f\u90e8\u5206\u662f\u4eba\u5de5\u6807\u8bb0\u7684\uff0c\u4f46 TVL \u6a21\u578b\u5728\u65b0\u7684\u89e6\u6478\u89c6\u89c9\u4e0a\u8868\u73b0\u51fa\u6bd4 GPT-4V (+12%) \u548c\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (+32%) \u66f4\u597d\u7684\u89c6\u89c9\u89e6\u89c9\u7406\u89e3\u4e86\u89e3\u57fa\u51c6\u3002\u4ee3\u7801\u548c\u6570\u636e\uff1ahttps://tactile-vlm.github.io\u3002|[2402.13232v1](http://arxiv.org/pdf/2402.13232v1)|null|\n", "2402.13220": "|**2024-02-20**|**How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts**|\u6b3a\u9a97\u4f60\u7684\u591a\u5f0f\u8054\u8fd0\u6cd5\u5b66\u7855\u58eb\u6709\u591a\u5bb9\u6613\uff1f\u6b3a\u9a97\u6027\u63d0\u793a\u7684\u5b9e\u8bc1\u5206\u6790|Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan|The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 5% to 35%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts.|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u663e\u7740\u8fdb\u6b65\u5e76\u6ca1\u6709\u4f7f\u5b83\u4eec\u514d\u53d7\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u63d0\u793a\u4e2d\u7684\u6b3a\u9a97\u6027\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u800c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u4ea7\u751f\u5e7b\u89c9\u53cd\u5e94\u3002\u4e3a\u4e86\u5b9a\u91cf\u8bc4\u4f30\u6b64\u6f0f\u6d1e\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 MAD-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b 850 \u4e2a\u6d4b\u8bd5\u6837\u672c\uff0c\u5206\u4e3a 6 \u4e2a\u7c7b\u522b\uff0c\u4f8b\u5982\u4e0d\u5b58\u5728\u7684\u5bf9\u8c61\u3001\u5bf9\u8c61\u8ba1\u6570\u3001\u7a7a\u95f4\u5173\u7cfb\u548c\u89c6\u89c9\u6df7\u4e71\u3002\u6211\u4eec\u63d0\u4f9b\u5bf9\u6d41\u884c MLLM \u7684\u5168\u9762\u5206\u6790\uff0c\u8303\u56f4\u4ece GPT-4V\u3001Gemini-Pro \u5230\u5f00\u6e90\u6a21\u578b\uff0c\u4f8b\u5982 LLaVA-1.5 \u548c CogVLM\u3002\u6839\u636e\u7ecf\u9a8c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230 GPT-4V \u4e0e\u5176\u4ed6\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u7740\u7684\u6027\u80fd\u5dee\u8ddd\uff1b\u4e4b\u524d\u5f3a\u5927\u7684\u6307\u4ee4\u8c03\u6574\u6a21\u578b\uff0c\u4f8b\u5982 LRV-Instruction \u548c LLaVA-RLHF\uff0c\u5728\u8fd9\u4e2a\u65b0\u57fa\u51c6\u4e0a\u65e0\u6548\u3002\u867d\u7136 GPT-4V \u5728 MAD-Bench \u4e0a\u8fbe\u5230\u4e86 75.02% \u7684\u51c6\u786e\u7387\uff0c\u4f46\u6211\u4eec\u5b9e\u9a8c\u4e2d\u4efb\u4f55\u5176\u4ed6\u6a21\u578b\u7684\u51c6\u786e\u7387\u5728 5% \u5230 35% \u4e4b\u95f4\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u8865\u6551\u63aa\u65bd\uff0c\u5728\u6b3a\u9a97\u6027\u63d0\u793a\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u989d\u5916\u7684\u6bb5\u843d\uff0c\u4ee5\u9f13\u52b1\u6a21\u578b\u5728\u56de\u7b54\u95ee\u9898\u4e4b\u524d\u4e09\u601d\u800c\u540e\u884c\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u8fd9\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u751a\u81f3\u53ef\u4ee5\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e00\u500d\uff1b\u7136\u800c\uff0c\u7edd\u5bf9\u6570\u5b57\u4ecd\u7136\u592a\u4f4e\uff0c\u65e0\u6cd5\u4ee4\u4eba\u6ee1\u610f\u3002\u6211\u4eec\u5e0c\u671b MAD-Bench \u80fd\u591f\u4f5c\u4e3a\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6765\u523a\u6fc0\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u62b5\u5fa1\u6b3a\u9a97\u6027\u63d0\u793a\u7684\u80fd\u529b\u3002|[2402.13220v1](http://arxiv.org/pdf/2402.13220v1)|null|\n", "2402.13146": "|**2024-02-20**|**OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog**|OLViT\uff1a\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5d4c\u5165\u8fdb\u884c\u89c6\u9891\u5bf9\u8bdd\u7684\u591a\u6a21\u6001\u72b6\u6001\u8ddf\u8e2a|Adnen Abdessaied, Manuel von Hochmeister, Andreas Bulling|We present the Object Language Video Transformer (OLViT) - a novel model for video dialog operating over a multi-modal attention-based dialog state tracker. Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal reasoning, and accurate object tracking across multiple dialog turns. OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns. In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous multi-modal dialog state representations of the most relevant objects and rounds. As a result, they can be seamlessly integrated into Large Language Models (LLMs) and offer high flexibility in dealing with different datasets and tasks. Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets.|\u6211\u4eec\u63d0\u51fa\u4e86\u5bf9\u8c61\u8bed\u8a00\u89c6\u9891\u8f6c\u6362\u5668\uff08OLViT\uff09\u2014\u2014\u4e00\u79cd\u5728\u57fa\u4e8e\u591a\u6a21\u5f0f\u6ce8\u610f\u7684\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u5668\u4e0a\u8fd0\u884c\u7684\u89c6\u9891\u5bf9\u8bdd\u7684\u65b0\u9896\u6a21\u578b\u3002\u73b0\u6709\u7684\u89c6\u9891\u5bf9\u8bdd\u6a21\u578b\u96be\u4ee5\u89e3\u51b3\u9700\u8981\u89c6\u9891\u5185\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u5b9a\u4f4d\u3001\u957f\u671f\u65f6\u95f4\u63a8\u7406\u4ee5\u53ca\u8de8\u591a\u4e2a\u5bf9\u8bdd\u56de\u5408\u7684\u51c6\u786e\u5bf9\u8c61\u8ddf\u8e2a\u7684\u95ee\u9898\u3002 OLViT \u901a\u8fc7\u6839\u636e\u5bf9\u8c61\u72b6\u6001\u8ddf\u8e2a\u5668 (OST) \u548c\u8bed\u8a00\u72b6\u6001\u8ddf\u8e2a\u5668 (LST) \u7684\u8f93\u51fa\u7ef4\u62a4\u5168\u5c40\u5bf9\u8bdd\u72b6\u6001\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff1aOST \u5173\u6ce8\u89c6\u9891\u4e2d\u6700\u91cd\u8981\u7684\u5bf9\u8c61\uff0c\u800c LST \u5219\u8ddf\u8e2a\u89c6\u9891\u4e2d\u6700\u91cd\u8981\u7684\u5bf9\u8c61\u3002\u5bf9\u5148\u524d\u5bf9\u8bdd\u8f6e\u6b21\u6700\u91cd\u8981\u7684\u8bed\u8a00\u5171\u540c\u5f15\u7528\u3002\u4e0e\u4ee5\u524d\u7684\u5de5\u4f5c\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u901a\u7528\u7684\uff0c\u56e0\u6b64\u80fd\u591f\u5b66\u4e60\u6700\u76f8\u5173\u7684\u5bf9\u8c61\u548c\u56de\u5408\u7684\u8fde\u7eed\u591a\u6a21\u5f0f\u5bf9\u8bdd\u72b6\u6001\u8868\u793a\u3002\u56e0\u6b64\uff0c\u5b83\u4eec\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\uff0c\u5e76\u5728\u5904\u7406\u4e0d\u540c\u6570\u636e\u96c6\u548c\u4efb\u52a1\u65f6\u63d0\u4f9b\u9ad8\u5ea6\u7075\u6d3b\u6027\u3002\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684 DVD\uff08\u54cd\u5e94\u5206\u7c7b\uff09\u548c SIMMC 2.1\uff08\u54cd\u5e94\u751f\u6210\uff09\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u8868\u660e\uff0cOLViT \u5728\u8fd9\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2402.13146v1](http://arxiv.org/pdf/2402.13146v1)|null|\n", "2402.12846": "|**2024-02-20**|**ConVQG: Contrastive Visual Question Generation with Multimodal Guidance**|ConVQG\uff1a\u5229\u7528\u591a\u6a21\u6001\u6307\u5bfc\u751f\u6210\u5bf9\u6bd4\u89c6\u89c9\u95ee\u9898|Li Mi, Syrielle Montariol, Javiera Castillo-Navarro, Xianjie Dai, Antoine Bosselut, Devis Tuia|Asking questions about visual environments is a crucial way for intelligent agents to understand rich multi-faceted scenes, raising the importance of Visual Question Generation (VQG) systems. Apart from being grounded to the image, existing VQG systems can use textual constraints, such as expected answers or knowledge triplets, to generate focused questions. These constraints allow VQG systems to specify the question content or leverage external commonsense knowledge that can not be obtained from the image content only. However, generating focused questions using textual constraints while enforcing a high relevance to the image content remains a challenge, as VQG systems often ignore one or both forms of grounding. In this work, we propose Contrastive Visual Question Generation (ConVQG), a method using a dual contrastive objective to discriminate questions generated using both modalities from those based on a single one. Experiments on both knowledge-aware and standard VQG benchmarks demonstrate that ConVQG outperforms the state-of-the-art methods and generates image-grounded, text-guided, and knowledge-rich questions. Our human evaluation results also show preference for ConVQG questions compared to non-contrastive baselines.|\u63d0\u51fa\u6709\u5173\u89c6\u89c9\u73af\u5883\u7684\u95ee\u9898\u662f\u667a\u80fd\u4ee3\u7406\u7406\u89e3\u4e30\u5bcc\u7684\u591a\u65b9\u9762\u573a\u666f\u7684\u91cd\u8981\u65b9\u5f0f\uff0c\u8fd9\u63d0\u9ad8\u4e86\u89c6\u89c9\u95ee\u9898\u751f\u6210\uff08VQG\uff09\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002\u9664\u4e86\u57fa\u4e8e\u56fe\u50cf\u4e4b\u5916\uff0c\u73b0\u6709\u7684 VQG \u7cfb\u7edf\u8fd8\u53ef\u4ee5\u4f7f\u7528\u6587\u672c\u7ea6\u675f\uff08\u4f8b\u5982\u9884\u671f\u7b54\u6848\u6216\u77e5\u8bc6\u4e09\u5143\u7ec4\uff09\u6765\u751f\u6210\u6709\u9488\u5bf9\u6027\u7684\u95ee\u9898\u3002\u8fd9\u4e9b\u7ea6\u675f\u5141\u8bb8 VQG \u7cfb\u7edf\u6307\u5b9a\u95ee\u9898\u5185\u5bb9\u6216\u5229\u7528\u65e0\u6cd5\u4ec5\u4ece\u56fe\u50cf\u5185\u5bb9\u83b7\u5f97\u7684\u5916\u90e8\u5e38\u8bc6\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u4f7f\u7528\u6587\u672c\u7ea6\u675f\u751f\u6210\u91cd\u70b9\u95ee\u9898\uff0c\u540c\u65f6\u5f3a\u5236\u4e0e\u56fe\u50cf\u5185\u5bb9\u4fdd\u6301\u9ad8\u5ea6\u76f8\u5173\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a VQG \u7cfb\u7edf\u7ecf\u5e38\u5ffd\u7565\u4e00\u79cd\u6216\u4e24\u79cd\u5f62\u5f0f\u7684\u57fa\u7840\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5bf9\u6bd4\u89c6\u89c9\u95ee\u9898\u751f\u6210\uff08ConVQG\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4f7f\u7528\u53cc\u91cd\u5bf9\u6bd4\u76ee\u6807\u6765\u533a\u5206\u4f7f\u7528\u4e24\u79cd\u6a21\u5f0f\u751f\u6210\u7684\u95ee\u9898\u548c\u57fa\u4e8e\u5355\u4e00\u6a21\u5f0f\u751f\u6210\u7684\u95ee\u9898\u7684\u65b9\u6cd5\u3002\u5bf9\u77e5\u8bc6\u611f\u77e5\u548c\u6807\u51c6 VQG \u57fa\u51c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0cConVQG \u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u751f\u6210\u57fa\u4e8e\u56fe\u50cf\u3001\u6587\u672c\u5f15\u5bfc\u548c\u77e5\u8bc6\u4e30\u5bcc\u7684\u95ee\u9898\u200b\u200b\u3002\u4e0e\u975e\u5bf9\u6bd4\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u4eba\u7c7b\u8bc4\u4f30\u7ed3\u679c\u8fd8\u663e\u793a\u51fa\u5bf9 ConVQG \u95ee\u9898\u7684\u504f\u597d\u3002|[2402.12846v1](http://arxiv.org/pdf/2402.12846v1)|null|\n", "2402.12750": "|**2024-02-20**|**Model Composition for Multimodal Large Language Models**|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u578b\u7ec4\u5408|Chi Chen, Yiyang Du, Zheng Fang, Ziyue Wang, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, et.al.|Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.|\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u6700\u65b0\u53d1\u5c55\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u6b63\u5728\u671d\u7740\u521b\u5efa\u80fd\u591f\u7406\u89e3\u5404\u79cd\u6a21\u6001\u8f93\u5165\u7684\u591a\u529f\u80fd MLLM \u7684\u76ee\u6807\u8fc8\u8fdb\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u914d\u5bf9\u591a\u6a21\u6001\u6307\u4ee4\u6570\u636e\u7684\u8054\u5408\u8bad\u7ec3\uff0c\u8fd9\u662f\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u5e76\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u65b0\u6a21\u6001\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u73b0\u6709 MLLM \u7684\u6a21\u578b\u7ec4\u5408\u6765\u521b\u5efa\u4e00\u4e2a\u4fdd\u7559\u6bcf\u4e2a\u539f\u59cb\u6a21\u578b\u7684\u6a21\u6001\u7406\u89e3\u80fd\u529b\u7684\u65b0\u6a21\u578b\u3002\u6211\u4eec\u7684\u57fa\u672c\u5b9e\u73b0 NaiveMC \u901a\u8fc7\u91cd\u7528\u6a21\u6001\u7f16\u7801\u5668\u548c\u5408\u5e76 LLM \u53c2\u6570\u5c55\u793a\u4e86\u8be5\u8303\u4f8b\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165DAMC\u6765\u89e3\u51b3\u5408\u5e76\u8fc7\u7a0b\u4e2d\u7684\u53c2\u6570\u5e72\u6270\u548c\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u4e3a\u4e86\u4fc3\u8fdb\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 MCUB\uff0c\u8fd9\u662f\u8bc4\u4f30 MLLM \u7406\u89e3\u4e0d\u540c\u6a21\u5f0f\u8f93\u5165\u7684\u80fd\u529b\u7684\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u548c\u5176\u4ed6\u56db\u4e2a\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u7684\u5b9e\u9a8c\u663e\u793a\u51fa\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u7684\u663e\u7740\u6539\u8fdb\uff0c\u8bc1\u660e\u6a21\u578b\u7ec4\u5408\u53ef\u4ee5\u521b\u5efa\u80fd\u591f\u5904\u7406\u6765\u81ea\u591a\u79cd\u6a21\u6001\u7684\u8f93\u5165\u7684\u901a\u7528\u6a21\u578b\u3002|[2402.12750v1](http://arxiv.org/pdf/2402.12750v1)|null|\n", "2402.12728": "|**2024-02-20**|**Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering**|\u6a21\u6001\u611f\u77e5\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u6210\uff0c\u7528\u4e8e\u57fa\u4e8e\u77e5\u8bc6\u7684\u89c6\u89c9\u95ee\u7b54|Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang|Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion. We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments on two benchmark datasets show the superiority of MAIL with 24x less resources.|\u57fa\u4e8e\u77e5\u8bc6\u7684\u89c6\u89c9\u95ee\u7b54\uff08KVQA\uff09\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4ee5\u5229\u7528\u5916\u90e8\u77e5\u8bc6\uff08\u4f8b\u5982\u77e5\u8bc6\u56fe\uff08KG\uff09\uff09\u56de\u7b54\u89c6\u89c9\u95ee\u9898\u3002\u5c3d\u7ba1\u5df2\u7ecf\u63d0\u51fa\u4e86\u4e00\u4e9b\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u9690\u5f0f\u77e5\u8bc6\u6e90\u7684\u5c1d\u8bd5\uff0c\u4f46\u7531\u4e8e LLM \u53ef\u80fd\u4f1a\u4ea7\u751f\u5e7b\u89c9\uff0c\u8fd9\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6b64\u5916\uff0c\u56fe\u50cf\u3001KG \u548c LLM \u7b49\u591a\u79cd\u77e5\u8bc6\u6e90\u65e0\u6cd5\u8f7b\u677e\u9488\u5bf9\u590d\u6742\u573a\u666f\u8fdb\u884c\u8c03\u6574\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e KVQA (MAIL) \u6cd5\u5b66\u7855\u58eb\u7684\u65b0\u9896\u7684\u6a21\u6001\u611f\u77e5\u96c6\u6210\u3002\u5b83\u4ed4\u7ec6\u5229\u7528\u591a\u6a21\u6001\u77e5\u8bc6\u8fdb\u884c\u56fe\u50cf\u7406\u89e3\u548c\u77e5\u8bc6\u63a8\u7406\u3002\u5177\u4f53\u6765\u8bf4\uff0c\uff08i\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528 LLM \u7684\u4e24\u9636\u6bb5\u63d0\u793a\u7b56\u7565\uff0c\u5c06\u56fe\u50cf\u5bc6\u96c6\u5730\u4f53\u73b0\u4e3a\u5177\u6709\u8be6\u7ec6\u89c6\u89c9\u7279\u5f81\u7684\u573a\u666f\u56fe\uff1b \uff08ii\uff09\u6211\u4eec\u901a\u8fc7\u5c06\u63d0\u5230\u7684\u5b9e\u4f53\u4e0e\u5916\u90e8\u4e8b\u5b9e\u8054\u7cfb\u8d77\u6765\u6784\u5efa\u8026\u5408\u6982\u5ff5\u56fe\u3002 (iii) \u4e3a\u5145\u5206\u7684\u591a\u6a21\u6001\u878d\u5408\u800c\u8bbe\u8ba1\u5b9a\u5236\u7684\u4f2a\u66b9\u7f57\u56fe\u4ecb\u8d28\u878d\u5408\u3002\u6211\u4eec\u5229\u7528\u4e24\u4e2a\u56fe\u4e2d\u5171\u4eab\u7684\u5b9e\u4f53\u4f5c\u4e3a\u5a92\u4ecb\u6765\u6865\u63a5\u7d27\u5bc6\u7684\u6a21\u6001\u95f4\u4ea4\u6362\uff0c\u540c\u65f6\u901a\u8fc7\u9650\u5236\u5a92\u4ecb\u5185\u7684\u878d\u5408\u6765\u6700\u5927\u9650\u5ea6\u5730\u4fdd\u7559\u6709\u6d1e\u5bdf\u529b\u7684\u6a21\u6001\u5185\u5b66\u4e60\u3002\u5bf9\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\u4e86 MAIL \u5728\u8d44\u6e90\u51cf\u5c11 24 \u500d\u7684\u60c5\u51b5\u4e0b\u7684\u4f18\u8d8a\u6027\u3002|[2402.12728v1](http://arxiv.org/pdf/2402.12728v1)|null|\n"}, "Nerf": {"2402.13255": "|**2024-02-20**|**How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey**|NeRF \u548c 3D \u9ad8\u65af\u5206\u5e03\u5982\u4f55\u91cd\u5851 SLAM\uff1a\u4e00\u9879\u8c03\u67e5|Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandstr\u00f6m, Stefano Mattoccia, Martin R. Oswald, Matteo Poggi|Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments. This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) representations. Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of SLAM progress through the lens of the latest advancements in radiance fields. It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges.|\u5728\u8fc7\u53bb\u7684\u4e8c\u5341\u5e74\u4e2d\uff0c\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5efa\u56fe (SLAM) \u9886\u57df\u7684\u7814\u7a76\u7ecf\u5386\u4e86\u91cd\u5927\u53d1\u5c55\uff0c\u51f8\u663e\u4e86\u5176\u5728\u5b9e\u73b0\u672a\u77e5\u73af\u5883\u81ea\u4e3b\u63a2\u7d22\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\u3002\u8fd9\u79cd\u6f14\u53d8\u7684\u8303\u56f4\u4ece\u624b\u5de5\u65b9\u6cd5\u5230\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\uff0c\u518d\u5230\u6700\u8fd1\u4ee5\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u548c 3D \u9ad8\u65af\u5206\u5e03 (3DGS) \u8868\u793a\u4e3a\u91cd\u70b9\u7684\u53d1\u5c55\u3002\u8ba4\u8bc6\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u4ee5\u53ca\u7f3a\u4e4f\u5bf9\u8be5\u4e3b\u9898\u7684\u5168\u9762\u8c03\u67e5\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8f90\u5c04\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u9996\u6b21\u5168\u9762\u6982\u8ff0 SLAM \u7684\u8fdb\u5c55\u3002\u5b83\u63ed\u793a\u4e86\u80cc\u666f\u3001\u6f14\u53d8\u8def\u5f84\u3001\u56fa\u6709\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u7a81\u51fa\u52a8\u6001\u8fdb\u5c55\u548c\u5177\u4f53\u6311\u6218\u63d0\u4f9b\u4e86\u57fa\u672c\u53c2\u8003\u3002|[2402.13255v1](http://arxiv.org/pdf/2402.13255v1)|null|\n", "2402.13252": "|**2024-02-20**|**Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields**|\u63d0\u9ad8\u76f8\u673a\u4f4d\u59ff\u548c\u5206\u89e3\u4f4e\u9636\u5f20\u91cf\u8f90\u5c04\u573a\u8054\u5408\u4f18\u5316\u7684\u9c81\u68d2\u6027|Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu|In this paper, we propose an algorithm that allows joint refinement of camera pose and scene geometry represented by decomposed low-rank tensor, using only 2D images as supervision. First, we conduct a pilot study based on a 1D signal and relate our findings to 3D scenarios, where the naive joint pose optimization on voxel-based NeRFs can easily lead to sub-optimal solutions. Moreover, based on the analysis of the frequency spectrum, we propose to apply convolutional Gaussian filters on 2D and 3D radiance fields for a coarse-to-fine training schedule that enables joint camera pose optimization. Leveraging the decomposition property in decomposed low-rank tensor, our method achieves an equivalent effect to brute-force 3D convolution with only incurring little computational overhead. To further improve the robustness and stability of joint optimization, we also propose techniques of smoothed 2D supervision, randomly scaled kernel parameters, and edge-guided loss mask. Extensive quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior performance in novel view synthesis as well as rapid convergence for optimization.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u5141\u8bb8\u4ec5\u4f7f\u7528 2D \u56fe\u50cf\u4f5c\u4e3a\u76d1\u7763\uff0c\u8054\u5408\u7ec6\u5316\u7531\u5206\u89e3\u7684\u4f4e\u79e9\u5f20\u91cf\u8868\u793a\u7684\u76f8\u673a\u59ff\u6001\u548c\u573a\u666f\u51e0\u4f55\u5f62\u72b6\u3002\u9996\u5148\uff0c\u6211\u4eec\u57fa\u4e8e 1D \u4fe1\u53f7\u8fdb\u884c\u4e86\u4e00\u9879\u8bd5\u70b9\u7814\u7a76\uff0c\u5e76\u5c06\u6211\u4eec\u7684\u53d1\u73b0\u4e0e 3D \u573a\u666f\u76f8\u5173\u8054\uff0c\u5176\u4e2d\u57fa\u4e8e\u4f53\u7d20\u7684 NeRF \u7684\u6734\u7d20\u8054\u5408\u59ff\u52bf\u4f18\u5316\u5f88\u5bb9\u6613\u5bfc\u81f4\u6b21\u4f18\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u9891\u8c31\u5206\u6790\uff0c\u6211\u4eec\u5efa\u8bae\u5728 2D \u548c 3D \u8f90\u5c04\u573a\u4e0a\u5e94\u7528\u5377\u79ef\u9ad8\u65af\u6ee4\u6ce2\u5668\uff0c\u4ee5\u5b9e\u73b0\u4ece\u7c97\u5230\u7ec6\u7684\u8bad\u7ec3\u8ba1\u5212\uff0c\u4ece\u800c\u5b9e\u73b0\u8054\u5408\u76f8\u673a\u59ff\u6001\u4f18\u5316\u3002\u5229\u7528\u5206\u89e3\u7684\u4f4e\u79e9\u5f20\u91cf\u7684\u5206\u89e3\u7279\u6027\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u5f3a\u529b 3D \u5377\u79ef\u7b49\u6548\u7684\u6548\u679c\uff0c\u5e76\u4e14\u53ea\u4ea7\u751f\u5f88\u5c11\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8054\u5408\u4f18\u5316\u7684\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u5e73\u6ed1\u4e8c\u7ef4\u76d1\u7763\u3001\u968f\u673a\u7f29\u653e\u5185\u6838\u53c2\u6570\u548c\u8fb9\u7f18\u5f15\u5bfc\u635f\u5931\u63a9\u6a21\u7684\u6280\u672f\u3002\u5e7f\u6cdb\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u5728\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u4ee5\u53ca\u5feb\u901f\u6536\u655b\u4f18\u5316\u65b9\u9762\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002|[2402.13252v1](http://arxiv.org/pdf/2402.13252v1)|**[link](https://github.com/nemo1999/joint-tensorf)**|\n", "2402.12792": "|**2024-02-20**|**OccFlowNet: Towards Self-supervised Occupancy Estimation via Differentiable Rendering and Occupancy Flow**|OccFlowNet\uff1a\u901a\u8fc7\u53ef\u5fae\u6e32\u67d3\u548c\u5360\u7528\u6d41\u5b9e\u73b0\u81ea\u76d1\u7763\u5360\u7528\u4f30\u8ba1|Simon Boeder, Fabian Gigengack, Benjamin Risse|Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain. In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire. In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only. To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps. Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches. When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards self-supervised learning in this domain.|\u8bed\u4e49\u5360\u7528\u4f5c\u4e3a\u4e00\u79cd\u7a81\u51fa\u7684 3D \u573a\u666f\u8868\u793a\u6700\u8fd1\u83b7\u5f97\u4e86\u5de8\u5927\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5177\u6709\u7ec6\u7c92\u5ea6 3D \u4f53\u7d20\u6807\u7b7e\u7684\u5927\u578b\u4e14\u6602\u8d35\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u589e\u52a0\u4e86\u8be5\u9886\u57df\u81ea\u6211\u76d1\u63a7\u5b66\u4e60\u7684\u9700\u6c42\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u542f\u53d1\u7684\u65b0\u9896\u7684\u5360\u7528\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528 2D \u6807\u7b7e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u66f4\u5bb9\u6613\u83b7\u53d6\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u91c7\u7528\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u6765\u9884\u6d4b\u6df1\u5ea6\u200b\u200b\u548c\u8bed\u4e49\u56fe\uff0c\u5e76\u4ec5\u57fa\u4e8e 2D \u76d1\u7763\u8bad\u7ec3 3D \u7f51\u7edc\u3002\u4e3a\u4e86\u63d0\u9ad8\u51e0\u4f55\u7cbe\u5ea6\u5e76\u589e\u52a0\u76d1\u7763\u4fe1\u53f7\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u76f8\u90bb\u65f6\u95f4\u6b65\u957f\u7684\u65f6\u95f4\u6e32\u67d3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u5360\u7528\u6d41\u4f5c\u4e3a\u5904\u7406\u573a\u666f\u4e2d\u52a8\u6001\u5bf9\u8c61\u5e76\u786e\u4fdd\u5176\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u673a\u5236\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\uff0c\u4e0e\u4f7f\u7528 3D \u6807\u7b7e\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4ec5 2D \u76d1\u7763\u5c31\u8db3\u4ee5\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4f18\u4e8e\u5e76\u53d1 2D \u65b9\u6cd5\u3002\u5f53\u5c06 2D \u76d1\u7763\u4e0e 3D \u6807\u7b7e\u3001\u65f6\u95f4\u6e32\u67d3\u548c\u5360\u7528\u6d41\u76f8\u7ed3\u5408\u65f6\uff0c\u6211\u4eec\u7684\u6027\u80fd\u663e\u7740\u4f18\u4e8e\u4e4b\u524d\u6240\u6709\u7684\u5360\u7528\u4f30\u8ba1\u6a21\u578b\u3002\u6211\u4eec\u7684\u7ed3\u8bba\u662f\uff0c\u6240\u63d0\u51fa\u7684\u6e32\u67d3\u76d1\u7763\u548c\u5360\u7528\u6d41\u7a0b\u4fc3\u8fdb\u4e86\u5360\u7528\u4f30\u8ba1\uff0c\u5e76\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4e86\u8be5\u9886\u57df\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u7684\u5dee\u8ddd\u3002|[2402.12792v1](http://arxiv.org/pdf/2402.12792v1)|null|\n"}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.13251": "|**2024-02-20**|**FlashTex: Fast Relightable Mesh Texturing with LightControlNet**|FlashTex\uff1a\u4f7f\u7528 LightControlNet \u8fdb\u884c\u5feb\u901f\u53ef\u91cd\u65b0\u7167\u660e\u7f51\u683c\u7eb9\u7406|Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, Maneesh Agrawala|Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surface material from lighting. Our pipeline is significantly faster than previous text-to-texture methods, while producing high-quality and relightable textures.|\u5373\u4f7f\u5bf9\u4e8e\u4e13\u4e1a\u7684\u89c6\u89c9\u5185\u5bb9\u521b\u5efa\u8005\u6765\u8bf4\uff0c\u624b\u52a8\u521b\u5efa 3D \u7f51\u683c\u7eb9\u7406\u4e5f\u975e\u5e38\u8017\u65f6\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u63d0\u4f9b\u7684\u6587\u672c\u63d0\u793a\u81ea\u52a8\u5bf9\u8f93\u5165 3D \u7f51\u683c\u8fdb\u884c\u7eb9\u7406\u5316\u7684\u5feb\u901f\u65b9\u6cd5\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u7167\u660e\u4e0e\u6700\u7ec8\u7eb9\u7406\u4e2d\u7684\u8868\u9762\u6750\u8d28/\u53cd\u5c04\u7387\u5206\u5f00\uff0c\u4ee5\u4fbf\u7f51\u683c\u53ef\u4ee5\u5728\u4efb\u4f55\u7167\u660e\u73af\u5883\u4e2d\u6b63\u786e\u5730\u91cd\u65b0\u7167\u4eae\u548c\u6e32\u67d3\u3002\u6211\u4eec\u5f15\u5165\u4e86 LightControlNet\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e ControlNet \u67b6\u6784\u7684\u65b0\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u5b83\u5141\u8bb8\u5c06\u6240\u9700\u7684\u7167\u660e\u6307\u5b9a\u4e3a\u6a21\u578b\u7684\u8c03\u8282\u56fe\u50cf\u3002\u7136\u540e\uff0c\u6211\u4eec\u7684\u6587\u672c\u5230\u7eb9\u7406\u7ba1\u9053\u5206\u4e24\u4e2a\u9636\u6bb5\u6784\u5efa\u7eb9\u7406\u3002\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528 LightControlNet \u751f\u6210\u4e00\u7ec4\u7a00\u758f\u7684\u89c6\u89c9\u4e00\u81f4\u7684\u7f51\u683c\u53c2\u8003\u89c6\u56fe\u3002\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u57fa\u4e8e\u5206\u6570\u84b8\u998f\u91c7\u6837 (SDS) \u7684\u7eb9\u7406\u4f18\u5316\uff0c\u4e0e LightControlNet \u914d\u5408\u4f7f\u7528\u4ee5\u63d0\u9ad8\u7eb9\u7406\u8d28\u91cf\uff0c\u540c\u65f6\u5c06\u8868\u9762\u6750\u8d28\u4e0e\u7167\u660e\u5206\u79bb\u3002\u6211\u4eec\u7684\u7ba1\u9053\u6bd4\u4ee5\u524d\u7684\u6587\u672c\u5230\u7eb9\u7406\u65b9\u6cd5\u8981\u5feb\u5f97\u591a\uff0c\u540c\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u53ef\u91cd\u65b0\u70b9\u4eae\u7684\u7eb9\u7406\u3002|[2402.13251v1](http://arxiv.org/pdf/2402.13251v1)|null|\n", "2402.13217": "|**2024-02-20**|**VideoPrism: A Foundational Visual Encoder for Video Understanding**|VideoPrism\uff1a\u7528\u4e8e\u89c6\u9891\u7406\u89e3\u7684\u57fa\u7840\u89c6\u89c9\u7f16\u7801\u5668|Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, et.al.|We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.|\u6211\u4eec\u5f15\u5165\u4e86 VideoPrism\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u7528\u89c6\u9891\u7f16\u7801\u5668\uff0c\u53ef\u4ee5\u4f7f\u7528\u5355\u4e2a\u51bb\u7ed3\u6a21\u578b\u6765\u5904\u7406\u5404\u79cd\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u3002\u6211\u4eec\u5728\u5305\u542b 36M \u9ad8\u8d28\u91cf\u89c6\u9891\u5b57\u5e55\u5bf9\u548c 582M \u5e26\u6709\u566a\u58f0\u5e76\u884c\u6587\u672c\uff08\u4f8b\u5982 ASR \u8f6c\u5f55\u672c\uff09\u7684\u89c6\u9891\u526a\u8f91\u7684\u5f02\u6784\u8bed\u6599\u5e93\u4e0a\u9884\u8bad\u7ec3 VideoPrism\u3002\u9884\u8bad\u7ec3\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u89c6\u9891\u5d4c\u5165\u7684\u5168\u5c40\u5c40\u90e8\u84b8\u998f\u548c\u4ee4\u724c\u6d17\u724c\u65b9\u6848\u6539\u8fdb\u4e86\u5c4f\u853d\u81ea\u52a8\u7f16\u7801\uff0c\u4f7f VideoPrism \u80fd\u591f\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u6a21\u6001\uff0c\u540c\u65f6\u5229\u7528\u4e0e\u89c6\u9891\u76f8\u5173\u7684\u5b9d\u8d35\u6587\u672c\u3002\u6211\u4eec\u5728\u56db\u7ec4\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u5e7f\u6cdb\u6d4b\u8bd5 VideoPrism\uff0c\u4ece\u7f51\u7edc\u89c6\u9891\u95ee\u7b54\u5230\u79d1\u5b66\u7b80\u5386\uff0c\u5728 33 \u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684 30 \u4e2a\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2402.13217v1](http://arxiv.org/pdf/2402.13217v1)|null|\n", "2402.13122": "|**2024-02-20**|**Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model**|\u4f7f\u7528 CoRTe \u8fdb\u884c\u8de8\u57df\u8fc1\u79fb\u5b66\u4e60\uff1a\u4ece\u9ed1\u76d2\u5230\u8f7b\u91cf\u7ea7\u5206\u5272\u6a21\u578b\u7684\u4e00\u81f4\u4e14\u53ef\u9760\u7684\u8fc1\u79fb|Claudia Cuttano, Antonio Tavera, Fabio Cermelli, Giuseppe Averta, Barbara Caputo|Many practical applications require training of semantic segmentation models on unlabelled datasets and their execution on low-resource hardware. Distillation from a trained source model may represent a solution for the first but does not account for the different distribution of the training data. Unsupervised domain adaptation (UDA) techniques claim to solve the domain shift, but in most cases assume the availability of the source data or an accessible white-box source model, which in practical applications are often unavailable for commercial and/or safety reasons. In this paper, we investigate a more challenging setting in which a lightweight model has to be trained on a target unlabelled dataset for semantic segmentation, under the assumption that we have access only to black-box source model predictions. Our method, named CoRTe, consists of (i) a pseudo-labelling function that extracts reliable knowledge from the black-box source model using its relative confidence, (ii) a pseudo label refinement method to retain and enhance the novel information learned by the student model on the target data, and (iii) a consistent training of the model using the extracted pseudo labels. We benchmark CoRTe on two synthetic-to-real settings, demonstrating remarkable results when using black-box models to transfer knowledge on lightweight models for a target data distribution.|\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u9700\u8981\u5728\u672a\u6807\u8bb0\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5e76\u5728\u4f4e\u8d44\u6e90\u786c\u4ef6\u4e0a\u6267\u884c\u3002\u4ece\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6e90\u6a21\u578b\u4e2d\u63d0\u53d6\u53ef\u80fd\u4ee3\u8868\u7b2c\u4e00\u4e2a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u6ca1\u6709\u8003\u8651\u8bad\u7ec3\u6570\u636e\u7684\u4e0d\u540c\u5206\u5e03\u3002\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u6280\u672f\u58f0\u79f0\u53ef\u4ee5\u89e3\u51b3\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u4f46\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5047\u8bbe\u6e90\u6570\u636e\u6216\u53ef\u8bbf\u95ee\u7684\u767d\u76d2\u6e90\u6a21\u578b\u53ef\u7528\uff0c\u800c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7531\u4e8e\u5546\u4e1a\u548c/\u6216\u5b89\u5168\u539f\u56e0\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u4e0d\u53ef\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u79cd\u66f4\u5177\u6311\u6218\u6027\u7684\u8bbe\u7f6e\uff0c\u5176\u4e2d\u5047\u8bbe\u6211\u4eec\u53ea\u80fd\u8bbf\u95ee\u9ed1\u76d2\u6e90\u6a21\u578b\u9884\u6d4b\uff0c\u5219\u5fc5\u987b\u5728\u76ee\u6807\u672a\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u578b\u4ee5\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u540d\u4e3a CoRTe\uff0c\u7531\uff08i\uff09\u4e00\u4e2a\u4f2a\u6807\u7b7e\u51fd\u6570\u7ec4\u6210\uff0c\u8be5\u51fd\u6570\u4f7f\u7528\u5176\u76f8\u5bf9\u7f6e\u4fe1\u5ea6\u4ece\u9ed1\u76d2\u6e90\u6a21\u578b\u4e2d\u63d0\u53d6\u53ef\u9760\u7684\u77e5\u8bc6\uff0c\uff08ii\uff09\u4e00\u4e2a\u4f2a\u6807\u7b7e\u7ec6\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4fdd\u7559\u548c\u589e\u5f3a\u4ece\u9ed1\u76d2\u6e90\u6a21\u578b\u4e2d\u5b66\u5230\u7684\u65b0\u4fe1\u606f\u3002\u76ee\u6807\u6570\u636e\u4e0a\u7684\u5b66\u751f\u6a21\u578b\uff0c\u4ee5\u53ca\uff08iii\uff09\u4f7f\u7528\u63d0\u53d6\u7684\u4f2a\u6807\u7b7e\u5bf9\u6a21\u578b\u8fdb\u884c\u4e00\u81f4\u7684\u8bad\u7ec3\u3002\u6211\u4eec\u5728\u4e24\u79cd\u5408\u6210\u5230\u771f\u5b9e\u7684\u8bbe\u7f6e\u4e0a\u5bf9 CoRTe \u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u4f7f\u7528\u9ed1\u76d2\u6a21\u578b\u4f20\u8f93\u76ee\u6807\u6570\u636e\u5206\u5e03\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u77e5\u8bc6\u65f6\u5c55\u793a\u4e86\u663e\u7740\u7684\u7ed3\u679c\u3002|[2402.13122v1](http://arxiv.org/pdf/2402.13122v1)|null|\n", "2402.13007": "|**2024-02-20**|**Improve Cross-Architecture Generalization on Dataset Distillation**|\u6539\u8fdb\u6570\u636e\u96c6\u84b8\u998f\u7684\u8de8\u67b6\u6784\u6cdb\u5316|Binglin Zhou, Linhao Zhong, Wentao Chen|Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed \"model pool\". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.|\u6570\u636e\u96c6\u84b8\u998f\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\uff0c\u65e8\u5728\u4ece\u8f83\u5927\u7684\u73b0\u6709\u6570\u636e\u96c6\u521b\u5efa\u8f83\u5c0f\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u8303\u4f8b\uff0c\u5176\u4e2d\u5408\u6210\u6570\u636e\u96c6\u7ee7\u627f\u4e86\u7279\u5b9a\u4e8e\u6a21\u578b\u7684\u504f\u5dee\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u66ff\u4ee3\u6a21\u578b\u7684\u6cdb\u5316\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u6a21\u578b\u6c60\u201d\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u6d89\u53ca\u5728\u6570\u636e\u84b8\u998f\u8fc7\u7a0b\u4e2d\u6839\u636e\u7279\u5b9a\u7684\u6982\u7387\u5206\u5e03\u4ece\u4e0d\u540c\u7684\u6a21\u578b\u6c60\u4e2d\u9009\u62e9\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u6a21\u578b\u6c60\u4e0e\u5df2\u5efa\u7acb\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u5e76\u5c06\u77e5\u8bc6\u84b8\u998f\u5e94\u7528\u4e8e\u84b8\u998f\u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u8fc7\u7a0b\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u5728\u6d4b\u8bd5\u65f6\u9a8c\u8bc1\u4e86\u6a21\u578b\u6c60\u65b9\u6cd5\u5728\u4e00\u7cfb\u5217\u73b0\u6709\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u7684\u5353\u8d8a\u6027\u80fd\u3002|[2402.13007v1](http://arxiv.org/pdf/2402.13007v1)|**[link](https://github.com/distill-generalization-group/distill-generalization)**|\n", "2402.12624": "|**2024-02-20**|**Efficient Parameter Mining and Freezing for Continual Object Detection**|\u7528\u4e8e\u6301\u7eed\u76ee\u6807\u68c0\u6d4b\u7684\u9ad8\u6548\u53c2\u6570\u6316\u6398\u548c\u51bb\u7ed3|Angelo G. Menezes, Augusto J. Peterlevitz, Mateus A. Chinelatto, Andr\u00e9 C. P. L. F. de Carvalho|Continual Object Detection is essential for enabling intelligent agents to interact proactively with humans in real-world settings. While parameter-isolation strategies have been extensively explored in the context of continual learning for classification, they have yet to be fully harnessed for incremental object detection scenarios. Drawing inspiration from prior research that focused on mining individual neuron responses and integrating insights from recent developments in neural pruning, we proposed efficient ways to identify which layers are the most important for a network to maintain the performance of a detector across sequential updates. The presented findings highlight the substantial advantages of layer-level parameter isolation in facilitating incremental learning within object detection models, offering promising avenues for future research and application in real-world scenarios.|\u6301\u7eed\u7684\u5bf9\u8c61\u68c0\u6d4b\u5bf9\u4e8e\u4f7f\u667a\u80fd\u4ee3\u7406\u80fd\u591f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e3b\u52a8\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u53c2\u6570\u9694\u79bb\u7b56\u7565\u5728\u6301\u7eed\u5b66\u4e60\u5206\u7c7b\u7684\u80cc\u666f\u4e0b\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u63a2\u7d22\uff0c\u4f46\u5b83\u4eec\u5c1a\u672a\u5b8c\u5168\u7528\u4e8e\u589e\u91cf\u5bf9\u8c61\u68c0\u6d4b\u573a\u666f\u3002\u4ece\u5148\u524d\u4e13\u6ce8\u4e8e\u6316\u6398\u5355\u4e2a\u795e\u7ecf\u5143\u54cd\u5e94\u7684\u7814\u7a76\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u5e76\u6574\u5408\u795e\u7ecf\u526a\u679d\u6700\u65b0\u53d1\u5c55\u7684\u89c1\u89e3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6709\u6548\u7684\u65b9\u6cd5\u6765\u786e\u5b9a\u54ea\u4e9b\u5c42\u5bf9\u4e8e\u7f51\u7edc\u5728\u8fde\u7eed\u66f4\u65b0\u4e2d\u7ef4\u6301\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u6700\u91cd\u8981\u3002\u6240\u63d0\u51fa\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5c42\u7ea7\u53c2\u6570\u9694\u79bb\u5728\u4fc3\u8fdb\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u4e2d\u7684\u589e\u91cf\u5b66\u4e60\u65b9\u9762\u7684\u5de8\u5927\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002|[2402.12624v1](http://arxiv.org/pdf/2402.12624v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.13250": "|**2024-02-20**|**Video ReCap: Recursive Captioning of Hour-Long Videos**|\u89c6\u9891\u56de\u987e\uff1a\u957f\u8fbe\u4e00\u5c0f\u65f6\u7684\u89c6\u9891\u7684\u9012\u5f52\u5b57\u5e55|Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius|Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g., objects, scenes, atomic actions). However, most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities. We propose Video ReCap, a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels. The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently. We utilize a curriculum learning training scheme to learn the hierarchical structure of videos, starting from clip-level captions describing atomic actions, then focusing on segment-level descriptions, and concluding with generating summaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by augmenting Ego4D with 8,267 manually collected long-range video summaries. Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks, such as VideoQA on EgoSchema. Data, code, and models are available at: https://sites.google.com/view/vidrecap|\u5927\u591a\u6570\u89c6\u9891\u5b57\u5e55\u6a21\u578b\u65e8\u5728\u5904\u7406\u51e0\u79d2\u949f\u7684\u77ed\u89c6\u9891\u526a\u8f91\u5e76\u8f93\u51fa\u63cf\u8ff0\u4f4e\u7ea7\u89c6\u89c9\u6982\u5ff5\uff08\u4f8b\u5982\u5bf9\u8c61\u3001\u573a\u666f\u3001\u539f\u5b50\u52a8\u4f5c\uff09\u7684\u6587\u672c\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u5b9e\u4e16\u754c\u7684\u89c6\u9891\u4f1a\u6301\u7eed\u51e0\u5206\u949f\u6216\u51e0\u5c0f\u65f6\uff0c\u5e76\u4e14\u5177\u6709\u8de8\u8d8a\u4e0d\u540c\u65f6\u95f4\u7c92\u5ea6\u7684\u590d\u6742\u5c42\u6b21\u7ed3\u6784\u3002\u6211\u4eec\u63d0\u51fa\u4e86 Video ReCap\uff0c\u8fd9\u662f\u4e00\u79cd\u9012\u5f52\u89c6\u9891\u5b57\u5e55\u6a21\u578b\uff0c\u53ef\u4ee5\u5904\u7406\u957f\u5ea6\u622a\u7136\u4e0d\u540c\uff08\u4ece 1 \u79d2\u5230 2 \u5c0f\u65f6\uff09\u7684\u89c6\u9891\u8f93\u5165\uff0c\u5e76\u5728\u591a\u4e2a\u5c42\u6b21\u7ed3\u6784\u7ea7\u522b\u8f93\u51fa\u89c6\u9891\u5b57\u5e55\u3002\u9012\u5f52\u89c6\u9891\u8bed\u8a00\u67b6\u6784\u5229\u7528\u4e86\u4e0d\u540c\u89c6\u9891\u5c42\u6b21\u7ed3\u6784\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5904\u7406\u957f\u8fbe\u4e00\u5c0f\u65f6\u7684\u89c6\u9891\u3002\u6211\u4eec\u5229\u7528\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3\u65b9\u6848\u6765\u5b66\u4e60\u89c6\u9891\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4ece\u63cf\u8ff0\u539f\u5b50\u52a8\u4f5c\u7684\u526a\u8f91\u7ea7\u5b57\u5e55\u5f00\u59cb\uff0c\u7136\u540e\u5173\u6ce8\u7247\u6bb5\u7ea7\u63cf\u8ff0\uff0c\u6700\u540e\u4e3a\u957f\u8fbe\u4e00\u5c0f\u65f6\u7684\u89c6\u9891\u751f\u6210\u6458\u8981\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u4f7f\u7528 8,267 \u4e2a\u624b\u52a8\u6536\u96c6\u7684\u8fdc\u7a0b\u89c6\u9891\u6458\u8981\u589e\u5f3a Ego4D \u6765\u5f15\u5165 Ego4D-HCap \u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u9012\u5f52\u6a21\u578b\u53ef\u4ee5\u7075\u6d3b\u5730\u751f\u6210\u4e0d\u540c\u5c42\u6b21\u7684\u5b57\u5e55\uff0c\u540c\u65f6\u4e5f\u53ef\u7528\u4e8e\u5176\u4ed6\u590d\u6742\u7684\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u4f8b\u5982 EgoSchema \u4e0a\u7684 VideoQA\u3002\u6570\u636e\u3001\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u53d6\uff1ahttps://sites.google.com/view/vidrecap|[2402.13250v1](http://arxiv.org/pdf/2402.13250v1)|null|\n", "2402.13172": "|**2024-02-20**|**3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data**|\u4f7f\u7528\u751f\u7269\u529b\u5b66\u6a21\u578b\u548c\u7efc\u5408\u8bad\u7ec3\u6570\u636e\u4ece\u89c6\u9891\u8fdb\u884c 3D \u8fd0\u52a8\u5b66\u4f30\u8ba1|Zhi-Yi Lin, Bofan Lyu, Judith Cueto Fernandez, Eline van der Kruk, Ajay Seth, Xucong Zhang|Accurate 3D kinematics estimation of human body is crucial in various applications for human health and mobility, such as rehabilitation, injury prevention, and diagnosis, as it helps to understand the biomechanical loading experienced during movement. Conventional marker-based motion capture is expensive in terms of financial investment, time, and the expertise required. Moreover, due to the scarcity of datasets with accurate annotations, existing markerless motion capture methods suffer from challenges including unreliable 2D keypoint detection, limited anatomic accuracy, and low generalization capability. In this work, we propose a novel biomechanics-aware network that directly outputs 3D kinematics from two input views with consideration of biomechanical prior and spatio-temporal information. To train the model, we create synthetic dataset ODAH with accurate kinematics annotations generated by aligning the body mesh from the SMPL-X model and a full-body OpenSim skeletal model. Our extensive experiments demonstrate that the proposed approach, only trained on synthetic data, outperforms previous state-of-the-art methods when evaluated across multiple datasets, revealing a promising direction for enhancing video-based human motion capture.|\u51c6\u786e\u7684\u4eba\u4f53 3D \u8fd0\u52a8\u5b66\u4f30\u8ba1\u5bf9\u4e8e\u4eba\u7c7b\u5065\u5eb7\u548c\u79fb\u52a8\u6027\u7684\u5404\u79cd\u5e94\u7528\uff08\u4f8b\u5982\u5eb7\u590d\u3001\u4f24\u5bb3\u9884\u9632\u548c\u8bca\u65ad\uff09\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u6709\u52a9\u4e8e\u4e86\u89e3\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u6240\u7ecf\u5386\u7684\u751f\u7269\u529b\u5b66\u8d1f\u8377\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u6807\u8bb0\u7684\u52a8\u4f5c\u6355\u6349\u5728\u8d22\u52a1\u6295\u8d44\u3001\u65f6\u95f4\u548c\u6240\u9700\u4e13\u4e1a\u77e5\u8bc6\u65b9\u9762\u90fd\u5f88\u6602\u8d35\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u7f3a\u4e4f\u51c6\u786e\u6ce8\u91ca\u7684\u6570\u636e\u96c6\uff0c\u73b0\u6709\u7684\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u65b9\u6cd5\u9762\u4e34\u7740\u4e8c\u7ef4\u5173\u952e\u70b9\u68c0\u6d4b\u4e0d\u53ef\u9760\u3001\u89e3\u5256\u7cbe\u5ea6\u6709\u9650\u548c\u6cdb\u5316\u80fd\u529b\u4f4e\u7b49\u6311\u6218\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u751f\u7269\u529b\u5b66\u611f\u77e5\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u53ef\u4ee5\u76f4\u63a5\u4ece\u4e24\u4e2a\u8f93\u5165\u89c6\u56fe\u8f93\u51fa 3D \u8fd0\u52a8\u5b66\uff0c\u540c\u65f6\u8003\u8651\u751f\u7269\u529b\u5b66\u5148\u9a8c\u4fe1\u606f\u548c\u65f6\u7a7a\u4fe1\u606f\u3002\u4e3a\u4e86\u8bad\u7ec3\u6a21\u578b\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u5408\u6210\u6570\u636e\u96c6 ODAH\uff0c\u5176\u4e2d\u5305\u542b\u901a\u8fc7\u5bf9\u9f50 SMPL-X \u6a21\u578b\u7684\u8eab\u4f53\u7f51\u683c\u548c\u5168\u8eab OpenSim \u9aa8\u9abc\u6a21\u578b\u751f\u6210\u7684\u7cbe\u786e\u8fd0\u52a8\u5b66\u6ce8\u91ca\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4ec5\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u8de8\u591a\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u65f6\u4f18\u4e8e\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u589e\u5f3a\u57fa\u4e8e\u89c6\u9891\u7684\u4eba\u4f53\u52a8\u4f5c\u6355\u6349\u7684\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002|[2402.13172v1](http://arxiv.org/pdf/2402.13172v1)|null|\n", "2402.13061": "|**2024-02-20**|**Toward Fairness via Maximum Mean Discrepancy Regularization on Logits Space**|\u901a\u8fc7 Logits \u7a7a\u95f4\u4e0a\u7684\u6700\u5927\u5747\u503c\u5dee\u5f02\u6b63\u5219\u5316\u5b9e\u73b0\u516c\u5e73|Hao-Wei Chung, Ching-Hao Chiu, Yu-Jen Chen, Yiyu Shi, Tsung-Yi Ho|Fairness has become increasingly pivotal in machine learning for high-risk applications such as machine learning in healthcare and facial recognition. However, we see the deficiency in the previous logits space constraint methods. Therefore, we propose a novel framework, Logits-MMD, that achieves the fairness condition by imposing constraints on output logits with Maximum Mean Discrepancy. Moreover, quantitative analysis and experimental results show that our framework has a better property that outperforms previous methods and achieves state-of-the-art on two facial recognition datasets and one animal dataset. Finally, we show experimental results and demonstrate that our debias approach achieves the fairness condition effectively.|\u516c\u5e73\u6027\u5728\u533b\u7597\u4fdd\u5065\u548c\u9762\u90e8\u8bc6\u522b\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u7684\u673a\u5668\u5b66\u4e60\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u6211\u4eec\u770b\u5230\u4e86\u5148\u524d\u7684 Logits \u7a7a\u95f4\u7ea6\u675f\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6 Logits-MMD\uff0c\u5b83\u901a\u8fc7\u5bf9\u5177\u6709\u6700\u5927\u5e73\u5747\u5dee\u5f02\u7684\u8f93\u51fa logits \u65bd\u52a0\u7ea6\u675f\u6765\u5b9e\u73b0\u516c\u5e73\u6761\u4ef6\u3002\u6b64\u5916\uff0c\u5b9a\u91cf\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u4e24\u4e2a\u9762\u90e8\u8bc6\u522b\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u52a8\u7269\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5b9e\u9a8c\u7ed3\u679c\u5e76\u8bc1\u660e\u6211\u4eec\u7684\u53bb\u504f\u65b9\u6cd5\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u516c\u5e73\u6761\u4ef6\u3002|[2402.13061v1](http://arxiv.org/pdf/2402.13061v1)|null|\n", "2402.13004": "|**2024-02-20**|**Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition**|\u7528\u4e8e\u8fde\u7eed\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\u7684\u4f20\u7edf\u6df7\u5408\u89e3\u7801\u5668\u548c CTC/Attention \u89e3\u7801\u5668\u7684\u6bd4\u8f83|David Gimeno-G\u00f3mez, Carlos-D. Mart\u00ednez-Hinarejos|Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR). Similar to other speech processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures. While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm. However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders. Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation. We also analyzed to what extent our visual speech features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language. Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters.|\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5174\u8d77\u548c\u5927\u89c4\u6a21\u89c6\u542c\u6570\u636e\u5e93\u7684\u53ef\u7528\u6027\uff0c\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\uff08VSR\uff09\u9886\u57df\u53d6\u5f97\u4e86\u6700\u65b0\u8fdb\u5c55\u3002\u4e0e\u5176\u4ed6\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u7c7b\u4f3c\uff0c\u8fd9\u4e9b\u7aef\u5230\u7aef VSR \u7cfb\u7edf\u901a\u5e38\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3002\u867d\u7136\u7f16\u7801\u5668\u6709\u4e9b\u901a\u7528\uff0c\u4f46\u5df2\u7ecf\u63a2\u7d22\u4e86\u591a\u79cd\u89e3\u7801\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0e\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u4f20\u7edf\u6df7\u5408\u6a21\u578b\uff08DNN-HMM\uff09\u6216\u8fde\u63a5\u4e3b\u4e49\u65f6\u95f4\u5206\u7c7b\uff08CTC\uff09\u8303\u4f8b\u3002\u7136\u800c\uff0c\u6709\u4e9b\u8bed\u8a00\u548c\u4efb\u52a1\u7684\u6570\u636e\u662f\u7a00\u7f3a\u7684\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u89e3\u7801\u5668\u4e4b\u95f4\u6ca1\u6709\u660e\u786e\u7684\u6bd4\u8f83\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u7814\u7a76\u91cd\u70b9\u662f\u4f20\u7edf\u7684 DNN-HMM \u89e3\u7801\u5668\u53ca\u5176\u6700\u5148\u8fdb\u7684 CTC/Attention \u89e3\u7801\u5668\u5982\u4f55\u6839\u636e\u7528\u4e8e\u4f30\u8ba1\u7684\u6570\u636e\u91cf\u800c\u8868\u73b0\u3002\u6211\u4eec\u8fd8\u5206\u6790\u4e86\u6211\u4eec\u7684\u89c6\u89c9\u8bed\u97f3\u7279\u5f81\u80fd\u591f\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u9002\u5e94\u672a\u7ecf\u8fc7\u660e\u786e\u8bad\u7ec3\u7684\u573a\u666f\uff0c\u65e0\u8bba\u662f\u8003\u8651\u7c7b\u4f3c\u7684\u6570\u636e\u96c6\u8fd8\u662f\u4e3a\u4e0d\u540c\u8bed\u8a00\u6536\u96c6\u7684\u53e6\u4e00\u4e2a\u6570\u636e\u96c6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4f20\u7edf\u8303\u5f0f\u8fbe\u5230\u4e86\u63d0\u9ad8\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u7684 CTC/Attention \u6a21\u578b\u7684\u8bc6\u522b\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u66f4\u5c11\u7684\u53c2\u6570\u3002|[2402.13004v1](http://arxiv.org/pdf/2402.13004v1)|null|\n", "2402.12968": "|**2024-02-20**|**MapTrack: Tracking in the Map**|MapTrack\uff1a\u5728\u5730\u56fe\u4e2d\u8ffd\u8e2a|Fei Wang, Ruohui Zhang, Chenglin Chen, Min Yang, Yun Bai|Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted trajectories for each target. Most state-of-the-art approaches first detect objects in each frame and then implement data association between new detections and existing tracks using motion models and appearance similarities. Despite achieving satisfactory results, occlusion and crowds can easily lead to missing and distorted detections, followed by missing and false associations. In this paper, we first revisit the classic tracker DeepSORT, enhancing its robustness over crowds and occlusion significantly by placing greater trust in predictions when detections are unavailable or of low quality in crowded and occluded scenes. Specifically, we propose a new framework comprising of three lightweight and plug-and-play algorithms: the probability map, the prediction map, and the covariance adaptive Kalman filter. The probability map identifies whether undetected objects have genuinely disappeared from view (e.g., out of the image or entered a building) or are only temporarily undetected due to occlusion or other reasons. Trajectories of undetected targets that are still within the probability map are extended by state estimations directly. The prediction map determines whether an object is in a crowd, and we prioritize state estimations over observations when severe deformation of observations occurs, accomplished through the covariance adaptive Kalman filter. The proposed method, named MapTrack, achieves state-of-the-art results on popular multi-object tracking benchmarks such as MOT17 and MOT20. Despite its superior performance, our method remains simple, online, and real-time. The code will be open-sourced later.|\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u65e8\u5728\u4e3a\u6bcf\u4e2a\u76ee\u6807\u4fdd\u6301\u7a33\u5b9a\u4e14\u4e0d\u95f4\u65ad\u7684\u8f68\u8ff9\u3002\u5927\u591a\u6570\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u9996\u5148\u68c0\u6d4b\u6bcf\u4e2a\u5e27\u4e2d\u7684\u5bf9\u8c61\uff0c\u7136\u540e\u4f7f\u7528\u8fd0\u52a8\u6a21\u578b\u548c\u5916\u89c2\u76f8\u4f3c\u6027\u5728\u65b0\u68c0\u6d4b\u548c\u73b0\u6709\u8f68\u8ff9\u4e4b\u95f4\u5b9e\u73b0\u6570\u636e\u5173\u8054\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\uff0c\u4f46\u906e\u6321\u548c\u4eba\u7fa4\u5f88\u5bb9\u6613\u5bfc\u81f4\u68c0\u6d4b\u7f3a\u5931\u548c\u626d\u66f2\uff0c\u8fdb\u800c\u5bfc\u81f4\u5173\u8054\u7f3a\u5931\u548c\u9519\u8bef\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u91cd\u65b0\u5ba1\u89c6\u7ecf\u5178\u7684\u8ddf\u8e2a\u5668 DeepSORT\uff0c\u901a\u8fc7\u5728\u62e5\u6324\u548c\u906e\u6321\u573a\u666f\u4e2d\u68c0\u6d4b\u4e0d\u53ef\u7528\u6216\u68c0\u6d4b\u8d28\u91cf\u4f4e\u65f6\u66f4\u52a0\u4fe1\u4efb\u9884\u6d4b\uff0c\u663e\u7740\u589e\u5f3a\u5176\u5bf9\u4eba\u7fa4\u548c\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531\u4e09\u79cd\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u7b97\u6cd5\u7ec4\u6210\u7684\u65b0\u6846\u67b6\uff1a\u6982\u7387\u56fe\u3001\u9884\u6d4b\u56fe\u548c\u534f\u65b9\u5dee\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u3002\u6982\u7387\u56fe\u8bc6\u522b\u672a\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u662f\u5426\u786e\u5b9e\u4ece\u89c6\u91ce\u4e2d\u6d88\u5931\uff08\u4f8b\u5982\uff0c\u4ece\u56fe\u50cf\u4e2d\u6d88\u5931\u6216\u8fdb\u5165\u5efa\u7b51\u7269\uff09\uff0c\u6216\u8005\u53ea\u662f\u7531\u4e8e\u906e\u6321\u6216\u5176\u4ed6\u539f\u56e0\u800c\u6682\u65f6\u672a\u88ab\u68c0\u6d4b\u5230\u3002\u4ecd\u5728\u6982\u7387\u56fe\u4e2d\u7684\u672a\u68c0\u6d4b\u5230\u7684\u76ee\u6807\u7684\u8f68\u8ff9\u901a\u8fc7\u72b6\u6001\u4f30\u8ba1\u76f4\u63a5\u6269\u5c55\u3002\u9884\u6d4b\u56fe\u786e\u5b9a\u4e00\u4e2a\u5bf9\u8c61\u662f\u5426\u5728\u4eba\u7fa4\u4e2d\uff0c\u5f53\u89c2\u5bdf\u53d1\u751f\u4e25\u91cd\u53d8\u5f62\u65f6\uff0c\u6211\u4eec\u4f18\u5148\u8003\u8651\u72b6\u6001\u4f30\u8ba1\u800c\u4e0d\u662f\u89c2\u5bdf\uff0c\u8fd9\u662f\u901a\u8fc7\u534f\u65b9\u5dee\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5b8c\u6210\u7684\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u540d\u4e3a MapTrack\uff0c\u5728\u6d41\u884c\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6\uff08\u4f8b\u5982 MOT17 \u548c MOT20\uff09\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u5c3d\u7ba1\u6027\u80fd\u4f18\u8d8a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ecd\u7136\u7b80\u5355\u3001\u5728\u7ebf\u548c\u5b9e\u65f6\u3002\u540e\u7eed\u4ee3\u7801\u5c06\u4f1a\u5f00\u6e90\u3002|[2402.12968v1](http://arxiv.org/pdf/2402.12968v1)|null|\n", "2402.12946": "|**2024-02-20**|**Cell Graph Transformer for Nuclei Classification**|\u7528\u4e8e\u7ec6\u80de\u6838\u5206\u7c7b\u7684\u7ec6\u80de\u56fe\u8f6c\u6362\u5668|Wei Lou, Guanbin Li, Xiang Wan, Haofeng Li|Nuclei classification is a critical step in computer-aided diagnosis with histopathology images. In the past, various methods have employed graph neural networks (GNN) to analyze cell graphs that model inter-cell relationships by considering nuclei as vertices. However, they are limited by the GNN mechanism that only passes messages among local nodes via fixed edges. To address the issue, we develop a cell graph transformer (CGT) that treats nodes and edges as input tokens to enable learnable adjacency and information exchange among all nodes. Nevertheless, training the transformer with a cell graph presents another challenge. Poorly initialized features can lead to noisy self-attention scores and inferior convergence, particularly when processing the cell graphs with numerous connections. Thus, we further propose a novel topology-aware pretraining method that leverages a graph convolutional network (GCN) to learn a feature extractor. The pre-trained features may suppress unreasonable correlations and hence ease the finetuning of CGT. Experimental results suggest that the proposed cell graph transformer with topology-aware pretraining significantly improves the nuclei classification results, and achieves the state-of-the-art performance. Code and models are available at https://github.com/lhaof/CGT|\u7ec6\u80de\u6838\u5206\u7c7b\u662f\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7684\u5173\u952e\u6b65\u9aa4\u3002\u8fc7\u53bb\uff0c\u5404\u79cd\u65b9\u6cd5\u90fd\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6765\u5206\u6790\u7ec6\u80de\u56fe\uff0c\u901a\u8fc7\u5c06\u7ec6\u80de\u6838\u89c6\u4e3a\u9876\u70b9\u6765\u5efa\u6a21\u7ec6\u80de\u95f4\u5173\u7cfb\u3002\u7136\u800c\uff0c\u5b83\u4eec\u53d7\u5230 GNN \u673a\u5236\u7684\u9650\u5236\uff0c\u4ec5\u901a\u8fc7\u56fa\u5b9a\u8fb9\u5728\u672c\u5730\u8282\u70b9\u4e4b\u95f4\u4f20\u9012\u6d88\u606f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5355\u5143\u56fe\u8f6c\u6362\u5668\uff08CGT\uff09\uff0c\u5b83\u5c06\u8282\u70b9\u548c\u8fb9\u89c6\u4e3a\u8f93\u5165\u6807\u8bb0\uff0c\u4ee5\u5b9e\u73b0\u6240\u6709\u8282\u70b9\u4e4b\u95f4\u7684\u53ef\u5b66\u4e60\u90bb\u63a5\u548c\u4fe1\u606f\u4ea4\u6362\u3002\u7136\u800c\uff0c\u4f7f\u7528\u5355\u5143\u56fe\u8bad\u7ec3\u53d8\u538b\u5668\u63d0\u51fa\u4e86\u53e6\u4e00\u4e2a\u6311\u6218\u3002\u521d\u59cb\u5316\u4e0d\u5f53\u7684\u7279\u5f81\u53ef\u80fd\u4f1a\u5bfc\u81f4\u81ea\u6ce8\u610f\u529b\u5206\u6570\u5608\u6742\u548c\u6536\u655b\u6027\u8f83\u5dee\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5177\u6709\u5927\u91cf\u8fde\u63a5\u7684\u5355\u5143\u56fe\u65f6\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62d3\u6251\u611f\u77e5\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u6765\u5b66\u4e60\u7279\u5f81\u63d0\u53d6\u5668\u3002\u9884\u8bad\u7ec3\u7684\u7279\u5f81\u53ef\u4ee5\u6291\u5236\u4e0d\u5408\u7406\u7684\u76f8\u5173\u6027\uff0c\u4ece\u800c\u7b80\u5316 CGT \u7684\u5fae\u8c03\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5177\u6709\u62d3\u6251\u611f\u77e5\u9884\u8bad\u7ec3\u7684\u5355\u5143\u56fe\u8f6c\u6362\u5668\u663e\u7740\u6539\u5584\u4e86\u6838\u5206\u7c7b\u7ed3\u679c\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 https://github.com/lhaof/CGT \u83b7\u53d6|[2402.12946v1](http://arxiv.org/pdf/2402.12946v1)|**[link](https://github.com/lhaof/cgt)**|\n", "2402.12938": "|**2024-02-20**|**UniCell: Universal Cell Nucleus Classification via Prompt Learning**|UniCell\uff1a\u901a\u8fc7\u5feb\u901f\u5b66\u4e60\u8fdb\u884c\u901a\u7528\u7ec6\u80de\u6838\u5206\u7c7b|Junjia Huang, Haofeng Li, Xiang Wan, Guanbin Li|The recognition of multi-class cell nuclei can significantly facilitate the process of histopathological diagnosis. Numerous pathological datasets are currently available, but their annotations are inconsistent. Most existing methods require individual training on each dataset to deduce the relevant labels and lack the use of common knowledge across datasets, consequently restricting the quality of recognition. In this paper, we propose a universal cell nucleus classification framework (UniCell), which employs a novel prompt learning mechanism to uniformly predict the corresponding categories of pathological images from different dataset domains. In particular, our framework adopts an end-to-end architecture for nuclei detection and classification, and utilizes flexible prediction heads for adapting various datasets. Moreover, we develop a Dynamic Prompt Module (DPM) that exploits the properties of multiple datasets to enhance features. The DPM first integrates the embeddings of datasets and semantic categories, and then employs the integrated prompts to refine image representations, efficiently harvesting the shared knowledge among the related cell types and data sources. Experimental results demonstrate that the proposed method effectively achieves the state-of-the-art results on four nucleus detection and classification benchmarks. Code and models are available at https://github.com/lhaof/UniCell|\u591a\u7c7b\u7ec6\u80de\u6838\u7684\u8bc6\u522b\u53ef\u4ee5\u663e\u7740\u4fc3\u8fdb\u7ec4\u7ec7\u75c5\u7406\u5b66\u8bca\u65ad\u7684\u8fc7\u7a0b\u3002\u76ee\u524d\u6709\u8bb8\u591a\u75c5\u7406\u6570\u636e\u96c6\u53ef\u7528\uff0c\u4f46\u5b83\u4eec\u7684\u6ce8\u91ca\u4e0d\u4e00\u81f4\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5bf9\u6bcf\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u5355\u72ec\u8bad\u7ec3\u4ee5\u63a8\u65ad\u76f8\u5173\u6807\u7b7e\uff0c\u5e76\u4e14\u7f3a\u4e4f\u8de8\u6570\u636e\u96c6\u901a\u7528\u77e5\u8bc6\u7684\u4f7f\u7528\uff0c\u4ece\u800c\u9650\u5236\u4e86\u8bc6\u522b\u8d28\u91cf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7ec6\u80de\u6838\u5206\u7c7b\u6846\u67b6\uff08UniCell\uff09\uff0c\u5b83\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u63d0\u793a\u5b66\u4e60\u673a\u5236\u6765\u7edf\u4e00\u9884\u6d4b\u6765\u81ea\u4e0d\u540c\u6570\u636e\u96c6\u57df\u7684\u75c5\u7406\u56fe\u50cf\u7684\u76f8\u5e94\u7c7b\u522b\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u6846\u67b6\u91c7\u7528\u7aef\u5230\u7aef\u67b6\u6784\u8fdb\u884c\u6838\u68c0\u6d4b\u548c\u5206\u7c7b\uff0c\u5e76\u5229\u7528\u7075\u6d3b\u7684\u9884\u6d4b\u5934\u6765\u9002\u5e94\u5404\u79cd\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u52a8\u6001\u63d0\u793a\u6a21\u5757\uff08DPM\uff09\uff0c\u5b83\u5229\u7528\u591a\u4e2a\u6570\u636e\u96c6\u7684\u5c5e\u6027\u6765\u589e\u5f3a\u529f\u80fd\u3002 DPM \u9996\u5148\u96c6\u6210\u6570\u636e\u96c6\u548c\u8bed\u4e49\u7c7b\u522b\u7684\u5d4c\u5165\uff0c\u7136\u540e\u5229\u7528\u96c6\u6210\u7684\u63d0\u793a\u6765\u7ec6\u5316\u56fe\u50cf\u8868\u793a\uff0c\u6709\u6548\u5730\u6536\u83b7\u76f8\u5173\u7ec6\u80de\u7c7b\u578b\u548c\u6570\u636e\u6e90\u4e4b\u95f4\u7684\u5171\u4eab\u77e5\u8bc6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u56db\u4e2a\u6838\u68c0\u6d4b\u548c\u5206\u7c7b\u57fa\u51c6\u4e0a\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 https://github.com/lhaof/UniCell \u83b7\u53d6|[2402.12938v1](http://arxiv.org/pdf/2402.12938v1)|**[link](https://github.com/lhaof/unicell)**|\n", "2402.12923": "|**2024-02-20**|**Advancements in Point Cloud-Based 3D Defect Detection and Classification for Industrial Systems: A Comprehensive Survey**|\u5de5\u4e1a\u7cfb\u7edf\u57fa\u4e8e\u70b9\u4e91\u7684 3D \u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u7c7b\u7684\u8fdb\u5c55\uff1a\u5168\u9762\u8c03\u67e5|Anju Rani, Daniel Ortiz-Arroyo, Petar Durdevic|In recent years, 3D point clouds (PCs) have gained significant attention due to their diverse applications across various fields such as computer vision (CV), condition monitoring, virtual reality, robotics, autonomous driving etc. Deep learning (DL) has proven effective in leveraging 3D PCs to address various challenges previously encountered in 2D vision. However, the application of deep neural networks (DNN) to process 3D PCs presents its own set of challenges. To address these challenges, numerous methods have been proposed. This paper provides an in-depth review of recent advancements in DL-based condition monitoring (CM) using 3D PCs, with a specific focus on defect shape classification and segmentation within industrial applications for operational and maintenance purposes. Recognizing the crucial role of these aspects in industrial maintenance, the paper provides insightful observations that offer perspectives on the strengths and limitations of the reviewed DL-based PC processing methods. This synthesis of knowledge aims to contribute to the understanding and enhancement of CM processes, particularly within the framework of remaining useful life (RUL), in industrial systems.|\u8fd1\u5e74\u6765\uff0c3D\u70b9\u4e91\uff08PC\uff09\u56e0\u5176\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u3001\u72b6\u6001\u76d1\u6d4b\u3001\u865a\u62df\u73b0\u5b9e\u3001\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u5404\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u5df2\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\u5229\u7528 3D PC \u89e3\u51b3\u4ee5\u524d\u5728 2D \u89c6\u89c9\u4e2d\u9047\u5230\u7684\u5404\u79cd\u6311\u6218\u3002\u7136\u800c\uff0c\u5e94\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u5904\u7406 3D PC \u4e5f\u9762\u4e34\u7740\u4e00\u7cfb\u5217\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u591a\u79cd\u65b9\u6cd5\u3002\u672c\u6587\u6df1\u5165\u56de\u987e\u4e86\u4f7f\u7528 3D PC \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u72b6\u6001\u76d1\u6d4b (CM) \u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u5173\u6ce8\u5de5\u4e1a\u5e94\u7528\u4e2d\u7528\u4e8e\u64cd\u4f5c\u548c\u7ef4\u62a4\u76ee\u7684\u7684\u7f3a\u9677\u5f62\u72b6\u5206\u7c7b\u548c\u5206\u5272\u3002\u8ba4\u8bc6\u5230\u8fd9\u4e9b\u65b9\u9762\u5728\u5de5\u4e1a\u7ef4\u62a4\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u5bcc\u6709\u6d1e\u5bdf\u529b\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u4e3a\u6240\u5ba1\u67e5\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684 PC \u5904\u7406\u65b9\u6cd5\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u89c2\u70b9\u3002\u8fd9\u79cd\u77e5\u8bc6\u7efc\u5408\u65e8\u5728\u4fc3\u8fdb\u5bf9 CM \u8fc7\u7a0b\u7684\u7406\u89e3\u548c\u589e\u5f3a\uff0c\u7279\u522b\u662f\u5728\u5de5\u4e1a\u7cfb\u7edf\u7684\u5269\u4f59\u4f7f\u7528\u5bff\u547d (RUL) \u6846\u67b6\u5185\u3002|[2402.12923v1](http://arxiv.org/pdf/2402.12923v1)|null|\n", "2402.12843": "|**2024-02-20**|**SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets**|SolarPanel \u5206\u5272\uff1a\u4e0d\u5b8c\u7f8e\u6570\u636e\u96c6\u7684\u81ea\u6211\u76d1\u7763\u5b66\u4e60|Sankarshanaa Sagaram, Aditya Kasliwal, Krish Didwania, Laven Srivastava, Pallavi Kailas, Ujjwal Verma|The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency. This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning. We explore and apply Self-Supervised Learning (SSL) to solve these challenges. We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions.|\u592a\u9633\u80fd\u7684\u65e5\u76ca\u666e\u53ca\u9700\u8981\u5148\u8fdb\u7684\u76d1\u63a7\u548c\u7ef4\u62a4\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u592a\u9633\u80fd\u7535\u6c60\u677f\u5b89\u88c5\u7684\u6700\u4f73\u6027\u80fd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e00\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\u662f\u4ece\u822a\u7a7a\u6216\u536b\u661f\u56fe\u50cf\u4e2d\u51c6\u786e\u5206\u5272\u592a\u9633\u80fd\u7535\u6c60\u677f\uff0c\u8fd9\u5bf9\u4e8e\u8bc6\u522b\u64cd\u4f5c\u95ee\u9898\u548c\u8bc4\u4f30\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u89e3\u51b3\u4e86\u9762\u677f\u5206\u5272\u4e2d\u7684\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u6ce8\u91ca\u6570\u636e\u7684\u7a00\u7f3a\u6027\u4ee5\u53ca\u76d1\u7763\u5b66\u4e60\u7684\u624b\u52a8\u6ce8\u91ca\u7684\u52b3\u52a8\u5bc6\u96c6\u578b\u6027\u8d28\u3002\u6211\u4eec\u63a2\u7d22\u5e76\u5e94\u7528\u81ea\u6211\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002\u6211\u4eec\u8bc1\u660e SSL \u663e\u7740\u589e\u5f3a\u4e86\u5404\u79cd\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u624b\u52a8\u6ce8\u91ca\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3a\u7a33\u5065\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u592a\u9633\u80fd\u7535\u6c60\u677f\u5206\u5272\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2402.12843v1](http://arxiv.org/pdf/2402.12843v1)|null|\n", "2402.12800": "|**2024-02-20**|**Radar-Based Recognition of Static Hand Gestures in American Sign Language**|\u57fa\u4e8e\u96f7\u8fbe\u7684\u7f8e\u56fd\u624b\u8bed\u9759\u6001\u624b\u52bf\u8bc6\u522b|Christian Schuessler, Wenxuan Zhang, Johanna Br\u00e4unig, Marcel Hoffmann, Michael Stelzig, Martin Vossiek|In the fast-paced field of human-computer interaction (HCI) and virtual reality (VR), automatic gesture recognition has become increasingly essential. This is particularly true for the recognition of hand signs, providing an intuitive way to effortlessly navigate and control VR and HCI applications. Considering increased privacy requirements, radar sensors emerge as a compelling alternative to cameras. They operate effectively in low-light conditions without capturing identifiable human details, thanks to their lower resolution and distinct wavelength compared to visible light.   While previous works predominantly deploy radar sensors for dynamic hand gesture recognition based on Doppler information, our approach prioritizes classification using an imaging radar that operates on spatial information, e.g. image-like data. However, generating large training datasets required for neural networks (NN) is a time-consuming and challenging process, often falling short of covering all potential scenarios. Acknowledging these challenges, this study explores the efficacy of synthetic data generated by an advanced radar ray-tracing simulator. This simulator employs an intuitive material model that can be adjusted to introduce data diversity.   Despite exclusively training the NN on synthetic data, it demonstrates promising performance when put to the test with real measurement data. This emphasizes the practicality of our methodology in overcoming data scarcity challenges and advancing the field of automatic gesture recognition in VR and HCI applications.|\u5728\u5feb\u8282\u594f\u7684\u4eba\u673a\u4ea4\u4e92\uff08HCI\uff09\u548c\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u9886\u57df\uff0c\u81ea\u52a8\u624b\u52bf\u8bc6\u522b\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5bf9\u4e8e\u624b\u52bf\u8bc6\u522b\u6765\u8bf4\u5c24\u5176\u5982\u6b64\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u7684\u65b9\u5f0f\u6765\u8f7b\u677e\u5bfc\u822a\u548c\u63a7\u5236 VR \u548c HCI \u5e94\u7528\u7a0b\u5e8f\u3002\u8003\u8651\u5230\u65e5\u76ca\u589e\u957f\u7684\u9690\u79c1\u8981\u6c42\uff0c\u96f7\u8fbe\u4f20\u611f\u5668\u6210\u4e3a\u6444\u50cf\u5934\u7684\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u66ff\u4ee3\u54c1\u3002\u7531\u4e8e\u4e0e\u53ef\u89c1\u5149\u76f8\u6bd4\uff0c\u5b83\u4eec\u7684\u5206\u8fa8\u7387\u8f83\u4f4e\u4e14\u6ce2\u957f\u4e0d\u540c\uff0c\u56e0\u6b64\u5b83\u4eec\u53ef\u4ee5\u5728\u5f31\u5149\u6761\u4ef6\u4e0b\u6709\u6548\u8fd0\u884c\uff0c\u800c\u4e0d\u4f1a\u6355\u83b7\u53ef\u8bc6\u522b\u7684\u4eba\u4f53\u7ec6\u8282\u3002\u867d\u7136\u4e4b\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u90e8\u7f72\u96f7\u8fbe\u4f20\u611f\u5668\u6765\u57fa\u4e8e\u591a\u666e\u52d2\u4fe1\u606f\u8fdb\u884c\u52a8\u6001\u624b\u52bf\u8bc6\u522b\uff0c\u4f46\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u5148\u4f7f\u7528\u57fa\u4e8e\u7a7a\u95f4\u4fe1\u606f\uff08\u4f8b\u5982\uff0c\u7a7a\u95f4\u4fe1\u606f\uff09\u7684\u6210\u50cf\u96f7\u8fbe\u8fdb\u884c\u5206\u7c7b\u3002\u7c7b\u4f3c\u56fe\u50cf\u7684\u6570\u636e\u3002\u7136\u800c\uff0c\u751f\u6210\u795e\u7ecf\u7f51\u7edc (NN) \u6240\u9700\u7684\u5927\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u8017\u65f6\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u8fc7\u7a0b\uff0c\u901a\u5e38\u65e0\u6cd5\u8986\u76d6\u6240\u6709\u6f5c\u5728\u573a\u666f\u3002\u8ba4\u8bc6\u5230\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5148\u8fdb\u96f7\u8fbe\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u5668\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u7684\u529f\u6548\u3002\u8be5\u6a21\u62df\u5668\u91c7\u7528\u76f4\u89c2\u7684\u6750\u6599\u6a21\u578b\uff0c\u53ef\u4ee5\u8c03\u6574\u8be5\u6a21\u578b\u4ee5\u5f15\u5165\u6570\u636e\u591a\u6837\u6027\u3002\u5c3d\u7ba1\u4e13\u95e8\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u4f46\u5728\u4f7f\u7528\u771f\u5b9e\u6d4b\u91cf\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\u65f6\uff0c\u5b83\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002\u8fd9\u5f3a\u8c03\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u514b\u670d\u6570\u636e\u7a00\u7f3a\u6311\u6218\u548c\u63a8\u8fdb VR \u548c HCI \u5e94\u7528\u4e2d\u81ea\u52a8\u624b\u52bf\u8bc6\u522b\u9886\u57df\u7684\u5b9e\u7528\u6027\u3002|[2402.12800v1](http://arxiv.org/pdf/2402.12800v1)|null|\n", "2402.12765": "|**2024-02-20**|**GOOD: Towards Domain Generalized Orientated Object Detection**|\u597d\uff1a\u8fc8\u5411\u9886\u57df\u5e7f\u4e49\u76ee\u6807\u68c0\u6d4b|Qi Bi, Beichen Zhou, Jingjun Yi, Wei Ji, Haolan Zhan, Gui-Song Xia|Oriented object detection has been rapidly developed in the past few years, but most of these methods assume the training and testing images are under the same statistical distribution, which is far from reality. In this paper, we propose the task of domain generalized oriented object detection, which intends to explore the generalization of oriented object detectors on arbitrary unseen target domains. Learning domain generalized oriented object detectors is particularly challenging, as the cross-domain style variation not only negatively impacts the content representation, but also leads to unreliable orientation predictions. To address these challenges, we propose a generalized oriented object detector (GOOD). After style hallucination by the emerging contrastive language-image pre-training (CLIP), it consists of two key components, namely, rotation-aware content consistency learning (RAC) and style consistency learning (SEC). The proposed RAC allows the oriented object detector to learn stable orientation representation from style-diversified samples. The proposed SEC further stabilizes the generalization ability of content representation from different image styles. Extensive experiments on multiple cross-domain settings show the state-of-the-art performance of GOOD. Source code will be publicly available.|\u9762\u5411\u76ee\u6807\u68c0\u6d4b\u5728\u8fc7\u53bb\u51e0\u5e74\u4e2d\u5f97\u5230\u4e86\u8fc5\u901f\u53d1\u5c55\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5927\u591a\u6570\u90fd\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u56fe\u50cf\u5904\u4e8e\u76f8\u540c\u7684\u7edf\u8ba1\u5206\u5e03\u4e0b\uff0c\u8fd9\u4e0e\u73b0\u5b9e\u76f8\u53bb\u751a\u8fdc\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57df\u5e7f\u4e49\u5b9a\u5411\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\uff0c\u65e8\u5728\u63a2\u7d22\u5b9a\u5411\u5bf9\u8c61\u68c0\u6d4b\u5668\u5728\u4efb\u610f\u4e0d\u53ef\u89c1\u76ee\u6807\u57df\u4e0a\u7684\u6cdb\u5316\u3002\u5b66\u4e60\u9886\u57df\u5e7f\u4e49\u9762\u5411\u5bf9\u8c61\u68c0\u6d4b\u5668\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u8de8\u57df\u98ce\u683c\u53d8\u5316\u4e0d\u4ec5\u5bf9\u5185\u5bb9\u8868\u793a\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u800c\u4e14\u8fd8\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u65b9\u5411\u9884\u6d4b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684\u9762\u5411\u5bf9\u8c61\u68c0\u6d4b\u5668\uff08GOOD\uff09\u3002\u5728\u65b0\u5174\u7684\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u7684\u98ce\u683c\u5e7b\u89c9\u4e4b\u540e\uff0c\u5b83\u7531\u4e24\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\u7ec4\u6210\uff0c\u5373\u65cb\u8f6c\u611f\u77e5\u7684\u5185\u5bb9\u4e00\u81f4\u6027\u5b66\u4e60\uff08RAC\uff09\u548c\u98ce\u683c\u4e00\u81f4\u6027\u5b66\u4e60\uff08SEC\uff09\u3002\u6240\u63d0\u51fa\u7684 RAC \u5141\u8bb8\u5b9a\u5411\u5bf9\u8c61\u68c0\u6d4b\u5668\u4ece\u98ce\u683c\u591a\u6837\u5316\u7684\u6837\u672c\u4e2d\u5b66\u4e60\u7a33\u5b9a\u7684\u65b9\u5411\u8868\u793a\u3002\u6240\u63d0\u51fa\u7684 SEC \u8fdb\u4e00\u6b65\u7a33\u5b9a\u4e86\u4e0d\u540c\u56fe\u50cf\u98ce\u683c\u5185\u5bb9\u8868\u793a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5bf9\u591a\u4e2a\u8de8\u57df\u8bbe\u7f6e\u7684\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\u4e86 GOOD \u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u6e90\u4ee3\u7801\u5c06\u516c\u5f00\u3002|[2402.12765v1](http://arxiv.org/pdf/2402.12765v1)|null|\n", "2402.12763": "|**2024-02-20**|**BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization**|BronchoTrack\uff1a\u7528\u4e8e\u5206\u652f\u7ea7\u652f\u6c14\u7ba1\u955c\u5b9a\u4f4d\u7684\u6c14\u9053\u7ba1\u8154\u8ddf\u8e2a|Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Jinlin Wu, Jian Chen, Lujie Li, Hongbin Liu|Localizing the bronchoscope in real time is essential for ensuring intervention quality. However, most existing methods struggle to balance between speed and generalization. To address these challenges, we present BronchoTrack, an innovative real-time framework for accurate branch-level localization, encompassing lumen detection, tracking, and airway association.To achieve real-time performance, we employ a benchmark lightweight detector for efficient lumen detection. We are the first to introduce multi-object tracking to bronchoscopic localization, mitigating temporal confusion in lumen identification caused by rapid bronchoscope movement and complex airway structures. To ensure generalization across patient cases, we propose a training-free detection-airway association method based on a semantic airway graph that encodes the hierarchy of bronchial tree structures.Experiments on nine patient datasets demonstrate BronchoTrack's localization accuracy of 85.64 \\%, while accessing up to the 4th generation of airways.Furthermore, we tested BronchoTrack in an in-vivo animal study using a porcine model, where it successfully localized the bronchoscope into the 8th generation airway.Experimental evaluation underscores BronchoTrack's real-time performance in both satisfying accuracy and generalization, demonstrating its potential for clinical applications.|\u5b9e\u65f6\u5b9a\u4f4d\u652f\u6c14\u7ba1\u955c\u5bf9\u4e8e\u786e\u4fdd\u5e72\u9884\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u90fd\u96be\u4ee5\u5728\u901f\u5ea6\u548c\u6cdb\u5316\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 BronchoTrack\uff0c\u8fd9\u662f\u4e00\u79cd\u521b\u65b0\u7684\u5b9e\u65f6\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u7684\u5206\u652f\u7ea7\u5b9a\u4f4d\uff0c\u5305\u62ec\u7ba1\u8154\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u6c14\u9053\u5173\u8054\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u51c6\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u5668\u6765\u8fdb\u884c\u9ad8\u6548\u7684\u7ba1\u8154\u68c0\u6d4b\u3002\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u5c06\u591a\u76ee\u6807\u8ddf\u8e2a\u5f15\u5165\u652f\u6c14\u7ba1\u955c\u5b9a\u4f4d\u7684\u516c\u53f8\uff0c\u51cf\u8f7b\u56e0\u652f\u6c14\u7ba1\u955c\u5feb\u901f\u79fb\u52a8\u548c\u590d\u6742\u6c14\u9053\u7ed3\u6784\u5f15\u8d77\u7684\u7ba1\u8154\u8bc6\u522b\u4e2d\u7684\u65f6\u95f4\u6df7\u4e71\u3002\u4e3a\u4e86\u786e\u4fdd\u8de8\u60a3\u8005\u75c5\u4f8b\u7684\u6cdb\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u6c14\u9053\u56fe\u7684\u514d\u8bad\u7ec3\u68c0\u6d4b\u6c14\u9053\u5173\u8054\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9\u652f\u6c14\u7ba1\u6811\u7ed3\u6784\u7684\u5c42\u6b21\u7ed3\u6784\u8fdb\u884c\u7f16\u7801\u3002\u5bf9\u4e5d\u4e2a\u60a3\u8005\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBronchoTrack \u7684\u5b9a\u4f4d\u7cbe\u5ea6\u4e3a 85.64%\uff0c\u540c\u65f6\u8bbf\u95ee\u6700\u591a\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u732a\u6a21\u578b\u5728\u4f53\u5185\u52a8\u7269\u7814\u7a76\u4e2d\u6d4b\u8bd5\u4e86 BronchoTrack\uff0c\u6210\u529f\u5730\u5c06\u652f\u6c14\u7ba1\u955c\u5b9a\u4f4d\u5230\u7b2c 8 \u4ee3\u6c14\u9053\u4e2d\u3002\u5b9e\u9a8c\u8bc4\u4f30\u5f3a\u8c03\u4e86 BronchoTrack \u5728\u4ee4\u4eba\u6ee1\u610f\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002|[2402.12763v1](http://arxiv.org/pdf/2402.12763v1)|null|\n", "2402.12754": "|**2024-02-20**|**Fingerprint Presentation Attack Detector Using Global-Local Model**|\u4f7f\u7528\u5168\u5c40\u5c40\u90e8\u6a21\u578b\u7684\u6307\u7eb9\u5448\u73b0\u653b\u51fb\u68c0\u6d4b\u5668|Haozhe Liu, Wentian Zhang, Feng Liu, Haoqian Wu, Linlin Shen|The vulnerability of automated fingerprint recognition systems (AFRSs) to presentation attacks (PAs) promotes the vigorous development of PA detection (PAD) technology. However, PAD methods have been limited by information loss and poor generalization ability, resulting in new PA materials and fingerprint sensors. This paper thus proposes a global-local model-based PAD (RTK-PAD) method to overcome those limitations to some extent. The proposed method consists of three modules, called: 1) the global module; 2) the local module; and 3) the rethinking module. By adopting the cut-out-based global module, a global spoofness score predicted from nonlocal features of the entire fingerprint images can be achieved. While by using the texture in-painting-based local module, a local spoofness score predicted from fingerprint patches is obtained. The two modules are not independent but connected through our proposed rethinking module by localizing two discriminative patches for the local module based on the global spoofness score. Finally, the fusion spoofness score by averaging the global and local spoofness scores is used for PAD. Our experimental results evaluated on LivDet 2017 show that the proposed RTK-PAD can achieve an average classification error (ACE) of 2.28% and a true detection rate (TDR) of 91.19% when the false detection rate (FDR) equals 1.0%, which significantly outperformed the state-of-the-art methods by $\\sim$10% in terms of TDR (91.19% versus 80.74%).|\u81ea\u52a8\u6307\u7eb9\u8bc6\u522b\u7cfb\u7edf\uff08AFRS\uff09\u6613\u53d7\u6f14\u793a\u653b\u51fb\uff08PA\uff09\u7684\u5f71\u54cd\u4fc3\u8fdb\u4e86PA\u68c0\u6d4b\uff08PAD\uff09\u6280\u672f\u7684\u84ec\u52c3\u53d1\u5c55\u3002\u7136\u800c\uff0cPAD\u65b9\u6cd5\u53d7\u5230\u4fe1\u606f\u4e22\u5931\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u9650\u5236\uff0c\u4ece\u800c\u50ac\u751f\u4e86\u65b0\u7684PA\u6750\u6599\u548c\u6307\u7eb9\u4f20\u611f\u5668\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u5c40-\u5c40\u90e8\u6a21\u578b\u7684PAD\uff08RTK-PAD\uff09\u65b9\u6cd5\u6765\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\uff0c\u79f0\u4e3a\uff1a1\uff09\u5168\u5c40\u6a21\u5757\uff1b 2\uff09\u672c\u5730\u6a21\u5757\uff1b 3\uff09\u53cd\u601d\u6a21\u5757\u3002\u901a\u8fc7\u91c7\u7528\u57fa\u4e8e\u526a\u5207\u7684\u5168\u5c40\u6a21\u5757\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4ece\u6574\u4e2a\u6307\u7eb9\u56fe\u50cf\u7684\u975e\u5c40\u90e8\u7279\u5f81\u9884\u6d4b\u7684\u5168\u5c40\u6b3a\u9a97\u5206\u6570\u3002\u800c\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u7eb9\u7406\u4fee\u590d\u7684\u672c\u5730\u6a21\u5757\uff0c\u83b7\u5f97\u4e86\u4ece\u6307\u7eb9\u8865\u4e01\u9884\u6d4b\u7684\u672c\u5730\u6b3a\u9a97\u5206\u6570\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u4e0d\u662f\u72ec\u7acb\u7684\uff0c\u800c\u662f\u901a\u8fc7\u6211\u4eec\u63d0\u51fa\u7684\u91cd\u65b0\u601d\u8003\u6a21\u5757\u8fde\u63a5\u8d77\u6765\uff0c\u6839\u636e\u5168\u5c40\u6b3a\u9a97\u5206\u6570\u4e3a\u672c\u5730\u6a21\u5757\u5b9a\u4f4d\u4e24\u4e2a\u5224\u522b\u6027\u8865\u4e01\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5e73\u5747\u5168\u5c40\u548c\u5c40\u90e8\u6b3a\u9a97\u5206\u6570\u5f97\u5230\u7684\u878d\u5408\u6b3a\u9a97\u5206\u6570\u7528\u4e8e PAD\u3002\u6211\u4eec\u5728 LivDet 2017 \u4e0a\u8bc4\u4f30\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u9519\u8bef\u68c0\u6d4b\u7387\uff08FDR\uff09\u7b49\u4e8e 1.0% \u65f6\uff0c\u6240\u63d0\u51fa\u7684 RTK-PAD \u53ef\u4ee5\u5b9e\u73b0 2.28% \u7684\u5e73\u5747\u5206\u7c7b\u8bef\u5dee\uff08ACE\uff09\u548c 91.19% \u7684\u771f\u5b9e\u68c0\u6d4b\u7387\uff08TDR\uff09\uff0c\u8fd9\u5c31 TDR \u800c\u8a00\uff0c\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5 $\\sim$10%\uff0891.19% \u4e0e 80.74%\uff09\u3002|[2402.12754v1](http://arxiv.org/pdf/2402.12754v1)|null|\n", "2402.12736": "|**2024-02-20**|**CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning**|CST\uff1a\u53c2\u6570\u548c\u5185\u5b58\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\u7684\u6821\u51c6\u4fa7\u8c03|Feng Chen|Achieving a universally high accuracy in object detection is quite challenging, and the mainstream focus in the industry currently lies on detecting specific classes of objects. However, deploying one or multiple object detection networks requires a certain amount of GPU memory for training and storage capacity for inference. This presents challenges in terms of how to effectively coordinate multiple object detection tasks under resource-constrained conditions. This paper introduces a lightweight fine-tuning strategy called Calibration side tuning, which integrates aspects of adapter tuning and side tuning to adapt the successful techniques employed in transformers for use with ResNet. The Calibration side tuning architecture that incorporates maximal transition calibration, utilizing a small number of additional parameters to enhance network performance while maintaining a smooth training process. Furthermore, this paper has conducted an analysis on multiple fine-tuning strategies and have implemented their application within ResNet, thereby expanding the research on fine-tuning strategies for object detection networks. Besides, this paper carried out extensive experiments using five benchmark datasets. The experimental results demonstrated that this method outperforms other compared state-of-the-art techniques, and a better balance between the complexity and performance of the finetune schemes is achieved.|\u5728\u7269\u4f53\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u666e\u904d\u7684\u9ad8\u7cbe\u5ea6\u662f\u76f8\u5f53\u5177\u6709\u6311\u6218\u6027\u7684\uff0c\u76ee\u524d\u4e1a\u754c\u7684\u4e3b\u6d41\u7126\u70b9\u5728\u4e8e\u68c0\u6d4b\u7279\u5b9a\u7c7b\u522b\u7684\u7269\u4f53\u3002\u7136\u800c\uff0c\u90e8\u7f72\u4e00\u4e2a\u6216\u591a\u4e2a\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\u9700\u8981\u4e00\u5b9a\u91cf\u7684 GPU \u5185\u5b58\u7528\u4e8e\u8bad\u7ec3\u548c\u5b58\u50a8\u5bb9\u91cf\u7528\u4e8e\u63a8\u7406\u3002\u8fd9\u5bf9\u5982\u4f55\u5728\u8d44\u6e90\u6709\u9650\u7684\u6761\u4ef6\u4e0b\u6709\u6548\u534f\u8c03\u591a\u4e2a\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u63d0\u51fa\u4e86\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u6821\u51c6\u4fa7\u8c03\u6574\u201d\u7684\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u96c6\u6210\u4e86\u9002\u914d\u5668\u8c03\u6574\u548c\u4fa7\u8c03\u6574\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u4ee5\u9002\u5e94 Transformer \u4e2d\u91c7\u7528\u7684\u6210\u529f\u6280\u672f\uff0c\u4ee5\u4fbf\u4e0e ResNet \u4e00\u8d77\u4f7f\u7528\u3002\u6821\u51c6\u4fa7\u8c03\u6574\u67b6\u6784\u5305\u542b\u6700\u5927\u8f6c\u6362\u6821\u51c6\uff0c\u5229\u7528\u5c11\u91cf\u9644\u52a0\u53c2\u6570\u6765\u589e\u5f3a\u7f51\u7edc\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5e73\u7a33\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u672c\u6587\u5bf9\u591a\u79cd\u5fae\u8c03\u7b56\u7565\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u5728ResNet\u4e2d\u5b9e\u73b0\u4e86\u5b83\u4eec\u7684\u5e94\u7528\uff0c\u4ece\u800c\u62d3\u5c55\u4e86\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\u5fae\u8c03\u7b56\u7565\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u672c\u6587\u4f7f\u7528\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u6280\u672f\uff0c\u5e76\u4e14\u5728\u5fae\u8c03\u65b9\u6848\u7684\u590d\u6742\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002|[2402.12736v1](http://arxiv.org/pdf/2402.12736v1)|null|\n", "2402.12721": "|**2024-02-20**|**PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images**|PAC-FNO\uff1a\u7528\u4e8e\u8bc6\u522b\u4f4e\u8d28\u91cf\u56fe\u50cf\u7684\u5e76\u884c\u7ed3\u6784\u5168\u5206\u91cf\u5085\u7acb\u53f6\u795e\u7ecf\u7b97\u5b50|Jinsung Jeon, Hyundong Jin, Jonghyun Choi, Sanghyun Hong, Dongeun Lee, Kookjin Lee, Noseong Park|A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice. To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem. Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the original, downstream model. Moreover, the proposed PAC-FNO is ready to work with existing image recognition models. Extensively evaluating methods with seven image recognition benchmarks, we show that the proposed PAC-FNO improves the performance of existing baseline models on images with various resolutions by up to 77.1% and various types of natural variations in the images at inference.|\u5f00\u53d1\u56fe\u50cf\u8bc6\u522b\u6a21\u578b\u7684\u6807\u51c6\u505a\u6cd5\u662f\u5728\u7279\u5b9a\u56fe\u50cf\u5206\u8fa8\u7387\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u90e8\u7f72\u5b83\u3002\u7136\u800c\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u63a8\u7406\u4e2d\uff0c\u6a21\u578b\u7ecf\u5e38\u9047\u5230\u5206\u8fa8\u7387\u4e0e\u8bad\u7ec3\u96c6\u4e0d\u540c\u7684\u56fe\u50cf\u548c/\u6216\u53d7\u5230\u81ea\u7136\u53d8\u5316\uff08\u4f8b\u5982\u5929\u6c14\u53d8\u5316\u3001\u566a\u58f0\u7c7b\u578b\u548c\u538b\u7f29\u4f2a\u5f71\uff09\u7684\u5f71\u54cd\u3002\u867d\u7136\u4f20\u7edf\u7684\u89e3\u51b3\u65b9\u6848\u6d89\u53ca\u9488\u5bf9\u4e0d\u540c\u5206\u8fa8\u7387\u6216\u8f93\u5165\u53d8\u5316\u8bad\u7ec3\u591a\u4e2a\u6a21\u578b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u5f88\u9ad8\uff0c\u56e0\u6b64\u5728\u5b9e\u8df5\u4e2d\u65e0\u6cd5\u6269\u5c55\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5373\u5e76\u884c\u7ed3\u6784\u548c\u5168\u7ec4\u4ef6\u5085\u7acb\u53f6\u795e\u7ecf\u7b97\u5b50\uff08PAC-FNO\uff09\u6765\u89e3\u51b3\u8be5\u95ee\u9898\u3002\u4e0e\u4f20\u7edf\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u4e0d\u540c\uff0cPAC-FNO \u5728\u9891\u57df\u4e2d\u8fd0\u884c\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5355\u4e2a\u6a21\u578b\u4e2d\u5904\u7406\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bad\u7ec3 PAC-FNO \u7684\u4e24\u9636\u6bb5\u7b97\u6cd5\uff0c\u5bf9\u539f\u59cb\u4e0b\u6e38\u6a21\u578b\u8fdb\u884c\u4e86\u6700\u5c0f\u7684\u4fee\u6539\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684 PAC-FNO \u5df2\u51c6\u5907\u597d\u4e0e\u73b0\u6709\u7684\u56fe\u50cf\u8bc6\u522b\u6a21\u578b\u4e00\u8d77\u4f7f\u7528\u3002\u901a\u8fc7\u4e03\u4e2a\u56fe\u50cf\u8bc6\u522b\u57fa\u51c6\u5bf9\u65b9\u6cd5\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u53d1\u73b0\u6240\u63d0\u51fa\u7684 PAC-FNO \u5c06\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u5728\u5404\u79cd\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u4e0a\u4ee5\u53ca\u63a8\u7406\u65f6\u56fe\u50cf\u4e2d\u5404\u79cd\u7c7b\u578b\u7684\u81ea\u7136\u53d8\u5316\u7684\u6027\u80fd\u63d0\u9ad8\u4e86\u9ad8\u8fbe 77.1%\u3002|[2402.12721v1](http://arxiv.org/pdf/2402.12721v1)|null|\n", "2402.12706": "|**2024-02-20**|**Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition**|\u5b66\u4e60\u7528\u4e8e\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u7684\u57df\u4e0d\u53d8\u65f6\u95f4\u52a8\u529b\u5b66|Yuke Li, Guangyi Chen, Ben Abramowitz, Stefano Anzellott, Donglai Wei|Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples. Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model. Our central hypothesis is that temporal invariance in the dynamic system between latent variables lends itself to transferability (domain-invariance). We therefore propose DITeD, or Domain-Invariant Temporal Dynamics for knowledge transfer. To detect the temporal invariance part, we propose a generative framework with a two-stage training strategy during pre-training. Specifically, we explicitly model invariant dynamics including temporal dynamic generation and transitions, and the variant visual and domain encoders. Then we pre-train the model with the self-supervised signals to learn the representation. After that, we fix the whole representation model and tune the classifier. During adaptation, we fix the transferable temporal dynamics and update the image encoder. The efficacy of our approach is revealed by the superior accuracy of DITeD over leading alternatives across standard few-shot action recognition datasets. Moreover, we validate that the learned temporal dynamic transition and temporal dynamic generation modules possess transferable qualities.|\u5c11\u955c\u5934\u52a8\u4f5c\u8bc6\u522b\u65e8\u5728\u4ec5\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u6837\u672c\u5feb\u901f\u4f7f\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u5177\u6709\u5206\u5e03\u53d8\u5316\u7684\u65b0\u6570\u636e\u3002\u4e3b\u8981\u6311\u6218\u5305\u62ec\u5982\u4f55\u8bc6\u522b\u548c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5b66\u5230\u7684\u53ef\u8f6c\u79fb\u77e5\u8bc6\u3002\u6211\u4eec\u7684\u4e2d\u5fc3\u5047\u8bbe\u662f\uff0c\u52a8\u6001\u7cfb\u7edf\u4e2d\u6f5c\u5728\u53d8\u91cf\u4e4b\u95f4\u7684\u65f6\u95f4\u4e0d\u53d8\u6027\u6709\u52a9\u4e8e\u53ef\u8f6c\u79fb\u6027\uff08\u57df\u4e0d\u53d8\u6027\uff09\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DITeD\uff0c\u5373\u57df\u4e0d\u53d8\u65f6\u95f4\u52a8\u529b\u5b66\u6765\u8fdb\u884c\u77e5\u8bc6\u8f6c\u79fb\u3002\u4e3a\u4e86\u68c0\u6d4b\u65f6\u95f4\u4e0d\u53d8\u6027\u90e8\u5206\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u7684\u751f\u6210\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u660e\u786e\u5730\u6a21\u62df\u4e0d\u53d8\u7684\u52a8\u6001\uff0c\u5305\u62ec\u65f6\u95f4\u52a8\u6001\u751f\u6210\u548c\u8f6c\u6362\uff0c\u4ee5\u53ca\u53d8\u4f53\u89c6\u89c9\u548c\u57df\u7f16\u7801\u5668\u3002\u7136\u540e\u6211\u4eec\u7528\u81ea\u76d1\u7763\u4fe1\u53f7\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u5b66\u4e60\u8868\u793a\u3002\u4e4b\u540e\uff0c\u6211\u4eec\u4fee\u590d\u6574\u4e2a\u8868\u793a\u6a21\u578b\u5e76\u8c03\u6574\u5206\u7c7b\u5668\u3002\u5728\u9002\u5e94\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4fee\u590d\u4e86\u53ef\u4f20\u8f93\u7684\u65f6\u95f4\u52a8\u6001\u5e76\u66f4\u65b0\u56fe\u50cf\u7f16\u7801\u5668\u3002 DITeD \u5728\u6807\u51c6\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\u4e2d\u4f18\u4e8e\u9886\u5148\u66ff\u4ee3\u65b9\u6848\u7684\u51c6\u786e\u6027\u63ed\u793a\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u5b66\u4e60\u5230\u7684\u65f6\u95f4\u52a8\u6001\u8f6c\u6362\u548c\u65f6\u95f4\u52a8\u6001\u751f\u6210\u6a21\u5757\u5177\u6709\u53ef\u8f6c\u79fb\u7684\u54c1\u8d28\u3002|[2402.12706v1](http://arxiv.org/pdf/2402.12706v1)|null|\n", "2402.12701": "|**2024-02-20**|**wmh_seg: Transformer based U-Net for Robust and Automatic White Matter Hyperintensity Segmentation across 1.5T, 3T and 7T**|wmh_seg\uff1a\u57fa\u4e8e Transformer \u7684 U-Net\uff0c\u7528\u4e8e 1.5T\u30013T \u548c 7T \u7684\u9c81\u68d2\u548c\u81ea\u52a8\u767d\u8d28\u9ad8\u4fe1\u53f7\u5206\u5272|Jinghang Li, Tales Santini, Yuanzhe Huang, Joseph M. Mettenburg, Tamer S. Ibrahima, Howard J. Aizensteina, Minjie Wu|White matter hyperintensity (WMH) remains the top imaging biomarker for neurodegenerative diseases. Robust and accurate segmentation of WMH holds paramount significance for neuroimaging studies. The growing shift from 3T to 7T MRI necessitates robust tools for harmonized segmentation across field strengths and artifacts. Recent deep learning models exhibit promise in WMH segmentation but still face challenges, including diverse training data representation and limited analysis of MRI artifacts' impact. To address these, we introduce wmh_seg, a novel deep learning model leveraging a transformer-based encoder from SegFormer. wmh_seg is trained on an unmatched dataset, including 1.5T, 3T, and 7T FLAIR images from various sources, alongside with artificially added MR artifacts. Our approach bridges gaps in training diversity and artifact analysis. Our model demonstrated stable performance across magnetic field strengths, scanner manufacturers, and common MR imaging artifacts. Despite the unique inhomogeneity artifacts on ultra-high field MR images, our model still offers robust and stable segmentation on 7T FLAIR images. Our model, to date, is the first that offers quality white matter lesion segmentation on 7T FLAIR images.|\u767d\u8d28\u9ad8\u4fe1\u53f7\uff08WMH\uff09\u4ecd\u7136\u662f\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u9996\u8981\u5f71\u50cf\u751f\u7269\u6807\u5fd7\u7269\u3002\u7a33\u5065\u800c\u51c6\u786e\u7684 WMH \u5206\u5272\u5bf9\u4e8e\u795e\u7ecf\u5f71\u50cf\u5b66\u7814\u7a76\u5177\u6709\u81f3\u5173\u91cd\u8981\u7684\u610f\u4e49\u3002\u4ece 3T MRI \u5230 7T MRI \u7684\u4e0d\u65ad\u8f6c\u53d8\u9700\u8981\u5f3a\u5927\u7684\u5de5\u5177\u6765\u534f\u8c03\u573a\u5f3a\u548c\u4f2a\u5f71\u7684\u5206\u5272\u3002\u6700\u8fd1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728 WMH \u5206\u5272\u65b9\u9762\u5c55\u73b0\u4e86\u524d\u666f\uff0c\u4f46\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u8868\u793a\u548c\u5bf9 MRI \u4f2a\u5f71\u5f71\u54cd\u7684\u6709\u9650\u5206\u6790\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 wmh_seg\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528 SegFormer \u57fa\u4e8e\u8f6c\u6362\u5668\u7684\u7f16\u7801\u5668\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002 wmh_seg \u5728\u65e0\u4e0e\u4f26\u6bd4\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5305\u62ec\u6765\u81ea\u5404\u79cd\u6765\u6e90\u7684 1.5T\u30013T \u548c 7T FLAIR \u56fe\u50cf\uff0c\u4ee5\u53ca\u4eba\u5de5\u6dfb\u52a0\u7684 MR \u4f2a\u50cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f25\u8865\u4e86\u8bad\u7ec3\u591a\u6837\u6027\u548c\u5de5\u4ef6\u5206\u6790\u65b9\u9762\u7684\u5dee\u8ddd\u3002\u6211\u4eec\u7684\u6a21\u578b\u5728\u78c1\u573a\u5f3a\u5ea6\u3001\u626b\u63cf\u4eea\u5236\u9020\u5546\u548c\u5e38\u89c1 MR \u6210\u50cf\u4f2a\u5f71\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u5c3d\u7ba1\u8d85\u9ad8\u573a MR \u56fe\u50cf\u4e0a\u5b58\u5728\u72ec\u7279\u7684\u4e0d\u5747\u5300\u4f2a\u5f71\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4ecd\u7136\u53ef\u4ee5\u5728 7T FLAIR \u56fe\u50cf\u4e0a\u63d0\u4f9b\u7a33\u5065\u4e14\u7a33\u5b9a\u7684\u5206\u5272\u3002\u8fc4\u4eca\u4e3a\u6b62\uff0c\u6211\u4eec\u7684\u6a21\u578b\u662f\u7b2c\u4e00\u4e2a\u5728 7T FLAIR \u56fe\u50cf\u4e0a\u63d0\u4f9b\u9ad8\u8d28\u91cf\u767d\u8d28\u75c5\u53d8\u5206\u5272\u7684\u6a21\u578b\u3002|[2402.12701v1](http://arxiv.org/pdf/2402.12701v1)|null|\n", "2402.12683": "|**2024-02-20**|**TorchCP: A Library for Conformal Prediction based on PyTorch**|TorchCP\uff1a\u57fa\u4e8e PyTorch \u7684\u5171\u5f62\u9884\u6d4b\u5e93|Hongxin Wei, Jianguo Huang|TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\\href{https://github.com/ml-stat-Sustech/TorchCP}{\\text{this https URL}}$.|TorchCP \u662f\u4e00\u4e2a\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4fdd\u5f62\u9884\u6d4b\u7814\u7a76\u7684 Python \u5de5\u5177\u7bb1\u3002\u5b83\u5305\u542b\u7528\u4e8e\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\uff08\u5305\u62ec\u591a\u7ef4\u8f93\u51fa\uff09\u7684\u4e8b\u540e\u548c\u8bad\u7ec3\u65b9\u6cd5\u7684\u5404\u79cd\u5b9e\u73b0\u3002 TorchCP \u57fa\u4e8e PyTorch (Paszke et al., 2019) \u6784\u5efa\uff0c\u5229\u7528\u77e9\u9635\u8ba1\u7b97\u7684\u4f18\u52bf\u63d0\u4f9b\u7b80\u6d01\u9ad8\u6548\u7684\u63a8\u7406\u5b9e\u73b0\u3002\u8be5\u4ee3\u7801\u6839\u636e LGPL \u8bb8\u53ef\u8bc1\u83b7\u5f97\u8bb8\u53ef\uff0c\u5e76\u5728 $\\href{https://github.com/ml-stat-Sustech/TorchCP}{\\text{this https URL}}$ \u4e0a\u5f00\u6e90\u3002|[2402.12683v1](http://arxiv.org/pdf/2402.12683v1)|**[link](https://github.com/ml-stat-sustech/torchcp)**|\n", "2402.12677": "|**2024-02-20**|**Object-level Geometric Structure Preserving for Natural Image Stitching**|\u81ea\u7136\u56fe\u50cf\u62fc\u63a5\u7684\u5bf9\u8c61\u7ea7\u51e0\u4f55\u7ed3\u6784\u4fdd\u7559|Wenxiao Cai, Wankou Yang|The topic of stitching images with globally natural structures holds paramount significance. Current methodologies exhibit the ability to preserve local geometric structures, yet fall short in maintaining relationships between these geometric structures. In this paper, we endeavor to safeguard the overall, OBJect-level structures within images based on Global Similarity Prior, while concurrently mitigating distortion and ghosting artifacts with OBJ-GSP. Our approach leverages the Segment Anything Model to extract geometric structures with semantic information, enhancing the algorithm's ability to preserve objects in a manner that aligns more intuitively with human perception. We seek to identify spatial constraints that govern the relationships between various geometric boundaries. Recognizing that multiple geometric boundaries collectively define complete objects, we employ triangular meshes to safeguard not only individual geometric structures but also the overall shapes of objects within the images. Empirical evaluations across multiple image stitching datasets demonstrate that our method establishes a new state-of-the-art benchmark in image stitching. Our implementation and dataset is publicly available at https://github.com/RussRobin/OBJ-GSP .|\u5c06\u56fe\u50cf\u4e0e\u5168\u5c40\u81ea\u7136\u7ed3\u6784\u62fc\u63a5\u7684\u4e3b\u9898\u5177\u6709\u81f3\u5173\u91cd\u8981\u7684\u610f\u4e49\u3002\u5f53\u524d\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u4fdd\u7559\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u4f46\u5728\u7ef4\u6301\u8fd9\u4e9b\u51e0\u4f55\u7ed3\u6784\u4e4b\u95f4\u7684\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u81f4\u529b\u4e8e\u57fa\u4e8e\u5168\u5c40\u76f8\u4f3c\u5148\u9a8c\u4fdd\u62a4\u56fe\u50cf\u5185\u7684\u6574\u4f53\u5bf9\u8c61\u7ea7\u7ed3\u6784\uff0c\u540c\u65f6\u4f7f\u7528 OBJ-GSP \u51cf\u8f7b\u5931\u771f\u548c\u91cd\u5f71\u4f2a\u5f71\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u5206\u6bb5\u4efb\u610f\u6a21\u578b\u6765\u63d0\u53d6\u5177\u6709\u8bed\u4e49\u4fe1\u606f\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u4ece\u800c\u589e\u5f3a\u7b97\u6cd5\u4ee5\u66f4\u76f4\u89c2\u5730\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u65b9\u5f0f\u4fdd\u7559\u5bf9\u8c61\u7684\u80fd\u529b\u3002\u6211\u4eec\u5bfb\u6c42\u8bc6\u522b\u63a7\u5236\u5404\u79cd\u51e0\u4f55\u8fb9\u754c\u4e4b\u95f4\u5173\u7cfb\u7684\u7a7a\u95f4\u7ea6\u675f\u3002\u8ba4\u8bc6\u5230\u591a\u4e2a\u51e0\u4f55\u8fb9\u754c\u5171\u540c\u5b9a\u4e49\u4e86\u5b8c\u6574\u7684\u5bf9\u8c61\uff0c\u6211\u4eec\u91c7\u7528\u4e09\u89d2\u5f62\u7f51\u683c\u4e0d\u4ec5\u53ef\u4ee5\u4fdd\u62a4\u5355\u4e2a\u51e0\u4f55\u7ed3\u6784\uff0c\u8fd8\u53ef\u4ee5\u4fdd\u62a4\u56fe\u50cf\u4e2d\u5bf9\u8c61\u7684\u6574\u4f53\u5f62\u72b6\u3002\u5bf9\u591a\u4e2a\u56fe\u50cf\u62fc\u63a5\u6570\u636e\u96c6\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u56fe\u50cf\u62fc\u63a5\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u57fa\u51c6\u3002\u6211\u4eec\u7684\u5b9e\u73b0\u548c\u6570\u636e\u96c6\u53ef\u5728 https://github.com/RussRobin/OBJ-GSP \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2402.12677v1](http://arxiv.org/pdf/2402.12677v1)|**[link](https://github.com/russrobin/obj-gsp)**|\n", "2402.12644": "|**2024-02-20**|**Neuromorphic Synergy for Video Binarization**|\u89c6\u9891\u4e8c\u503c\u5316\u7684\u795e\u7ecf\u5f62\u6001\u534f\u540c|Shijie Lin, Xiang Zhang, Lei Yang, Lei Yu, Bin Zhou, Xiaowei Luo, Wenping Wang, Jia Pan|Bimodal objects, such as the checkerboard pattern used in camera calibration, markers for object tracking, and text on road signs, to name a few, are prevalent in our daily lives and serve as a visual form to embed information that can be easily recognized by vision systems. While binarization from intensity images is crucial for extracting the embedded information in the bimodal objects, few previous works consider the task of binarization of blurry images due to the relative motion between the vision sensor and the environment. The blurry images can result in a loss in the binarization quality and thus degrade the downstream applications where the vision system is in motion. Recently, neuromorphic cameras offer new capabilities for alleviating motion blur, but it is non-trivial to first deblur and then binarize the images in a real-time manner. In this work, we propose an event-based binary reconstruction method that leverages the prior knowledge of the bimodal target's properties to perform inference independently in both event space and image space and merge the results from both domains to generate a sharp binary image. We also develop an efficient integration method to propagate this binary image to high frame rate binary video. Finally, we develop a novel method to naturally fuse events and images for unsupervised threshold identification. The proposed method is evaluated in publicly available and our collected data sequence, and shows the proposed method can outperform the SOTA methods to generate high frame rate binary video in real-time on CPU-only devices.|\u53cc\u5cf0\u5bf9\u8c61\uff0c\u4f8b\u5982\u7528\u4e8e\u76f8\u673a\u6821\u51c6\u7684\u68cb\u76d8\u56fe\u6848\u3001\u7528\u4e8e\u5bf9\u8c61\u8ddf\u8e2a\u7684\u6807\u8bb0\u4ee5\u53ca\u8def\u6807\u4e0a\u7684\u6587\u672c\u7b49\uff0c\u5728\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\u4e2d\u5f88\u666e\u904d\uff0c\u5e76\u4e14\u4f5c\u4e3a\u4e00\u79cd\u89c6\u89c9\u5f62\u5f0f\u6765\u5d4c\u5165\u53ef\u4ee5\u8f7b\u677e\u8bc6\u522b\u7684\u4fe1\u606f\u3002\u89c6\u89c9\u7cfb\u7edf\u3002\u867d\u7136\u5f3a\u5ea6\u56fe\u50cf\u7684\u4e8c\u503c\u5316\u5bf9\u4e8e\u63d0\u53d6\u53cc\u5cf0\u5bf9\u8c61\u4e2d\u7684\u5d4c\u5165\u4fe1\u606f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e4b\u524d\u5f88\u5c11\u6709\u5de5\u4f5c\u8003\u8651\u7531\u4e8e\u89c6\u89c9\u4f20\u611f\u5668\u548c\u73af\u5883\u4e4b\u95f4\u7684\u76f8\u5bf9\u8fd0\u52a8\u800c\u5bfc\u81f4\u7684\u6a21\u7cca\u56fe\u50cf\u7684\u4e8c\u503c\u5316\u4efb\u52a1\u3002\u6a21\u7cca\u7684\u56fe\u50cf\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e8c\u503c\u5316\u8d28\u91cf\u4e0b\u964d\uff0c\u4ece\u800c\u964d\u4f4e\u89c6\u89c9\u7cfb\u7edf\u8fd0\u884c\u7684\u4e0b\u6e38\u5e94\u7528\u7684\u6027\u80fd\u3002\u6700\u8fd1\uff0c\u795e\u7ecf\u5f62\u6001\u76f8\u673a\u63d0\u4f9b\u4e86\u51cf\u8f7b\u8fd0\u52a8\u6a21\u7cca\u7684\u65b0\u529f\u80fd\uff0c\u4f46\u9996\u5148\u5bf9\u56fe\u50cf\u8fdb\u884c\u53bb\u6a21\u7cca\u7136\u540e\u4ee5\u5b9e\u65f6\u65b9\u5f0f\u4e8c\u503c\u5316\u5e76\u975e\u6613\u4e8b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u7684\u4e8c\u503c\u91cd\u5efa\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u53cc\u5cf0\u76ee\u6807\u5c5e\u6027\u7684\u5148\u9a8c\u77e5\u8bc6\u5728\u4e8b\u4ef6\u7a7a\u95f4\u548c\u56fe\u50cf\u7a7a\u95f4\u4e2d\u72ec\u7acb\u5730\u6267\u884c\u63a8\u7406\uff0c\u5e76\u5c06\u4e24\u4e2a\u57df\u7684\u7ed3\u679c\u5408\u5e76\u4ee5\u751f\u6210\u6e05\u6670\u7684\u4e8c\u503c\u56fe\u50cf\u3002\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u96c6\u6210\u65b9\u6cd5\u6765\u5c06\u8be5\u4e8c\u8fdb\u5236\u56fe\u50cf\u4f20\u64ad\u5230\u9ad8\u5e27\u7387\u4e8c\u8fdb\u5236\u89c6\u9891\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u81ea\u7136\u5730\u878d\u5408\u4e8b\u4ef6\u548c\u56fe\u50cf\u4ee5\u8fdb\u884c\u65e0\u76d1\u7763\u9608\u503c\u8bc6\u522b\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u516c\u5f00\u53ef\u7528\u7684\u548c\u6211\u4eec\u6536\u96c6\u7684\u6570\u636e\u5e8f\u5217\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u4f18\u4e8e SOTA \u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u7eaf CPU \u8bbe\u5907\u4e0a\u5b9e\u65f6\u751f\u6210\u9ad8\u5e27\u7387\u4e8c\u8fdb\u5236\u89c6\u9891\u3002|[2402.12644v1](http://arxiv.org/pdf/2402.12644v1)|**[link](https://github.com/eleboss/ebr)**|\n", "2402.12641": "|**2024-02-20**|**YOLO-Ant: A Lightweight Detector via Depthwise Separable Convolutional and Large Kernel Design for Antenna Interference Source Detection**|YOLO-Ant\uff1a\u91c7\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u548c\u5927\u5185\u6838\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u5668\uff0c\u7528\u4e8e\u5929\u7ebf\u5e72\u6270\u6e90\u68c0\u6d4b|Xiaoyu Tang, Xingming Chen, Jintao Cheng, Jin Wu, Rui Fan, Chengxi Zhang, Zebo Zhou|In the era of 5G communication, removing interference sources that affect communication is a resource-intensive task. The rapid development of computer vision has enabled unmanned aerial vehicles to perform various high-altitude detection tasks. Because the field of object detection for antenna interference sources has not been fully explored, this industry lacks dedicated learning samples and detection models for this specific task. In this article, an antenna dataset is created to address important antenna interference source detection issues and serves as the basis for subsequent research. We introduce YOLO-Ant, a lightweight CNN and transformer hybrid detector specifically designed for antenna interference source detection. Specifically, we initially formulated a lightweight design for the network depth and width, ensuring that subsequent investigations were conducted within a lightweight framework. Then, we propose a DSLK-Block module based on depthwise separable convolution and large convolution kernels to enhance the network's feature extraction ability, effectively improving small object detection. To address challenges such as complex backgrounds and large interclass differences in antenna detection, we construct DSLKVit-Block, a powerful feature extraction module that combines DSLK-Block and transformer structures. Considering both its lightweight design and accuracy, our method not only achieves optimal performance on the antenna dataset but also yields competitive results on public datasets.|\u57285G\u901a\u4fe1\u65f6\u4ee3\uff0c\u53bb\u9664\u5f71\u54cd\u901a\u4fe1\u7684\u5e72\u6270\u6e90\u662f\u4e00\u9879\u8d44\u6e90\u5bc6\u96c6\u578b\u4efb\u52a1\u3002\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u65e0\u4eba\u673a\u80fd\u591f\u6267\u884c\u5404\u79cd\u9ad8\u7a7a\u63a2\u6d4b\u4efb\u52a1\u3002\u7531\u4e8e\u5929\u7ebf\u5e72\u6270\u6e90\u7684\u7269\u4f53\u68c0\u6d4b\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u8be5\u884c\u4e1a\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u8be5\u7279\u5b9a\u4efb\u52a1\u7684\u5b66\u4e60\u6837\u672c\u548c\u68c0\u6d4b\u6a21\u578b\u3002\u672c\u6587\u521b\u5efa\u4e86\u4e00\u4e2a\u5929\u7ebf\u6570\u636e\u96c6\u6765\u89e3\u51b3\u91cd\u8981\u7684\u5929\u7ebf\u5e72\u6270\u6e90\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u4f5c\u4e3a\u540e\u7eed\u7814\u7a76\u7684\u57fa\u7840\u3002\u6211\u4eec\u63a8\u51fa YOLO-Ant\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7 CNN \u548c\u53d8\u538b\u5668\u6df7\u5408\u68c0\u6d4b\u5668\uff0c\u4e13\u4e3a\u5929\u7ebf\u5e72\u6270\u6e90\u68c0\u6d4b\u800c\u8bbe\u8ba1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u6700\u521d\u5bf9\u7f51\u7edc\u6df1\u5ea6\u548c\u5bbd\u5ea6\u8fdb\u884c\u4e86\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\uff0c\u786e\u4fdd\u540e\u7eed\u7814\u7a76\u5728\u8f7b\u91cf\u7ea7\u6846\u67b6\u5185\u8fdb\u884c\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u548c\u5927\u5377\u79ef\u6838\u7684DSLK-Block\u6a21\u5757\u6765\u589e\u5f3a\u7f51\u7edc\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u6709\u6548\u63d0\u9ad8\u5c0f\u76ee\u6807\u68c0\u6d4b\u3002\u4e3a\u4e86\u89e3\u51b3\u5929\u7ebf\u68c0\u6d4b\u4e2d\u590d\u6742\u80cc\u666f\u548c\u8f83\u5927\u7c7b\u95f4\u5dee\u5f02\u7b49\u6311\u6218\uff0c\u6211\u4eec\u6784\u5efa\u4e86 DSLKVit-Block\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86 DSLK-Block \u548c Transformer \u7ed3\u6784\u7684\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u3002\u8003\u8651\u5230\u5176\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u548c\u51c6\u786e\u6027\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u5728\u5929\u7ebf\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u800c\u4e14\u8fd8\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u4ea7\u751f\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002|[2402.12641v1](http://arxiv.org/pdf/2402.12641v1)|**[link](https://github.com/scnu-rislab/yolo-ant)**|\n"}, "OCR": {}, "GNN": {"2402.12675": "|**2024-02-20**|**Visual Reasoning in Object-Centric Deep Neural Networks: A Comparative Cognition Approach**|\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u89c6\u89c9\u63a8\u7406\uff1a\u4e00\u79cd\u6bd4\u8f83\u8ba4\u77e5\u65b9\u6cd5|Guillermo Puebla, Jeffrey S. Bowers|Achieving visual reasoning is a long-term goal of artificial intelligence. In the last decade, several studies have applied deep neural networks (DNNs) to the task of learning visual relations from images, with modest results in terms of generalization of the relations learned. However, in recent years, object-centric representation learning has been put forward as a way to achieve visual reasoning within the deep learning framework. Object-centric models attempt to model input scenes as compositions of objects and relations between them. To this end, these models use several kinds of attention mechanisms to segregate the individual objects in a scene from the background and from other objects. In this work we tested relation learning and generalization in several object-centric models, as well as a ResNet-50 baseline. In contrast to previous research, which has focused heavily in the same-different task in order to asses relational reasoning in DNNs, we use a set of tasks -- with varying degrees of difficulty -- derived from the comparative cognition literature. Our results show that object-centric models are able to segregate the different objects in a scene, even in many out-of-distribution cases. In our simpler tasks, this improves their capacity to learn and generalize visual relations in comparison to the ResNet-50 baseline. However, object-centric models still struggle in our more difficult tasks and conditions. We conclude that abstract visual reasoning remains an open challenge for DNNs, including object-centric models.|\u5b9e\u73b0\u89c6\u89c9\u63a8\u7406\u662f\u4eba\u5de5\u667a\u80fd\u7684\u957f\u671f\u76ee\u6807\u3002\u5728\u8fc7\u53bb\u7684\u5341\u5e74\u4e2d\uff0c\u4e00\u4e9b\u7814\u7a76\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5e94\u7528\u4e8e\u4ece\u56fe\u50cf\u4e2d\u5b66\u4e60\u89c6\u89c9\u5173\u7cfb\u7684\u4efb\u52a1\uff0c\u5728\u6240\u5b66\u5173\u7cfb\u7684\u6cdb\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u8fd1\u5e74\u6765\uff0c\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8868\u793a\u5b66\u4e60\u88ab\u63d0\u51fa\u4f5c\u4e3a\u5728\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5185\u5b9e\u73b0\u89c6\u89c9\u63a8\u7406\u7684\u4e00\u79cd\u65b9\u6cd5\u3002\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u5c1d\u8bd5\u5c06\u8f93\u5165\u573a\u666f\u5efa\u6a21\u4e3a\u5bf9\u8c61\u7684\u7ec4\u5408\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u4e3a\u6b64\uff0c\u8fd9\u4e9b\u6a21\u578b\u4f7f\u7528\u591a\u79cd\u6ce8\u610f\u673a\u5236\u5c06\u573a\u666f\u4e2d\u7684\u5404\u4e2a\u5bf9\u8c61\u4e0e\u80cc\u666f\u548c\u5176\u4ed6\u5bf9\u8c61\u5206\u5f00\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5728\u51e0\u4e2a\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u4ee5\u53ca ResNet-50 \u57fa\u7ebf\u4e2d\u6d4b\u8bd5\u4e86\u5173\u7cfb\u5b66\u4e60\u548c\u6cdb\u5316\u3002\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u540c\u5f02\u4efb\u52a1\u4ee5\u8bc4\u4f30 DNN \u4e2d\u7684\u5173\u7cfb\u63a8\u7406\uff0c\u4e0e\u6b64\u76f8\u53cd\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e00\u7ec4\u6765\u81ea\u6bd4\u8f83\u8ba4\u77e5\u6587\u732e\u7684\u5177\u6709\u4e0d\u540c\u96be\u5ea6\u7684\u4efb\u52a1\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u8bb8\u591a\u4e0d\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u4e5f\u80fd\u591f\u5206\u79bb\u573a\u666f\u4e2d\u7684\u4e0d\u540c\u5bf9\u8c61\u3002\u5728\u6211\u4eec\u66f4\u7b80\u5355\u7684\u4efb\u52a1\u4e2d\uff0c\u4e0e ResNet-50 \u57fa\u7ebf\u76f8\u6bd4\uff0c\u8fd9\u63d0\u9ad8\u4e86\u4ed6\u4eec\u5b66\u4e60\u548c\u6982\u62ec\u89c6\u89c9\u5173\u7cfb\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u5728\u6211\u4eec\u66f4\u56f0\u96be\u7684\u4efb\u52a1\u548c\u6761\u4ef6\u4e0b\u4ecd\u7136\u4e3e\u6b65\u7ef4\u8270\u3002\u6211\u4eec\u7684\u7ed3\u8bba\u662f\uff0c\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u4ecd\u7136\u662f DNN \u9762\u4e34\u7684\u4e00\u4e2a\u5f00\u653e\u6311\u6218\uff0c\u5305\u62ec\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u3002|[2402.12675v1](http://arxiv.org/pdf/2402.12675v1)|**[link](https://github.com/GuillermoPuebla/object-centric-reasoning)**|\n"}, "\u56fe\u50cf\u7406\u89e3": {}, "LLM": {"2402.13088": "|**2024-02-20**|**Slot-VLM: SlowFast Slots for Video-Language Modeling**|Slot-VLM\uff1a\u7528\u4e8e\u89c6\u9891\u8bed\u8a00\u5efa\u6a21\u7684 SlowFast \u63d2\u69fd|Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu|Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure. The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information. Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.|\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8fdb\u6b65\u7684\u63a8\u52a8\u4e0b\uff0c\u89c6\u9891\u8bed\u8a00\u6a21\u578b (VLM) \u6b63\u5728\u5f00\u8f9f\u89c6\u9891\u7406\u89e3\u7684\u65b0\u9886\u57df\u3002\u4e00\u4e2a\u5173\u952e\u7684\u6311\u6218\u662f\u5f00\u53d1\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c06\u89c6\u9891\u5185\u5bb9\u5c01\u88c5\u5230\u4e00\u7ec4\u4ee3\u8868\u6027\u4ee4\u724c\u4e2d\uff0c\u4ee5\u4e0e\u6cd5\u5b66\u7855\u58eb\u4fdd\u6301\u4e00\u81f4\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86 Slot-VLM\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u65e8\u5728\u6839\u636e\u5bf9\u8c61\u548c\u4e8b\u4ef6\u7684\u89c6\u89c9\u8868\u793a\u751f\u6210\u8bed\u4e49\u5206\u89e3\u7684\u89c6\u9891\u6807\u8bb0\uff0c\u4ee5\u4fc3\u8fdb LLM \u63a8\u7406\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a SlowFast Slots \u6a21\u5757\uff0c\u5373 SF-Slots\uff0c\u5b83\u81ea\u9002\u5e94\u5730\u5c06\u6765\u81ea CLIP \u89c6\u89c9\u7f16\u7801\u5668\u7684\u5bc6\u96c6\u89c6\u9891\u6807\u8bb0\u805a\u5408\u5230\u4e00\u7ec4\u4ee3\u8868\u6027\u63d2\u69fd\u4e2d\u3002\u4e3a\u4e86\u540c\u65f6\u8003\u8651\u7a7a\u95f4\u5bf9\u8c61\u7ec6\u8282\u548c\u53d8\u5316\u7684\u65f6\u95f4\u52a8\u6001\uff0cSF-Slots \u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\u6784\u5efa\u3002 Slow-Slots \u5206\u652f\u4e13\u6ce8\u4e8e\u4ece\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u4f46\u4f4e\uff08\u6162\uff09\u5e27\u91c7\u6837\u7387\u7684\u7279\u5f81\u4e2d\u63d0\u53d6\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u69fd\uff0c\u5f3a\u8c03\u8be6\u7ec6\u7684\u5bf9\u8c61\u4fe1\u606f\u3002\u76f8\u53cd\uff0cFast-Slots \u5206\u652f\u88ab\u8bbe\u8ba1\u4e3a\u4ece\u9ad8\u65f6\u95f4\u91c7\u6837\u7387\u4f46\u4f4e\u7a7a\u95f4\u5206\u8fa8\u7387\u7279\u5f81\u4e2d\u5b66\u4e60\u4ee5\u4e8b\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u65f6\u9699\u3002\u8fd9\u4e9b\u4e92\u8865\u7684\u69fd\u4f4d\u7ec4\u5408\u5728\u4e00\u8d77\u5f62\u6210\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u4f5c\u4e3a\u6cd5\u5b66\u7855\u58eb\u7684\u8f93\u5165\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u95ee\u9898\u56de\u7b54\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86 Slot-VLM \u7684\u6709\u6548\u6027\uff0c\u5b83\u5728\u89c6\u9891\u95ee\u7b54\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2402.13088v1](http://arxiv.org/pdf/2402.13088v1)|null|\n"}, "Transformer": {"2402.13185": "|**2024-02-20**|**UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing**|UniEdit\uff1a\u7528\u4e8e\u89c6\u9891\u8fd0\u52a8\u548c\u5916\u89c2\u7f16\u8f91\u7684\u7edf\u4e00\u514d\u8c03\u4f18\u6846\u67b6|Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, Jiang Bian|Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization). However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored. In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework. To realize motion editing while preserving source video content, based on the insights that temporal and spatial self-attention layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively. The obtained features are then injected into the main editing path via temporal and spatial self-attention layers. Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods. Our code will be publicly available.|\u6587\u672c\u5f15\u5bfc\u89c6\u9891\u7f16\u8f91\u7684\u6700\u65b0\u8fdb\u5c55\u5728\u5916\u89c2\u7f16\u8f91\uff08\u4f8b\u5982\u98ce\u683c\u5316\uff09\u65b9\u9762\u5c55\u793a\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u5c06\u89c6\u9891\u7f16\u8f91\u4e0e\u56fe\u50cf\u7f16\u8f91\u533a\u5206\u5f00\u6765\u7684\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u89c6\u9891\u52a8\u4f5c\u7f16\u8f91\uff08\u4f8b\u5982\uff0c\u4ece\u5403\u4e1c\u897f\u5230\u6325\u624b\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 UniEdit\uff0c\u8fd9\u662f\u4e00\u4e2a\u514d\u8c03\u6574\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u53cd\u8f6c\u751f\u6210\u6846\u67b6\u4e2d\u9884\u5148\u8bad\u7ec3\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u7684\u529f\u80fd\uff0c\u652f\u6301\u89c6\u9891\u8fd0\u52a8\u548c\u5916\u89c2\u7f16\u8f91\u3002\u4e3a\u4e86\u5728\u4fdd\u7559\u6e90\u89c6\u9891\u5185\u5bb9\u7684\u540c\u65f6\u5b9e\u73b0\u8fd0\u52a8\u7f16\u8f91\uff0c\u57fa\u4e8e\u65f6\u95f4\u548c\u7a7a\u95f4\u81ea\u6ce8\u610f\u529b\u5c42\u5206\u522b\u7f16\u7801\u5e27\u95f4\u548c\u5e27\u5185\u4f9d\u8d56\u6027\u7684\u89c1\u89e3\uff0c\u6211\u4eec\u5f15\u5165\u8f85\u52a9\u8fd0\u52a8\u53c2\u8003\u548c\u91cd\u5efa\u5206\u652f\u6765\u4ea7\u751f\u6587\u672c\u5f15\u5bfc\u7684\u8fd0\u52a8\u548c\u6e90\u5206\u522b\u5177\u6709\u7279\u70b9\u3002\u7136\u540e\u5c06\u83b7\u5f97\u7684\u7279\u5f81\u901a\u8fc7\u65f6\u95f4\u548c\u7a7a\u95f4\u81ea\u6ce8\u610f\u529b\u5c42\u6ce8\u5165\u5230\u4e3b\u7f16\u8f91\u8def\u5f84\u4e2d\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUniEdit \u6db5\u76d6\u4e86\u89c6\u9891\u52a8\u4f5c\u7f16\u8f91\u548c\u5404\u79cd\u5916\u89c2\u7f16\u8f91\u573a\u666f\uff0c\u5e76\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u516c\u5f00\u3002|[2402.13185v1](http://arxiv.org/pdf/2402.13185v1)|null|\n", "2402.12810": "|**2024-02-20**|**PIP-Net: Pedestrian Intention Prediction in the Wild**|PIP-Net\uff1a\u91ce\u5916\u884c\u4eba\u610f\u56fe\u9884\u6d4b|Mohsen Azarmi, Mahdi Rezaei, He Wang, Sebastien Glaser|Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to enhancement in the model's contextual perception. Depending on the traffic scenario and road environment, the model excels in predicting pedestrian crossing intentions up to 4 seconds in advance which is a breakthrough in current research studies in pedestrian intention prediction. Finally, for the first time, we present the Urban-PIP dataset, a customised pedestrian intention prediction dataset, with multi-camera annotations in real-world automated driving scenarios.|\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08AV\uff09\u51c6\u786e\u7684\u884c\u4eba\u610f\u56fe\u9884\u6d4b\uff08PIP\uff09\u662f\u8be5\u9886\u57df\u5f53\u524d\u7684\u7814\u7a76\u6311\u6218\u4e4b\u4e00\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 PIP-Net\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u65e8\u5728\u9884\u6d4b\u73b0\u5b9e\u57ce\u5e02\u573a\u666f\u4e2d\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u884c\u4eba\u8fc7\u8def\u610f\u56fe\u3002\u6211\u4eec\u63d0\u4f9b\u4e24\u79cd PIP-Net \u53d8\u4f53\uff0c\u4e13\u4e3a\u4e0d\u540c\u7684\u6444\u50cf\u673a\u5b89\u88c5\u548c\u8bbe\u7f6e\u800c\u8bbe\u8ba1\u3002\u5229\u7528\u9a7e\u9a76\u573a\u666f\u7684\u8fd0\u52a8\u5b66\u6570\u636e\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u91c7\u7528\u4e86\u57fa\u4e8e\u5faa\u73af\u548c\u65f6\u95f4\u6ce8\u610f\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u589e\u5f3a\u9053\u8def\u4f7f\u7528\u8005\u7684\u89c6\u89c9\u8868\u793a\u53ca\u200b\u200b\u5176\u4e0e\u81ea\u6211\u8f66\u8f86\u7684\u63a5\u8fd1\u5ea6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5206\u7c7b\u6df1\u5ea6\u7279\u5f81\u56fe\uff0c\u7ed3\u5408\u5c40\u90e8\u8fd0\u52a8\u6d41\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u5bf9\u573a\u666f\u52a8\u6001\u7684\u4e30\u5bcc\u6d1e\u5bdf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86\u6269\u5927\u6444\u50cf\u5934\u89c6\u91ce\u7684\u5f71\u54cd\uff0c\u4ece\u56f4\u7ed5\u81ea\u6211\u8f66\u8f86\u7684\u4e00\u4e2a\u6444\u50cf\u5934\u589e\u52a0\u5230\u4e09\u4e2a\u6444\u50cf\u5934\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u60c5\u5883\u611f\u77e5\u3002\u8be5\u6a21\u578b\u6839\u636e\u4ea4\u901a\u573a\u666f\u548c\u9053\u8def\u73af\u5883\uff0c\u80fd\u591f\u63d0\u524d4\u79d2\u9884\u6d4b\u884c\u4eba\u8fc7\u8857\u610f\u56fe\uff0c\u8fd9\u662f\u5f53\u524d\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u7814\u7a76\u7684\u7a81\u7834\u3002\u6700\u540e\uff0c\u6211\u4eec\u9996\u6b21\u63d0\u51fa\u4e86 Urban-PIP \u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b9a\u5236\u7684\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u6570\u636e\u96c6\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5177\u6709\u591a\u6444\u50cf\u5934\u6ce8\u91ca\u3002|[2402.12810v1](http://arxiv.org/pdf/2402.12810v1)|null|\n", "2402.12788": "|**2024-02-20**|**RhythmFormer: Extracting rPPG Signals Based on Hierarchical Temporal Periodic Transformer**|RhythmFormer\uff1a\u57fa\u4e8e\u5206\u5c42\u65f6\u95f4\u5468\u671f\u53d8\u6362\u5668\u63d0\u53d6 rPPG \u4fe1\u53f7|Bochao Zou, Zizheng Guo, Jiansheng Chen, Huimin Ma|Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals based on facial videos, holding high potential in various applications such as healthcare, affective computing, anti-spoofing, etc. Due to the periodicity nature of rPPG, the long-range dependency capturing capacity of the Transformer was assumed to be advantageous for such signals. However, existing approaches have not conclusively demonstrated the superior performance of Transformer over traditional convolutional neural network methods, this gap may stem from a lack of thorough exploration of rPPG periodicity. In this paper, we propose RhythmFormer, a fully end-to-end transformer-based method for extracting rPPG signals by explicitly leveraging the quasi-periodic nature of rPPG. The core module, Hierarchical Temporal Periodic Transformer, hierarchically extracts periodic features from multiple temporal scales. It utilizes dynamic sparse attention based on periodicity in the temporal domain, allowing for fine-grained modeling of rPPG features. Furthermore, a fusion stem is proposed to guide self-attention to rPPG features effectively, and it can be easily transferred to existing methods to enhance their performance significantly. RhythmFormer achieves state-of-the-art performance with fewer parameters and reduced computational complexity in comprehensive experiments compared to previous approaches. The codes are available at https://github.com/zizheng-guo/RhythmFormer.|\u8fdc\u7a0b\u5149\u7535\u4f53\u79ef\u63cf\u8bb0\u6cd5\uff08rPPG\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u9762\u90e8\u89c6\u9891\u68c0\u6d4b\u751f\u7406\u4fe1\u53f7\u7684\u975e\u63a5\u89e6\u5f0f\u65b9\u6cd5\uff0c\u5728\u533b\u7597\u4fdd\u5065\u3001\u60c5\u611f\u8ba1\u7b97\u3001\u53cd\u6b3a\u9a97\u7b49\u5404\u79cd\u5e94\u7528\u4e2d\u5177\u6709\u5f88\u9ad8\u7684\u6f5c\u529b\u3002\u7531\u4e8erPPG\u7684\u5468\u671f\u6027\uff0c\u957f\u671f\u5047\u8bbe Transformer \u7684\u8303\u56f4\u4f9d\u8d56\u6027\u6355\u83b7\u80fd\u529b\u5bf9\u4e8e\u6b64\u7c7b\u4fe1\u53f7\u662f\u6709\u5229\u7684\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5c1a\u672a\u6700\u7ec8\u8bc1\u660e Transformer \u76f8\u5bf9\u4e8e\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u8fd9\u79cd\u5dee\u8ddd\u53ef\u80fd\u6e90\u4e8e\u7f3a\u4e4f\u5bf9 rPPG \u5468\u671f\u6027\u7684\u5f7b\u5e95\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 RhythmFormer\uff0c\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u7aef\u5230\u7aef\u53d8\u538b\u5668\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u901a\u8fc7\u660e\u786e\u5229\u7528 rPPG \u7684\u51c6\u5468\u671f\u6027\u8d28\u6765\u63d0\u53d6 rPPG \u4fe1\u53f7\u3002\u6838\u5fc3\u6a21\u5757Hierarchical Temporal periodic Transformer\u4ece\u591a\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u5206\u5c42\u63d0\u53d6\u5468\u671f\u6027\u7279\u5f81\u3002\u5b83\u5229\u7528\u57fa\u4e8e\u65f6\u57df\u5468\u671f\u6027\u7684\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u5141\u8bb8\u5bf9 rPPG \u7279\u5f81\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5efa\u6a21\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u5e72\u6765\u6709\u6548\u5730\u5f15\u5bfc\u81ea\u6ce8\u610f\u529b\u5230rPPG\u7279\u5f81\uff0c\u5e76\u4e14\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u8f6c\u79fb\u5230\u73b0\u6709\u65b9\u6cd5\u4e2d\u4ee5\u663e\u7740\u63d0\u9ad8\u5176\u6027\u80fd\u3002\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cRhythmFormer \u5728\u7efc\u5408\u5b9e\u9a8c\u4e2d\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/zizheng-guo/RhythmFormer \u83b7\u53d6\u3002|[2402.12788v1](http://arxiv.org/pdf/2402.12788v1)|**[link](https://github.com/zizheng-guo/rhythmformer)**|\n"}, "3D/CG": {}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {}, "\u5176\u4ed6": {"2402.13243": "|**2024-02-20**|**VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning**|VADv2\uff1a\u901a\u8fc7\u6982\u7387\u89c4\u5212\u5b9e\u73b0\u7aef\u5230\u7aef\u77e2\u91cf\u5316\u81ea\u52a8\u9a7e\u9a76|Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, Xinggang Wang|Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging. In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle. Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods. It runs stably in a fully end-to-end manner, even without the rule-based wrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.|\u4ece\u5927\u89c4\u6a21\u9a7e\u9a76\u6f14\u793a\u4e2d\u5b66\u4e60\u7c7b\u4eba\u9a7e\u9a76\u7b56\u7565\u662f\u6709\u524d\u9014\u7684\uff0c\u4f46\u89c4\u5212\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u975e\u786e\u5b9a\u6027\u4f7f\u5176\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u4e3a\u4e86\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86VADv2\uff0c\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u89c4\u5212\u7684\u7aef\u5230\u7aef\u9a7e\u9a76\u6a21\u578b\u3002 VADv2\u4ee5\u6d41\u5f0f\u65b9\u5f0f\u5c06\u591a\u89c6\u56fe\u56fe\u50cf\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u5c06\u4f20\u611f\u5668\u6570\u636e\u8f6c\u6362\u4e3a\u73af\u5883\u4ee4\u724c\u5d4c\u5165\uff0c\u8f93\u51fa\u52a8\u4f5c\u7684\u6982\u7387\u5206\u5e03\uff0c\u5e76\u5bf9\u4e00\u4e2a\u52a8\u4f5c\u8fdb\u884c\u91c7\u6837\u6765\u63a7\u5236\u8f66\u8f86\u3002\u4ec5\u4f7f\u7528\u6444\u50cf\u5934\u4f20\u611f\u5668\uff0cVADv2 \u5c31\u80fd\u5728 CARLA Town05 \u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u95ed\u73af\u6027\u80fd\uff0c\u663e\u7740\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u3002\u5373\u4f7f\u6ca1\u6709\u57fa\u4e8e\u89c4\u5219\u7684\u5305\u88c5\u5668\uff0c\u5b83\u4e5f\u80fd\u4ee5\u5b8c\u5168\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u7a33\u5b9a\u8fd0\u884c\u3002\u95ed\u73af\u6f14\u793a\u4f4d\u4e8e https://hgao-cv.github.io/VADv2\u3002|[2402.13243v1](http://arxiv.org/pdf/2402.13243v1)|null|\n", "2402.13195": "|**2024-02-20**|**Design and Flight Demonstration of a Quadrotor for Urban Mapping and Target Tracking Research**|\u7528\u4e8e\u57ce\u5e02\u6d4b\u7ed8\u548c\u76ee\u6807\u8ddf\u8e2a\u7814\u7a76\u7684\u56db\u65cb\u7ffc\u98de\u884c\u5668\u7684\u8bbe\u8ba1\u548c\u98de\u884c\u6f14\u793a|Collin Hague, Nick Kakavitsas, Jincheng Zhang, Chris Beam, Andrew Willis, Artur Wolek|This paper describes the hardware design and flight demonstration of a small quadrotor with imaging sensors for urban mapping, hazard avoidance, and target tracking research. The vehicle is equipped with five cameras, including two pairs of fisheye stereo cameras that enable a nearly omnidirectional view and a two-axis gimbaled camera. An onboard NVIDIA Jetson Orin Nano computer running the Robot Operating System software is used for data collection. An autonomous tracking behavior was implemented to coordinate the motion of the quadrotor and gimbaled camera to track a moving GPS coordinate. The data collection system was demonstrated through a flight test that tracked a moving GPS-tagged vehicle through a series of roads and parking lots. A map of the environment was reconstructed from the collected images using the Direct Sparse Odometry (DSO) algorithm. The performance of the quadrotor was also characterized by acoustic noise, communication range, battery voltage in hover, and maximum speed tests.|\u672c\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8e\u57ce\u5e02\u6d4b\u7ed8\u3001\u5371\u9669\u89c4\u907f\u548c\u76ee\u6807\u8ddf\u8e2a\u7814\u7a76\u7684\u5e26\u6709\u6210\u50cf\u4f20\u611f\u5668\u7684\u5c0f\u578b\u56db\u65cb\u7ffc\u98de\u884c\u5668\u7684\u786c\u4ef6\u8bbe\u8ba1\u548c\u98de\u884c\u6f14\u793a\u3002\u8be5\u8f66\u914d\u5907\u4e86\u4e94\u4e2a\u6444\u50cf\u5934\uff0c\u5305\u62ec\u4e24\u5bf9\u53ef\u5b9e\u73b0\u8fd1\u4e4e\u5168\u5411\u89c6\u89d2\u7684\u9c7c\u773c\u7acb\u4f53\u6444\u50cf\u5934\u548c\u4e00\u4e2a\u4e24\u8f74\u4e07\u5411\u6444\u50cf\u5934\u3002\u8fd0\u884c\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u8f6f\u4ef6\u7684\u677f\u8f7d NVIDIA Jetson Orin Nano \u8ba1\u7b97\u673a\u7528\u4e8e\u6570\u636e\u6536\u96c6\u3002\u5b9e\u73b0\u4e86\u81ea\u4e3b\u8ddf\u8e2a\u884c\u4e3a\u6765\u534f\u8c03\u56db\u65cb\u7ffc\u98de\u884c\u5668\u548c\u4e07\u5411\u6444\u50cf\u673a\u7684\u8fd0\u52a8\uff0c\u4ee5\u8ddf\u8e2a\u79fb\u52a8\u7684 GPS \u5750\u6807\u3002\u8be5\u6570\u636e\u6536\u96c6\u7cfb\u7edf\u901a\u8fc7\u98de\u884c\u6d4b\u8bd5\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u8be5\u6d4b\u8bd5\u8ddf\u8e2a\u4e00\u8f86\u79fb\u52a8\u7684\u5e26\u6709 GPS \u6807\u7b7e\u7684\u8f66\u8f86\u7a7f\u8fc7\u4e00\u7cfb\u5217\u9053\u8def\u548c\u505c\u8f66\u573a\u3002\u4f7f\u7528\u76f4\u63a5\u7a00\u758f\u91cc\u7a0b\u8ba1 (DSO) \u7b97\u6cd5\u6839\u636e\u6536\u96c6\u7684\u56fe\u50cf\u91cd\u5efa\u73af\u5883\u5730\u56fe\u3002\u56db\u65cb\u7ffc\u98de\u884c\u5668\u7684\u6027\u80fd\u8fd8\u901a\u8fc7\u566a\u58f0\u3001\u901a\u4fe1\u8303\u56f4\u3001\u60ac\u505c\u65f6\u7684\u7535\u6c60\u7535\u538b\u548c\u6700\u5927\u901f\u5ea6\u6d4b\u8bd5\u6765\u8868\u5f81\u3002|[2402.13195v1](http://arxiv.org/pdf/2402.13195v1)|null|\n", "2402.13131": "|**2024-02-20**|**exploreCOSMOS: Interactive Exploration of Conditional Statistical Shape Models in the Web-Browser**|exploreCOSMOS\uff1a\u7f51\u7edc\u6d4f\u89c8\u5668\u4e2d\u6761\u4ef6\u7edf\u8ba1\u5f62\u72b6\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u63a2\u7d22|Maximilian Hahn, Bernhard Egger|Statistical Shape Models of faces and various body parts are heavily used in medical image analysis, computer vision and visualization. Whilst the field is well explored with many existing tools, all of them aim at experts, which limits their applicability. We demonstrate the first tool that enables the convenient exploration of statistical shape models in the browser, with the capability to manipulate the faces in a targeted manner. This manipulation is performed via a posterior model given partial observations. We release our code and application on GitHub https://github.com/maximilian-hahn/exploreCOSMOS|\u9762\u90e8\u548c\u8eab\u4f53\u5404\u4e2a\u90e8\u4f4d\u7684\u7edf\u8ba1\u5f62\u72b6\u6a21\u578b\u5927\u91cf\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u53ef\u89c6\u5316\u3002\u867d\u7136\u8bb8\u591a\u73b0\u6709\u5de5\u5177\u5bf9\u8be5\u9886\u57df\u8fdb\u884c\u4e86\u5f88\u597d\u7684\u63a2\u7d22\uff0c\u4f46\u6240\u6709\u8fd9\u4e9b\u5de5\u5177\u90fd\u9488\u5bf9\u4e13\u5bb6\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u9002\u7528\u6027\u3002\u6211\u4eec\u6f14\u793a\u4e86\u7b2c\u4e00\u4e2a\u5de5\u5177\uff0c\u53ef\u4ee5\u5728\u6d4f\u89c8\u5668\u4e2d\u65b9\u4fbf\u5730\u63a2\u7d22\u7edf\u8ba1\u5f62\u72b6\u6a21\u578b\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u9488\u5bf9\u6027\u5730\u64cd\u7eb5\u9762\u90e8\u3002\u8fd9\u79cd\u64cd\u4f5c\u662f\u901a\u8fc7\u7ed9\u5b9a\u90e8\u5206\u89c2\u5bdf\u7ed3\u679c\u7684\u540e\u9a8c\u6a21\u578b\u6765\u6267\u884c\u7684\u3002\u6211\u4eec\u5728 GitHub \u4e0a\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u5e94\u7528\u7a0b\u200b\u200b\u5e8f https://github.com/maximilian-hahn/exploreCOSMOS|[2402.13131v1](http://arxiv.org/pdf/2402.13131v1)|**[link](https://github.com/maximilian-hahn/explorecosmos)**|\n", "2402.12891": "|**2024-02-20**|**Mind the Exit Pupil Gap: Revisiting the Intrinsics of a Standard Plenoptic Camera**|\u6ce8\u610f\u51fa\u77b3\u95f4\u9699\uff1a\u91cd\u65b0\u5ba1\u89c6\u6807\u51c6\u5168\u5149\u76f8\u673a\u7684\u672c\u8d28|Tim Michels, Daniel M\u00e4ckelmann, Reinhard Koch|Among the common applications of plenoptic cameras are depth reconstruction and post-shot refocusing. These require a calibration relating the camera-side light field to that of the scene. Numerous methods with this goal have been developed based on thin lens models for the plenoptic camera's main lens and microlenses. Our work addresses the often-overlooked role of the main lens exit pupil in these models and specifically in the decoding process of standard plenoptic camera (SPC) images. We formally deduce the connection between the refocusing distance and the resampling parameter for the decoded light field and provide an analysis of the errors that arise when the exit pupil is not considered. In addition, previous work is revisited with respect to the exit pupil's role and all theoretical results are validated through a ray-tracing-based simulation. With the public release of the evaluated SPC designs alongside our simulation and experimental data we aim to contribute to a more accurate and nuanced understanding of plenoptic camera optics.|\u5168\u5149\u76f8\u673a\u7684\u5e38\u89c1\u5e94\u7528\u5305\u62ec\u6df1\u5ea6\u91cd\u5efa\u548c\u62cd\u6444\u540e\u91cd\u65b0\u805a\u7126\u3002\u8fd9\u4e9b\u9700\u8981\u5c06\u76f8\u673a\u4fa7\u5149\u573a\u4e0e\u573a\u666f\u5149\u573a\u76f8\u5173\u8054\u7684\u6821\u51c6\u3002\u57fa\u4e8e\u5168\u5149\u76f8\u673a\u4e3b\u900f\u955c\u548c\u5fae\u900f\u955c\u7684\u8584\u900f\u955c\u6a21\u578b\uff0c\u5df2\u7ecf\u5f00\u53d1\u4e86\u8bb8\u591a\u5b9e\u73b0\u6b64\u76ee\u6807\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u89e3\u51b3\u4e86\u4e3b\u955c\u5934\u51fa\u5c04\u5149\u77b3\u5728\u8fd9\u4e9b\u6a21\u578b\u4e2d\u7ecf\u5e38\u88ab\u5ffd\u89c6\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u6807\u51c6\u5168\u5149\u76f8\u673a (SPC) \u56fe\u50cf\u7684\u89e3\u7801\u8fc7\u7a0b\u4e2d\u3002\u6211\u4eec\u6b63\u5f0f\u63a8\u5bfc\u4e86\u89e3\u7801\u5149\u573a\u7684\u91cd\u805a\u7126\u8ddd\u79bb\u548c\u91cd\u91c7\u6837\u53c2\u6570\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u5bf9\u4e0d\u8003\u8651\u51fa\u77b3\u65f6\u51fa\u73b0\u7684\u8bef\u5dee\u8fdb\u884c\u4e86\u5206\u6790\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u4e4b\u524d\u5173\u4e8e\u51fa\u77b3\u4f5c\u7528\u7684\u5de5\u4f5c\uff0c\u5e76\u4e14\u6240\u6709\u7406\u8bba\u7ed3\u679c\u90fd\u901a\u8fc7\u57fa\u4e8e\u5149\u7ebf\u8ffd\u8e2a\u7684\u6a21\u62df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u901a\u8fc7\u516c\u5f00\u53d1\u5e03\u7ecf\u8fc7\u8bc4\u4f30\u7684 SPC \u8bbe\u8ba1\u4ee5\u53ca\u6211\u4eec\u7684\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u5e2e\u52a9\u4eba\u4eec\u66f4\u51c6\u786e\u3001\u66f4\u7ec6\u81f4\u5730\u4e86\u89e3\u5168\u5149\u76f8\u673a\u5149\u5b66\u5668\u4ef6\u3002|[2402.12891v1](http://arxiv.org/pdf/2402.12891v1)|**[link](https://gitlab.com/ungetym/blender-camera-generator)**|\n", "2402.12844": "|**2024-02-20**|**ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation**|ICON\uff1a\u901a\u8fc7\u75c5\u53d8\u611f\u77e5\u6df7\u5408\u589e\u5f3a\u63d0\u9ad8\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u62a5\u544a\u95f4\u4e00\u81f4\u6027|Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, Jiang Liu|Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesions, our approach involves first extracting lesions from input images and examining their characteristics. Then, we introduce a lesion-aware mix-up augmentation technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, by linearly interpolating them during the training phase. Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports.|\u5148\u524d\u5173\u4e8e\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u7814\u7a76\u5728\u63d0\u9ad8\u751f\u6210\u62a5\u544a\u7684\u4e34\u5e8a\u51c6\u786e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f3a\u8c03\u5b83\u5e94\u5177\u5907\u7684\u53e6\u4e00\u4e2a\u5173\u952e\u54c1\u8d28\uff0c\u5373\u62a5\u544a\u95f4\u4e00\u81f4\u6027\uff0c\u6307\u7684\u662f\u4e3a\u8bed\u4e49\u7b49\u6548\u7684\u5c04\u7ebf\u7167\u7247\u751f\u6210\u4e00\u81f4\u62a5\u544a\u7684\u80fd\u529b\u3002\u5bf9\u4e8e\u786e\u4fdd\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u800c\u8a00\uff0c\u8fd9\u79cd\u8d28\u91cf\u751a\u81f3\u6bd4\u6574\u4f53\u62a5\u544a\u7684\u51c6\u786e\u6027\u66f4\u91cd\u8981\uff0c\u56e0\u4e3a\u5bb9\u6613\u63d0\u4f9b\u76f8\u4e92\u77db\u76fe\u7684\u7ed3\u679c\u7684\u7cfb\u7edf\u4f1a\u4e25\u91cd\u524a\u5f31\u7528\u6237\u7684\u4fe1\u4efb\u3002\u9057\u61be\u7684\u662f\uff0c\u73b0\u6709\u65b9\u6cd5\u5f88\u96be\u4fdd\u6301\u62a5\u544a\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u8868\u73b0\u51fa\u5bf9\u5e38\u89c1\u6a21\u5f0f\u7684\u504f\u89c1\u548c\u5bf9\u75c5\u53d8\u53d8\u5f02\u7684\u654f\u611f\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ICON\uff0c\u5b83\u63d0\u9ad8\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u62a5\u544a\u95f4\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u589e\u5f3a\u7cfb\u7edf\u6355\u83b7\u8bed\u4e49\u7b49\u6548\u75c5\u53d8\u76f8\u4f3c\u6027\u7684\u80fd\u529b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u9996\u5148\u4ece\u8f93\u5165\u56fe\u50cf\u4e2d\u63d0\u53d6\u75c5\u53d8\u5e76\u68c0\u67e5\u5176\u7279\u5f81\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u75c5\u53d8\u611f\u77e5\u6df7\u5408\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u9636\u6bb5\u5bf9\u5b83\u4eec\u8fdb\u884c\u7ebf\u6027\u63d2\u503c\uff0c\u786e\u4fdd\u8bed\u4e49\u7b49\u6548\u75c5\u53d8\u7684\u8868\u793a\u4e0e\u76f8\u540c\u7684\u5c5e\u6027\u5bf9\u9f50\u3002\u5bf9\u4e09\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u80f8\u90e8 X \u5c04\u7ebf\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u65e0\u8bba\u662f\u5728\u63d0\u9ad8\u751f\u6210\u62a5\u544a\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u3002|[2402.12844v1](http://arxiv.org/pdf/2402.12844v1)|**[link](https://github.com/wjhou/icon)**|\n", "2402.12760": "|**2024-02-20**|**A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis**|\u7528\u4e8e\u5728\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u751f\u6210\u6a21\u578b\u9996\u9009\u63d0\u793a\u7684\u7528\u6237\u53cb\u597d\u6846\u67b6|Nailei Hei, Qianyu Guo, Zihao Wang, Yan Wang, Haofen Wang, Wenqiang Zhang|Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images. Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts. To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization. For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts. Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs. Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts. Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results. Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics.|\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u5df2\u7ecf\u8bc1\u660e\u4e86\u6307\u5bfc\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u4ee4\u4eba\u60ca\u53f9\u7684\u56fe\u50cf\u7684\u6f5c\u529b\u3002\u5c3d\u7ba1\u73b0\u6709\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u53ef\u4ee5\u63d0\u4f9b\u9ad8\u7ea7\u6307\u5bfc\uff0c\u4f46\u7531\u4e8e\u65b0\u624b\u7528\u6237\u8f93\u5165\u7684\u63d0\u793a\u4e0e\u6a21\u578b\u504f\u597d\u7684\u63d0\u793a\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u65b0\u624b\u7528\u6237\u901a\u8fc7\u624b\u52a8\u8f93\u5165\u63d0\u793a\u6765\u8fbe\u5230\u9884\u671f\u7ed3\u679c\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5f25\u5408\u7528\u6237\u8f93\u5165\u884c\u4e3a\u548c\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u8ddd\uff0c\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u7c97\u7ec6\u7c92\u5ea6\u63d0\u793a\u6570\u636e\u96c6\uff08CFP\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7528\u6237\u53cb\u597d\u7684\u7ec6\u7c92\u5ea6\u6587\u672c\u751f\u6210\u6846\u67b6\uff08UF-FGTG\uff09\u7528\u4e8e\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u3002\u5bf9\u4e8e CFP\uff0c\u6211\u4eec\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u7ed3\u5408\u4e86\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7684\u63d0\u793a\uff0c\u4ee5\u4fc3\u8fdb\u81ea\u52a8\u63d0\u793a\u751f\u6210\u65b9\u6cd5\u7684\u5f00\u53d1\u3002\u5bf9\u4e8e UF-FGTG\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u81ea\u52a8\u5c06\u7528\u6237\u8f93\u5165\u63d0\u793a\u8f6c\u6362\u4e3a\u6a21\u578b\u9996\u9009\u63d0\u793a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u63d0\u793a\u7ec6\u5316\u5668\uff0c\u5b83\u4e0d\u65ad\u91cd\u5199\u63d0\u793a\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u9009\u62e9\u7b26\u5408\u5176\u72ec\u7279\u9700\u6c42\u7684\u7ed3\u679c\u3002\u540c\u65f6\uff0c\u6211\u4eec\u5c06\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u4e0e\u56fe\u50cf\u76f8\u5173\u7684\u635f\u5931\u51fd\u6570\u96c6\u6210\u5230\u6587\u672c\u751f\u6210\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u751f\u6210\u6a21\u578b\u9996\u9009\u7684\u63d0\u793a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff0c\u4ee5\u786e\u4fdd\u751f\u6210\u7ed3\u679c\u7684\u591a\u6837\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u5177\u89c6\u89c9\u5438\u5f15\u529b\u548c\u591a\u6837\u5316\u7684\u56fe\u50cf\uff0c\u5728\u516d\u4e2a\u8d28\u91cf\u548c\u7f8e\u5b66\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e86 5%\u3002|[2402.12760v1](http://arxiv.org/pdf/2402.12760v1)|**[link](https://github.com/naylenv/uf-fgtg)**|\n", "2402.12735": "|**2024-02-20**|**Denoising OCT Images Using Steered Mixture of Experts with Multi-Model Inference**|\u4f7f\u7528\u591a\u6a21\u578b\u63a8\u7406\u4e13\u5bb6\u7684\u5f15\u5bfc\u7ec4\u5408\u5bf9 OCT \u56fe\u50cf\u8fdb\u884c\u53bb\u566a|Ayta\u00e7 \u00d6zkan, Elena Stoykova, Thomas Sikora, Violeta Madjarova|In Optical Coherence Tomography (OCT), speckle noise significantly hampers image quality, affecting diagnostic accuracy. Current methods, including traditional filtering and deep learning techniques, have limitations in noise reduction and detail preservation. Addressing these challenges, this study introduces a novel denoising algorithm, Block-Matching Steered-Mixture of Experts with Multi-Model Inference and Autoencoder (BM-SMoE-AE). This method combines block-matched implementation of the SMoE algorithm with an enhanced autoencoder architecture, offering efficient speckle noise reduction while retaining critical image details. Our method stands out by providing improved edge definition and reduced processing time. Comparative analysis with existing denoising techniques demonstrates the superior performance of BM-SMoE-AE in maintaining image integrity and enhancing OCT image usability for medical diagnostics.|\u5728\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf (OCT) \u4e2d\uff0c\u6563\u6591\u566a\u58f0\u4e25\u91cd\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u3002\u5f53\u524d\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u4f20\u7edf\u7684\u6ee4\u6ce2\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5728\u964d\u566a\u548c\u7ec6\u8282\u4fdd\u7559\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53bb\u566a\u7b97\u6cd5\uff0c\u5373\u5177\u6709\u591a\u6a21\u578b\u63a8\u7406\u548c\u81ea\u52a8\u7f16\u7801\u5668\u7684\u4e13\u5bb6\u5757\u5339\u914d\u5f15\u5bfc\u6df7\u5408\u7b97\u6cd5 (BM-SMoE-AE)\u3002\u8be5\u65b9\u6cd5\u5c06 SMoE \u7b97\u6cd5\u7684\u5757\u5339\u914d\u5b9e\u73b0\u4e0e\u589e\u5f3a\u7684\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\u76f8\u7ed3\u5408\uff0c\u5728\u4fdd\u7559\u5173\u952e\u56fe\u50cf\u7ec6\u8282\u7684\u540c\u65f6\u63d0\u4f9b\u9ad8\u6548\u7684\u6563\u6591\u566a\u58f0\u6291\u5236\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u56e0\u63d0\u4f9b\u6539\u8fdb\u7684\u8fb9\u7f18\u6e05\u6670\u5ea6\u548c\u51cf\u5c11\u5904\u7406\u65f6\u95f4\u800c\u8131\u9896\u800c\u51fa\u3002\u4e0e\u73b0\u6709\u53bb\u566a\u6280\u672f\u7684\u6bd4\u8f83\u5206\u6790\u8868\u660e\uff0cBM-SMoE-AE \u5728\u4fdd\u6301\u56fe\u50cf\u5b8c\u6574\u6027\u548c\u589e\u5f3a OCT \u56fe\u50cf\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u53ef\u7528\u6027\u65b9\u9762\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\u3002|[2402.12735v1](http://arxiv.org/pdf/2402.12735v1)|null|\n", "2402.12676": "|**2024-02-20**|**Advancing Monocular Video-Based Gait Analysis Using Motion Imitation with Physics-Based Simulation**|\u4f7f\u7528\u8fd0\u52a8\u6a21\u4eff\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\u63a8\u8fdb\u57fa\u4e8e\u5355\u76ee\u89c6\u9891\u7684\u6b65\u6001\u5206\u6790|Nikolaos Smyrnakis, Tasos Karakostas, R. James Cotton|Gait analysis from videos obtained from a smartphone would open up many clinical opportunities for detecting and quantifying gait impairments. However, existing approaches for estimating gait parameters from videos can produce physically implausible results. To overcome this, we train a policy using reinforcement learning to control a physics simulation of human movement to replicate the movement seen in video. This forces the inferred movements to be physically plausible, while improving the accuracy of the inferred step length and walking velocity.|\u5bf9\u4ece\u667a\u80fd\u624b\u673a\u83b7\u5f97\u7684\u89c6\u9891\u8fdb\u884c\u6b65\u6001\u5206\u6790\u5c06\u4e3a\u68c0\u6d4b\u548c\u91cf\u5316\u6b65\u6001\u969c\u788d\u63d0\u4f9b\u8bb8\u591a\u4e34\u5e8a\u673a\u4f1a\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4ece\u89c6\u9891\u4e2d\u4f30\u8ba1\u6b65\u6001\u53c2\u6570\u7684\u65b9\u6cd5\u53ef\u80fd\u4f1a\u4ea7\u751f\u7269\u7406\u4e0a\u4ee4\u4eba\u96be\u4ee5\u7f6e\u4fe1\u7684\u7ed3\u679c\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u8bad\u7ec3\u7b56\u7565\u6765\u63a7\u5236\u4eba\u4f53\u8fd0\u52a8\u7684\u7269\u7406\u6a21\u62df\uff0c\u4ee5\u590d\u5236\u89c6\u9891\u4e2d\u770b\u5230\u7684\u8fd0\u52a8\u3002\u8fd9\u8feb\u4f7f\u63a8\u65ad\u7684\u8fd0\u52a8\u5728\u7269\u7406\u4e0a\u5408\u7406\uff0c\u540c\u65f6\u63d0\u9ad8\u63a8\u65ad\u7684\u6b65\u957f\u548c\u884c\u8d70\u901f\u5ea6\u7684\u51c6\u786e\u6027\u3002|[2402.12676v1](http://arxiv.org/pdf/2402.12676v1)|null|\n", "2402.12627": "|**2024-02-20**|**A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective**|\u673a\u5668\u5b66\u4e60\u5728\u6570\u636e\u53d8\u5316\u65b9\u9762\u7684\u8fdb\u5c55\u7684\u5168\u9762\u56de\u987e\uff1a\u8de8\u9886\u57df\u89c6\u89d2|Jeng-Lin Li, Chih-Fan Hsu, Ming-Ching Chang, Wei-Chao Chen|Recent artificial intelligence (AI) technologies show remarkable evolution in various academic fields and industries. However, in the real world, dynamic data lead to principal challenges for deploying AI models. An unexpected data change brings about severe performance degradation in AI models. We identify two major related research fields, domain shift and concept drift according to the setting of the data change. Although these two popular research fields aim to solve distribution shift and non-stationary data stream problems, the underlying properties remain similar which also encourages similar technical approaches. In this review, we regroup domain shift and concept drift into a single research problem, namely the data change problem, with a systematic overview of state-of-the-art methods in the two research fields. We propose a three-phase problem categorization scheme to link the key ideas in the two technical fields. We thus provide a novel scope for researchers to explore contemporary technical strategies, learn industrial applications, and identify future directions for addressing data change challenges.|\u6700\u8fd1\u7684\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6280\u672f\u5728\u5404\u4e2a\u5b66\u672f\u9886\u57df\u548c\u884c\u4e1a\u4e2d\u5c55\u73b0\u51fa\u663e\u7740\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u52a8\u6001\u6570\u636e\u7ed9\u90e8\u7f72\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5e26\u6765\u4e86\u4e3b\u8981\u6311\u6218\u3002\u610f\u5916\u7684\u6570\u636e\u53d8\u5316\u4f1a\u5bfc\u81f4\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002\u6839\u636e\u6570\u636e\u53d8\u5316\u7684\u80cc\u666f\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u4e24\u4e2a\u4e3b\u8981\u7684\u76f8\u5173\u7814\u7a76\u9886\u57df\uff1a\u9886\u57df\u8f6c\u79fb\u548c\u6982\u5ff5\u6f02\u79fb\u3002\u5c3d\u7ba1\u8fd9\u4e24\u4e2a\u6d41\u884c\u7684\u7814\u7a76\u9886\u57df\u65e8\u5728\u89e3\u51b3\u5206\u5e03\u53d8\u5316\u548c\u975e\u5e73\u7a33\u6570\u636e\u6d41\u95ee\u9898\uff0c\u4f46\u5176\u57fa\u672c\u5c5e\u6027\u4ecd\u7136\u76f8\u4f3c\uff0c\u8fd9\u4e5f\u9f13\u52b1\u4e86\u7c7b\u4f3c\u7684\u6280\u672f\u65b9\u6cd5\u3002\u5728\u8fd9\u7bc7\u7efc\u8ff0\u4e2d\uff0c\u6211\u4eec\u5c06\u9886\u57df\u8f6c\u79fb\u548c\u6982\u5ff5\u6f02\u79fb\u91cd\u65b0\u7ec4\u5408\u4e3a\u4e00\u4e2a\u7814\u7a76\u95ee\u9898\uff0c\u5373\u6570\u636e\u53d8\u5316\u95ee\u9898\uff0c\u5e76\u7cfb\u7edf\u5730\u6982\u8ff0\u4e86\u8fd9\u4e24\u4e2a\u7814\u7a76\u9886\u57df\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u95ee\u9898\u5206\u7c7b\u65b9\u6848\u6765\u94fe\u63a5\u4e24\u4e2a\u6280\u672f\u9886\u57df\u7684\u5173\u952e\u601d\u60f3\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4e3a\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u5f53\u4ee3\u6280\u672f\u7b56\u7565\u3001\u5b66\u4e60\u5de5\u4e1a\u5e94\u7528\u5e76\u786e\u5b9a\u5e94\u5bf9\u6570\u636e\u53d8\u5316\u6311\u6218\u7684\u672a\u6765\u65b9\u5411\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u8303\u56f4\u3002|[2402.12627v1](http://arxiv.org/pdf/2402.12627v1)|null|\n"}}