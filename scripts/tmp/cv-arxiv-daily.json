{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.12217": "|**2024-01-22**|**Exploring Simple Open-Vocabulary Semantic Segmentation**|\u63a2\u7d22\u7b80\u5355\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272|Zihang Lai|Open-vocabulary semantic segmentation models aim to accurately assign a semantic label to each pixel in an image from a set of arbitrary open-vocabulary texts. In order to learn such pixel-level alignment, current approaches typically rely on a combination of (i) image-level VL model (e.g. CLIP), (ii) ground truth masks, and (iii) custom grouping encoders. In this paper, we introduce S-Seg, a novel model that can achieve surprisingly strong performance without depending on any of the above elements. S-Seg leverages pseudo-mask and language to train a MaskFormer, and can be easily trained from publicly available image-text datasets. Contrary to prior works, our model directly trains for pixel-level features and language alignment. Once trained, S-Seg generalizes well to multiple testing datasets without requiring fine-tuning. In addition, S-Seg has the extra benefits of scalability with data and consistently improvement when augmented with self-training. We believe that our simple yet effective approach will serve as a solid baseline for future research.|\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u6a21\u578b\u65e8\u5728\u4ece\u4e00\u7ec4\u4efb\u610f\u5f00\u653e\u8bcd\u6c47\u6587\u672c\u4e2d\u51c6\u786e\u5730\u5c06\u8bed\u4e49\u6807\u7b7e\u5206\u914d\u7ed9\u56fe\u50cf\u4e2d\u7684\u6bcf\u4e2a\u50cf\u7d20\u3002\u4e3a\u4e86\u5b66\u4e60\u8fd9\u79cd\u50cf\u7d20\u7ea7\u5bf9\u9f50\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e (i) \u56fe\u50cf\u7ea7 VL \u6a21\u578b\uff08\u4f8b\u5982 CLIP\uff09\u3001(ii) \u5730\u9762\u771f\u5b9e\u63a9\u6a21\u548c (iii) \u81ea\u5b9a\u4e49\u5206\u7ec4\u7f16\u7801\u5668\u7684\u7ec4\u5408\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 S-Seg\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u4e0a\u8ff0\u4efb\u4f55\u5143\u7d20\u5373\u53ef\u5b9e\u73b0\u4ee4\u4eba\u60ca\u8bb6\u7684\u5f3a\u5927\u6027\u80fd\u3002 S-Seg \u5229\u7528\u4f2a\u63a9\u6a21\u548c\u8bed\u8a00\u6765\u8bad\u7ec3 MaskFormer\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ece\u516c\u5f00\u53ef\u7528\u7684\u56fe\u50cf\u6587\u672c\u6570\u636e\u96c6\u8f7b\u677e\u8fdb\u884c\u8bad\u7ec3\u3002\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u53cd\uff0c\u6211\u4eec\u7684\u6a21\u578b\u76f4\u63a5\u8bad\u7ec3\u50cf\u7d20\u7ea7\u7279\u5f81\u548c\u8bed\u8a00\u5bf9\u9f50\u3002\u7ecf\u8fc7\u8bad\u7ec3\u540e\uff0cS-Seg \u53ef\u4ee5\u5f88\u597d\u5730\u63a8\u5e7f\u5230\u591a\u4e2a\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u65e0\u9700\u8fdb\u884c\u5fae\u8c03\u3002\u6b64\u5916\uff0cS-Seg \u8fd8\u5177\u6709\u6570\u636e\u53ef\u6269\u5c55\u6027\u4ee5\u53ca\u901a\u8fc7\u81ea\u6211\u8bad\u7ec3\u589e\u5f3a\u540e\u6301\u7eed\u6539\u8fdb\u7684\u989d\u5916\u4f18\u52bf\u3002\u6211\u4eec\u76f8\u4fe1\uff0c\u6211\u4eec\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\u5c06\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u575a\u5b9e\u7684\u57fa\u7840\u3002|[2401.12217v1](http://arxiv.org/pdf/2401.12217v1)|null|\n", "2401.12210": "|**2024-01-22**|**Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks for Accurate Bangla Sign Language Recognition**|\u8fde\u63a5\u70b9\uff1a\u5229\u7528\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u51c6\u786e\u7684\u5b5f\u52a0\u62c9\u624b\u8bed\u8bc6\u522b|Haz Sameen Shahgir, Khondker Salman Sayeed, Md Toki Tahmid, Tanjeem Azwad Zaman, Md. Zarif Ul Alam|Recent advances in Deep Learning and Computer Vision have been successfully leveraged to serve marginalized communities in various contexts. One such area is Sign Language - a primary means of communication for the deaf community. However, so far, the bulk of research efforts and investments have gone into American Sign Language, and research activity into low-resource sign languages - especially Bangla Sign Language - has lagged significantly. In this research paper, we present a new word-level Bangla Sign Language dataset - BdSL40 - consisting of 611 videos over 40 words, along with two different approaches: one with a 3D Convolutional Neural Network model and another with a novel Graph Neural Network approach for the classification of BdSL40 dataset. This is the first study on word-level BdSL recognition, and the dataset was transcribed from Indian Sign Language (ISL) using the Bangla Sign Language Dictionary (1997). The proposed GNN model achieved an F1 score of 89%. The study highlights the significant lexical and semantic similarity between BdSL, West Bengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in the literature. We release the dataset and source code to stimulate further research.|\u6df1\u5ea6\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6700\u65b0\u8fdb\u5c55\u5df2\u88ab\u6210\u529f\u5229\u7528\u6765\u4e3a\u5404\u79cd\u60c5\u51b5\u4e0b\u7684\u8fb9\u7f18\u5316\u793e\u533a\u63d0\u4f9b\u670d\u52a1\u3002\u5176\u4e2d\u4e00\u4e2a\u9886\u57df\u662f\u624b\u8bed\u2014\u2014\u804b\u4eba\u793e\u533a\u7684\u4e3b\u8981\u4ea4\u6d41\u65b9\u5f0f\u3002\u7136\u800c\uff0c\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u5927\u90e8\u5206\u7814\u7a76\u5de5\u4f5c\u548c\u6295\u8d44\u90fd\u6295\u5165\u5230\u4e86\u7f8e\u56fd\u624b\u8bed\u65b9\u9762\uff0c\u800c\u5bf9\u8d44\u6e90\u532e\u4e4f\u7684\u624b\u8bed\uff08\u5c24\u5176\u662f\u5b5f\u52a0\u62c9\u624b\u8bed\uff09\u7684\u7814\u7a76\u6d3b\u52a8\u5374\u660e\u663e\u6ede\u540e\u3002\u5728\u8fd9\u7bc7\u7814\u7a76\u8bba\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5355\u8bcd\u7ea7\u5b5f\u52a0\u62c9\u624b\u8bed\u6570\u636e\u96c6 - BdSL40 - \u7531\u8d85\u8fc7 40 \u4e2a\u5355\u8bcd\u7684 611 \u4e2a\u89c6\u9891\u7ec4\u6210\uff0c\u4ee5\u53ca\u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\uff1a\u4e00\u79cd\u91c7\u7528 3D \u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u53e6\u4e00\u79cd\u91c7\u7528\u65b0\u9896\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7528\u4e8e BdSL40 \u6570\u636e\u96c6\u7684\u5206\u7c7b\u3002\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5173\u4e8e\u5355\u8bcd\u7ea7 BdSL \u8bc6\u522b\u7684\u7814\u7a76\uff0c\u6570\u636e\u96c6\u662f\u4f7f\u7528\u5b5f\u52a0\u62c9\u624b\u8bed\u8bcd\u5178 (1997) \u4ece\u5370\u5ea6\u624b\u8bed (ISL) \u8f6c\u5f55\u800c\u6765\u3002\u6240\u63d0\u51fa\u7684 GNN \u6a21\u578b\u53d6\u5f97\u4e86 89% \u7684 F1 \u5206\u6570\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86 BdSL\u3001\u897f\u5b5f\u52a0\u62c9\u624b\u8bed\u548c ISL \u4e4b\u95f4\u663e\u7740\u7684\u8bcd\u6c47\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u4ee5\u53ca\u6587\u732e\u4e2d\u7f3a\u4e4f BdSL \u7684\u5355\u8bcd\u7ea7\u6570\u636e\u96c6\u3002\u6211\u4eec\u53d1\u5e03\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002|[2401.12210v1](http://arxiv.org/pdf/2401.12210v1)|null|\n", "2401.12202": "|**2024-01-22**|**OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics**|OK-Robot\uff1a\u96c6\u6210\u673a\u5668\u4eba\u5f00\u653e\u77e5\u8bc6\u6a21\u578b\u771f\u6b63\u91cd\u8981\u7684\u662f\u4ec0\u4e48|Peiqi Liu, Yaswanth Orru, Chris Paxton, Nur Muhammad Mahi Shafiullah, Lerrel Pinto|Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot's performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules. Videos of our experiments are available on our website: https://ok-robot.github.io|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u673a\u5668\u4eba\u9886\u57df\u53d6\u5f97\u4e86\u663e\u7740\u8fdb\u5c55\u3002\u6211\u4eec\u73b0\u5728\u62e5\u6709\u80fd\u591f\u57fa\u4e8e\u8bed\u8a00\u67e5\u8be2\u8bc6\u522b\u7269\u4f53\u7684\u89c6\u89c9\u6a21\u578b\u3001\u80fd\u591f\u6709\u6548\u63a7\u5236\u79fb\u52a8\u7cfb\u7edf\u7684\u5bfc\u822a\u7cfb\u7edf\u4ee5\u53ca\u80fd\u591f\u5904\u7406\u5404\u79cd\u7269\u4f53\u7684\u6293\u53d6\u6a21\u578b\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fd9\u4e9b\u8fdb\u6b65\uff0c\u673a\u5668\u4eba\u6280\u672f\u7684\u901a\u7528\u5e94\u7528\u4ecd\u7136\u843d\u540e\uff0c\u5c3d\u7ba1\u5b83\u4eec\u4f9d\u8d56\u4e8e\u8bc6\u522b\u3001\u5bfc\u822a\u548c\u6293\u53d6\u7b49\u57fa\u672c\u529f\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u7cfb\u7edf\u4f18\u5148\u7684\u65b9\u6cd5\u6765\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5f00\u653e\u77e5\u8bc6\u7684\u673a\u5668\u4eba\u6846\u67b6\uff0c\u79f0\u4e3a OK-Robot\u3002\u901a\u8fc7\u7ed3\u5408\u7528\u4e8e\u7269\u4f53\u68c0\u6d4b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM)\u3001\u7528\u4e8e\u8fd0\u52a8\u7684\u5bfc\u822a\u539f\u8bed\u4ee5\u53ca\u7528\u4e8e\u7269\u4f53\u64cd\u4f5c\u7684\u6293\u53d6\u539f\u8bed\uff0cOK-Robot \u4e3a\u62fe\u53d6\u548c\u653e\u7f6e\u64cd\u4f5c\u63d0\u4f9b\u4e86\u96c6\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4efb\u4f55\u57f9\u8bad\u3002\u4e3a\u4e86\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u6211\u4eec\u5728 10 \u4e2a\u771f\u5b9e\u5bb6\u5ead\u73af\u5883\u4e2d\u8fd0\u884c OK-Robot\u3002\u7ed3\u679c\u8868\u660e\uff0cOK-Robot \u5728\u5f00\u653e\u5f0f\u62fe\u653e\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86 58.5% \u7684\u6210\u529f\u7387\uff0c\u4ee3\u8868\u4e86\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c (OVMM) \u9886\u57df\u7684\u6700\u65b0\u6280\u672f\uff0c\u5176\u6027\u80fd\u51e0\u4e4e\u662f\u4e4b\u524d\u7684 1.8 \u500d\u5de5\u4f5c\u3002\u5728\u66f4\u5e72\u51c0\u3001\u6574\u6d01\u7684\u73af\u5883\u4e2d\uff0cOK-Robot \u7684\u6027\u80fd\u63d0\u5347\u81f3 82%\u3002\u7136\u800c\uff0c\u4ece OK-Robot \u83b7\u5f97\u7684\u6700\u91cd\u8981\u7684\u89c1\u89e3\u662f\u5c06 VLM \u7b49\u5f00\u653e\u77e5\u8bc6\u7cfb\u7edf\u4e0e\u673a\u5668\u4eba\u6a21\u5757\u76f8\u7ed3\u5408\u65f6\uff0c\u7ec6\u5fae\u7ec6\u8282\u7684\u5173\u952e\u4f5c\u7528\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u89c6\u9891\u53ef\u5728\u6211\u4eec\u7684\u7f51\u7ad9\u4e0a\u89c2\u770b\uff1ahttps://ok-robot.github.io|[2401.12202v1](http://arxiv.org/pdf/2401.12202v1)|null|\n", "2401.12176": "|**2024-01-22**|**Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses**|Broiler-Net\uff1a\u7528\u4e8e\u5bb6\u79bd\u820d\u4e2d\u8089\u9e21\u884c\u4e3a\u5206\u6790\u200b\u200b\u7684\u6df1\u5ea6\u5377\u79ef\u6846\u67b6|Tahereh Zarrat Ehsan, Seyed Mehdi Mohtavipour|Detecting anomalies in poultry houses is crucial for maintaining optimal chicken health conditions, minimizing economic losses and bolstering profitability. This paper presents a novel real-time framework for analyzing chicken behavior in cage-free poultry houses to detect abnormal behaviors. Specifically, two significant abnormalities, namely inactive broiler and huddling behavior, are investigated in this study. The proposed framework comprises three key steps: (1) chicken detection utilizing a state-of-the-art deep learning model, (2) tracking individual chickens across consecutive frames with a fast tracker module, and (3) detecting abnormal behaviors within the video stream. Experimental studies are conducted to evaluate the efficacy of the proposed algorithm in accurately assessing chicken behavior. The results illustrate that our framework provides a precise and efficient solution for real-time anomaly detection, facilitating timely interventions to maintain chicken health and enhance overall productivity on poultry farms. Github: https://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis|\u68c0\u6d4b\u79bd\u820d\u4e2d\u7684\u5f02\u5e38\u5bf9\u4e8e\u7ef4\u6301\u9e21\u7684\u6700\u4f73\u5065\u5eb7\u72b6\u51b5\u3001\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u7ecf\u6d4e\u635f\u5931\u548c\u63d0\u9ad8\u76c8\u5229\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u65f6\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u6563\u517b\u79bd\u820d\u4e2d\u9e21\u7684\u884c\u4e3a\u4ee5\u68c0\u6d4b\u5f02\u5e38\u884c\u4e3a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u4e24\u79cd\u663e\u7740\u7684\u5f02\u5e38\u73b0\u8c61\uff0c\u5373\u4e0d\u6d3b\u8dc3\u7684\u8089\u9e21\u548c\u6324\u4f5c\u4e00\u56e2\u7684\u884c\u4e3a\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a\uff081\uff09\u5229\u7528\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9e21\u68c0\u6d4b\uff0c\uff082\uff09\u4f7f\u7528\u5feb\u901f\u8ddf\u8e2a\u5668\u6a21\u5757\u5728\u8fde\u7eed\u5e27\u4e2d\u8ddf\u8e2a\u5355\u4e2a\u9e21\uff0c\u4ee5\u53ca\uff083\uff09\u68c0\u6d4b\u9e21\u5185\u7684\u5f02\u5e38\u884c\u4e3a\u89c6\u9891\u6d41\u3002\u8fdb\u884c\u5b9e\u9a8c\u7814\u7a76\u4ee5\u8bc4\u4f30\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u51c6\u786e\u8bc4\u4f30\u9e21\u884c\u4e3a\u65b9\u9762\u7684\u529f\u6548\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4e3a\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7cbe\u786e\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u53ca\u65f6\u91c7\u53d6\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u4fdd\u6301\u9e21\u7684\u5065\u5eb7\u5e76\u63d0\u9ad8\u5bb6\u79bd\u517b\u6b96\u573a\u7684\u6574\u4f53\u751f\u4ea7\u529b\u3002 Github\uff1ahttps://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis|[2401.12176v1](http://arxiv.org/pdf/2401.12176v1)|null|\n", "2401.12164": "|**2024-01-22**|**Semi-supervised segmentation of land cover images using nonlinear canonical correlation analysis with multiple features and t-SNE**|\u4f7f\u7528\u591a\u7279\u5f81\u975e\u7ebf\u6027\u5178\u578b\u76f8\u5173\u5206\u6790\u548c t-SNE \u5bf9\u571f\u5730\u8986\u76d6\u56fe\u50cf\u8fdb\u884c\u534a\u76d1\u7763\u5206\u5272|Hong Wei, James Xiao, Yichao Zhang, Xia Hong|Image segmentation is a clustering task whereby each pixel is assigned a cluster label. Remote sensing data usually consists of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other source data such as LIDAR (LIght Detection And Ranging) data, where available. This suggests that, in order to account for spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing the multiple bands and a local patch as appropriate. Similarly, multiple types of texture features based on a pixel's local patch would also be beneficial for encoding locally statistical information and spatial variations, without necessarily labelling pixel-wise a large amount of ground truth, then training a supervised model, which is sometimes impractical. In this work, by resorting to label only a small quantity of pixels, a new semi-supervised segmentation approach is proposed. Initially, over all pixels, an image data matrix is created in high dimensional feature space. Then, t-SNE projects the high dimensional data onto 3D embedding. By using radial basis functions as input features, which use the labelled data samples as centres, to pair with the output class labels, a modified canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced which learns the associated projection matrix via the small labelled data set. The associated canonical variables, obtained for the full image, are applied by k-means clustering algorithm. The proposed semi-supervised RBF-CCA algorithm has been implemented on several remotely sensed multispectral images, demonstrating excellent segmentation results.|\u56fe\u50cf\u5206\u5272\u662f\u4e00\u9879\u805a\u7c7b\u4efb\u52a1\uff0c\u5176\u4e2d\u6bcf\u4e2a\u50cf\u7d20\u90fd\u88ab\u5206\u914d\u4e00\u4e2a\u805a\u7c7b\u6807\u7b7e\u3002\u9065\u611f\u6570\u636e\u901a\u5e38\u7531\u591a\u4e2a\u5149\u8c31\u56fe\u50cf\u6ce2\u6bb5\u7ec4\u6210\uff0c\u5176\u4e2d\u5b58\u5728\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u571f\u5730\u8986\u76d6\u5b50\u533a\u57df\uff0c\u5e76\u4e0e\u5176\u4ed6\u6e90\u6570\u636e\uff08\u5982\u53ef\u7528\u7684\u6fc0\u5149\u96f7\u8fbe\uff08\u5149\u63a2\u6d4b\u548c\u6d4b\u8ddd\uff09\u6570\u636e\uff09\u5171\u540c\u6ce8\u518c\u3002\u8fd9\u8868\u660e\uff0c\u4e3a\u4e86\u8003\u8651\u50cf\u7d20\u4e4b\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u4e0e\u6bcf\u4e2a\u50cf\u7d20\u76f8\u5173\u8054\u7684\u7279\u5f81\u5411\u91cf\u53ef\u4ee5\u662f\u8868\u793a\u591a\u4e2a\u9891\u5e26\u548c\u9002\u5f53\u7684\u5c40\u90e8\u8865\u4e01\u7684\u5411\u91cf\u5316\u5f20\u91cf\u3002\u540c\u6837\uff0c\u57fa\u4e8e\u50cf\u7d20\u5c40\u90e8\u8865\u4e01\u7684\u591a\u79cd\u7c7b\u578b\u7684\u7eb9\u7406\u7279\u5f81\u4e5f\u6709\u5229\u4e8e\u7f16\u7801\u5c40\u90e8\u7edf\u8ba1\u4fe1\u606f\u548c\u7a7a\u95f4\u53d8\u5316\uff0c\u800c\u4e0d\u5fc5\u9010\u50cf\u7d20\u6807\u8bb0\u5927\u91cf\u5730\u9762\u5b9e\u51b5\uff0c\u7136\u540e\u8bad\u7ec3\u76d1\u7763\u6a21\u578b\uff0c\u8fd9\u6709\u65f6\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u901a\u8fc7\u4ec5\u6807\u8bb0\u5c11\u91cf\u50cf\u7d20\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u3002\u6700\u521d\uff0c\u5728\u6240\u6709\u50cf\u7d20\u4e0a\uff0c\u5728\u9ad8\u7ef4\u7279\u5f81\u7a7a\u95f4\u4e2d\u521b\u5efa\u56fe\u50cf\u6570\u636e\u77e9\u9635\u3002\u7136\u540e\uff0ct-SNE \u5c06\u9ad8\u7ef4\u6570\u636e\u6295\u5f71\u5230 3D \u5d4c\u5165\u4e0a\u3002\u901a\u8fc7\u4f7f\u7528\u5f84\u5411\u57fa\u51fd\u6570\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\uff0c\u4f7f\u7528\u6807\u8bb0\u6570\u636e\u6837\u672c\u4f5c\u4e3a\u4e2d\u5fc3\uff0c\u4e0e\u8f93\u51fa\u7c7b\u6807\u7b7e\u914d\u5bf9\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u89c4\u8303\u76f8\u5173\u5206\u6790\u7b97\u6cd5\uff0c\u79f0\u4e3a RBF-CCA\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5c0f\u6807\u8bb0\u6570\u636e\u96c6\u3002\u901a\u8fc7 k \u5747\u503c\u805a\u7c7b\u7b97\u6cd5\u5e94\u7528\u9488\u5bf9\u5b8c\u6574\u56fe\u50cf\u83b7\u5f97\u7684\u76f8\u5173\u5178\u578b\u53d8\u91cf\u3002\u6240\u63d0\u51fa\u7684\u534a\u76d1\u7763 RBF-CCA \u7b97\u6cd5\u5df2\u5728\u591a\u5e45\u9065\u611f\u591a\u5149\u8c31\u56fe\u50cf\u4e0a\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u51fa\u8272\u7684\u5206\u5272\u7ed3\u679c\u3002|[2401.12164v1](http://arxiv.org/pdf/2401.12164v1)|null|\n", "2401.12161": "|**2024-01-22**|**Automated facial recognition system using deep learning for pain assessment in adults with cerebral palsy**|\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\u5bf9\u8111\u762b\u6210\u4eba\u60a3\u8005\u8fdb\u884c\u75bc\u75db\u8bc4\u4f30|\u00c1lvaro Sabater-G\u00e1rriz, F. Xavier Gaya-Morey, Jos\u00e9 Mar\u00eda Buades-Rubio, Cristina Manresa Yee, Pedro Montoya, Inmaculada Riquelme|Background: Pain assessment in individuals with neurological conditions, especially those with limited self-report ability and altered facial expressions, presents challenges. Existing measures, relying on direct observation by caregivers, lack sensitivity and specificity. In cerebral palsy, pain is a common comorbidity and a reliable evaluation protocol is crucial. Thus, having an automatic system that recognizes facial expressions could be of enormous help when diagnosing pain in this type of patient.   Objectives: 1) to build a dataset of facial pain expressions in individuals with cerebral palsy, and 2) to develop an automated facial recognition system based on deep learning for pain assessment addressed to this population.   Methods: Ten neural networks were trained on three pain image databases, including the UNBC-McMaster Shoulder Pain Expression Archive Database, the Multimodal Intensity Pain Dataset, and the Delaware Pain Database. Additionally, a curated dataset (CPPAIN) was created, consisting of 109 preprocessed facial pain expression images from individuals with cerebral palsy, categorized by two physiotherapists using the Facial Action Coding System observational scale.   Results: InceptionV3 exhibited promising performance on the CP-PAIN dataset, achieving an accuracy of 62.67% and an F1 score of 61.12%. Explainable artificial intelligence techniques revealed consistent essential features for pain identification across models.   Conclusion: This study demonstrates the potential of deep learning models for robust pain detection in populations with neurological conditions and communication disabilities. The creation of a larger dataset specific to cerebral palsy would further enhance model accuracy, offering a valuable tool for discerning subtle and idiosyncratic pain expressions. The insights gained could extend to other complex neurological conditions.|\u80cc\u666f\uff1a\u60a3\u6709\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u7684\u4e2a\u4f53\u7684\u75bc\u75db\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u81ea\u6211\u62a5\u544a\u80fd\u529b\u6709\u9650\u548c\u9762\u90e8\u8868\u60c5\u6539\u53d8\u7684\u4e2a\u4f53\uff0c\u9762\u4e34\u7740\u6311\u6218\u3002\u73b0\u6709\u7684\u63aa\u65bd\u4f9d\u8d56\u4e8e\u62a4\u7406\u4eba\u5458\u7684\u76f4\u63a5\u89c2\u5bdf\uff0c\u7f3a\u4e4f\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u3002\u5728\u8111\u762b\u4e2d\uff0c\u75bc\u75db\u662f\u4e00\u79cd\u5e38\u89c1\u7684\u5408\u5e76\u75c7\uff0c\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6848\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u62e5\u6709\u4e00\u4e2a\u8bc6\u522b\u9762\u90e8\u8868\u60c5\u7684\u81ea\u52a8\u7cfb\u7edf\u5728\u8bca\u65ad\u6b64\u7c7b\u60a3\u8005\u7684\u75bc\u75db\u65f6\u53ef\u80fd\u4f1a\u6709\u5de8\u5927\u7684\u5e2e\u52a9\u3002\u76ee\u6807\uff1a1) \u5efa\u7acb\u8111\u762b\u60a3\u8005\u9762\u90e8\u75bc\u75db\u8868\u60c5\u6570\u636e\u96c6\uff0c2) \u5f00\u53d1\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\uff0c\u7528\u4e8e\u9488\u5bf9\u8be5\u4eba\u7fa4\u7684\u75bc\u75db\u8bc4\u4f30\u3002\u65b9\u6cd5\uff1a\u5728\u4e09\u4e2a\u75bc\u75db\u56fe\u50cf\u6570\u636e\u5e93\u4e0a\u8bad\u7ec3 10 \u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u5305\u62ec UNBC-McMaster \u80a9\u90e8\u75bc\u75db\u8868\u8fbe\u6863\u6848\u6570\u636e\u5e93\u3001\u591a\u6a21\u6001\u5f3a\u5ea6\u75bc\u75db\u6570\u636e\u96c6\u548c\u7279\u62c9\u534e\u5dde\u75bc\u75db\u6570\u636e\u5e93\u3002\u6b64\u5916\uff0c\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a\u7cbe\u9009\u6570\u636e\u96c6 (CPPAIN)\uff0c\u5176\u4e2d\u5305\u542b\u6765\u81ea\u8111\u762b\u60a3\u8005\u7684 109 \u5f20\u7ecf\u8fc7\u9884\u5904\u7406\u7684\u9762\u90e8\u75bc\u75db\u8868\u60c5\u56fe\u50cf\uff0c\u7531\u4e24\u540d\u7269\u7406\u6cbb\u7597\u5e08\u4f7f\u7528\u9762\u90e8\u52a8\u4f5c\u7f16\u7801\u7cfb\u7edf\u89c2\u5bdf\u91cf\u8868\u8fdb\u884c\u5206\u7c7b\u3002\u7ed3\u679c\uff1aInceptionV3 \u5728 CP-PAIN \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u8fbe\u5230 62.67%\uff0cF1 \u5206\u6570\u8fbe\u5230 61.12%\u3002\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\u63ed\u793a\u4e86\u8de8\u6a21\u578b\u75bc\u75db\u8bc6\u522b\u7684\u4e00\u81f4\u57fa\u672c\u7279\u5f81\u3002\u7ed3\u8bba\uff1a\u8fd9\u9879\u7814\u7a76\u8bc1\u660e\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u548c\u6c9f\u901a\u969c\u788d\u4eba\u7fa4\u4e2d\u8fdb\u884c\u7a33\u5065\u75bc\u75db\u68c0\u6d4b\u7684\u6f5c\u529b\u3002\u521b\u5efa\u9488\u5bf9\u8111\u762b\u7684\u66f4\u5927\u6570\u636e\u96c6\u5c06\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u8fa8\u522b\u5fae\u5999\u548c\u7279\u6b8a\u7684\u75bc\u75db\u8868\u8fbe\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002\u83b7\u5f97\u7684\u89c1\u89e3\u53ef\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u590d\u6742\u7684\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u3002|[2401.12161v1](http://arxiv.org/pdf/2401.12161v1)|null|\n", "2401.12133": "|**2024-01-22**|**VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games**|VRMN-bD\uff1aVR \u5355\u53e3\u4e92\u52a8\u6e38\u620f\u4e2d\u6c89\u6d78\u5f0f\u4eba\u7c7b\u6050\u60e7\u53cd\u5e94\u7684\u591a\u6a21\u6001\u81ea\u7136\u884c\u4e3a\u6570\u636e\u96c6|He Zhang, Xinyang Li, Yuanxi Sun, Xinyi Fu, Christine Qiu, John M. Carroll|Understanding and recognizing emotions are important and challenging issues in the metaverse era. Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications. In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players. We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively. We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets. The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope. We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments. Moreover, we discussed the implications of this work for communities and applications. The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD.|\u7406\u89e3\u548c\u8bc6\u522b\u60c5\u7eea\u662f\u865a\u62df\u5b87\u5b99\u65f6\u4ee3\u91cd\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u5728\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u73af\u5883\u4e2d\u7406\u89e3\u3001\u8bc6\u522b\u548c\u9884\u6d4b\u6050\u60e7\u662f\u4eba\u7c7b\u57fa\u672c\u60c5\u611f\u4e4b\u4e00\uff0c\u5728\u6c89\u6d78\u5f0f\u6e38\u620f\u5f00\u53d1\u3001\u573a\u666f\u5f00\u53d1\u548c\u4e0b\u4e00\u4ee3\u865a\u62df\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ee5 VR \u6050\u6016\u6e38\u620f\u4e3a\u5a92\u4ecb\uff0c\u901a\u8fc7\u6536\u96c6 23 \u540d\u73a9\u5bb6\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u59ff\u52bf\u3001\u97f3\u9891\u548c\u751f\u7406\u4fe1\u53f7\uff09\u6765\u5206\u6790\u6050\u60e7\u60c5\u7eea\u3002\u6211\u4eec\u4f7f\u7528\u57fa\u4e8e LSTM \u7684\u6a21\u578b\u6765\u9884\u6d4b\u6050\u60e7\uff0c\u5728 6 \u7ea7\u5206\u7c7b\uff08\u65e0\u6050\u60e7\u548c\u4e94\u4e2a\u4e0d\u540c\u7ea7\u522b\u7684\u6050\u60e7\uff09\u548c 2 \u7ea7\u5206\u7c7b\uff08\u65e0\u6050\u60e7\u548c\u6050\u60e7\uff09\u4e0b\uff0c\u51c6\u786e\u7387\u5206\u522b\u4e3a 65.31% \u548c 90.47%\u3002\u6211\u4eec\u6784\u5efa\u4e86\u6c89\u6d78\u5f0f\u4eba\u7c7b\u6050\u60e7\u53cd\u5e94\u7684\u591a\u6a21\u6001\u81ea\u7136\u884c\u4e3a\u6570\u636e\u96c6\uff08VRMN-bD\uff09\uff0c\u5e76\u5c06\u5176\u4e0e\u73b0\u6709\u7684\u76f8\u5173\u9ad8\u7ea7\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6570\u636e\u96c6\u5728\u6536\u96c6\u65b9\u6cd5\u3001\u6570\u636e\u89c4\u6a21\u548c\u53d7\u4f17\u8303\u56f4\u65b9\u9762\u7684\u9650\u5236\u8f83\u5c11\u3002\u6211\u4eec\u5728\u9488\u5bf9 VR \u7ad9\u7acb\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u6050\u60e7\u548c\u884c\u4e3a\u7684\u591a\u6a21\u5f0f\u6570\u636e\u96c6\u65b9\u9762\u5177\u6709\u72ec\u7279\u6027\u548c\u5148\u8fdb\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86\u8fd9\u9879\u5de5\u4f5c\u5bf9\u793e\u533a\u548c\u5e94\u7528\u7a0b\u5e8f\u7684\u5f71\u54cd\u3002\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728 https://github.com/KindOPSTAR/VRMN-bD \u83b7\u53d6\u3002|[2401.12133v1](http://arxiv.org/pdf/2401.12133v1)|null|\n", "2401.12129": "|**2024-01-22**|**Out-of-Distribution Detection & Applications With Ablated Learned Temperature Energy**|\u5177\u6709\u6d88\u878d\u5b66\u4e60\u6e29\u5ea6\u80fd\u91cf\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u548c\u5e94\u7528|Will LeVine, Benjamin Pikus, Jacob Phillips, Berk Norman, Fernando Amat Gil, Sean Hendryx|As deep neural networks become adopted in high-stakes domains, it is crucial to be able to identify when inference inputs are Out-of-Distribution (OOD) so that users can be alerted of likely drops in performance and calibration despite high confidence. Among many others, existing methods use the following two scores to do so without training on any apriori OOD examples: a learned temperature and an energy score. In this paper we introduce Ablated Learned Temperature Energy (or \"AbeT\" for short), a method which combines these prior methods in novel ways with effective modifications. Due to these contributions, AbeT lowers the False Positive Rate at $95\\%$ True Positive Rate (FPR@95) by $35.39\\%$ in classification (averaged across all ID and OOD datasets measured) compared to state of the art without training networks in multiple stages or requiring hyperparameters or test-time backward passes. We additionally provide empirical insights as to how our model learns to distinguish between In-Distribution (ID) and OOD samples while only being explicitly trained on ID samples via exposure to misclassified ID examples at training time. Lastly, we show the efficacy of our method in identifying predicted bounding boxes and pixels corresponding to OOD objects in object detection and semantic segmentation, respectively - with an AUROC increase of $5.15\\%$ in object detection and both a decrease in FPR@95 of $41.48\\%$ and an increase in AUPRC of $34.20\\%$ on average in semantic segmentation compared to previous state of the art.|\u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5f97\u5230\u91c7\u7528\uff0c\u80fd\u591f\u8bc6\u522b\u63a8\u7406\u8f93\u5165\u4f55\u65f6\u51fa\u73b0\u5206\u5e03\u5916 (OOD) \u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u4fbf\u7528\u6237\u5728\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u6536\u5230\u6027\u80fd\u548c\u6821\u51c6\u53ef\u80fd\u4e0b\u964d\u7684\u8b66\u62a5\u3002\u9664\u8bb8\u591a\u5176\u4ed6\u65b9\u6cd5\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u4ee5\u4e0b\u4e24\u4e2a\u5206\u6570\u6765\u5b9e\u73b0\u6b64\u76ee\u7684\uff0c\u800c\u65e0\u9700\u5bf9\u4efb\u4f55\u5148\u9a8c OOD \u793a\u4f8b\u8fdb\u884c\u8bad\u7ec3\uff1a\u5b66\u4e60\u6e29\u5ea6\u548c\u80fd\u91cf\u5206\u6570\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u6d88\u878d\u5b66\u4e60\u6e29\u5ea6\u80fd\u91cf\uff08Ablated LearnedTemperature Energy\uff0c\u7b80\u79f0\u201cAbeT\u201d\uff09\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4ee5\u65b0\u9896\u7684\u65b9\u5f0f\u7ed3\u5408\u4e86\u8fd9\u4e9b\u73b0\u6709\u65b9\u6cd5\u5e76\u8fdb\u884c\u4e86\u6709\u6548\u7684\u4fee\u6539\u3002\u7531\u4e8e\u8fd9\u4e9b\u8d21\u732e\uff0c\u4e0e\u6ca1\u6709\u8bad\u7ec3\u7f51\u7edc\u7684\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0cAbeT \u5c06\u5206\u7c7b\u4e2d\u7684\u5047\u9633\u6027\u7387 (FPR@95) \u964d\u4f4e\u4e86 35.39\\%$\uff08\u6d4b\u91cf\u7684\u6240\u6709 ID \u548c OOD \u6570\u636e\u96c6\u7684\u5e73\u5747\u503c\uff09\uff0c\u4e3a $95\\%$ \u771f\u9633\u6027\u7387 (FPR@95)\u5904\u4e8e\u591a\u4e2a\u9636\u6bb5\u6216\u9700\u8981\u8d85\u53c2\u6570\u6216\u6d4b\u8bd5\u65f6\u95f4\u5411\u540e\u4f20\u9012\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e\u6211\u4eec\u7684\u6a21\u578b\u5982\u4f55\u5b66\u4e60\u533a\u5206\u5206\u5e03\u5185 (ID) \u548c OOD \u6837\u672c\u7684\u7ecf\u9a8c\u89c1\u89e3\uff0c\u540c\u65f6\u4ec5\u901a\u8fc7\u5728\u8bad\u7ec3\u65f6\u63a5\u89e6\u9519\u8bef\u5206\u7c7b\u7684 ID \u793a\u4f8b\u6765\u5bf9 ID \u6837\u672c\u8fdb\u884c\u663e\u5f0f\u8bad\u7ec3\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5bf9\u8c61\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u4e2d\u5206\u522b\u8bc6\u522b\u4e0e OOD \u5bf9\u8c61\u76f8\u5bf9\u5e94\u7684\u9884\u6d4b\u8fb9\u754c\u6846\u548c\u50cf\u7d20\u7684\u529f\u6548 - \u5bf9\u8c61\u68c0\u6d4b\u4e2d\u7684 AUROC \u589e\u52a0\u4e86 5.15\\%$\uff0c\u800c FPR@95 \u5747\u51cf\u5c11\u4e86\u4e0e\u4e4b\u524d\u7684\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u8bed\u4e49\u5206\u5272\u5728\u8bed\u4e49\u5206\u5272\u65b9\u9762\u5e73\u5747\u589e\u52a0\u4e86 41.48\\%$\uff0cAUPRC \u5e73\u5747\u589e\u52a0\u4e86 $34.20\\%$\u3002|[2401.12129v1](http://arxiv.org/pdf/2401.12129v1)|null|\n", "2401.12074": "|**2024-01-22**|**DeepCERES: A Deep learning method for cerebellar lobule segmentation using ultra-high resolution multimodal MRI**|DeepCERES\uff1a\u4f7f\u7528\u8d85\u9ad8\u5206\u8fa8\u7387\u591a\u6a21\u6001 MRI \u8fdb\u884c\u5c0f\u8111\u5c0f\u53f6\u5206\u5272\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5|Sergio Morell-Ortega, Marina Ruiz-Perez, Marien Gadea, Roberto Vivo-Hernando, Gregorio Rubio, Fernando Aparici, Mariam de la Iglesia-Vaya, Gwenaelle Catheline, Pierrick Coup\u00e9, Jos\u00e9 V. Manj\u00f3n|This paper introduces a novel multimodal and high-resolution human brain cerebellum lobule segmentation method. Unlike current tools that operate at standard resolution ($1 \\text{ mm}^{3}$) or using mono-modal data, the proposed method improves cerebellum lobule segmentation through the use of a multimodal and ultra-high resolution ($0.125 \\text{ mm}^{3}$) training dataset. To develop the method, first, a database of semi-automatically labelled cerebellum lobules was created to train the proposed method with ultra-high resolution T1 and T2 MR images. Then, an ensemble of deep networks has been designed and developed, allowing the proposed method to excel in the complex cerebellum lobule segmentation task, improving precision while being memory efficient. Notably, our approach deviates from the traditional U-Net model by exploring alternative architectures. We have also integrated deep learning with classical machine learning methods incorporating a priori knowledge from multi-atlas segmentation, which improved precision and robustness. Finally, a new online pipeline, named DeepCERES, has been developed to make available the proposed method to the scientific community requiring as input only a single T1 MR image at standard resolution.|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u3001\u9ad8\u5206\u8fa8\u7387\u4eba\u8111\u5c0f\u8111\u5c0f\u53f6\u5206\u5272\u65b9\u6cd5\u3002\u4e0e\u5f53\u524d\u4ee5\u6807\u51c6\u5206\u8fa8\u7387\uff08$1 \\text{ mm}^{3}$\uff09\u6216\u4f7f\u7528\u5355\u6a21\u6001\u6570\u636e\u8fd0\u884c\u7684\u5de5\u5177\u4e0d\u540c\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u591a\u6a21\u6001\u548c\u8d85\u9ad8\u5206\u8fa8\u7387\uff08$0.125 \\text { mm}^{3}$) \u8bad\u7ec3\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u5f00\u53d1\u8be5\u65b9\u6cd5\uff0c\u9996\u5148\u521b\u5efa\u4e86\u534a\u81ea\u52a8\u6807\u8bb0\u5c0f\u8111\u5c0f\u53f6\u7684\u6570\u636e\u5e93\uff0c\u4ee5\u4f7f\u7528\u8d85\u9ad8\u5206\u8fa8\u7387 T1 \u548c T2 MR \u56fe\u50cf\u6765\u8bad\u7ec3\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u7136\u540e\uff0c\u8bbe\u8ba1\u548c\u5f00\u53d1\u4e86\u4e00\u4e2a\u6df1\u5ea6\u7f51\u7edc\u96c6\u5408\uff0c\u4f7f\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u590d\u6742\u7684\u5c0f\u8111\u5c0f\u53f6\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u540c\u65f6\u63d0\u9ad8\u5185\u5b58\u6548\u7387\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u63a2\u7d22\u66ff\u4ee3\u67b6\u6784\u6765\u504f\u79bb\u4f20\u7edf\u7684 U-Net \u6a21\u578b\u3002\u6211\u4eec\u8fd8\u5c06\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u7ed3\u5408\u4e86\u591a\u56fe\u8c31\u5206\u5272\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002\u6700\u540e\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3a DeepCERES \u7684\u65b0\u5728\u7ebf\u7ba1\u9053\uff0c\u4ee5\u4fbf\u5411\u79d1\u5b66\u754c\u63d0\u4f9b\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u4ec5\u9700\u8981\u6807\u51c6\u5206\u8fa8\u7387\u7684\u5355\u4e2a T1 MR \u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u3002|[2401.12074v1](http://arxiv.org/pdf/2401.12074v1)|null|\n", "2401.12051": "|**2024-01-22**|**CloSe: A 3D Clothing Segmentation Dataset and Model**|CloSe\uff1a3D \u670d\u88c5\u5206\u5272\u6570\u636e\u96c6\u548c\u6a21\u578b|Dimitrije Anti\u0107, Garvita Tiwari, Batuhan Ozcomlekci, Riccardo Marin, Gerard Pons-Moll|3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries. Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personalization. To address this, we first introduce CloSe-D: a novel large-scale dataset containing 3D clothing segmentation of 3167 scans, covering a range of 18 distinct clothing classes. Additionally, we propose CloSe-Net, the first learning-based 3D clothing segmentation model for fine-grained segmentation from colored point clouds. CloSe-Net uses local point features, body-clothing correlation, and a garment-class and point features-based attention module, improving performance over baselines and prior work. The proposed attention module enables our model to learn appearance and geometry-dependent clothing prior from data. We further validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing. We also introduce CloSe-T, a 3D interactive tool for refining segmentation labels. Combining the tool with CloSe-T in a continual learning setup demonstrates improved generalization on real-world data. Dataset, model, and tool can be found at https://virtualhumans.mpi-inf.mpg.de/close3dv24/.|3D \u670d\u88c5\u5efa\u6a21\u548c\u6570\u636e\u96c6\u5728\u5a31\u4e50\u3001\u52a8\u753b\u548c\u6570\u5b57\u65f6\u5c1a\u884c\u4e1a\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u73b0\u6709\u7684\u5de5\u4f5c\u901a\u5e38\u7f3a\u4e4f\u8be6\u7ec6\u7684\u8bed\u4e49\u7406\u89e3\u6216\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u73b0\u5b9e\u6027\u548c\u4e2a\u6027\u5316\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165 CloSe-D\uff1a\u4e00\u4e2a\u65b0\u9896\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b 3167 \u6b21\u626b\u63cf\u7684 3D \u670d\u88c5\u5206\u5272\uff0c\u6db5\u76d6 18 \u4e2a\u4e0d\u540c\u7684\u670d\u88c5\u7c7b\u522b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 CloSe-Net\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684 3D \u670d\u88c5\u5206\u5272\u6a21\u578b\uff0c\u7528\u4e8e\u5bf9\u5f69\u8272\u70b9\u4e91\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u5272\u3002 CloSe-Net \u4f7f\u7528\u5c40\u90e8\u70b9\u7279\u5f81\u3001\u8eab\u4f53-\u670d\u88c5\u76f8\u5173\u6027\u4ee5\u53ca\u57fa\u4e8e\u670d\u88c5\u7c7b\u548c\u70b9\u7279\u5f81\u7684\u6ce8\u610f\u6a21\u5757\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u548c\u4e4b\u524d\u7684\u5de5\u4f5c\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u6240\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u6a21\u5757\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u5148\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5916\u89c2\u548c\u4f9d\u8d56\u4e8e\u51e0\u4f55\u5f62\u72b6\u7684\u670d\u88c5\u3002\u6211\u4eec\u901a\u8fc7\u6210\u529f\u5206\u5272\u516c\u5f00\u7684\u7a7f\u7740\u670d\u88c5\u7684\u4eba\u6570\u636e\u96c6\uff0c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86 CloSe-T\uff0c\u4e00\u79cd\u7528\u4e8e\u7ec6\u5316\u5206\u5272\u6807\u7b7e\u7684 3D \u4ea4\u4e92\u5f0f\u5de5\u5177\u3002\u5728\u6301\u7eed\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5c06\u8be5\u5de5\u5177\u4e0e CloSe-T \u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5bf9\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u5de5\u5177\u53ef\u4ee5\u5728 https://virtual humans.mpi-inf.mpg.de/close3dv24/ \u627e\u5230\u3002|[2401.12051v1](http://arxiv.org/pdf/2401.12051v1)|null|\n", "2401.12048": "|**2024-01-22**|**HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report (Team KuzHum)**|HomeRobot \u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u6311\u6218\u8d5b 2023 \u53c2\u8d5b\u8005\u62a5\u544a\uff08KuzHum \u56e2\u961f\uff09|Volodymyr Kuzma, Vladyslav Humennyy, Ruslan Partsey|We report an improvements to NeurIPS 2023 HomeRobot: Open Vocabulary Mobile Manipulation (OVMM) Challenge reinforcement learning baseline. More specifically, we propose more accurate semantic segmentation module, along with better place skill policy, and high-level heuristic that outperforms the baseline by 2.4% of overall success rate (sevenfold improvement) and 8.2% of partial success rate (1.75 times improvement) on Test Standard split of the challenge dataset. With aforementioned enhancements incorporated our agent scored 3rd place in the challenge on both simulation and real-world stages.|\u6211\u4eec\u62a5\u544a\u4e86 NeurIPS 2023 HomeRobot\uff1a\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c (OVMM) \u6311\u6218\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u7684\u6539\u8fdb\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u66f4\u51c6\u786e\u7684\u8bed\u4e49\u5206\u5272\u6a21\u5757\uff0c\u4ee5\u53ca\u66f4\u597d\u7684\u4f4d\u7f6e\u6280\u80fd\u7b56\u7565\u548c\u9ad8\u7ea7\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5176\u6574\u4f53\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u9ad8\u51fa 2.4%\uff08\u63d0\u9ad8\u4e86 7 \u500d\uff09\uff0c\u90e8\u5206\u6210\u529f\u7387\u63d0\u9ad8\u4e86 8.2%\uff08\u63d0\u9ad8\u4e86 1.75 \u500d\uff09\u6311\u6218\u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u6807\u51c6\u5206\u5272\u3002\u901a\u8fc7\u4e0a\u8ff0\u589e\u5f3a\u529f\u80fd\uff0c\u6211\u4eec\u7684\u4ee3\u7406\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u9636\u6bb5\u7684\u6311\u6218\u4e2d\u90fd\u83b7\u5f97\u4e86\u7b2c\u4e09\u540d\u3002|[2401.12048v1](http://arxiv.org/pdf/2401.12048v1)|null|\n", "2401.12039": "|**2024-01-22**|**Look, Listen and Recognise: Character-Aware Audio-Visual Subtitling**|\u770b\u3001\u542c\u3001\u8ba4\uff1a\u89d2\u8272\u611f\u77e5\u89c6\u542c\u5b57\u5e55|Bruno Korbar, Jaesung Huh, Andrew Zisserman|The goal of this paper is automatic character-aware subtitle generation. Given a video and a minimal amount of metadata, we propose an audio-visual method that generates a full transcript of the dialogue, with precise speech timestamps, and the character speaking identified. The key idea is to first use audio-visual cues to select a set of high-precision audio exemplars for each character, and then use these exemplars to classify all speech segments by speaker identity. Notably, the method does not require face detection or tracking. We evaluate the method over a variety of TV sitcoms, including Seinfeld, Fraiser and Scrubs. We envision this system being useful for the automatic generation of subtitles to improve the accessibility of the vast amount of videos available on modern streaming services. Project page : \\url{https://www.robots.ox.ac.uk/~vgg/research/look-listen-recognise/}|\u672c\u6587\u7684\u76ee\u6807\u662f\u81ea\u52a8\u751f\u6210\u5b57\u7b26\u611f\u77e5\u5b57\u5e55\u3002\u7ed9\u5b9a\u89c6\u9891\u548c\u6700\u5c11\u91cf\u7684\u5143\u6570\u636e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u542c\u65b9\u6cd5\uff0c\u53ef\u4ee5\u751f\u6210\u5b8c\u6574\u7684\u5bf9\u8bdd\u8bb0\u5f55\uff0c\u5177\u6709\u7cbe\u786e\u7684\u8bed\u97f3\u65f6\u95f4\u6233\u548c\u5df2\u8bc6\u522b\u7684\u8bf4\u8bdd\u89d2\u8272\u3002\u5173\u952e\u601d\u60f3\u662f\u9996\u5148\u4f7f\u7528\u89c6\u542c\u7ebf\u7d22\u4e3a\u6bcf\u4e2a\u89d2\u8272\u9009\u62e9\u4e00\u7ec4\u9ad8\u7cbe\u5ea6\u7684\u97f3\u9891\u6837\u672c\uff0c\u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u6837\u672c\u6839\u636e\u8bf4\u8bdd\u8005\u8eab\u4efd\u5bf9\u6240\u6709\u8bed\u97f3\u7247\u6bb5\u8fdb\u884c\u5206\u7c7b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u9762\u90e8\u68c0\u6d4b\u6216\u8ddf\u8e2a\u3002\u6211\u4eec\u5728\u5404\u79cd\u7535\u89c6\u60c5\u666f\u559c\u5267\u4e2d\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u5305\u62ec\u300a\u5b8b\u98de\u6b63\u4f20\u300b\u3001\u300a\u5f17\u83b1\u6cfd\u300b\u548c\u300a\u5b9e\u4e60\u533b\u751f\u98ce\u4e91\u300b\u3002\u6211\u4eec\u9884\u8ba1\u8be5\u7cfb\u7edf\u53ef\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5b57\u5e55\uff0c\u4ee5\u63d0\u9ad8\u73b0\u4ee3\u6d41\u5a92\u4f53\u670d\u52a1\u4e0a\u5927\u91cf\u89c6\u9891\u7684\u53ef\u8bbf\u95ee\u6027\u3002\u9879\u76ee\u9875\u9762\uff1a\\url{https://www.robots.ox.ac.uk/~vgg/research/look-listen-recognise/}|[2401.12039v1](http://arxiv.org/pdf/2401.12039v1)|null|\n", "2401.11914": "|**2024-01-22**|**A Saliency Enhanced Feature Fusion based multiscale RGB-D Salient Object Detection Network**|\u57fa\u4e8e\u663e\u7740\u6027\u589e\u5f3a\u7279\u5f81\u878d\u5408\u7684\u591a\u5c3a\u5ea6 RGB-D \u663e\u7740\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc|Rui Huang, Qingyi Zhao, Yan Xing, Sihua Gao, Weifeng Xu, Yuxiang Zhang, Wei Fan|Multiscale convolutional neural network (CNN) has demonstrated remarkable capabilities in solving various vision problems. However, fusing features of different scales alwaysresults in large model sizes, impeding the application of multiscale CNNs in RGB-D saliency detection. In this paper, we propose a customized feature fusion module, called Saliency Enhanced Feature Fusion (SEFF), for RGB-D saliency detection. SEFF utilizes saliency maps of the neighboring scales to enhance the necessary features for fusing, resulting in more representative fused features. Our multiscale RGB-D saliency detector uses SEFF and processes images with three different scales. SEFF is used to fuse the features of RGB and depth images, as well as the features of decoders at different scales. Extensive experiments on five benchmark datasets have demonstrated the superiority of our method over ten SOTA saliency detectors.|\u591a\u5c3a\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u89e3\u51b3\u5404\u79cd\u89c6\u89c9\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u878d\u5408\u4e0d\u540c\u5c3a\u5ea6\u7684\u7279\u5f81\u603b\u662f\u4f1a\u5bfc\u81f4\u6a21\u578b\u5c3a\u5bf8\u8fc7\u5927\uff0c\u963b\u788d\u4e86\u591a\u5c3a\u5ea6 CNN \u5728 RGB-D \u663e\u7740\u6027\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u5236\u7684\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u79f0\u4e3a\u663e\u7740\u6027\u589e\u5f3a\u7279\u5f81\u878d\u5408\uff08SEFF\uff09\uff0c\u7528\u4e8e RGB-D \u663e\u7740\u6027\u68c0\u6d4b\u3002 SEFF \u5229\u7528\u76f8\u90bb\u5c3a\u5ea6\u7684\u663e\u7740\u6027\u56fe\u6765\u589e\u5f3a\u878d\u5408\u6240\u9700\u7684\u7279\u5f81\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u5177\u4ee3\u8868\u6027\u7684\u878d\u5408\u7279\u5f81\u3002\u6211\u4eec\u7684\u591a\u5c3a\u5ea6 RGB-D \u663e\u7740\u6027\u68c0\u6d4b\u5668\u4f7f\u7528 SEFF \u5e76\u5904\u7406\u5177\u6709\u4e09\u79cd\u4e0d\u540c\u5c3a\u5ea6\u7684\u56fe\u50cf\u3002 SEFF\u7528\u4e8e\u878d\u5408RGB\u548c\u6df1\u5ea6\u56fe\u50cf\u7684\u7279\u5f81\uff0c\u4ee5\u53ca\u4e0d\u540c\u5c3a\u5ea6\u7684\u89e3\u7801\u5668\u7684\u7279\u5f81\u3002\u5bf9\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u5341\u4e2a SOTA \u663e\u7740\u6027\u68c0\u6d4b\u5668\u7684\u4f18\u8d8a\u6027\u3002|[2401.11914v1](http://arxiv.org/pdf/2401.11914v1)|null|\n", "2401.11913": "|**2024-01-22**|**Large receptive field strategy and important feature extraction strategy in 3D object detection**|3D\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u5927\u611f\u53d7\u91ce\u7b56\u7565\u548c\u91cd\u8981\u7279\u5f81\u63d0\u53d6\u7b56\u7565|Leichao Cui, Xiuxian Li, Min Meng|The enhancement of 3D object detection is pivotal for precise environmental perception and improved task execution capabilities in autonomous driving. LiDAR point clouds, offering accurate depth information, serve as a crucial information for this purpose. Our study focuses on key challenges in 3D target detection. To tackle the challenge of expanding the receptive field of a 3D convolutional kernel, we introduce the Dynamic Feature Fusion Module (DFFM). This module achieves adaptive expansion of the 3D convolutional kernel's receptive field, balancing the expansion with acceptable computational loads. This innovation reduces operations, expands the receptive field, and allows the model to dynamically adjust to different object requirements. Simultaneously, we identify redundant information in 3D features. Employing the Feature Selection Module (FSM) quantitatively evaluates and eliminates non-important features, achieving the separation of output box fitting and feature extraction. This innovation enables the detector to focus on critical features, resulting in model compression, reduced computational burden, and minimized candidate frame interference. Extensive experiments confirm that both DFFM and FSM not only enhance current benchmarks, particularly in small target detection, but also accelerate network performance. Importantly, these modules exhibit effective complementarity.|3D \u7269\u4f53\u68c0\u6d4b\u7684\u589e\u5f3a\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7cbe\u786e\u7684\u73af\u5883\u611f\u77e5\u548c\u63d0\u9ad8\u4efb\u52a1\u6267\u884c\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u63d0\u4f9b\u51c6\u786e\u7684\u6df1\u5ea6\u4fe1\u606f\uff0c\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u7684\u7684\u5173\u952e\u4fe1\u606f\u3002\u6211\u4eec\u7684\u7814\u7a76\u91cd\u70b9\u662f 3D \u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u6269\u5c55 3D \u5377\u79ef\u6838\u611f\u53d7\u91ce\u7684\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u52a8\u6001\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08DFFM\uff09\u3002\u8be5\u6a21\u5757\u5b9e\u73b0\u4e86 3D \u5377\u79ef\u6838\u611f\u53d7\u91ce\u7684\u81ea\u9002\u5e94\u6269\u5c55\uff0c\u5e73\u8861\u6269\u5c55\u4e0e\u53ef\u63a5\u53d7\u7684\u8ba1\u7b97\u8d1f\u8f7d\u3002\u8fd9\u4e00\u521b\u65b0\u51cf\u5c11\u4e86\u64cd\u4f5c\uff0c\u6269\u5927\u4e86\u611f\u53d7\u91ce\uff0c\u5e76\u5141\u8bb8\u6a21\u578b\u52a8\u6001\u8c03\u6574\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u5bf9\u8c61\u8981\u6c42\u3002\u540c\u65f6\uff0c\u6211\u4eec\u8bc6\u522b 3D \u7279\u5f81\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u3002\u91c7\u7528\u7279\u5f81\u9009\u62e9\u6a21\u5757\uff08FSM\uff09\u5b9a\u91cf\u8bc4\u4f30\u5e76\u5254\u9664\u4e0d\u91cd\u8981\u7279\u5f81\uff0c\u5b9e\u73b0\u8f93\u51fa\u6846\u62df\u5408\u548c\u7279\u5f81\u63d0\u53d6\u7684\u5206\u79bb\u3002\u8fd9\u9879\u521b\u65b0\u4f7f\u68c0\u6d4b\u5668\u80fd\u591f\u4e13\u6ce8\u4e8e\u5173\u952e\u7279\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u6a21\u578b\u538b\u7f29\u3001\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u5e76\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5019\u9009\u5e27\u5e72\u6270\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\uff0cDFFM \u548c FSM \u4e0d\u4ec5\u589e\u5f3a\u4e86\u5f53\u524d\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u5728\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\uff0c\u800c\u4e14\u8fd8\u63d0\u9ad8\u4e86\u7f51\u7edc\u6027\u80fd\u3002\u91cd\u8981\u7684\u662f\uff0c\u8fd9\u4e9b\u6a21\u5757\u8868\u73b0\u51fa\u6709\u6548\u7684\u4e92\u8865\u6027\u3002|[2401.11913v1](http://arxiv.org/pdf/2401.11913v1)|null|\n", "2401.11877": "|**2024-01-22**|**Evaluating the Feasibility of Standard Facial Expression Recognition in Individuals with Moderate to Severe Intellectual Disabilities**|\u8bc4\u4f30\u6807\u51c6\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u5bf9\u4e2d\u5ea6\u81f3\u91cd\u5ea6\u667a\u529b\u969c\u788d\u4e2a\u4f53\u7684\u53ef\u884c\u6027|F. Xavier Gaya-Morey, Silvia Ramis, Jose M. Buades-Rubio, Cristina Manresa-Yee|Recent research has underscored the increasing preference of users for human-like interactions with machines. Consequently, facial expression recognition has gained significance as a means of imparting social robots with the capacity to discern the emotional states of users. In this investigation, we assess the suitability of deep learning approaches, known for their remarkable performance in this domain, for recognizing facial expressions in individuals with intellectual disabilities, which has not been yet studied in the literature, to the best of our knowledge. To address this objective, we train a set of twelve distinct convolutional neural networks in different approaches, including an ensemble of datasets without individuals with intellectual disabilities and a dataset featuring such individuals. Our examination of the outcomes achieved by the various models under distinct training conditions, coupled with a comprehensive analysis of critical facial regions during expression recognition facilitated by explainable artificial intelligence techniques, revealed significant distinctions in facial expressions between individuals with and without intellectual disabilities, as well as among individuals with intellectual disabilities. Remarkably, our findings demonstrate the feasibility of facial expression recognition within this population through tailored user-specific training methodologies, which enable the models to effectively address the unique expressions of each user.|\u6700\u8fd1\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u7528\u6237\u8d8a\u6765\u8d8a\u503e\u5411\u4e8e\u4e0e\u673a\u5668\u8fdb\u884c\u7c7b\u4eba\u4ea4\u4e92\u3002\u56e0\u6b64\uff0c\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4f5c\u4e3a\u8d4b\u4e88\u793e\u4ea4\u673a\u5668\u4eba\u8fa8\u522b\u7528\u6237\u60c5\u7eea\u72b6\u6001\u80fd\u529b\u7684\u4e00\u79cd\u624b\u6bb5\uff0c\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u5728\u8fd9\u9879\u8c03\u67e5\u4e2d\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u9002\u7528\u6027\uff0c\u8be5\u65b9\u6cd5\u4ee5\u5176\u5728\u8be5\u9886\u57df\u7684\u5353\u8d8a\u8868\u73b0\u800c\u95fb\u540d\uff0c\u7528\u4e8e\u8bc6\u522b\u667a\u969c\u4eba\u58eb\u7684\u9762\u90e8\u8868\u60c5\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u5c1a\u672a\u5728\u6587\u732e\u4e2d\u8fdb\u884c\u8fc7\u7814\u7a76\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u7528\u4e0d\u540c\u7684\u65b9\u6cd5\u8bad\u7ec3\u4e86\u4e00\u7ec4\u5341\u4e8c\u4e2a\u4e0d\u540c\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5305\u62ec\u4e00\u7ec4\u6ca1\u6709\u667a\u969c\u4eba\u58eb\u7684\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5305\u542b\u667a\u969c\u4eba\u58eb\u7684\u6570\u636e\u96c6\u3002\u6211\u4eec\u5bf9\u5404\u79cd\u6a21\u578b\u5728\u4e0d\u540c\u8bad\u7ec3\u6761\u4ef6\u4e0b\u53d6\u5f97\u7684\u7ed3\u679c\u8fdb\u884c\u4e86\u68c0\u67e5\uff0c\u518d\u52a0\u4e0a\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\u4fc3\u8fdb\u7684\u8868\u60c5\u8bc6\u522b\u8fc7\u7a0b\u4e2d\u5173\u952e\u9762\u90e8\u533a\u57df\u7684\u5168\u9762\u5206\u6790\uff0c\u63ed\u793a\u4e86\u667a\u529b\u969c\u788d\u8005\u548c\u975e\u667a\u529b\u969c\u788d\u8005\u4e4b\u95f4\u9762\u90e8\u8868\u60c5\u7684\u663e\u7740\u5dee\u5f02\u3002\u5c31\u50cf\u667a\u529b\u969c\u788d\u4eba\u58eb\u4e00\u6837\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u901a\u8fc7\u9488\u5bf9\u7279\u5b9a\u7528\u6237\u91cf\u8eab\u5b9a\u5236\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u8be5\u4eba\u7fa4\u4e2d\u8fdb\u884c\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u53ef\u884c\u6027\uff0c\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u6bcf\u4e2a\u7528\u6237\u7684\u72ec\u7279\u8868\u60c5\u3002|[2401.11877v1](http://arxiv.org/pdf/2401.11877v1)|null|\n", "2401.11874": "|**2024-01-22**|**Detect-Order-Construct: A Tree Construction based Approach for Hierarchical Document Structure Analysis**|\u68c0\u6d4b-\u987a\u5e8f-\u6784\u9020\uff1a\u4e00\u79cd\u57fa\u4e8e\u6811\u6784\u9020\u7684\u5206\u5c42\u6587\u6863\u7ed3\u6784\u5206\u6790\u65b9\u6cd5|Jiawei Wang, Kai Hu, Zhuoyao Zhong, Lei Sun, Qiang Huo|Document structure analysis (aka document layout analysis) is crucial for understanding the physical layout and logical structure of documents, with applications in information retrieval, document summarization, knowledge extraction, etc. In this paper, we concentrate on Hierarchical Document Structure Analysis (HDSA) to explore hierarchical relationships within structured documents created using authoring software employing hierarchical schemas, such as LaTeX, Microsoft Word, and HTML. To comprehensively analyze hierarchical document structures, we propose a tree construction based approach that addresses multiple subtasks concurrently, including page object detection (Detect), reading order prediction of identified objects (Order), and the construction of intended hierarchical structure (Construct). We present an effective end-to-end solution based on this framework to demonstrate its performance. To assess our approach, we develop a comprehensive benchmark called Comp-HRDoc, which evaluates the above subtasks simultaneously. Our end-to-end system achieves state-of-the-art performance on two large-scale document layout analysis datasets (PubLayNet and DocLayNet), a high-quality hierarchical document structure reconstruction dataset (HRDoc), and our Comp-HRDoc benchmark. The Comp-HRDoc benchmark will be released to facilitate further research in this field.|\u6587\u6863\u7ed3\u6784\u5206\u6790\uff08\u53c8\u540d\u6587\u6863\u5e03\u5c40\u5206\u6790\uff09\u5bf9\u4e8e\u7406\u89e3\u6587\u6863\u7684\u7269\u7406\u5e03\u5c40\u548c\u903b\u8f91\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u5e94\u7528\u4e8e\u4fe1\u606f\u68c0\u7d22\u3001\u6587\u6863\u6458\u8981\u3001\u77e5\u8bc6\u63d0\u53d6\u7b49\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8\u5206\u5c42\u6587\u6863\u7ed3\u6784\u5206\u6790\uff08HDSA\uff09\u63a2\u7d22\u4f7f\u7528\u91c7\u7528\u5206\u5c42\u6a21\u5f0f\u7684\u521b\u4f5c\u8f6f\u4ef6\uff08\u4f8b\u5982 LaTeX\u3001Microsoft Word \u548c HTML\uff09\u521b\u5efa\u7684\u7ed3\u6784\u5316\u6587\u6863\u4e2d\u7684\u5206\u5c42\u5173\u7cfb\u3002\u4e3a\u4e86\u5168\u9762\u5206\u6790\u5c42\u6b21\u6587\u6863\u7ed3\u6784\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u540c\u65f6\u5904\u7406\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5305\u62ec\u9875\u9762\u5bf9\u8c61\u68c0\u6d4b\uff08Detect\uff09\u3001\u8bc6\u522b\u5bf9\u8c61\u7684\u9605\u8bfb\u987a\u5e8f\u9884\u6d4b\uff08Order\uff09\u4ee5\u53ca\u6784\u5efa\u9884\u671f\u7684\u5c42\u6b21\u7ed3\u6784\uff08Construct\uff09\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8be5\u6846\u67b6\u7684\u6709\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u6765\u5c55\u793a\u5176\u6027\u80fd\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3a Comp-HRDoc \u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5b83\u540c\u65f6\u8bc4\u4f30\u4e0a\u8ff0\u5b50\u4efb\u52a1\u3002\u6211\u4eec\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u5728\u4e24\u4e2a\u5927\u578b\u6587\u6863\u5e03\u5c40\u5206\u6790\u6570\u636e\u96c6\uff08PubLayNet \u548c DocLayNet\uff09\u3001\u9ad8\u8d28\u91cf\u5206\u5c42\u6587\u6863\u7ed3\u6784\u91cd\u5efa\u6570\u636e\u96c6\uff08HRDoc\uff09\u4ee5\u53ca\u6211\u4eec\u7684 Comp-HRDoc \u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002 Comp-HRDoc \u57fa\u51c6\u6d4b\u8bd5\u5c06\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002|[2401.11874v1](http://arxiv.org/pdf/2401.11874v1)|null|\n", "2401.11856": "|**2024-01-22**|**MOSformer: Momentum encoder-based inter-slice fusion transformer for medical image segmentation**|MOSformer\uff1a\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u57fa\u4e8e\u52a8\u91cf\u7f16\u7801\u5668\u7684\u5c42\u95f4\u878d\u5408\u53d8\u538b\u5668|De-Xing Huang, Xiao-Hu Zhou, Xiao-Liang Xie, Shi-Qi Liu, Zhen-Qiu Feng, Mei-Jiang Gui, Hao Li, Tian-Yu Xiang, Xiu-Ling Liu, Zeng-Guang Hou|Medical image segmentation takes an important position in various clinical applications. Deep learning has emerged as the predominant solution for automated segmentation of volumetric medical images. 2.5D-based segmentation models bridge computational efficiency of 2D-based models and spatial perception capabilities of 3D-based models. However, prevailing 2.5D-based models often treat each slice equally, failing to effectively learn and exploit inter-slice information, resulting in suboptimal segmentation performances. In this paper, a novel Momentum encoder-based inter-slice fusion transformer (MOSformer) is proposed to overcome this issue by leveraging inter-slice information at multi-scale feature maps extracted by different encoders. Specifically, dual encoders are employed to enhance feature distinguishability among different slices. One of the encoders is moving-averaged to maintain the consistency of slice representations. Moreover, an IF-Swin transformer module is developed to fuse inter-slice multi-scale features. The MOSformer is evaluated on three benchmark datasets (Synapse, ACDC, and AMOS), establishing a new state-of-the-art with 85.63%, 92.19%, and 85.43% of DSC, respectively. These promising results indicate its competitiveness in medical image segmentation. Codes and models of MOSformer will be made publicly available upon acceptance.|\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u5404\u79cd\u4e34\u5e8a\u5e94\u7528\u4e2d\u5360\u6709\u91cd\u8981\u5730\u4f4d\u3002\u6df1\u5ea6\u5b66\u4e60\u5df2\u6210\u4e3a\u4f53\u79ef\u533b\u5b66\u56fe\u50cf\u81ea\u52a8\u5206\u5272\u7684\u4e3b\u8981\u89e3\u51b3\u65b9\u6848\u3002\u57fa\u4e8e 2.5D \u7684\u5206\u5272\u6a21\u578b\u5c06\u57fa\u4e8e 2D \u7684\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u57fa\u4e8e 3D \u7684\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u7ed3\u5408\u8d77\u6765\u3002\u7136\u800c\uff0c\u6d41\u884c\u7684\u57fa\u4e8e 2.5D \u7684\u6a21\u578b\u901a\u5e38\u5e73\u7b49\u5730\u5bf9\u5f85\u6bcf\u4e2a\u5207\u7247\uff0c\u65e0\u6cd5\u6709\u6548\u5730\u5b66\u4e60\u548c\u5229\u7528\u5207\u7247\u95f4\u4fe1\u606f\uff0c\u5bfc\u81f4\u5206\u5272\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u52a8\u91cf\u7f16\u7801\u5668\u7684\u7247\u95f4\u878d\u5408\u53d8\u538b\u5668\uff08MOSformer\uff09\uff0c\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u7f16\u7801\u5668\u63d0\u53d6\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u56fe\u7684\u7247\u95f4\u4fe1\u606f\u6765\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u91c7\u7528\u53cc\u7f16\u7801\u5668\u6765\u589e\u5f3a\u4e0d\u540c\u5207\u7247\u4e4b\u95f4\u7684\u7279\u5f81\u53ef\u533a\u5206\u6027\u3002\u5176\u4e2d\u4e00\u4e2a\u7f16\u7801\u5668\u662f\u79fb\u52a8\u5e73\u5747\u7684\uff0c\u4ee5\u4fdd\u6301\u5207\u7247\u8868\u793a\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86 IF-Swin \u53d8\u538b\u5668\u6a21\u5757\u6765\u878d\u5408\u7247\u95f4\u591a\u5c3a\u5ea6\u7279\u5f81\u3002 MOSformer \u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08Synapse\u3001ACDC \u548c AMOS\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5206\u522b\u4ee5 DSC \u7684 85.63%\u300192.19% \u548c 85.43% \u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u8fd9\u4e9b\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u8868\u660e\u4e86\u5176\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u9762\u7684\u7ade\u4e89\u529b\u3002 MOSformer \u7684\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728\u63a5\u53d7\u540e\u516c\u5f00\u53d1\u5e03\u3002|[2401.11856v1](http://arxiv.org/pdf/2401.11856v1)|null|\n", "2401.11847": "|**2024-01-22**|**SignVTCL: Multi-Modal Continuous Sign Language Recognition Enhanced by Visual-Textual Contrastive Learning**|SignVTCL\uff1a\u901a\u8fc7\u89c6\u89c9\u6587\u672c\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u591a\u6a21\u5f0f\u8fde\u7eed\u624b\u8bed\u8bc6\u522b|Hao Chen, Jiaze Wang, Ziyu Guo, Jinpeng Li, Donghao Zhou, Bian Wu, Chenyong Guan, Guangyong Chen, Pheng-Ann Heng|Sign language recognition (SLR) plays a vital role in facilitating communication for the hearing-impaired community. SLR is a weakly supervised task where entire videos are annotated with glosses, making it challenging to identify the corresponding gloss within a video segment. Recent studies indicate that the main bottleneck in SLR is the insufficient training caused by the limited availability of large-scale datasets. To address this challenge, we present SignVTCL, a multi-modal continuous sign language recognition framework enhanced by visual-textual contrastive learning, which leverages the full potential of multi-modal data and the generalization ability of language model. SignVTCL integrates multi-modal data (video, keypoints, and optical flow) simultaneously to train a unified visual backbone, thereby yielding more robust visual representations. Furthermore, SignVTCL contains a visual-textual alignment approach incorporating gloss-level and sentence-level alignment to ensure precise correspondence between visual features and glosses at the level of individual glosses and sentence. Experimental results conducted on three datasets, Phoenix-2014, Phoenix-2014T, and CSL-Daily, demonstrate that SignVTCL achieves state-of-the-art results compared with previous methods.|\u624b\u8bed\u8bc6\u522b (SLR) \u5728\u4fc3\u8fdb\u542c\u529b\u969c\u788d\u793e\u533a\u7684\u6c9f\u901a\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002 SLR \u662f\u4e00\u9879\u5f31\u76d1\u7763\u4efb\u52a1\uff0c\u6574\u4e2a\u89c6\u9891\u90fd\u7528\u6ce8\u91ca\u8fdb\u884c\u6ce8\u91ca\uff0c\u56e0\u6b64\u5f88\u96be\u8bc6\u522b\u89c6\u9891\u7247\u6bb5\u4e2d\u76f8\u5e94\u7684\u6ce8\u91ca\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cSLR \u7684\u4e3b\u8981\u74f6\u9888\u662f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u53ef\u7528\u6027\u6709\u9650\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u8db3\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 SignVTCL\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u89c6\u89c9\u6587\u672c\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684\u591a\u6a21\u6001\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\u6846\u67b6\uff0c\u5b83\u5145\u5206\u5229\u7528\u4e86\u591a\u6a21\u6001\u6570\u636e\u7684\u6f5c\u529b\u548c\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002 SignVTCL \u540c\u65f6\u96c6\u6210\u591a\u6a21\u6001\u6570\u636e\uff08\u89c6\u9891\u3001\u5173\u952e\u70b9\u548c\u5149\u6d41\uff09\u6765\u8bad\u7ec3\u7edf\u4e00\u7684\u89c6\u89c9\u4e3b\u5e72\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u8868\u793a\u3002\u6b64\u5916\uff0cSignVTCL \u5305\u542b\u4e00\u79cd\u89c6\u89c9\u6587\u672c\u5bf9\u9f50\u65b9\u6cd5\uff0c\u7ed3\u5408\u6ce8\u91ca\u7ea7\u522b\u548c\u53e5\u5b50\u7ea7\u522b\u5bf9\u9f50\uff0c\u4ee5\u786e\u4fdd\u89c6\u89c9\u7279\u5f81\u548c\u6ce8\u91ca\u5728\u5355\u4e2a\u6ce8\u91ca\u548c\u53e5\u5b50\u7ea7\u522b\u4e0a\u7684\u7cbe\u786e\u5bf9\u5e94\u3002\u5728 Phoenix-2014\u3001Phoenix-2014T \u548c CSL-Daily \u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cSignVTCL \u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002|[2401.11847v1](http://arxiv.org/pdf/2401.11847v1)|null|\n", "2401.11835": "|**2024-01-22**|**Unveiling the Human-like Similarities of Automatic Facial Expression Recognition: An Empirical Exploration through Explainable AI**|\u63ed\u793a\u81ea\u52a8\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u7c7b\u4eba\u76f8\u4f3c\u6027\uff1a\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\u8fdb\u884c\u5b9e\u8bc1\u63a2\u7d22|F. Xavier Gaya-Morey, Silvia Ramis-Guarinos, Cristina Manresa-Yee, Jose M. Buades-Rubio|Facial expression recognition is vital for human behavior analysis, and deep learning has enabled models that can outperform humans. However, it is unclear how closely they mimic human processing. This study aims to explore the similarity between deep neural networks and human perception by comparing twelve different networks, including both general object classifiers and FER-specific models. We employ an innovative global explainable AI method to generate heatmaps, revealing crucial facial regions for the twelve networks trained on six facial expressions. We assess these results both quantitatively and qualitatively, comparing them to ground truth masks based on Friesen and Ekman's description and among them. We use Intersection over Union (IoU) and normalized correlation coefficients for comparisons. We generate 72 heatmaps to highlight critical regions for each expression and architecture. Qualitatively, models with pre-trained weights show more similarity in heatmaps compared to those without pre-training. Specifically, eye and nose areas influence certain facial expressions, while the mouth is consistently important across all models and expressions. Quantitatively, we find low average IoU values (avg. 0.2702) across all expressions and architectures. The best-performing architecture averages 0.3269, while the worst-performing one averages 0.2066. Dendrograms, built with the normalized correlation coefficient, reveal two main clusters for most expressions: models with pre-training and models without pre-training. Findings suggest limited alignment between human and AI facial expression recognition, with network architectures influencing the similarity, as similar architectures prioritize similar facial regions.|\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u5bf9\u4e8e\u4eba\u7c7b\u884c\u4e3a\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u6df1\u5ea6\u5b66\u4e60\u4f7f\u6a21\u578b\u80fd\u591f\u8d85\u8d8a\u4eba\u7c7b\u3002\u7136\u800c\uff0c\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u6a21\u4eff\u4eba\u7c7b\u5904\u7406\u7684\u7a0b\u5ea6\u5982\u4f55\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6bd4\u8f83 12 \u79cd\u4e0d\u540c\u7684\u7f51\u7edc\uff08\u5305\u62ec\u901a\u7528\u5bf9\u8c61\u5206\u7c7b\u5668\u548c FER \u7279\u5b9a\u6a21\u578b\uff09\u6765\u63a2\u7d22\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u4eba\u7c7b\u611f\u77e5\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u6211\u4eec\u91c7\u7528\u521b\u65b0\u7684\u5168\u5c40\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u6765\u751f\u6210\u70ed\u56fe\uff0c\u63ed\u793a\u53d7\u516d\u79cd\u9762\u90e8\u8868\u60c5\u8bad\u7ec3\u7684\u5341\u4e8c\u4e2a\u7f51\u7edc\u7684\u5173\u952e\u9762\u90e8\u533a\u57df\u3002\u6211\u4eec\u5bf9\u8fd9\u4e9b\u7ed3\u679c\u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\uff0c\u5c06\u5b83\u4eec\u4e0e\u57fa\u4e8e Friesen \u548c Ekman \u7684\u63cf\u8ff0\u4ee5\u53ca\u5176\u4e2d\u7684\u771f\u5b9e\u63a9\u6a21\u8fdb\u884c\u6bd4\u8f83\u3002\u6211\u4eec\u4f7f\u7528\u4ea4\u5e76\u5e76\u96c6\uff08IoU\uff09\u548c\u5f52\u4e00\u5316\u76f8\u5173\u7cfb\u6570\u8fdb\u884c\u6bd4\u8f83\u3002\u6211\u4eec\u751f\u6210 72 \u4e2a\u70ed\u56fe\u6765\u7a81\u51fa\u663e\u793a\u6bcf\u4e2a\u8868\u8fbe\u5f0f\u548c\u67b6\u6784\u7684\u5173\u952e\u533a\u57df\u3002\u5b9a\u6027\u5730\u8bb2\uff0c\u4e0e\u6ca1\u6709\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u5177\u6709\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u6a21\u578b\u5728\u70ed\u56fe\u4e2d\u8868\u73b0\u51fa\u66f4\u591a\u7684\u76f8\u4f3c\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u773c\u775b\u548c\u9f3b\u5b50\u533a\u57df\u4f1a\u5f71\u54cd\u67d0\u4e9b\u9762\u90e8\u8868\u60c5\uff0c\u800c\u5634\u5df4\u5728\u6240\u6709\u6a21\u578b\u548c\u8868\u60c5\u4e2d\u59cb\u7ec8\u5f88\u91cd\u8981\u3002\u5b9a\u91cf\u5730\uff0c\u6211\u4eec\u53d1\u73b0\u6240\u6709\u8868\u8fbe\u5f0f\u548c\u67b6\u6784\u7684\u5e73\u5747 IoU \u503c\u8f83\u4f4e\uff08\u5e73\u5747 0.2702\uff09\u3002\u6027\u80fd\u6700\u597d\u7684\u67b6\u6784\u5e73\u5747\u4e3a 0.3269\uff0c\u800c\u6027\u80fd\u6700\u5dee\u7684\u67b6\u6784\u5e73\u5747\u4e3a 0.2066\u3002\u4f7f\u7528\u5f52\u4e00\u5316\u76f8\u5173\u7cfb\u6570\u6784\u5efa\u7684\u6811\u72b6\u56fe\u63ed\u793a\u4e86\u5927\u591a\u6570\u8868\u8fbe\u5f0f\u7684\u4e24\u4e2a\u4e3b\u8981\u7c07\uff1a\u7ecf\u8fc7\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u672a\u7ecf\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u6709\u9650\uff0c\u7f51\u7edc\u67b6\u6784\u4f1a\u5f71\u54cd\u76f8\u4f3c\u6027\uff0c\u56e0\u4e3a\u76f8\u4f3c\u7684\u67b6\u6784\u4f1a\u4f18\u5148\u8003\u8651\u76f8\u4f3c\u7684\u9762\u90e8\u533a\u57df\u3002|[2401.11835v1](http://arxiv.org/pdf/2401.11835v1)|null|\n", "2401.11824": "|**2024-01-22**|**Rethinking Centered Kernel Alignment in Knowledge Distillation**|\u91cd\u65b0\u601d\u8003\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u4e2d\u5fc3\u5185\u6838\u5bf9\u9f50|Zikai Zhou, Yunhang Shen, Shitong Shao, Huanran Chen, Linrui Gong, Shaohui Lin|Knowledge distillation has emerged as a highly effective method for bridging the representation discrepancy between large-scale models and lightweight models. Prevalent approaches involve leveraging appropriate metrics to minimize the divergence or distance between the knowledge extracted from the teacher model and the knowledge learned by the student model. Centered Kernel Alignment (CKA) is widely used to measure representation similarity and has been applied in several knowledge distillation methods. However, these methods are complex and fail to uncover the essence of CKA, thus not answering the question of how to use CKA to achieve simple and effective distillation properly. This paper first provides a theoretical perspective to illustrate the effectiveness of CKA, which decouples CKA to the upper bound of Maximum Mean Discrepancy~(MMD) and a constant term. Drawing from this, we propose a novel Relation-Centered Kernel Alignment~(RCKA) framework, which practically establishes a connection between CKA and MMD. Furthermore, we dynamically customize the application of CKA based on the characteristics of each task, with less computational source yet comparable performance than the previous methods. The extensive experiments on the CIFAR-100, ImageNet-1k, and MS-COCO demonstrate that our method achieves state-of-the-art performance on almost all teacher-student pairs for image classification and object detection, validating the effectiveness of our approaches.|\u77e5\u8bc6\u84b8\u998f\u5df2\u6210\u4e3a\u5f25\u5408\u5927\u89c4\u6a21\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e4b\u95f4\u8868\u793a\u5dee\u5f02\u7684\u9ad8\u6548\u65b9\u6cd5\u3002\u6d41\u884c\u7684\u65b9\u6cd5\u6d89\u53ca\u5229\u7528\u9002\u5f53\u7684\u6307\u6807\u6765\u6700\u5c0f\u5316\u4ece\u6559\u5e08\u6a21\u578b\u63d0\u53d6\u7684\u77e5\u8bc6\u4e0e\u5b66\u751f\u6a21\u578b\u5b66\u5230\u7684\u77e5\u8bc6\u4e4b\u95f4\u7684\u5206\u6b67\u6216\u8ddd\u79bb\u3002\u4e2d\u5fc3\u6838\u5bf9\u9f50\uff08CKA\uff09\u5e7f\u6cdb\u7528\u4e8e\u6d4b\u91cf\u8868\u793a\u76f8\u4f3c\u6027\uff0c\u5e76\u5df2\u5e94\u7528\u4e8e\u591a\u79cd\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e2d\u3002\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u6bd4\u8f83\u590d\u6742\uff0c\u672a\u80fd\u63ed\u793aCKA\u7684\u672c\u8d28\uff0c\u65e0\u6cd5\u6b63\u786e\u56de\u7b54\u5982\u4f55\u5229\u7528CKA\u5b9e\u73b0\u7b80\u5355\u6709\u6548\u7684\u84b8\u998f\u7684\u95ee\u9898\u3002\u672c\u6587\u9996\u5148\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u89c6\u89d2\u6765\u8bf4\u660e CKA \u7684\u6709\u6548\u6027\uff0c\u5b83\u5c06 CKA \u89e3\u8026\u5230\u6700\u5927\u5e73\u5747\u5dee\u5f02~\uff08MMD\uff09\u7684\u4e0a\u9650\u548c\u5e38\u6570\u9879\u3002\u7531\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ee5\u5173\u7cfb\u4e3a\u4e2d\u5fc3\u7684\u5185\u6838\u5bf9\u9f50\uff08RCKA\uff09\u6846\u67b6\uff0c\u5b83\u5b9e\u9645\u4e0a\u5728CKA\u548cMMD\u4e4b\u95f4\u5efa\u7acb\u4e86\u8054\u7cfb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6839\u636e\u6bcf\u4e2a\u4efb\u52a1\u7684\u7279\u70b9\u52a8\u6001\u5b9a\u5236CKA\u7684\u5e94\u7528\uff0c\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8ba1\u7b97\u8d44\u6e90\u66f4\u5c11\uff0c\u4f46\u6027\u80fd\u76f8\u5f53\u3002\u5728 CIFAR-100\u3001ImageNet-1k \u548c MS-COCO \u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u5e08\u751f\u5bf9\u7684\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2401.11824v1](http://arxiv.org/pdf/2401.11824v1)|null|\n", "2401.11814": "|**2024-01-22**|**Symbrain: A large-scale dataset of MRI images for neonatal brain symmetry analysis**|Symbrain\uff1a\u7528\u4e8e\u65b0\u751f\u513f\u5927\u8111\u5bf9\u79f0\u6027\u5206\u6790\u7684\u5927\u89c4\u6a21 MRI \u56fe\u50cf\u6570\u636e\u96c6|Arnaud Gucciardi, Safouane El Ghazouali, Francesca Venturini, Vida Groznik, Umberto Michelucci|This paper presents an annotated dataset of brain MRI images designed to advance the field of brain symmetry study. Magnetic resonance imaging (MRI) has gained interest in analyzing brain symmetry in neonatal infants, and challenges remain due to the vast size differences between fetal and adult brains. Classification methods for brain structural MRI use scales and visual cues to assess hemisphere symmetry, which can help diagnose neonatal patients by comparing hemispheres and anatomical regions of interest in the brain. Using the Developing Human Connectome Project dataset, this work presents a dataset comprising cerebral images extracted as slices across selected portions of interest for clinical evaluation . All the extracted images are annotated with the brain's midline. All the extracted images are annotated with the brain's midline. From the assumption that a decrease in symmetry is directly related to possible clinical pathologies, the dataset can contribute to a more precise diagnosis because it can be used to train deep learning model application in neonatal cerebral MRI anomaly detection from postnatal infant scans thanks to computer vision. Such models learn to identify and classify anomalies by identifying potential asymmetrical patterns in medical MRI images. Furthermore, this dataset can contribute to the research and development of methods using the relative symmetry of the two brain hemispheres for crucial diagnosis and treatment planning.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e26\u6ce8\u91ca\u7684\u8111 MRI \u56fe\u50cf\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u8fdb\u8111\u5bf9\u79f0\u6027\u7814\u7a76\u9886\u57df\u3002\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u5728\u5206\u6790\u65b0\u751f\u513f\u5927\u8111\u5bf9\u79f0\u6027\u65b9\u9762\u5f15\u8d77\u4e86\u4eba\u4eec\u7684\u5174\u8da3\uff0c\u4f46\u7531\u4e8e\u80ce\u513f\u548c\u6210\u4eba\u5927\u8111\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u7684\u5c3a\u5bf8\u5dee\u5f02\uff0c\u6311\u6218\u4ecd\u7136\u5b58\u5728\u3002\u8111\u7ed3\u6784 MRI \u7684\u5206\u7c7b\u65b9\u6cd5\u4f7f\u7528\u5c3a\u5ea6\u548c\u89c6\u89c9\u7ebf\u7d22\u6765\u8bc4\u4f30\u534a\u7403\u5bf9\u79f0\u6027\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u6bd4\u8f83\u5927\u8111\u534a\u7403\u548c\u611f\u5174\u8da3\u7684\u89e3\u5256\u533a\u57df\u6765\u5e2e\u52a9\u8bca\u65ad\u65b0\u751f\u513f\u60a3\u8005\u3002\u8fd9\u9879\u5de5\u4f5c\u4f7f\u7528\u5f00\u53d1\u4eba\u7c7b\u8fde\u63a5\u7ec4\u9879\u76ee\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u63d0\u53d6\u4e3a\u8de8\u9009\u5b9a\u611f\u5174\u8da3\u90e8\u5206\u7684\u5207\u7247\u7684\u5927\u8111\u56fe\u50cf\uff0c\u7528\u4e8e\u4e34\u5e8a\u8bc4\u4f30\u3002\u6240\u6709\u63d0\u53d6\u7684\u56fe\u50cf\u90fd\u7528\u5927\u8111\u4e2d\u7ebf\u6ce8\u91ca\u3002\u6240\u6709\u63d0\u53d6\u7684\u56fe\u50cf\u90fd\u7528\u5927\u8111\u4e2d\u7ebf\u6ce8\u91ca\u3002\u5047\u8bbe\u5bf9\u79f0\u6027\u964d\u4f4e\u4e0e\u53ef\u80fd\u7684\u4e34\u5e8a\u75c5\u7406\u76f4\u63a5\u76f8\u5173\uff0c\u8be5\u6570\u636e\u96c6\u53ef\u4ee5\u6709\u52a9\u4e8e\u66f4\u7cbe\u786e\u7684\u8bca\u65ad\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e94\u7528\uff0c\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u4ece\u4ea7\u540e\u5a74\u513f\u626b\u63cf\u4e2d\u68c0\u6d4b\u65b0\u751f\u513f\u8111 MRI \u5f02\u5e38\u3002\u6b64\u7c7b\u6a21\u578b\u901a\u8fc7\u8bc6\u522b\u533b\u5b66 MRI \u56fe\u50cf\u4e2d\u6f5c\u5728\u7684\u4e0d\u5bf9\u79f0\u6a21\u5f0f\u6765\u5b66\u4e60\u8bc6\u522b\u548c\u5206\u7c7b\u5f02\u5e38\u3002\u6b64\u5916\uff0c\u8be5\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u7814\u7a76\u548c\u5f00\u53d1\u5229\u7528\u4e24\u4e2a\u5927\u8111\u534a\u7403\u7684\u76f8\u5bf9\u5bf9\u79f0\u6027\u8fdb\u884c\u5173\u952e\u8bca\u65ad\u548c\u6cbb\u7597\u8ba1\u5212\u7684\u65b9\u6cd5\u3002|[2401.11814v1](http://arxiv.org/pdf/2401.11814v1)|null|\n", "2401.11791": "|**2024-01-22**|**SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation**|SemPLeS\uff1a\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u8bed\u4e49\u63d0\u793a\u5b66\u4e60|Ci-Siang Lin, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen|Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using training image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may only capture discriminative image regions of target object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Class-associated Semantic Refinement to learn the prompts that adequately describe and suppress the image backgrounds associated with each target object category. In this way, our proposed framework is able to perform better semantic matching between object regions and the associated text labels, resulting in desired pseudo masks for training the segmentation model. The proposed SemPLeS framework achieves SOTA performance on the standard WSSS benchmarks, PASCAL VOC and MS COCO, and demonstrated interpretability with the semantic visualization of our learned prompts. The codes will be released.|\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\uff08WSSS\uff09\u65e8\u5728\u4f7f\u7528\u4ec5\u5177\u6709\u56fe\u50cf\u7ea7\u76d1\u7763\u7684\u8bad\u7ec3\u56fe\u50cf\u6570\u636e\u6765\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u3002\u7531\u4e8e\u65e0\u6cd5\u83b7\u5f97\u7cbe\u786e\u7684\u50cf\u7d20\u7ea7\u6ce8\u91ca\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4fa7\u91cd\u4e8e\u901a\u8fc7\u7ec6\u5316\u7c7b\u4f3c CAM \u7684\u70ed\u56fe\u6765\u751f\u6210\u7528\u4e8e\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u7684\u4f2a\u63a9\u6a21\u3002\u7136\u800c\uff0c\u751f\u6210\u7684\u70ed\u56fe\u53ef\u80fd\u4ec5\u6355\u83b7\u76ee\u6807\u5bf9\u8c61\u7c7b\u522b\u6216\u76f8\u5173\u8054\u7684\u5171\u73b0\u80cc\u666f\u7684\u8fa8\u522b\u56fe\u50cf\u533a\u57df\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cdWSSS\u8bed\u4e49\u63d0\u793a\u5b66\u4e60\uff08SemPLeS\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5b66\u4e60\u6709\u6548\u63d0\u793aCLIP\u7a7a\u95f4\u4ee5\u589e\u5f3a\u5206\u5272\u533a\u57df\u548c\u76ee\u6807\u5bf9\u8c61\u7c7b\u522b\u4e4b\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u5bf9\u6bd4\u63d0\u793a\u5b66\u4e60\u548c\u7c7b\u76f8\u5173\u8bed\u4e49\u7ec6\u5316\u6765\u5b66\u4e60\u5145\u5206\u63cf\u8ff0\u548c\u6291\u5236\u4e0e\u6bcf\u4e2a\u76ee\u6807\u5bf9\u8c61\u7c7b\u522b\u76f8\u5173\u7684\u56fe\u50cf\u80cc\u666f\u7684\u63d0\u793a\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u5728\u5bf9\u8c61\u533a\u57df\u548c\u76f8\u5173\u6587\u672c\u6807\u7b7e\u4e4b\u95f4\u6267\u884c\u66f4\u597d\u7684\u8bed\u4e49\u5339\u914d\uff0c\u4ece\u800c\u4ea7\u751f\u7528\u4e8e\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u7684\u6240\u9700\u4f2a\u63a9\u6a21\u3002\u6240\u63d0\u51fa\u7684 SemPLeS \u6846\u67b6\u5728\u6807\u51c6 WSSS \u57fa\u51c6\u3001PASCAL VOC \u548c MS COCO \u4e0a\u5b9e\u73b0\u4e86 SOTA \u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6211\u4eec\u5b66\u4e60\u7684\u63d0\u793a\u7684\u8bed\u4e49\u53ef\u89c6\u5316\u5c55\u793a\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u4ee3\u7801\u5c06\u88ab\u91ca\u653e\u3002|[2401.11791v1](http://arxiv.org/pdf/2401.11791v1)|null|\n", "2401.11790": "|**2024-01-22**|**Deep Learning for Computer Vision based Activity Recognition and Fall Detection of the Elderly: a Systematic Review**|\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6df1\u5ea6\u5b66\u4e60\u8001\u5e74\u4eba\u6d3b\u52a8\u8bc6\u522b\u548c\u8dcc\u5012\u68c0\u6d4b\uff1a\u7cfb\u7edf\u7efc\u8ff0|F. Xavier Gaya-Morey, Cristina Manresa-Yee, Jose M. Buades-Rubio|As the percentage of elderly people in developed countries increases worldwide, the healthcare of this collective is a worrying matter, especially if it includes the preservation of their autonomy. In this direction, many studies are being published on Ambient Assisted Living (AAL) systems, which help to reduce the preoccupations raised by the independent living of the elderly. In this study, a systematic review of the literature is presented on fall detection and Human Activity Recognition (HAR) for the elderly, as the two main tasks to solve to guarantee the safety of elderly people living alone. To address the current tendency to perform these two tasks, the review focuses on the use of Deep Learning (DL) based approaches on computer vision data. In addition, different collections of data like DL models, datasets or hardware (e.g. depth or thermal cameras) are gathered from the reviewed studies and provided for reference in future studies. Strengths and weaknesses of existing approaches are also discussed and, based on them, our recommendations for future works are provided.|\u968f\u7740\u5168\u7403\u53d1\u8fbe\u56fd\u5bb6\u8001\u5e74\u4eba\u53e3\u6bd4\u4f8b\u7684\u589e\u52a0\uff0c\u8fd9\u4e2a\u7fa4\u4f53\u7684\u533b\u7597\u4fdd\u5065\u662f\u4e00\u4e2a\u4ee4\u4eba\u62c5\u5fe7\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5982\u679c\u5b83\u5305\u62ec\u4fdd\u62a4\u4ed6\u4eec\u7684\u81ea\u4e3b\u6743\u7684\u8bdd\u3002\u5728\u8fd9\u4e2a\u65b9\u5411\u4e0a\uff0c\u8bb8\u591a\u5173\u4e8e\u73af\u5883\u8f85\u52a9\u751f\u6d3b\uff08AAL\uff09\u7cfb\u7edf\u7684\u7814\u7a76\u6b63\u5728\u53d1\u8868\uff0c\u8fd9\u6709\u52a9\u4e8e\u51cf\u5c11\u8001\u5e74\u4eba\u72ec\u7acb\u751f\u6d3b\u5f15\u8d77\u7684\u5173\u6ce8\u3002\u672c\u7814\u7a76\u5bf9\u8001\u5e74\u4eba\u8dcc\u5012\u68c0\u6d4b\u548c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u7684\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u56de\u987e\uff0c\u4f5c\u4e3a\u4fdd\u969c\u72ec\u5c45\u8001\u5e74\u4eba\u5b89\u5168\u9700\u8981\u89e3\u51b3\u7684\u4e24\u4e2a\u4e3b\u8981\u4efb\u52a1\u3002\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u6267\u884c\u8fd9\u4e24\u9879\u4efb\u52a1\u7684\u8d8b\u52bf\uff0c\u672c\u6b21\u5ba1\u67e5\u91cd\u70b9\u5173\u6ce8\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u4e0a\u4f7f\u7528\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60 (DL) \u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u4ece\u5df2\u5ba1\u67e5\u7684\u7814\u7a76\u4e2d\u6536\u96c6\u4e86\u4e0d\u540c\u7684\u6570\u636e\u96c6\u5408\uff0c\u4f8b\u5982\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3001\u6570\u636e\u96c6\u6216\u786c\u4ef6\uff08\u4f8b\u5982\u6df1\u5ea6\u76f8\u673a\u6216\u70ed\u611f\u76f8\u673a\uff09\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002\u8fd8\u8ba8\u8bba\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u70b9\u548c\u7f3a\u70b9\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u6211\u4eec\u5bf9\u672a\u6765\u5de5\u4f5c\u7684\u5efa\u8bae\u3002|[2401.11790v1](http://arxiv.org/pdf/2401.11790v1)|null|\n", "2401.11775": "|**2024-01-22**|**Collaborative Position Reasoning Network for Referring Image Segmentation**|\u7528\u4e8e\u53c2\u8003\u56fe\u50cf\u5206\u5272\u7684\u534f\u4f5c\u4f4d\u7f6e\u63a8\u7406\u7f51\u7edc|Jianjian Cao, Beiya Dai, Yulin Li, Xiameng Qin, Jingdong Wang|Given an image and a natural language expression as input, the goal of referring image segmentation is to segment the foreground masks of the entities referred by the expression. Existing methods mainly focus on interactive learning between vision and language to enhance the multi-modal representations for global context reasoning. However, predicting directly in pixel-level space can lead to collapsed positioning and poor segmentation results. Its main challenge lies in how to explicitly model entity localization, especially for non-salient entities. In this paper, we tackle this problem by executing a Collaborative Position Reasoning Network (CPRN) via the proposed novel Row-and-Column interactive (RoCo) and Guided Holistic interactive (Holi) modules. Specifically, RoCo aggregates the visual features into the row- and column-wise features corresponding two directional axes respectively. It offers a fine-grained matching behavior that perceives the associations between the linguistic features and two decoupled visual features to perform position reasoning over a hierarchical space. Holi integrates features of the two modalities by a cross-modal attention mechanism, which suppresses the irrelevant redundancy under the guide of positioning information from RoCo. Thus, with the incorporation of RoCo and Holi modules, CPRN captures the visual details of position reasoning so that the model can achieve more accurate segmentation. To our knowledge, this is the first work that explicitly focuses on position reasoning modeling. We also validate the proposed method on three evaluation datasets. It consistently outperforms existing state-of-the-art methods.|\u7ed9\u5b9a\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u5f0f\u4f5c\u4e3a\u8f93\u5165\uff0c\u5f15\u7528\u56fe\u50cf\u5206\u5272\u7684\u76ee\u6807\u662f\u5206\u5272\u8868\u8fbe\u5f0f\u5f15\u7528\u7684\u5b9e\u4f53\u7684\u524d\u666f\u63a9\u6a21\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u89c6\u89c9\u548c\u8bed\u8a00\u4e4b\u95f4\u7684\u4ea4\u4e92\u5b66\u4e60\uff0c\u4ee5\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u591a\u6a21\u6001\u8868\u793a\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5728\u50cf\u7d20\u7ea7\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9884\u6d4b\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5b9a\u4f4d\u5d29\u6e83\u548c\u5206\u5272\u7ed3\u679c\u4e0d\u4f73\u3002\u5b83\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5982\u4f55\u663e\u5f0f\u5730\u5efa\u6a21\u5b9e\u4f53\u672c\u5730\u5316\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u975e\u663e\u7740\u5b9e\u4f53\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u7684\u65b0\u9896\u7684\u884c\u5217\u4ea4\u4e92\uff08RoCo\uff09\u548c\u5f15\u5bfc\u6574\u4f53\u4ea4\u4e92\uff08Holi\uff09\u6a21\u5757\u6267\u884c\u534f\u4f5c\u4f4d\u7f6e\u63a8\u7406\u7f51\u7edc\uff08CPRN\uff09\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0cRoCo\u5c06\u89c6\u89c9\u7279\u5f81\u805a\u5408\u6210\u5206\u522b\u5bf9\u5e94\u4e24\u4e2a\u65b9\u5411\u8f74\u7684\u884c\u548c\u5217\u7279\u5f81\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u5339\u914d\u884c\u4e3a\uff0c\u53ef\u4ee5\u611f\u77e5\u8bed\u8a00\u7279\u5f81\u548c\u4e24\u4e2a\u89e3\u8026\u7684\u89c6\u89c9\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u4ee5\u5728\u5206\u5c42\u7a7a\u95f4\u4e0a\u6267\u884c\u4f4d\u7f6e\u63a8\u7406\u3002 Holi\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u673a\u5236\u6574\u5408\u4e86\u4e24\u79cd\u6a21\u6001\u7684\u7279\u5f81\uff0c\u5728RoCo\u5b9a\u4f4d\u4fe1\u606f\u7684\u6307\u5bfc\u4e0b\u6291\u5236\u4e86\u4e0d\u76f8\u5173\u7684\u5197\u4f59\u3002\u56e0\u6b64\uff0c\u901a\u8fc7RoCo\u548cHoli\u6a21\u5757\u7684\u7ed3\u5408\uff0cCPRN\u6355\u83b7\u4f4d\u7f6e\u63a8\u7406\u7684\u89c6\u89c9\u7ec6\u8282\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u5206\u5272\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u660e\u786e\u5173\u6ce8\u4f4d\u7f6e\u63a8\u7406\u5efa\u6a21\u7684\u5de5\u4f5c\u3002\u6211\u4eec\u8fd8\u5728\u4e09\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u5b83\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002|[2401.11775v1](http://arxiv.org/pdf/2401.11775v1)|null|\n", "2401.11767": "|**2024-01-22**|**Concealed Object Segmentation with Hierarchical Coherence Modeling**|\u4f7f\u7528\u5206\u5c42\u4e00\u81f4\u6027\u5efa\u6a21\u7684\u9690\u85cf\u5bf9\u8c61\u5206\u5272|Fengyang Xiao, Pan Zhang, Chunming He, Runze Hu, Yutao Liu|Concealed object segmentation (COS) is a challenging task that involves localizing and segmenting those concealed objects that are visually blended with their surrounding environments. Despite achieving remarkable success, existing COS segmenters still struggle to achieve complete segmentation results in extremely concealed scenarios. In this paper, we propose a Hierarchical Coherence Modeling (HCM) segmenter for COS, aiming to address this incomplete segmentation limitation. In specific, HCM promotes feature coherence by leveraging the intra-stage coherence and cross-stage coherence modules, exploring feature correlations at both the single-stage and contextual levels. Additionally, we introduce the reversible re-calibration decoder to detect previously undetected parts in low-confidence regions, resulting in further enhancing segmentation performance. Extensive experiments conducted on three COS tasks, including camouflaged object detection, polyp image segmentation, and transparent object detection, demonstrate the promising results achieved by the proposed HCM segmenter.|\u9690\u85cf\u5bf9\u8c61\u5206\u5272\uff08COS\uff09\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u6d89\u53ca\u5b9a\u4f4d\u548c\u5206\u5272\u90a3\u4e9b\u5728\u89c6\u89c9\u4e0a\u4e0e\u5468\u56f4\u73af\u5883\u878d\u5408\u7684\u9690\u85cf\u5bf9\u8c61\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u529f\uff0c\u73b0\u6709\u7684 COS \u5206\u5272\u5668\u4ecd\u7136\u96be\u4ee5\u5728\u6781\u5176\u9690\u853d\u7684\u573a\u666f\u4e0b\u83b7\u5f97\u5b8c\u6574\u7684\u5206\u5272\u7ed3\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e COS \u7684\u5206\u5c42\u4e00\u81f4\u6027\u5efa\u6a21\uff08HCM\uff09\u5206\u5272\u5668\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u79cd\u4e0d\u5b8c\u6574\u7684\u5206\u5272\u9650\u5236\u3002\u5177\u4f53\u6765\u8bf4\uff0cHCM \u901a\u8fc7\u5229\u7528\u9636\u6bb5\u5185\u4e00\u81f4\u6027\u548c\u8de8\u9636\u6bb5\u4e00\u81f4\u6027\u6a21\u5757\u6765\u4fc3\u8fdb\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u63a2\u7d22\u5355\u9636\u6bb5\u548c\u4e0a\u4e0b\u6587\u7ea7\u522b\u7684\u7279\u5f81\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u53ef\u9006\u91cd\u65b0\u6821\u51c6\u89e3\u7801\u5668\u6765\u68c0\u6d4b\u4f4e\u7f6e\u4fe1\u533a\u57df\u4e2d\u4ee5\u524d\u672a\u68c0\u6d4b\u5230\u7684\u90e8\u5206\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u589e\u5f3a\u5206\u5272\u6027\u80fd\u3002\u5bf9\u4e09\u4e2a COS \u4efb\u52a1\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5305\u62ec\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u3001\u606f\u8089\u56fe\u50cf\u5206\u5272\u548c\u900f\u660e\u76ee\u6807\u68c0\u6d4b\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 HCM \u5206\u5272\u5668\u53d6\u5f97\u7684\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002|[2401.11767v1](http://arxiv.org/pdf/2401.11767v1)|null|\n", "2401.11739": "|**2024-01-22**|**EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models**|EmerDiff\uff1a\u6269\u6563\u6a21\u578b\u4e2d\u65b0\u5174\u7684\u50cf\u7d20\u7ea7\u8bed\u4e49\u77e5\u8bc6|Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, Seung Wook Kim|Diffusion models have recently received increasing research attention for their remarkable transfer abilities in semantic segmentation tasks. However, generating fine-grained segmentation masks with diffusion models often requires additional training on annotated datasets, leaving it unclear to what extent pre-trained diffusion models alone understand the semantic relations of their generated images. To address this question, we leverage the semantic knowledge extracted from Stable Diffusion (SD) and aim to develop an image segmentor capable of generating fine-grained segmentation maps without any additional training. The primary difficulty stems from the fact that semantically meaningful feature maps typically exist only in the spatially lower-dimensional layers, which poses a challenge in directly extracting pixel-level semantic relations from these feature maps. To overcome this issue, our framework identifies semantic correspondences between image pixels and spatial locations of low-dimensional feature maps by exploiting SD's generation process and utilizes them for constructing image-resolution segmentation maps. In extensive experiments, the produced segmentation maps are demonstrated to be well delineated and capture detailed parts of the images, indicating the existence of highly accurate pixel-level semantic knowledge in diffusion models.|\u6269\u6563\u6a21\u578b\u6700\u8fd1\u56e0\u5176\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u5353\u8d8a\u7684\u8fc1\u79fb\u80fd\u529b\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u5173\u6ce8\u3002\u7136\u800c\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u5206\u5272\u63a9\u6a21\u901a\u5e38\u9700\u8981\u5bf9\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u8fdb\u884c\u989d\u5916\u7684\u8bad\u7ec3\uff0c\u56e0\u6b64\u4e0d\u6e05\u695a\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u5355\u72ec\u7406\u89e3\u5176\u751f\u6210\u56fe\u50cf\u7684\u8bed\u4e49\u5173\u7cfb\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5229\u7528\u4ece\u7a33\u5b9a\u6269\u6563\uff08SD\uff09\u4e2d\u63d0\u53d6\u7684\u8bed\u4e49\u77e5\u8bc6\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u751f\u6210\u7ec6\u7c92\u5ea6\u5206\u5272\u56fe\u7684\u56fe\u50cf\u5206\u5272\u5668\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u989d\u5916\u7684\u8bad\u7ec3\u3002\u4e3b\u8981\u56f0\u96be\u6e90\u4e8e\u8fd9\u6837\u4e00\u4e2a\u4e8b\u5b9e\uff1a\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u7279\u5f81\u56fe\u901a\u5e38\u53ea\u5b58\u5728\u4e8e\u7a7a\u95f4\u8f83\u4f4e\u7ef4\u7684\u5c42\u4e2d\uff0c\u8fd9\u5bf9\u4ece\u8fd9\u4e9b\u7279\u5f81\u56fe\u4e2d\u76f4\u63a5\u63d0\u53d6\u50cf\u7d20\u7ea7\u8bed\u4e49\u5173\u7cfb\u63d0\u51fa\u4e86\u6311\u6218\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u7684\u6846\u67b6\u901a\u8fc7\u5229\u7528 SD \u7684\u751f\u6210\u8fc7\u7a0b\u6765\u8bc6\u522b\u56fe\u50cf\u50cf\u7d20\u548c\u4f4e\u7ef4\u7279\u5f81\u56fe\u7684\u7a7a\u95f4\u4f4d\u7f6e\u4e4b\u95f4\u7684\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u5b83\u4eec\u6765\u6784\u5efa\u56fe\u50cf\u5206\u8fa8\u7387\u5206\u5272\u56fe\u3002\u5728\u5927\u91cf\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6240\u751f\u6210\u7684\u5206\u5272\u56fe\u88ab\u8bc1\u660e\u53ef\u4ee5\u5f88\u597d\u5730\u63cf\u7ed8\u5e76\u6355\u83b7\u56fe\u50cf\u7684\u8be6\u7ec6\u90e8\u5206\uff0c\u8fd9\u8868\u660e\u6269\u6563\u6a21\u578b\u4e2d\u5b58\u5728\u9ad8\u5ea6\u51c6\u786e\u7684\u50cf\u7d20\u7ea7\u8bed\u4e49\u77e5\u8bc6\u3002|[2401.11739v1](http://arxiv.org/pdf/2401.11739v1)|null|\n", "2401.11738": "|**2024-01-22**|**MetaSeg: Content-Aware Meta-Net for Omni-Supervised Semantic Segmentation**|MetaSeg\uff1a\u7528\u4e8e\u5168\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u5185\u5bb9\u611f\u77e5\u5143\u7f51\u7edc|Shenwang Jiang, Jianan Li, Ying Wang, Wenxuan Wu, Jizhou Zhang, Bo Huang, Tingfa Xu|Noisy labels, inevitably existing in pseudo segmentation labels generated from weak object-level annotations, severely hampers model optimization for semantic segmentation. Previous works often rely on massive hand-crafted losses and carefully-tuned hyper-parameters to resist noise, suffering poor generalization capability and high model complexity. Inspired by recent advances in meta learning, we argue that rather than struggling to tolerate noise hidden behind clean labels passively, a more feasible solution would be to find out the noisy regions actively, so as to simply ignore them during model optimization. With this in mind, this work presents a novel meta learning based semantic segmentation method, MetaSeg, that comprises a primary content-aware meta-net (CAM-Net) to sever as a noise indicator for an arbitrary segmentation model counterpart. Specifically, CAM-Net learns to generate pixel-wise weights to suppress noisy regions with incorrect pseudo labels while highlighting clean ones by exploiting hybrid strengthened features from image content, providing straightforward and reliable guidance for optimizing the segmentation model. Moreover, to break the barrier of time-consuming training when applying meta learning to common large segmentation models, we further present a new decoupled training strategy that optimizes different model layers in a divide-and-conquer manner. Extensive experiments on object, medical, remote sensing and human segmentation shows that our method achieves superior performance, approaching that of fully supervised settings, which paves a new promising way for omni-supervised semantic segmentation.|\u7531\u5f31\u5bf9\u8c61\u7ea7\u6ce8\u91ca\u751f\u6210\u7684\u4f2a\u5206\u5272\u6807\u7b7e\u4e2d\u4e0d\u53ef\u907f\u514d\u5730\u5b58\u5728\u566a\u58f0\u6807\u7b7e\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u8bed\u4e49\u5206\u5272\u7684\u6a21\u578b\u4f18\u5316\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u901a\u5e38\u4f9d\u8d56\u5927\u91cf\u7684\u624b\u5de5\u635f\u5931\u548c\u7cbe\u5fc3\u8c03\u6574\u7684\u8d85\u53c2\u6570\u6765\u62b5\u6297\u566a\u58f0\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\u3002\u53d7\u5143\u5b66\u4e60\u6700\u65b0\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u6211\u4eec\u8ba4\u4e3a\uff0c\u4e0e\u5176\u88ab\u52a8\u5730\u5fcd\u53d7\u9690\u85cf\u5728\u5e72\u51c0\u6807\u7b7e\u540e\u9762\u7684\u566a\u58f0\uff0c\u66f4\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u662f\u4e3b\u52a8\u627e\u51fa\u566a\u58f0\u533a\u57df\uff0c\u4ee5\u4fbf\u5728\u6a21\u578b\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5ffd\u7565\u5b83\u4eec\u3002\u8003\u8651\u5230\u8fd9\u4e00\u70b9\uff0c\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u8bed\u4e49\u5206\u5272\u65b9\u6cd5 MetaSeg\uff0c\u8be5\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u4e3b\u8981\u5185\u5bb9\u611f\u77e5\u5143\u7f51\u7edc\uff08CAM-Net\uff09\uff0c\u7528\u4f5c\u4efb\u610f\u5206\u5272\u6a21\u578b\u5bf9\u5e94\u7269\u7684\u566a\u58f0\u6307\u793a\u5668\u3002\u5177\u4f53\u6765\u8bf4\uff0cCAM-Net \u5b66\u4e60\u751f\u6210\u50cf\u7d20\u7ea7\u6743\u91cd\uff0c\u4ee5\u6291\u5236\u5177\u6709\u4e0d\u6b63\u786e\u4f2a\u6807\u7b7e\u7684\u566a\u58f0\u533a\u57df\uff0c\u540c\u65f6\u901a\u8fc7\u5229\u7528\u56fe\u50cf\u5185\u5bb9\u7684\u6df7\u5408\u5f3a\u5316\u7279\u5f81\u6765\u7a81\u51fa\u663e\u793a\u5e72\u51c0\u7684\u533a\u57df\uff0c\u4e3a\u4f18\u5316\u5206\u5272\u6a21\u578b\u63d0\u4f9b\u76f4\u63a5\u800c\u53ef\u9760\u7684\u6307\u5bfc\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u6253\u7834\u5c06\u5143\u5b66\u4e60\u5e94\u7528\u4e8e\u5e38\u89c1\u5927\u578b\u5206\u5272\u6a21\u578b\u65f6\u8017\u65f6\u8bad\u7ec3\u7684\u969c\u788d\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u8026\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u5206\u800c\u6cbb\u4e4b\u7684\u65b9\u5f0f\u4f18\u5316\u4e0d\u540c\u6a21\u578b\u5c42\u3002\u5728\u7269\u4f53\u3001\u533b\u5b66\u3001\u9065\u611f\u548c\u4eba\u4f53\u5206\u5272\u65b9\u9762\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u63a5\u8fd1\u5b8c\u5168\u76d1\u7763\u7684\u8bbe\u7f6e\uff0c\u8fd9\u4e3a\u5168\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5f00\u8f9f\u4e86\u4e00\u6761\u65b0\u7684\u6709\u524d\u666f\u7684\u9053\u8def\u3002|[2401.11738v1](http://arxiv.org/pdf/2401.11738v1)|null|\n", "2401.11734": "|**2024-01-22**|**Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey**|\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\u7684\u7ed3\u76f4\u80a0\u606f\u8089\u5206\u5272\uff1a\u7efc\u5408\u8c03\u67e5|Zhenyu Wu, Fengmao Lv, Chenglizhao Chen, Aimin Hao, Shuo Li|Colorectal polyp segmentation (CPS), an essential problem in medical image analysis, has garnered growing research attention. Recently, the deep learning-based model completely overwhelmed traditional methods in the field of CPS, and more and more deep CPS methods have emerged, bringing the CPS into the deep learning era. To help the researchers quickly grasp the main techniques, datasets, evaluation metrics, challenges, and trending of deep CPS, this paper presents a systematic and comprehensive review of deep-learning-based CPS methods from 2014 to 2023, a total of 115 technical papers. In particular, we first provide a comprehensive review of the current deep CPS with a novel taxonomy, including network architectures, level of supervision, and learning paradigm. More specifically, network architectures include eight subcategories, the level of supervision comprises six subcategories, and the learning paradigm encompasses 12 subcategories, totaling 26 subcategories. Then, we provided a comprehensive analysis the characteristics of each dataset, including the number of datasets, annotation types, image resolution, polyp size, contrast values, and polyp location. Following that, we summarized CPS's commonly used evaluation metrics and conducted a detailed analysis of 40 deep SOTA models, including out-of-distribution generalization and attribute-based performance analysis. Finally, we discussed deep learning-based CPS methods' main challenges and opportunities.|\u7ed3\u76f4\u80a0\u606f\u8089\u5206\u5272\uff08CPS\uff09\u662f\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u5df2\u7ecf\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u5173\u6ce8\u3002\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u5728CPS\u9886\u57df\u5b8c\u5168\u538b\u5012\u4e86\u4f20\u7edf\u65b9\u6cd5\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u6df1\u5ea6CPS\u65b9\u6cd5\u6d8c\u73b0\uff0c\u5c06CPS\u5e26\u5165\u4e86\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\u3002\u4e3a\u4e86\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5feb\u901f\u638c\u63e1\u6df1\u5ea6CPS\u7684\u4e3b\u8981\u6280\u672f\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u3001\u6311\u6218\u548c\u8d8b\u52bf\uff0c\u672c\u6587\u5bf92014\u5e74\u81f32023\u5e74\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684CPS\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u3001\u5168\u9762\u7684\u56de\u987e\uff0c\u5171115\u7bc7\u6280\u672f\u8bba\u6587\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u65b0\u9896\u7684\u5206\u7c7b\u6cd5\u5bf9\u5f53\u524d\u7684\u6df1\u5ea6 CPS \u8fdb\u884c\u4e86\u5168\u9762\u7684\u56de\u987e\uff0c\u5305\u62ec\u7f51\u7edc\u67b6\u6784\u3001\u76d1\u7763\u7ea7\u522b\u548c\u5b66\u4e60\u8303\u5f0f\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u7f51\u7edc\u67b6\u6784\u5305\u62ec8\u4e2a\u5b50\u7c7b\u522b\uff0c\u76d1\u7763\u7ea7\u522b\u5305\u62ec6\u4e2a\u5b50\u7c7b\u522b\uff0c\u5b66\u4e60\u8303\u5f0f\u5305\u62ec12\u4e2a\u5b50\u7c7b\u522b\uff0c\u603b\u517126\u4e2a\u5b50\u7c7b\u522b\u3002\u7136\u540e\uff0c\u6211\u4eec\u5bf9\u6bcf\u4e2a\u6570\u636e\u96c6\u7684\u7279\u5f81\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\uff0c\u5305\u62ec\u6570\u636e\u96c6\u7684\u6570\u91cf\u3001\u6ce8\u91ca\u7c7b\u578b\u3001\u56fe\u50cf\u5206\u8fa8\u7387\u3001\u606f\u8089\u5927\u5c0f\u3001\u5bf9\u6bd4\u5ea6\u503c\u548c\u606f\u8089\u4f4d\u7f6e\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u603b\u7ed3\u4e86CPS\u5e38\u7528\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5bf940\u4e2a\u6df1\u5ea6SOTA\u6a21\u578b\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\uff0c\u5305\u62ec\u5206\u5e03\u5916\u6cdb\u5316\u548c\u57fa\u4e8e\u5c5e\u6027\u7684\u6027\u80fd\u5206\u6790\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684 CPS \u65b9\u6cd5\u7684\u4e3b\u8981\u6311\u6218\u548c\u673a\u9047\u3002|[2401.11734v1](http://arxiv.org/pdf/2401.11734v1)|null|\n", "2401.11726": "|**2024-01-22**|**Detecting Out-of-Distribution Samples via Conditional Distribution Entropy with Optimal Transport**|\u901a\u8fc7\u5177\u6709\u6700\u4f73\u4f20\u8f93\u7684\u6761\u4ef6\u5206\u5e03\u71b5\u68c0\u6d4b\u5206\u5e03\u5916\u6837\u672c|Chuanwen Feng, Wenlong Chen, Ao Ke, Yilong Ren, Xike Xie, S. Kevin Zhou|When deploying a trained machine learning model in the real world, it is inevitable to receive inputs from out-of-distribution (OOD) sources. For instance, in continual learning settings, it is common to encounter OOD samples due to the non-stationarity of a domain. More generally, when we have access to a set of test inputs, the existing rich line of OOD detection solutions, especially the recent promise of distance-based methods, falls short in effectively utilizing the distribution information from training samples and test inputs. In this paper, we argue that empirical probability distributions that incorporate geometric information from both training samples and test inputs can be highly beneficial for OOD detection in the presence of test inputs available. To address this, we propose to model OOD detection as a discrete optimal transport problem. Within the framework of optimal transport, we propose a novel score function known as the \\emph{conditional distribution entropy} to quantify the uncertainty of a test input being an OOD sample. Our proposal inherits the merits of certain distance-based methods while eliminating the reliance on distribution assumptions, a-prior knowledge, and specific training mechanisms. Extensive experiments conducted on benchmark datasets demonstrate that our method outperforms its competitors in OOD detection.|\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u7ecf\u8fc7\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u8981\u63a5\u6536\u6765\u81ea\u5206\u5e03\u5916\uff08OOD\uff09\u6e90\u7684\u8f93\u5165\u3002\u4f8b\u5982\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u57df\u7684\u975e\u5e73\u7a33\u6027\uff0c\u7ecf\u5e38\u4f1a\u9047\u5230 OOD \u6837\u672c\u3002\u66f4\u4e00\u822c\u5730\u8bf4\uff0c\u5f53\u6211\u4eec\u80fd\u591f\u8bbf\u95ee\u4e00\u7ec4\u6d4b\u8bd5\u8f93\u5165\u65f6\uff0c\u73b0\u6709\u4e30\u5bcc\u7684 OOD \u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u6700\u8fd1\u627f\u8bfa\u7684\u57fa\u4e8e\u8ddd\u79bb\u7684\u65b9\u6cd5\uff0c\u5728\u6709\u6548\u5229\u7528\u6765\u81ea\u8bad\u7ec3\u6837\u672c\u548c\u6d4b\u8bd5\u8f93\u5165\u7684\u5206\u5e03\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8ba4\u4e3a\uff0c\u5728\u5b58\u5728\u53ef\u7528\u6d4b\u8bd5\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u7ed3\u5408\u6765\u81ea\u8bad\u7ec3\u6837\u672c\u548c\u6d4b\u8bd5\u8f93\u5165\u7684\u51e0\u4f55\u4fe1\u606f\u7684\u7ecf\u9a8c\u6982\u7387\u5206\u5e03\u5bf9\u4e8e OOD \u68c0\u6d4b\u975e\u5e38\u6709\u76ca\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u5c06 OOD \u68c0\u6d4b\u5efa\u6a21\u4e3a\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u3002\u5728\u6700\u4f18\u4f20\u8f93\u7684\u6846\u67b6\u5185\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a \\emph{\u6761\u4ef6\u5206\u5e03\u71b5} \u7684\u65b0\u9896\u8bc4\u5206\u51fd\u6570\uff0c\u7528\u4e8e\u91cf\u5316 OOD \u6837\u672c\u6d4b\u8bd5\u8f93\u5165\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u6211\u4eec\u7684\u5efa\u8bae\u7ee7\u627f\u4e86\u67d0\u4e9b\u57fa\u4e8e\u8ddd\u79bb\u7684\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u540c\u65f6\u6d88\u9664\u4e86\u5bf9\u5206\u5e03\u5047\u8bbe\u3001\u5148\u9a8c\u77e5\u8bc6\u548c\u7279\u5b9a\u8bad\u7ec3\u673a\u5236\u7684\u4f9d\u8d56\u3002\u5bf9\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 OOD \u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u7ade\u4e89\u5bf9\u624b\u3002|[2401.11726v1](http://arxiv.org/pdf/2401.11726v1)|null|\n", "2401.11724": "|**2024-01-22**|**Augmenting Prototype Network with TransMix for Few-shot Hyperspectral Image Classification**|\u4f7f\u7528 TransMix \u589e\u5f3a\u539f\u578b\u7f51\u7edc\u4ee5\u5b9e\u73b0\u5c11\u6837\u672c\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b|Chun Liu, Longwei Yang, Dongmei Dong, Zheng Li, Wei Yang, Zhigang Han, Jiayao Wang|Few-shot hyperspectral image classification aims to identify the classes of each pixel in the images by only marking few of these pixels. And in order to obtain the spatial-spectral joint features of each pixel, the fixed-size patches centering around each pixel are often used for classification. However, observing the classification results of existing methods, we found that boundary patches corresponding to the pixels which are located at the boundary of the objects in the hyperspectral images, are hard to classify. These boundary patchs are mixed with multi-class spectral information. Inspired by this, we propose to augment the prototype network with TransMix for few-shot hyperspectrial image classification(APNT). While taking the prototype network as the backbone, it adopts the transformer as feature extractor to learn the pixel-to-pixel relation and pay different attentions to different pixels. At the same time, instead of directly using the patches which are cut from the hyperspectral images for training, it randomly mixs up two patches to imitate the boundary patches and uses the synthetic patches to train the model, with the aim to enlarge the number of hard training samples and enhance their diversity. And by following the data agumentation technique TransMix, the attention returned by the transformer is also used to mix up the labels of two patches to generate better labels for synthetic patches. Compared with existing methods, the proposed method has demonstrated sate of the art performance and better robustness for few-shot hyperspectral image classification in our experiments.|\u5c11\u955c\u5934\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u65e8\u5728\u901a\u8fc7\u4ec5\u6807\u8bb0\u5c11\u6570\u50cf\u7d20\u6765\u8bc6\u522b\u56fe\u50cf\u4e2d\u6bcf\u4e2a\u50cf\u7d20\u7684\u7c7b\u522b\u3002\u4e3a\u4e86\u83b7\u5f97\u6bcf\u4e2a\u50cf\u7d20\u7684\u7a7a\u95f4-\u5149\u8c31\u8054\u5408\u7279\u5f81\uff0c\u901a\u5e38\u4f7f\u7528\u4ee5\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u7684\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\u8fdb\u884c\u5206\u7c7b\u3002\u7136\u800c\uff0c\u89c2\u5bdf\u73b0\u6709\u65b9\u6cd5\u7684\u5206\u7c7b\u7ed3\u679c\uff0c\u6211\u4eec\u53d1\u73b0\u9ad8\u5149\u8c31\u56fe\u50cf\u4e2d\u4f4d\u4e8e\u76ee\u6807\u8fb9\u754c\u7684\u50cf\u7d20\u5bf9\u5e94\u7684\u8fb9\u754c\u6591\u5757\u5f88\u96be\u5206\u7c7b\u3002\u8fd9\u4e9b\u8fb9\u754c\u6591\u5757\u4e0e\u591a\u7c7b\u5149\u8c31\u4fe1\u606f\u6df7\u5408\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528 TransMix \u589e\u5f3a\u539f\u578b\u7f51\u7edc\uff0c\u4ee5\u5b9e\u73b0\u5c11\u6837\u672c\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\uff08APNT\uff09\u3002\u4ee5\u539f\u578b\u7f51\u7edc\u4e3a\u9aa8\u5e72\uff0c\u91c7\u7528\u53d8\u538b\u5668\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\u6765\u5b66\u4e60\u50cf\u7d20\u5230\u50cf\u7d20\u7684\u5173\u7cfb\uff0c\u5e76\u5bf9\u4e0d\u540c\u7684\u50cf\u7d20\u7ed9\u4e88\u4e0d\u540c\u7684\u5173\u6ce8\u3002\u540c\u65f6\uff0c\u5b83\u4e0d\u662f\u76f4\u63a5\u4f7f\u7528\u4ece\u9ad8\u5149\u8c31\u56fe\u50cf\u4e2d\u5207\u4e0b\u7684\u8865\u4e01\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u662f\u968f\u673a\u6df7\u5408\u4e24\u4e2a\u8865\u4e01\u6765\u6a21\u62df\u8fb9\u754c\u8865\u4e01\uff0c\u5e76\u4f7f\u7528\u5408\u6210\u7684\u8865\u4e01\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u76ee\u7684\u662f\u6269\u5927\u6a21\u578b\u7684\u6570\u91cf\u3002\u52aa\u529b\u8bad\u7ec3\u6837\u672c\u5e76\u589e\u5f3a\u5176\u591a\u6837\u6027\u3002\u5e76\u4e14\u901a\u8fc7\u9075\u5faa\u6570\u636e\u589e\u5f3a\u6280\u672f TransMix\uff0c\u53d8\u538b\u5668\u8fd4\u56de\u7684\u6ce8\u610f\u529b\u4e5f\u7528\u4e8e\u6df7\u5408\u4e24\u4e2a\u8865\u4e01\u7684\u6807\u7b7e\uff0c\u4ee5\u4fbf\u4e3a\u5408\u6210\u8865\u4e01\u751f\u6210\u66f4\u597d\u7684\u6807\u7b7e\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u66f4\u597d\u7684\u5c11\u6837\u672c\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u9c81\u68d2\u6027\u3002|[2401.11724v1](http://arxiv.org/pdf/2401.11724v1)|null|\n", "2401.11719": "|**2024-01-22**|**SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation**|SFC\uff1a\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5171\u4eab\u7279\u5f81\u6821\u51c6|Xinqiao Zhao, Feilong Tang, Xiaoyang Wang, Jimin Xiao|Image-level weakly supervised semantic segmentation has received increasing attention due to its low annotation cost. Existing methods mainly rely on Class Activation Mapping (CAM) to obtain pseudo-labels for training semantic segmentation models. In this work, we are the first to demonstrate that long-tailed distribution in training data can cause the CAM calculated through classifier weights over-activated for head classes and under-activated for tail classes due to the shared features among head- and tail- classes. This degrades pseudo-label quality and further influences final semantic segmentation performance. To address this issue, we propose a Shared Feature Calibration (SFC) method for CAM generation. Specifically, we leverage the class prototypes that carry positive shared features and propose a Multi-Scaled Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the CAMs generated through classifier weights and class prototypes during training. The MSDW loss counterbalances over-activation and under-activation by calibrating the shared features in head-/tail-class classifier weights. Experimental results show that our SFC significantly improves CAM boundaries and achieves new state-of-the-art performances. The project is available at https://github.com/Barrett-python/SFC.|\u56fe\u50cf\u7ea7\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7531\u4e8e\u5176\u6ce8\u91ca\u6210\u672c\u4f4e\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u9760\u7c7b\u6fc0\u6d3b\u6620\u5c04\uff08CAM\uff09\u6765\u83b7\u53d6\u4f2a\u6807\u7b7e\u6765\u8bad\u7ec3\u8bed\u4e49\u5206\u5272\u6a21\u578b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u9996\u6b21\u8bc1\u660e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u4f1a\u5bfc\u81f4\u901a\u8fc7\u5206\u7c7b\u5668\u6743\u91cd\u8ba1\u7b97\u51fa\u7684 CAM \u7531\u4e8e\u5934\u7c7b\u548c\u5c3e\u7c7b\u4e4b\u95f4\u7684\u5171\u4eab\u7279\u5f81\u800c\u5bf9\u5934\u7c7b\u8fc7\u5ea6\u6fc0\u6d3b\uff0c\u800c\u5bf9\u5c3e\u7c7b\u6fc0\u6d3b\u4e0d\u8db3\u3002\u7c7b\u3002\u8fd9\u4f1a\u964d\u4f4e\u4f2a\u6807\u7b7e\u8d28\u91cf\u5e76\u8fdb\u4e00\u6b65\u5f71\u54cd\u6700\u7ec8\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e CAM \u751f\u6210\u7684\u5171\u4eab\u7279\u5f81\u6821\u51c6 (SFC) \u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528\u5177\u6709\u79ef\u6781\u5171\u4eab\u7279\u5f81\u7684\u7c7b\u539f\u578b\uff0c\u5e76\u63d0\u51fa\u591a\u5c3a\u5ea6\u5206\u5e03\u52a0\u6743\uff08MSDW\uff09\u4e00\u81f4\u6027\u635f\u5931\uff0c\u4ee5\u7f29\u5c0f\u8bad\u7ec3\u671f\u95f4\u901a\u8fc7\u5206\u7c7b\u5668\u6743\u91cd\u751f\u6210\u7684 CAM \u4e0e\u7c7b\u539f\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002 MSDW \u635f\u5931\u901a\u8fc7\u6821\u51c6\u5934/\u5c3e\u7c7b\u5206\u7c7b\u5668\u6743\u91cd\u4e2d\u7684\u5171\u4eab\u7279\u5f81\u6765\u5e73\u8861\u8fc7\u5ea6\u6fc0\u6d3b\u548c\u6fc0\u6d3b\u4e0d\u8db3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684 SFC \u663e\u7740\u6539\u5584\u4e86 CAM \u8fb9\u754c\u5e76\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u8be5\u9879\u76ee\u4f4d\u4e8e https://github.com/Barrett-python/SFC\u3002|[2401.11719v1](http://arxiv.org/pdf/2401.11719v1)|null|\n", "2401.11718": "|**2024-01-22**|**MsSVT++: Mixed-scale Sparse Voxel Transformer with Center Voting for 3D Object Detection**|MsSVT++\uff1a\u7528\u4e8e 3D \u5bf9\u8c61\u68c0\u6d4b\u7684\u5177\u6709\u4e2d\u5fc3\u6295\u7968\u529f\u80fd\u7684\u6df7\u5408\u5c3a\u5ea6\u7a00\u758f\u4f53\u7d20\u53d8\u6362\u5668|Jianan Li, Shaocong Dong, Lihe Ding, Tingfa Xu|Accurate 3D object detection in large-scale outdoor scenes, characterized by considerable variations in object scales, necessitates features rich in both long-range and fine-grained information. While recent detectors have utilized window-based transformers to model long-range dependencies, they tend to overlook fine-grained details. To bridge this gap, we propose MsSVT++, an innovative Mixed-scale Sparse Voxel Transformer that simultaneously captures both types of information through a divide-and-conquer approach. This approach involves explicitly dividing attention heads into multiple groups, each responsible for attending to information within a specific range. The outputs of these groups are subsequently merged to obtain final mixed-scale features. To mitigate the computational complexity associated with applying a window-based transformer in 3D voxel space, we introduce a novel Chessboard Sampling strategy and implement voxel sampling and gathering operations sparsely using a hash map. Moreover, an important challenge stems from the observation that non-empty voxels are primarily located on the surface of objects, which impedes the accurate estimation of bounding boxes. To overcome this challenge, we introduce a Center Voting module that integrates newly voted voxels enriched with mixed-scale contextual information towards the centers of the objects, thereby improving precise object localization. Extensive experiments demonstrate that our single-stage detector, built upon the foundation of MsSVT++, consistently delivers exceptional performance across diverse datasets.|\u5927\u89c4\u6a21\u6237\u5916\u573a\u666f\u4e2d\u7684\u7cbe\u786e 3D \u7269\u4f53\u68c0\u6d4b\uff0c\u5176\u7279\u70b9\u662f\u7269\u4f53\u5c3a\u5ea6\u53d8\u5316\u5f88\u5927\uff0c\u9700\u8981\u5177\u6709\u4e30\u5bcc\u7684\u957f\u8ddd\u79bb\u548c\u7ec6\u7c92\u5ea6\u4fe1\u606f\u7684\u7279\u5f81\u3002\u867d\u7136\u6700\u8fd1\u7684\u68c0\u6d4b\u5668\u5229\u7528\u57fa\u4e8e\u7a97\u53e3\u7684\u53d8\u538b\u5668\u6765\u6a21\u62df\u8fdc\u7a0b\u4f9d\u8d56\u6027\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u4f1a\u5ffd\u7565\u7ec6\u7c92\u5ea6\u7684\u7ec6\u8282\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 MsSVT++\uff0c\u8fd9\u662f\u4e00\u79cd\u521b\u65b0\u7684\u6df7\u5408\u5c3a\u5ea6\u7a00\u758f\u4f53\u7d20\u53d8\u6362\u5668\uff0c\u5b83\u901a\u8fc7\u5206\u800c\u6cbb\u4e4b\u7684\u65b9\u6cd5\u540c\u65f6\u6355\u83b7\u4e24\u79cd\u7c7b\u578b\u7684\u4fe1\u606f\u3002\u8fd9\u79cd\u65b9\u6cd5\u6d89\u53ca\u660e\u786e\u5730\u5c06\u6ce8\u610f\u529b\u5934\u5206\u4e3a\u591a\u4e2a\u7ec4\uff0c\u6bcf\u4e2a\u7ec4\u8d1f\u8d23\u5173\u6ce8\u7279\u5b9a\u8303\u56f4\u5185\u7684\u4fe1\u606f\u3002\u968f\u540e\u5408\u5e76\u8fd9\u4e9b\u7ec4\u7684\u8f93\u51fa\u4ee5\u83b7\u5f97\u6700\u7ec8\u7684\u6df7\u5408\u5c3a\u5ea6\u7279\u5f81\u3002\u4e3a\u4e86\u51cf\u8f7b\u4e0e\u5728 3D \u4f53\u7d20\u7a7a\u95f4\u4e2d\u5e94\u7528\u57fa\u4e8e\u7a97\u53e3\u7684\u53d8\u6362\u5668\u76f8\u5173\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u68cb\u76d8\u91c7\u6837\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u54c8\u5e0c\u56fe\u7a00\u758f\u5730\u5b9e\u73b0\u4f53\u7d20\u91c7\u6837\u548c\u6536\u96c6\u64cd\u4f5c\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u91cd\u8981\u7684\u6311\u6218\u6e90\u4e8e\u89c2\u5bdf\u5230\u975e\u7a7a\u4f53\u7d20\u4e3b\u8981\u4f4d\u4e8e\u7269\u4f53\u8868\u9762\uff0c\u8fd9\u963b\u788d\u4e86\u8fb9\u754c\u6846\u7684\u51c6\u786e\u4f30\u8ba1\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e2d\u5fc3\u6295\u7968\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5c06\u65b0\u6295\u7968\u7684\u4f53\u7d20\uff08\u5bcc\u542b\u6df7\u5408\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff09\u96c6\u6210\u5230\u5bf9\u8c61\u7684\u4e2d\u5fc3\uff0c\u4ece\u800c\u63d0\u9ad8\u7cbe\u786e\u7684\u5bf9\u8c61\u5b9a\u4f4d\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u5355\u7ea7\u68c0\u6d4b\u5668\u5efa\u7acb\u5728 MsSVT++ \u7684\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u4f9b\u5353\u8d8a\u7684\u6027\u80fd\u3002|[2401.11718v1](http://arxiv.org/pdf/2401.11718v1)|null|\n", "2401.11713": "|**2024-01-22**|**Medical Image Debiasing by Learning Adaptive Agreement from a Biased Council**|\u901a\u8fc7\u4ece\u6709\u504f\u89c1\u7684\u59d4\u5458\u4f1a\u5b66\u4e60\u81ea\u9002\u5e94\u534f\u8bae\u6765\u6d88\u9664\u533b\u5b66\u56fe\u50cf\u504f\u89c1|Luyang Luo, Xin Huang, Minghao Wang, Zhuoyue Wan, Hao Chen|Deep learning could be prone to learning shortcuts raised by dataset bias and result in inaccurate, unreliable, and unfair models, which impedes its adoption in real-world clinical applications. Despite its significance, there is a dearth of research in the medical image classification domain to address dataset bias. Furthermore, the bias labels are often agnostic, as identifying biases can be laborious and depend on post-hoc interpretation. This paper proposes learning Adaptive Agreement from a Biased Council (Ada-ABC), a debiasing framework that does not rely on explicit bias labels to tackle dataset bias in medical images. Ada-ABC develops a biased council consisting of multiple classifiers optimized with generalized cross entropy loss to learn the dataset bias. A debiasing model is then simultaneously trained under the guidance of the biased council. Specifically, the debiasing model is required to learn adaptive agreement with the biased council by agreeing on the correctly predicted samples and disagreeing on the wrongly predicted samples by the biased council. In this way, the debiasing model could learn the target attribute on the samples without spurious correlations while also avoiding ignoring the rich information in samples with spurious correlations. We theoretically demonstrated that the debiasing model could learn the target features when the biased model successfully captures dataset bias. Moreover, to our best knowledge, we constructed the first medical debiasing benchmark from four datasets containing seven different bias scenarios. Our extensive experiments practically showed that our proposed Ada-ABC outperformed competitive approaches, verifying its effectiveness in mitigating dataset bias for medical image classification. The codes and organized benchmark datasets will be made publicly available.|\u6df1\u5ea6\u5b66\u4e60\u53ef\u80fd\u5bb9\u6613\u51fa\u73b0\u6570\u636e\u96c6\u504f\u5dee\u5e26\u6765\u7684\u5b66\u4e60\u6377\u5f84\uff0c\u5e76\u5bfc\u81f4\u6a21\u578b\u4e0d\u51c6\u786e\u3001\u4e0d\u53ef\u9760\u548c\u4e0d\u516c\u5e73\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u91c7\u7528\u3002\u5c3d\u7ba1\u5176\u610f\u4e49\u91cd\u5927\uff0c\u4f46\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u9886\u57df\u4ecd\u7f3a\u4e4f\u89e3\u51b3\u6570\u636e\u96c6\u504f\u5dee\u7684\u7814\u7a76\u3002\u6b64\u5916\uff0c\u504f\u89c1\u6807\u7b7e\u901a\u5e38\u662f\u4e0d\u53ef\u77e5\u7684\uff0c\u56e0\u4e3a\u8bc6\u522b\u504f\u89c1\u53ef\u80fd\u5f88\u8d39\u529b\u5e76\u4e14\u4f9d\u8d56\u4e8e\u4e8b\u540e\u89e3\u91ca\u3002\u672c\u6587\u63d0\u51fa\u4ece\u504f\u7f6e\u59d4\u5458\u4f1a\uff08Ada-ABC\uff09\u5b66\u4e60\u81ea\u9002\u5e94\u534f\u8bae\uff0c\u8fd9\u662f\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4e8e\u663e\u5f0f\u504f\u5dee\u6807\u7b7e\u6765\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u4e2d\u6570\u636e\u96c6\u504f\u5dee\u7684\u53bb\u504f\u5dee\u6846\u67b6\u3002 Ada-ABC \u5f00\u53d1\u4e86\u4e00\u4e2a\u7531\u591a\u4e2a\u5206\u7c7b\u5668\u7ec4\u6210\u7684\u6709\u504f\u5dee\u59d4\u5458\u4f1a\uff0c\u8fd9\u4e9b\u5206\u7c7b\u5668\u901a\u8fc7\u5e7f\u4e49\u4ea4\u53c9\u71b5\u635f\u5931\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4ee5\u5b66\u4e60\u6570\u636e\u96c6\u504f\u5dee\u3002\u7136\u540e\u5728\u6709\u504f\u5dee\u59d4\u5458\u4f1a\u7684\u6307\u5bfc\u4e0b\u540c\u65f6\u8bad\u7ec3\u53bb\u504f\u5dee\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u53bb\u504f\u5dee\u6a21\u578b\u9700\u8981\u901a\u8fc7\u5bf9\u6709\u504f\u5dee\u59d4\u5458\u4f1a\u7684\u6b63\u786e\u9884\u6d4b\u6837\u672c\u8fbe\u6210\u4e00\u81f4\u5e76\u5728\u9519\u8bef\u9884\u6d4b\u6837\u672c\u4e0a\u8fbe\u6210\u4e00\u81f4\u6765\u5b66\u4e60\u4e0e\u6709\u504f\u5dee\u59d4\u5458\u4f1a\u7684\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u3002\u8fd9\u6837\uff0c\u53bb\u504f\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u6ca1\u6709\u865a\u5047\u76f8\u5173\u6027\u7684\u6837\u672c\u4e0a\u7684\u76ee\u6807\u5c5e\u6027\uff0c\u540c\u65f6\u4e5f\u907f\u514d\u5ffd\u7565\u5177\u6709\u865a\u5047\u76f8\u5173\u6027\u7684\u6837\u672c\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\uff0c\u5f53\u504f\u7f6e\u6a21\u578b\u6210\u529f\u6355\u83b7\u6570\u636e\u96c6\u504f\u5dee\u65f6\uff0c\u53bb\u504f\u7f6e\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u76ee\u6807\u7279\u5f81\u3002\u6b64\u5916\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u4ece\u5305\u542b\u4e03\u79cd\u4e0d\u540c\u504f\u5dee\u573a\u666f\u7684\u56db\u4e2a\u6570\u636e\u96c6\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u533b\u5b66\u53bb\u504f\u5dee\u57fa\u51c6\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u5b9e\u9645\u4e0a\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 Ada-ABC \u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u51cf\u8f7b\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u504f\u5dee\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u548c\u7ec4\u7ec7\u7684\u57fa\u51c6\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002|[2401.11713v1](http://arxiv.org/pdf/2401.11713v1)|null|\n", "2401.11704": "|**2024-01-22**|**EK-Net:Real-time Scene Text Detection with Expand Kernel Distance**|EK-Net\uff1a\u6269\u5c55\u6838\u8ddd\u79bb\u7684\u5b9e\u65f6\u573a\u666f\u6587\u672c\u68c0\u6d4b|Boyuan Zhu, Fagui Liu, Xi Chen, Quan Tang|Recently, scene text detection has received significant attention due to its wide application. However, accurate detection in complex scenes of multiple scales, orientations, and curvature remains a challenge. Numerous detection methods adopt the Vatti clipping (VC) algorithm for multiple-instance training to address the issue of arbitrary-shaped text. Yet we identify several bias results from these approaches called the \"shrinked kernel\". Specifically, it refers to a decrease in accuracy resulting from an output that overly favors the text kernel. In this paper, we propose a new approach named Expand Kernel Network (EK-Net) with expand kernel distance to compensate for the previous deficiency, which includes three-stages regression to complete instance detection. Moreover, EK-Net not only realize the precise positioning of arbitrary-shaped text, but also achieve a trade-off between performance and speed. Evaluation results demonstrate that EK-Net achieves state-of-the-art or competitive performance compared to other advanced methods, e.g., F-measure of 85.72% at 35.42 FPS on ICDAR 2015, F-measure of 85.75% at 40.13 FPS on CTW1500.|\u8fd1\u5e74\u6765\uff0c\u573a\u666f\u6587\u672c\u68c0\u6d4b\u7531\u4e8e\u5176\u5e7f\u6cdb\u7684\u5e94\u7528\u800c\u53d7\u5230\u4e86\u5e7f\u6cdb\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5728\u591a\u4e2a\u5c3a\u5ea6\u3001\u65b9\u5411\u548c\u66f2\u7387\u7684\u590d\u6742\u573a\u666f\u4e2d\u8fdb\u884c\u51c6\u786e\u68c0\u6d4b\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u8bb8\u591a\u68c0\u6d4b\u65b9\u6cd5\u91c7\u7528\u534e\u5e1d\u88c1\u526a\uff08VC\uff09\u7b97\u6cd5\u8fdb\u884c\u591a\u5b9e\u4f8b\u8bad\u7ec3\uff0c\u4ee5\u89e3\u51b3\u4efb\u610f\u5f62\u72b6\u6587\u672c\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u4ea7\u751f\u7684\u4e00\u4e9b\u504f\u5dee\u7ed3\u679c\u88ab\u79f0\u4e3a\u201c\u6536\u7f29\u5185\u6838\u201d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u6307\u7684\u662f\u7531\u4e8e\u8fc7\u4e8e\u504f\u5411\u6587\u672c\u5185\u6838\u7684\u8f93\u51fa\u800c\u5bfc\u81f4\u7684\u51c6\u786e\u6027\u4e0b\u964d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6269\u5c55\u6838\u7f51\u7edc\uff08EK-Net\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u6838\u8ddd\u79bb\u6765\u5f25\u8865\u4e4b\u524d\u7684\u7f3a\u9677\uff0c\u5176\u4e2d\u5305\u62ec\u4e09\u9636\u6bb5\u56de\u5f52\u6765\u5b8c\u6210\u5b9e\u4f8b\u68c0\u6d4b\u3002\u800c\u4e14\uff0cEK-Net\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u4efb\u610f\u5f62\u72b6\u6587\u672c\u7684\u7cbe\u786e\u5b9a\u4f4d\uff0c\u8fd8\u5b9e\u73b0\u4e86\u6027\u80fd\u548c\u901f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0cEK-Net \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6216\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4f8b\u5982\uff0cICDAR 2015 \u4e0a 35.42 FPS \u4e0b\u7684 F \u6d4b\u91cf\u4e3a 85.72%\uff0cCTW1500 \u4e0a 40.13 FPS \u4e0a\u7684 F \u6d4b\u91cf\u4e3a 85.75% \u3002|[2401.11704v1](http://arxiv.org/pdf/2401.11704v1)|null|\n", "2401.11674": "|**2024-01-22**|**Memory-Efficient Prompt Tuning for Incremental Histopathology Classification**|\u7528\u4e8e\u589e\u91cf\u7ec4\u7ec7\u75c5\u7406\u5b66\u5206\u7c7b\u7684\u5185\u5b58\u9ad8\u6548\u63d0\u793a\u8c03\u6574|Yu Zhu, Kang Li, Lequan Yu, Pheng-Ann Heng|Recent studies have made remarkable progress in histopathology classification. Based on current successes, contemporary works proposed to further upgrade the model towards a more generalizable and robust direction through incrementally learning from the sequentially delivered domains. Unlike previous parameter isolation based approaches that usually demand massive computation resources during model updating, we present a memory-efficient prompt tuning framework to cultivate model generalization potential in economical memory cost. For each incoming domain, we reuse the existing parameters of the initial classification model and attach lightweight trainable prompts into it for customized tuning. Considering the domain heterogeneity, we perform decoupled prompt tuning, where we adopt a domain-specific prompt for each domain to independently investigate its distinctive characteristics, and one domain-invariant prompt shared across all domains to continually explore the common content embedding throughout time. All domain-specific prompts will be appended to the prompt bank and isolated from further changes to prevent forgetting the distinctive features of early-seen domains. While the domain-invariant prompt will be passed on and iteratively evolve by style-augmented prompt refining to improve model generalization capability over time. In specific, we construct a graph with existing prompts and build a style-augmented graph attention network to guide the domain-invariant prompt exploring the overlapped latent embedding among all delivered domains for more domain generic representations. We have extensively evaluated our framework with two histopathology tasks, i.e., breast cancer metastasis classification and epithelium-stroma tissue classification, where our approach yielded superior performance and memory efficiency over the competing methods.|\u6700\u8fd1\u7684\u7814\u7a76\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u5206\u7c7b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u8fdb\u5c55\u3002\u57fa\u4e8e\u5f53\u524d\u7684\u6210\u529f\uff0c\u5f53\u4ee3\u7684\u5de5\u4f5c\u63d0\u51fa\u901a\u8fc7\u4ece\u987a\u5e8f\u4ea4\u4ed8\u7684\u9886\u57df\u4e2d\u589e\u91cf\u5b66\u4e60\uff0c\u8fdb\u4e00\u6b65\u5c06\u6a21\u578b\u5347\u7ea7\u5230\u66f4\u901a\u7528\u548c\u66f4\u7a33\u5065\u7684\u65b9\u5411\u3002\u4e0e\u4e4b\u524d\u57fa\u4e8e\u53c2\u6570\u9694\u79bb\u7684\u65b9\u6cd5\u5728\u6a21\u578b\u66f4\u65b0\u8fc7\u7a0b\u4e2d\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u4e0d\u540c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u63d0\u793a\u8c03\u6574\u6846\u67b6\uff0c\u4ee5\u7ecf\u6d4e\u7684\u5185\u5b58\u6210\u672c\u57f9\u517b\u6a21\u578b\u6cdb\u5316\u6f5c\u529b\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u4f20\u5165\u57df\uff0c\u6211\u4eec\u91cd\u7528\u521d\u59cb\u5206\u7c7b\u6a21\u578b\u7684\u73b0\u6709\u53c2\u6570\uff0c\u5e76\u5c06\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u63d0\u793a\u9644\u52a0\u5230\u5176\u4e2d\u4ee5\u8fdb\u884c\u5b9a\u5236\u8c03\u6574\u3002\u8003\u8651\u5230\u9886\u57df\u7684\u5f02\u6784\u6027\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u89e3\u8026\u7684\u63d0\u793a\u8c03\u6574\uff0c\u5176\u4e2d\u6211\u4eec\u4e3a\u6bcf\u4e2a\u9886\u57df\u91c7\u7528\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u63d0\u793a\u6765\u72ec\u7acb\u7814\u7a76\u5176\u72ec\u7279\u7684\u7279\u5f81\uff0c\u5e76\u5728\u6240\u6709\u9886\u57df\u4e4b\u95f4\u5171\u4eab\u4e00\u4e2a\u9886\u57df\u4e0d\u53d8\u7684\u63d0\u793a\uff0c\u4ee5\u4e0d\u65ad\u63a2\u7d22\u6574\u4e2a\u65f6\u95f4\u5d4c\u5165\u7684\u5171\u540c\u5185\u5bb9\u3002\u6240\u6709\u7279\u5b9a\u4e8e\u57df\u7684\u63d0\u793a\u90fd\u5c06\u9644\u52a0\u5230\u63d0\u793a\u5e93\u4e2d\uff0c\u5e76\u4e0e\u8fdb\u4e00\u6b65\u7684\u66f4\u6539\u9694\u79bb\uff0c\u4ee5\u9632\u6b62\u5fd8\u8bb0\u65e9\u671f\u770b\u5230\u7684\u57df\u7684\u72ec\u7279\u7279\u5f81\u3002\u800c\u9886\u57df\u4e0d\u53d8\u7684\u63d0\u793a\u5c06\u901a\u8fc7\u98ce\u683c\u589e\u5f3a\u7684\u63d0\u793a\u7ec6\u5316\u6765\u4f20\u9012\u548c\u8fed\u4ee3\u53d1\u5c55\uff0c\u4ee5\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7528\u73b0\u6709\u7684\u63d0\u793a\u6784\u5efa\u4e00\u4e2a\u56fe\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u98ce\u683c\u589e\u5f3a\u7684\u56fe\u6ce8\u610f\u7f51\u7edc\uff0c\u4ee5\u6307\u5bfc\u57df\u4e0d\u53d8\u7684\u63d0\u793a\u63a2\u7d22\u6240\u6709\u4ea4\u4ed8\u57df\u4e4b\u95f4\u7684\u91cd\u53e0\u6f5c\u5728\u5d4c\u5165\uff0c\u4ee5\u83b7\u5f97\u66f4\u591a\u57df\u901a\u7528\u8868\u793a\u3002\u6211\u4eec\u901a\u8fc7\u4e24\u9879\u7ec4\u7ec7\u75c5\u7406\u5b66\u4efb\u52a1\uff08\u5373\u4e73\u817a\u764c\u8f6c\u79fb\u5206\u7c7b\u548c\u4e0a\u76ae-\u95f4\u8d28\u7ec4\u7ec7\u5206\u7c7b\uff09\u5e7f\u6cdb\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u6846\u67b6\uff0c\u5176\u4e2d\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u7ade\u4e89\u65b9\u6cd5\u4ea7\u751f\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u8bb0\u5fc6\u6548\u7387\u3002|[2401.11674v1](http://arxiv.org/pdf/2401.11674v1)|null|\n", "2401.11671": "|**2024-01-22**|**RTA-Former: Reverse Transformer Attention for Polyp Segmentation**|RTA-Former\uff1a\u7528\u4e8e\u606f\u8089\u5206\u5272\u7684\u53cd\u5411\u53d8\u538b\u5668\u6ce8\u610f\u529b|Zhikai Li, Murong Yi, Ali Uneri, Sihan Niu, Craig Jones|Polyp segmentation is a key aspect of colorectal cancer prevention, enabling early detection and guiding subsequent treatments. Intelligent diagnostic tools, including deep learning solutions, are widely explored to streamline and potentially automate this process. However, even with many powerful network architectures, there still comes the problem of producing accurate edge segmentation. In this paper, we introduce a novel network, namely RTA-Former, that employs a transformer model as the encoder backbone and innovatively adapts Reverse Attention (RA) with a transformer stage in the decoder for enhanced edge segmentation. The results of the experiments illustrate that RTA-Former achieves state-of-the-art (SOTA) performance in five polyp segmentation datasets. The strong capability of RTA-Former holds promise in improving the accuracy of Transformer-based polyp segmentation, potentially leading to better clinical decisions and patient outcomes. Our code will be publicly available on GitHub.|\u606f\u8089\u5206\u5272\u662f\u7ed3\u76f4\u80a0\u764c\u9884\u9632\u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u53ef\u4ee5\u5b9e\u73b0\u65e9\u671f\u53d1\u73b0\u5e76\u6307\u5bfc\u540e\u7eed\u6cbb\u7597\u3002\u4eba\u4eec\u5e7f\u6cdb\u63a2\u7d22\u5305\u62ec\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u5728\u5185\u7684\u667a\u80fd\u8bca\u65ad\u5de5\u5177\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u5e76\u53ef\u80fd\u5b9e\u73b0\u81ea\u52a8\u5316\u3002\u7136\u800c\uff0c\u5373\u4f7f\u6709\u8bb8\u591a\u5f3a\u5927\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u4ecd\u7136\u5b58\u5728\u4ea7\u751f\u51c6\u786e\u8fb9\u7f18\u5206\u5272\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u7edc\uff0c\u5373 RTA-Former\uff0c\u5b83\u91c7\u7528 Transformer \u6a21\u578b\u4f5c\u4e3a\u7f16\u7801\u5668\u4e3b\u5e72\uff0c\u5e76\u521b\u65b0\u6027\u5730\u91c7\u7528\u89e3\u7801\u5668\u4e2d Transformer \u7ea7\u7684\u53cd\u5411\u6ce8\u610f\u529b\uff08RA\uff09\u6765\u589e\u5f3a\u8fb9\u7f18\u5206\u5272\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRTA-Former \u5728\u4e94\u4e2a\u606f\u8089\u5206\u5272\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684 (SOTA) \u6027\u80fd\u3002 RTA-Former \u7684\u5f3a\u5927\u529f\u80fd\u6709\u671b\u63d0\u9ad8\u57fa\u4e8e Transformer \u7684\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u6709\u53ef\u80fd\u5e26\u6765\u66f4\u597d\u7684\u4e34\u5e8a\u51b3\u7b56\u548c\u60a3\u8005\u7ed3\u679c\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728 GitHub \u4e0a\u516c\u5f00\u53d1\u5e03\u3002|[2401.11671v1](http://arxiv.org/pdf/2401.11671v1)|null|\n", "2401.11654": "|**2024-01-22**|**ActionHub: A Large-scale Action Video Description Dataset for Zero-shot Action Recognition**|ActionHub\uff1a\u7528\u4e8e\u96f6\u955c\u5934\u52a8\u4f5c\u8bc6\u522b\u7684\u5927\u89c4\u6a21\u52a8\u4f5c\u89c6\u9891\u63cf\u8ff0\u6570\u636e\u96c6|Jiaming Zhou, Junwei Liang, Kun-Yu Lin, Jinrui Yang, Wei-Shi Zheng|Zero-shot action recognition (ZSAR) aims to learn an alignment model between videos and class descriptions of seen actions that is transferable to unseen actions. The text queries (class descriptions) used in existing ZSAR works, however, are often short action names that fail to capture the rich semantics in the videos, leading to misalignment. With the intuition that video content descriptions (e.g., video captions) can provide rich contextual information of visual concepts in videos, we propose to utilize human annotated video descriptions to enrich the semantics of the class descriptions of each action. However, all existing action video description datasets are limited in terms of the number of actions, the semantics of video descriptions, etc. To this end, we collect a large-scale action video descriptions dataset named ActionHub, which covers a total of 1,211 common actions and provides 3.6 million action video descriptions. With the proposed ActionHub dataset, we further propose a novel Cross-modality and Cross-action Modeling (CoCo) framework for ZSAR, which consists of a Dual Cross-modality Alignment module and a Cross-action Invariance Mining module. Specifically, the Dual Cross-modality Alignment module utilizes both action labels and video descriptions from ActionHub to obtain rich class semantic features for feature alignment. The Cross-action Invariance Mining module exploits a cycle-reconstruction process between the class semantic feature spaces of seen actions and unseen actions, aiming to guide the model to learn cross-action invariant representations. Extensive experimental results demonstrate that our CoCo framework significantly outperforms the state-of-the-art on three popular ZSAR benchmarks (i.e., Kinetics-ZSAR, UCF101 and HMDB51) under two different learning protocols in ZSAR. We will release our code, models, and the proposed ActionHub dataset.|\u96f6\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\uff08ZSAR\uff09\u65e8\u5728\u5b66\u4e60\u89c6\u9891\u548c\u5df2\u89c1\u52a8\u4f5c\u7684\u7c7b\u63cf\u8ff0\u4e4b\u95f4\u7684\u5bf9\u9f50\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u8f6c\u79fb\u5230\u672a\u89c1\u52a8\u4f5c\u3002\u7136\u800c\uff0c\u73b0\u6709 ZSAR \u4f5c\u54c1\u4e2d\u4f7f\u7528\u7684\u6587\u672c\u67e5\u8be2\uff08\u7c7b\u63cf\u8ff0\uff09\u901a\u5e38\u662f\u7b80\u77ed\u7684\u52a8\u4f5c\u540d\u79f0\uff0c\u65e0\u6cd5\u6355\u83b7\u89c6\u9891\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\uff0c\u4ece\u800c\u5bfc\u81f4\u9519\u4f4d\u3002\u51ed\u501f\u89c6\u9891\u5185\u5bb9\u63cf\u8ff0\uff08\u4f8b\u5982\u89c6\u9891\u5b57\u5e55\uff09\u53ef\u4ee5\u63d0\u4f9b\u89c6\u9891\u4e2d\u89c6\u89c9\u6982\u5ff5\u7684\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u76f4\u89c9\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528\u4eba\u7c7b\u6ce8\u91ca\u7684\u89c6\u9891\u63cf\u8ff0\u6765\u4e30\u5bcc\u6bcf\u4e2a\u52a8\u4f5c\u7684\u7c7b\u63cf\u8ff0\u7684\u8bed\u4e49\u3002\u7136\u800c\uff0c\u6240\u6709\u73b0\u6709\u7684\u52a8\u4f5c\u89c6\u9891\u63cf\u8ff0\u6570\u636e\u96c6\u5728\u52a8\u4f5c\u6570\u91cf\u3001\u89c6\u9891\u63cf\u8ff0\u8bed\u4e49\u7b49\u65b9\u9762\u90fd\u53d7\u5230\u9650\u5236\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e00\u4e2a\u540d\u4e3a ActionHub \u7684\u5927\u89c4\u6a21\u52a8\u4f5c\u89c6\u9891\u63cf\u8ff0\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u603b\u5171\u6db5\u76d6\u4e86 1,211 \u4e2a\u5e38\u89c1\u7684\u52a8\u4f5c\u89c6\u9891\u63cf\u8ff0\u6570\u636e\u96c6\u3002\u52a8\u4f5c\u5e76\u63d0\u4f9b360\u4e07\u6761\u52a8\u4f5c\u89c6\u9891\u63cf\u8ff0\u3002\u5229\u7528\u6240\u63d0\u51fa\u7684 ActionHub \u6570\u636e\u96c6\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 ZSAR \u8de8\u6a21\u6001\u548c\u4ea4\u53c9\u52a8\u4f5c\u5efa\u6a21\uff08CoCo\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7531\u53cc\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\u548c\u8de8\u52a8\u4f5c\u4e0d\u53d8\u6027\u6316\u6398\u6a21\u5757\u7ec4\u6210\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u53cc\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\u5229\u7528\u6765\u81ea ActionHub \u7684\u52a8\u4f5c\u6807\u7b7e\u548c\u89c6\u9891\u63cf\u8ff0\u6765\u83b7\u53d6\u4e30\u5bcc\u7684\u7c7b\u522b\u8bed\u4e49\u7279\u5f81\u4ee5\u8fdb\u884c\u7279\u5f81\u5bf9\u9f50\u3002\u4ea4\u53c9\u52a8\u4f5c\u4e0d\u53d8\u6027\u6316\u6398\u6a21\u5757\u5229\u7528\u5df2\u89c1\u52a8\u4f5c\u548c\u672a\u89c1\u52a8\u4f5c\u7684\u7c7b\u8bed\u4e49\u7279\u5f81\u7a7a\u95f4\u4e4b\u95f4\u7684\u5faa\u73af\u91cd\u5efa\u8fc7\u7a0b\uff0c\u65e8\u5728\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u4ea4\u53c9\u52a8\u4f5c\u4e0d\u53d8\u8868\u793a\u3002\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728 ZSAR \u7684\u4e24\u79cd\u4e0d\u540c\u5b66\u4e60\u534f\u8bae\u4e0b\uff0c\u6211\u4eec\u7684 CoCo \u6846\u67b6\u5728\u4e09\u4e2a\u6d41\u884c\u7684 ZSAR \u57fa\u51c6\uff08\u5373 Kinetics-ZSAR\u3001UCF101 \u548c HMDB51\uff09\u4e0a\u7684\u6027\u80fd\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6846\u67b6\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u6211\u4eec\u7684\u4ee3\u7801\u3001\u6a21\u578b\u548c\u63d0\u8bae\u7684 ActionHub \u6570\u636e\u96c6\u3002|[2401.11654v1](http://arxiv.org/pdf/2401.11654v1)|null|\n", "2401.11649": "|**2024-01-22**|**M2-CLIP: A Multimodal, Multi-task Adapting Framework for Video Action Recognition**|M2-CLIP\uff1a\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u7684\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u9002\u5e94\u6846\u67b6|Mengmeng Wang, Jiazheng Xing, Boyuan Jiang, Jun Chen, Jianbiao Mei, Xingxing Zuo, Guang Dai, Jingdong Wang, Yong Liu|Recently, the rise of large-scale vision-language pretrained models like CLIP, coupled with the technology of Parameter-Efficient FineTuning (PEFT), has captured substantial attraction in video action recognition. Nevertheless, prevailing approaches tend to prioritize strong supervised performance at the expense of compromising the models' generalization capabilities during transfer. In this paper, we introduce a novel Multimodal, Multi-task CLIP adapting framework named \\name to address these challenges, preserving both high supervised performance and robust transferability. Firstly, to enhance the individual modality architectures, we introduce multimodal adapters to both the visual and text branches. Specifically, we design a novel visual TED-Adapter, that performs global Temporal Enhancement and local temporal Difference modeling to improve the temporal representation capabilities of the visual encoder. Moreover, we adopt text encoder adapters to strengthen the learning of semantic label information. Secondly, we design a multi-task decoder with a rich set of supervisory signals to adeptly satisfy the need for strong supervised performance and generalization within a multimodal framework. Experimental results validate the efficacy of our approach, demonstrating exceptional performance in supervised learning while maintaining strong generalization in zero-shot scenarios.|\u6700\u8fd1\uff0c\u50cf CLIP \u8fd9\u6837\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5174\u8d77\uff0c\u52a0\u4e0a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\uff0c\u5728\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u9886\u57df\u5f15\u8d77\u4e86\u5de8\u5927\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u6d41\u884c\u7684\u65b9\u6cd5\u503e\u5411\u4e8e\u4f18\u5148\u8003\u8651\u5f3a\u76d1\u7763\u6027\u80fd\uff0c\u4f46\u4ee3\u4ef7\u662f\u5728\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u635f\u5bb3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a \\name \u7684\u65b0\u9896\u7684\u591a\u6a21\u5f0f\u3001\u591a\u4efb\u52a1 CLIP \u9002\u5e94\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u76d1\u7763\u6027\u80fd\u548c\u5f3a\u5927\u7684\u53ef\u8f6c\u79fb\u6027\u3002\u9996\u5148\uff0c\u4e3a\u4e86\u589e\u5f3a\u5355\u72ec\u7684\u6a21\u6001\u67b6\u6784\uff0c\u6211\u4eec\u5c06\u591a\u6a21\u6001\u9002\u914d\u5668\u5f15\u5165\u89c6\u89c9\u548c\u6587\u672c\u5206\u652f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9 TED \u9002\u914d\u5668\uff0c\u5b83\u6267\u884c\u5168\u5c40\u65f6\u95f4\u589e\u5f3a\u548c\u5c40\u90e8\u65f6\u95f4\u5dee\u5f02\u5efa\u6a21\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u7f16\u7801\u5668\u7684\u65f6\u95f4\u8868\u793a\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528\u6587\u672c\u7f16\u7801\u5668\u9002\u914d\u5668\u6765\u52a0\u5f3a\u8bed\u4e49\u6807\u7b7e\u4fe1\u606f\u7684\u5b66\u4e60\u3002\u5176\u6b21\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5177\u6709\u4e30\u5bcc\u76d1\u7763\u4fe1\u53f7\u96c6\u7684\u591a\u4efb\u52a1\u89e3\u7801\u5668\uff0c\u4ee5\u5de7\u5999\u5730\u6ee1\u8db3\u591a\u6a21\u6001\u6846\u67b6\u5185\u5bf9\u5f3a\u76d1\u7763\u6027\u80fd\u548c\u6cdb\u5316\u7684\u9700\u6c42\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u76d1\u7763\u5b66\u4e60\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u540c\u65f6\u5728\u96f6\u6837\u672c\u573a\u666f\u4e2d\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002|[2401.11649v1](http://arxiv.org/pdf/2401.11649v1)|null|\n", "2401.11644": "|**2024-01-22**|**Friends Across Time: Multi-Scale Action Segmentation Transformer for Surgical Phase Recognition**|\u8de8\u8d8a\u65f6\u95f4\u7684\u670b\u53cb\uff1a\u7528\u4e8e\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u591a\u5c3a\u5ea6\u52a8\u4f5c\u5206\u6bb5\u53d8\u538b\u5668|Bokai Zhang, Jiayuan Meng, Bin Cheng, Dean Biskup, Svetlana Petculescu, Angela Chapman|Automatic surgical phase recognition is a core technology for modern operating rooms and online surgical video assessment platforms. Current state-of-the-art methods use both spatial and temporal information to tackle the surgical phase recognition task. Building on this idea, we propose the Multi-Scale Action Segmentation Transformer (MS-AST) for offline surgical phase recognition and the Multi-Scale Action Segmentation Causal Transformer (MS-ASCT) for online surgical phase recognition. We use ResNet50 or EfficientNetV2-M for spatial feature extraction. Our MS-AST and MS-ASCT can model temporal information at different scales with multi-scale temporal self-attention and multi-scale temporal cross-attention, which enhances the capture of temporal relationships between frames and segments. We demonstrate that our method can achieve 95.26% and 96.15% accuracy on the Cholec80 dataset for online and offline surgical phase recognition, respectively, which achieves new state-of-the-art results. Our method can also achieve state-of-the-art results on non-medical datasets in the video action segmentation domain.|\u81ea\u52a8\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u662f\u73b0\u4ee3\u624b\u672f\u5ba4\u548c\u5728\u7ebf\u624b\u672f\u89c6\u9891\u8bc4\u4f30\u5e73\u53f0\u7684\u6838\u5fc3\u6280\u672f\u3002\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u4f7f\u7528\u7a7a\u95f4\u548c\u65f6\u95f4\u4fe1\u606f\u6765\u5904\u7406\u624b\u672f\u76f8\u4f4d\u8bc6\u522b\u4efb\u52a1\u3002\u57fa\u4e8e\u8fd9\u4e2a\u60f3\u6cd5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u79bb\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u591a\u5c3a\u5ea6\u52a8\u4f5c\u5206\u5272\u53d8\u538b\u5668\uff08MS-AST\uff09\u548c\u7528\u4e8e\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u591a\u5c3a\u5ea6\u52a8\u4f5c\u5206\u5272\u56e0\u679c\u53d8\u538b\u5668\uff08MS-ASCT\uff09\u3002\u6211\u4eec\u4f7f\u7528ResNet50\u6216EfficientNetV2-M\u8fdb\u884c\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u3002\u6211\u4eec\u7684 MS-AST \u548c MS-ASCT \u53ef\u4ee5\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u95f4\u81ea\u6ce8\u610f\u529b\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u4e0d\u540c\u5c3a\u5ea6\u7684\u65f6\u95f4\u4fe1\u606f\u8fdb\u884c\u5efa\u6a21\uff0c\u4ece\u800c\u589e\u5f3a\u5bf9\u5e27\u548c\u7247\u6bb5\u4e4b\u95f4\u65f6\u95f4\u5173\u7cfb\u7684\u6355\u83b7\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728 Cholec80 \u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u5728\u7ebf\u548c\u79bb\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684 95.26% \u548c 96.15% \u51c6\u786e\u7387\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u8fd8\u53ef\u4ee5\u5728\u89c6\u9891\u52a8\u4f5c\u5206\u5272\u9886\u57df\u7684\u975e\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002|[2401.11644v1](http://arxiv.org/pdf/2401.11644v1)|null|\n", "2401.11633": "|**2024-01-22**|**Zoom-shot: Fast and Efficient Unsupervised Zero-Shot Transfer of CLIP to Vision Encoders with Multimodal Loss**|Zoom-shot\uff1a\u5feb\u901f\u9ad8\u6548\u7684\u65e0\u76d1\u7763\u96f6\u6837\u672c\u5c06 CLIP \u4f20\u8f93\u5230\u5177\u6709\u591a\u6a21\u6001\u635f\u5931\u7684\u89c6\u89c9\u7f16\u7801\u5668|Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, Clinton Fookes|The fusion of vision and language has brought about a transformative shift in computer vision through the emergence of Vision-Language Models (VLMs). However, the resource-intensive nature of existing VLMs poses a significant challenge. We need an accessible method for developing the next generation of VLMs. To address this issue, we propose Zoom-shot, a novel method for transferring the zero-shot capabilities of CLIP to any pre-trained vision encoder. We do this by exploiting the multimodal information (i.e. text and image) present in the CLIP latent space through the use of specifically designed multimodal loss functions. These loss functions are (1) cycle-consistency loss and (2) our novel prompt-guided knowledge distillation loss (PG-KD). PG-KD combines the concept of knowledge distillation with CLIP's zero-shot classification, to capture the interactions between text and image features. With our multimodal losses, we train a $\\textbf{linear mapping}$ between the CLIP latent space and the latent space of a pre-trained vision encoder, for only a $\\textbf{single epoch}$. Furthermore, Zoom-shot is entirely unsupervised and is trained using $\\textbf{unpaired}$ data. We test the zero-shot capabilities of a range of vision encoders augmented as new VLMs, on coarse and fine-grained classification datasets, outperforming the previous state-of-the-art in this problem domain. In our ablations, we find Zoom-shot allows for a trade-off between data and compute during training; and our state-of-the-art results can be obtained by reducing training from 20% to 1% of the ImageNet training data with 20 epochs. All code and models are available on GitHub.|\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u51fa\u73b0\uff0c\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u878d\u5408\u7ed9\u8ba1\u7b97\u673a\u89c6\u89c9\u5e26\u6765\u4e86\u9769\u547d\u6027\u7684\u8f6c\u53d8\u3002\u7136\u800c\uff0c\u73b0\u6709 VLM \u7684\u8d44\u6e90\u5bc6\u96c6\u578b\u6027\u8d28\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u6211\u4eec\u9700\u8981\u4e00\u79cd\u53ef\u8bbf\u95ee\u7684\u65b9\u6cd5\u6765\u5f00\u53d1\u4e0b\u4e00\u4ee3 VLM\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Zoom-shot\uff0c\u8fd9\u662f\u4e00\u79cd\u5c06 CLIP \u7684\u96f6\u6837\u672c\u529f\u80fd\u8f6c\u79fb\u5230\u4efb\u4f55\u9884\u5148\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u4e13\u95e8\u8bbe\u8ba1\u7684\u591a\u6a21\u6001\u635f\u5931\u51fd\u6570\u6765\u5229\u7528 CLIP \u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b58\u5728\u7684\u591a\u6a21\u6001\u4fe1\u606f\uff08\u5373\u6587\u672c\u548c\u56fe\u50cf\uff09\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u8fd9\u4e9b\u635f\u5931\u51fd\u6570\u662f\uff081\uff09\u5faa\u73af\u4e00\u81f4\u6027\u635f\u5931\u548c\uff082\uff09\u6211\u4eec\u65b0\u9896\u7684\u63d0\u793a\u5f15\u5bfc\u77e5\u8bc6\u84b8\u998f\u635f\u5931\uff08PG-KD\uff09\u3002 PG-KD\u5c06\u77e5\u8bc6\u84b8\u998f\u7684\u6982\u5ff5\u4e0eCLIP\u7684\u96f6\u6837\u672c\u5206\u7c7b\u76f8\u7ed3\u5408\uff0c\u4ee5\u6355\u83b7\u6587\u672c\u548c\u56fe\u50cf\u7279\u5f81\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u901a\u8fc7\u591a\u6a21\u6001\u635f\u5931\uff0c\u6211\u4eec\u5728 CLIP \u6f5c\u5728\u7a7a\u95f4\u548c\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u4e4b\u95f4\u8bad\u7ec3 $\\textbf{\u7ebf\u6027\u6620\u5c04}$\uff0c\u4ec5\u9002\u7528\u4e8e $\\textbf{single epoch}$\u3002\u6b64\u5916\uff0cZoom-shot \u5b8c\u5168\u662f\u65e0\u76d1\u7763\u7684\uff0c\u5e76\u4e14\u4f7f\u7528 $\\textbf{unpaired}$ \u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u6211\u4eec\u5728\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u4e00\u7cfb\u5217\u4f5c\u4e3a\u65b0 VLM \u589e\u5f3a\u7684\u89c6\u89c9\u7f16\u7801\u5668\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5728\u8be5\u95ee\u9898\u9886\u57df\u7684\u8868\u73b0\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002\u5728\u6211\u4eec\u7684\u6d88\u878d\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0 Zoom-shot \u5141\u8bb8\u5728\u8bad\u7ec3\u671f\u95f4\u5728\u6570\u636e\u548c\u8ba1\u7b97\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff1b\u6211\u4eec\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u53ef\u4ee5\u901a\u8fc7\u5c06 20 \u4e2a epoch \u7684 ImageNet \u8bad\u7ec3\u6570\u636e\u7684\u8bad\u7ec3\u91cf\u4ece 20% \u51cf\u5c11\u5230 1% \u6765\u83b7\u5f97\u3002\u6240\u6709\u4ee3\u7801\u548c\u6a21\u578b\u5747\u53ef\u5728 GitHub \u4e0a\u83b7\u53d6\u3002|[2401.11633v1](http://arxiv.org/pdf/2401.11633v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2401.12198": "|**2024-01-22**|**LONEStar: The Lunar Flashlight Optical Navigation Experiment**|LONEStar\uff1a\u6708\u7403\u624b\u7535\u7b52\u5149\u5b66\u5bfc\u822a\u5b9e\u9a8c|Michael Krause, Ava Thrasher, Priyal Soni, Liam Smego, Reuben Isaac, Jennifer Nolan, Micah Pledger, E. Glenn Lightsey, W. Jud Ready, John Christian|This paper documents the results from the highly successful Lunar flashlight Optical Navigation Experiment with a Star tracker (LONEStar). Launched in December 2022, Lunar Flashlight (LF) was a NASA-funded technology demonstration mission. After a propulsion system anomaly prevented capture in lunar orbit, LF was ejected from the Earth-Moon system and into heliocentric space. NASA subsequently transferred ownership of LF to Georgia Tech to conduct an unfunded extended mission to demonstrate further advanced technology objectives, including LONEStar. From August-December 2023, the LONEStar team performed on-orbit calibration of the optical instrument and a number of different OPNAV experiments. This campaign included the processing of nearly 400 images of star fields, Earth and Moon, and four other planets (Mercury, Mars, Jupiter, and Saturn). LONEStar provided the first on-orbit demonstrations of heliocentric navigation using only optical observations of planets. Of special note is the successful in-flight demonstration of (1) instantaneous triangulation with simultaneous sightings of two planets with the LOST algorithm and (2) dynamic triangulation with sequential sightings of multiple planets.|\u672c\u6587\u8bb0\u5f55\u4e86\u4f7f\u7528\u661f\u8ddf\u8e2a\u5668 (LONEStar) \u8fdb\u884c\u7684\u975e\u5e38\u6210\u529f\u7684\u6708\u7403\u624b\u7535\u7b52\u5149\u5b66\u5bfc\u822a\u5b9e\u9a8c\u7684\u7ed3\u679c\u3002\u6708\u7403\u624b\u7535\u7b52 (LF) \u4e8e 2022 \u5e74 12 \u6708\u53d1\u5c04\uff0c\u662f NASA \u8d44\u52a9\u7684\u4e00\u9879\u6280\u672f\u6f14\u793a\u4efb\u52a1\u3002\u5728\u63a8\u8fdb\u7cfb\u7edf\u5f02\u5e38\u5bfc\u81f4\u65e0\u6cd5\u6355\u83b7\u6708\u7403\u8f68\u9053\u540e\uff0cLF \u88ab\u4ece\u5730\u6708\u7cfb\u7edf\u4e2d\u5f39\u51fa\u5e76\u8fdb\u5165\u65e5\u5fc3\u7a7a\u95f4\u3002 NASA \u968f\u540e\u5c06 LF \u7684\u6240\u6709\u6743\u8f6c\u8ba9\u7ed9\u4f50\u6cbb\u4e9a\u7406\u5de5\u5b66\u9662\uff0c\u4ee5\u6267\u884c\u4e00\u9879\u65e0\u8d44\u91d1\u652f\u6301\u7684\u6269\u5c55\u4efb\u52a1\uff0c\u4ee5\u5c55\u793a\u8fdb\u4e00\u6b65\u7684\u5148\u8fdb\u6280\u672f\u76ee\u6807\uff0c\u5305\u62ec LONEStar\u3002 2023\u5e748\u6708\u81f312\u6708\uff0cLONEStar\u56e2\u961f\u8fdb\u884c\u4e86\u5149\u5b66\u4eea\u5668\u5728\u8f68\u6821\u51c6\u548c\u591a\u9879\u4e0d\u540c\u7684OPNAV\u5b9e\u9a8c\u3002\u8be5\u6d3b\u52a8\u5305\u62ec\u5904\u7406\u8fd1 400 \u5f20\u661f\u57df\u3001\u5730\u7403\u548c\u6708\u7403\u4ee5\u53ca\u5176\u4ed6\u56db\u9897\u884c\u661f\uff08\u6c34\u661f\u3001\u706b\u661f\u3001\u6728\u661f\u548c\u571f\u661f\uff09\u7684\u56fe\u50cf\u3002 LONEStar \u9996\u6b21\u4ec5\u4f7f\u7528\u884c\u661f\u5149\u5b66\u89c2\u6d4b\u8fdb\u884c\u4e86\u65e5\u5fc3\u5bfc\u822a\u5728\u8f68\u6f14\u793a\u3002\u7279\u522b\u503c\u5f97\u6ce8\u610f\u7684\u662f\u6210\u529f\u7684\u98de\u884c\u6f14\u793a\uff1a(1) \u4f7f\u7528 LOST \u7b97\u6cd5\u540c\u65f6\u89c2\u6d4b\u4e24\u9897\u884c\u661f\u7684\u77ac\u65f6\u4e09\u89d2\u6d4b\u91cf\u548c (2) \u8fde\u7eed\u89c2\u6d4b\u591a\u4e2a\u884c\u661f\u7684\u52a8\u6001\u4e09\u89d2\u6d4b\u91cf\u3002|[2401.12198v1](http://arxiv.org/pdf/2401.12198v1)|null|\n", "2401.12019": "|**2024-01-22**|**Stereo-Matching Knowledge Distilled Monocular Depth Estimation Filtered by Multiple Disparity Consistency**|\u901a\u8fc7\u591a\u91cd\u89c6\u5dee\u4e00\u81f4\u6027\u8fc7\u6ee4\u7684\u7acb\u4f53\u5339\u914d\u77e5\u8bc6\u84b8\u998f\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1|Woonghyun Ka, Jae Young Lee, Jaehyun Choi, Junmo Kim|In stereo-matching knowledge distillation methods of the self-supervised monocular depth estimation, the stereo-matching network's knowledge is distilled into a monocular depth network through pseudo-depth maps. In these methods, the learning-based stereo-confidence network is generally utilized to identify errors in the pseudo-depth maps to prevent transferring the errors. However, the learning-based stereo-confidence networks should be trained with ground truth (GT), which is not feasible in a self-supervised setting. In this paper, we propose a method to identify and filter errors in the pseudo-depth map using multiple disparity maps by checking their consistency without the need for GT and a training process. Experimental results show that the proposed method outperforms the previous methods and works well on various configurations by filtering out erroneous areas where the stereo-matching is vulnerable, especially such as textureless regions, occlusion boundaries, and reflective surfaces.|\u5728\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u7acb\u4f53\u5339\u914d\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e2d\uff0c\u7acb\u4f53\u5339\u914d\u7f51\u7edc\u7684\u77e5\u8bc6\u901a\u8fc7\u4f2a\u6df1\u5ea6\u56fe\u88ab\u84b8\u998f\u4e3a\u5355\u76ee\u6df1\u5ea6\u7f51\u7edc\u3002\u5728\u8fd9\u4e9b\u65b9\u6cd5\u4e2d\uff0c\u901a\u5e38\u5229\u7528\u57fa\u4e8e\u5b66\u4e60\u7684\u7acb\u4f53\u7f6e\u4fe1\u7f51\u7edc\u6765\u8bc6\u522b\u4f2a\u6df1\u5ea6\u56fe\u4e2d\u7684\u9519\u8bef\uff0c\u4ee5\u9632\u6b62\u9519\u8bef\u8f6c\u79fb\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u7acb\u4f53\u7f6e\u4fe1\u7f51\u7edc\u5e94\u8be5\u4f7f\u7528\u5730\u9762\u5b9e\u51b5\uff08GT\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u5728\u81ea\u6211\u76d1\u7763\u7684\u73af\u5883\u4e2d\u662f\u4e0d\u53ef\u884c\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u591a\u4e2a\u89c6\u5dee\u56fe\u901a\u8fc7\u68c0\u67e5\u5b83\u4eec\u7684\u4e00\u81f4\u6027\u6765\u8bc6\u522b\u548c\u8fc7\u6ee4\u4f2a\u6df1\u5ea6\u56fe\u4e2d\u7684\u9519\u8bef\u7684\u65b9\u6cd5\uff0c\u800c\u4e0d\u9700\u8981 GT \u548c\u8bad\u7ec3\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u901a\u8fc7\u8fc7\u6ee4\u6389\u7acb\u4f53\u5339\u914d\u8106\u5f31\u7684\u9519\u8bef\u533a\u57df\uff0c\u7279\u522b\u662f\u65e0\u7eb9\u7406\u533a\u57df\u3001\u906e\u6321\u8fb9\u754c\u548c\u53cd\u5c04\u8868\u9762\u7b49\uff0c\u5728\u5404\u79cd\u914d\u7f6e\u4e0a\u90fd\u8868\u73b0\u826f\u597d\u3002|[2401.12019v1](http://arxiv.org/pdf/2401.12019v1)|null|\n", "2401.12014": "|**2024-01-22**|**Robustness to distribution shifts of compressed networks for edge devices**|\u8fb9\u7f18\u8bbe\u5907\u538b\u7f29\u7f51\u7edc\u5206\u5e03\u53d8\u5316\u7684\u9c81\u68d2\u6027|Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark|It is necessary to develop efficient DNNs deployed on edge devices with limited computation resources. However, the compressed networks often execute new tasks in the target domain, which is different from the source domain where the original network is trained. It is important to investigate the robustness of compressed networks in two types of data distribution shifts: domain shifts and adversarial perturbations. In this study, we discover that compressed models are less robust to distribution shifts than their original networks. Interestingly, larger networks are more vulnerable to losing robustness than smaller ones, even when they are compressed to a similar size as the smaller networks. Furthermore, compact networks obtained by knowledge distillation are much more robust to distribution shifts than pruned networks. Finally, post-training quantization is a reliable method for achieving significant robustness to distribution shifts, and it outperforms both pruned and distilled models in terms of robustness.|\u6709\u5fc5\u8981\u5f00\u53d1\u90e8\u7f72\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548 DNN\u3002\u7136\u800c\uff0c\u538b\u7f29\u7f51\u7edc\u901a\u5e38\u5728\u76ee\u6807\u57df\u4e2d\u6267\u884c\u65b0\u4efb\u52a1\uff0c\u8be5\u76ee\u6807\u57df\u4e0e\u8bad\u7ec3\u539f\u59cb\u7f51\u7edc\u7684\u6e90\u57df\u4e0d\u540c\u3002\u7814\u7a76\u538b\u7f29\u7f51\u7edc\u5728\u4e24\u79cd\u7c7b\u578b\u7684\u6570\u636e\u5206\u5e03\u53d8\u5316\u4e2d\u7684\u9c81\u68d2\u6027\u975e\u5e38\u91cd\u8981\uff1a\u57df\u53d8\u5316\u548c\u5bf9\u6297\u6027\u6270\u52a8\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\u538b\u7f29\u6a21\u578b\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u9c81\u68d2\u6027\u4e0d\u5982\u539f\u59cb\u7f51\u7edc\u3002\u6709\u8da3\u7684\u662f\uff0c\u8f83\u5927\u7684\u7f51\u7edc\u6bd4\u8f83\u5c0f\u7684\u7f51\u7edc\u66f4\u5bb9\u6613\u5931\u53bb\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5b83\u4eec\u88ab\u538b\u7f29\u5230\u4e0e\u8f83\u5c0f\u7684\u7f51\u7edc\u76f8\u4f3c\u7684\u5927\u5c0f\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u83b7\u5f97\u7684\u7d27\u51d1\u7f51\u7edc\u6bd4\u4fee\u526a\u7f51\u7edc\u5bf9\u5206\u5e03\u53d8\u5316\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u6700\u540e\uff0c\u8bad\u7ec3\u540e\u91cf\u5316\u662f\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u663e\u7740\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u4fee\u526a\u6a21\u578b\u548c\u84b8\u998f\u6a21\u578b\u3002|[2401.12014v1](http://arxiv.org/pdf/2401.12014v1)|null|\n"}, "OCR": {"2401.11944": "|**2024-01-22**|**CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark**|CMMMU\uff1a\u4e2d\u56fd\u5927\u89c4\u6a21\u591a\u5b66\u79d1\u591a\u6a21\u6001\u7406\u89e3\u57fa\u51c6|Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, et.al.|As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures.   CMMMU focuses on complex perception and reasoning with domain-specific knowledge in the Chinese context. We evaluate 11 open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%, indicating a large space for improvement. CMMMU will boost the community to build the next-generation LMMs towards expert artificial intelligence and promote the democratization of LMMs by providing diverse language contexts.|\u968f\u7740\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u7684\u529f\u80fd\u4e0d\u65ad\u8fdb\u6b65\uff0c\u8bc4\u4f30 LMM \u6027\u80fd\u7684\u9700\u6c42\u4e5f\u65e5\u76ca\u589e\u957f\u3002\u6b64\u5916\uff0c\u5728\u6c49\u8bed\u7b49\u975e\u82f1\u8bed\u73af\u5883\u4e2d\u8bc4\u4f30 LMM \u7684\u9ad8\u7ea7\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u5b58\u5728\u66f4\u5927\u7684\u5dee\u8ddd\u3002\u6211\u4eec\u63a8\u51fa\u4e86 CMMMU\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u4e2d\u56fd\u5927\u89c4\u6a21\u591a\u5b66\u79d1\u591a\u6a21\u6001\u7406\u89e3\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30 LMM \u5728\u4e2d\u56fd\u80cc\u666f\u4e0b\u9700\u8981\u5927\u5b66\u6c34\u5e73\u5b66\u79d1\u77e5\u8bc6\u548c\u6df1\u601d\u719f\u8651\u63a8\u7406\u7684\u4efb\u52a1\u3002 CMMMU\u53d7\u5230MMMU\u7684\u542f\u53d1\u5e76\u4e25\u683c\u9075\u5faaMMMU\u7684\u6ce8\u91ca\u548c\u5206\u6790\u6a21\u5f0f\u3002 CMMMU \u5305\u62ec\u4ece\u5927\u5b66\u8003\u8bd5\u3001\u6d4b\u9a8c\u548c\u6559\u79d1\u4e66\u4e2d\u624b\u52a8\u6536\u96c6\u7684 12k \u591a\u6a21\u6001\u95ee\u9898\uff0c\u6db5\u76d6\u516d\u4e2a\u6838\u5fc3\u5b66\u79d1\uff1a\u827a\u672f\u4e0e\u8bbe\u8ba1\u3001\u5546\u4e1a\u3001\u79d1\u5b66\u3001\u5065\u5eb7\u4e0e\u533b\u5b66\u3001\u4eba\u6587\u4e0e\u793e\u4f1a\u79d1\u5b66\u4ee5\u53ca\u6280\u672f\u4e0e\u5de5\u7a0b\uff0c\u4e0e\u5176\u59ca\u59b9\u7bc7 MMMU \u4e00\u6837\u3002\u8fd9\u4e9b\u95ee\u9898\u6db5\u76d6 30 \u4e2a\u4e3b\u9898\uff0c\u5305\u542b 39 \u79cd\u9ad8\u5ea6\u5f02\u6784\u7684\u56fe\u50cf\u7c7b\u578b\uff0c\u4f8b\u5982\u56fe\u8868\u3001\u56fe\u8868\u3001\u5730\u56fe\u3001\u8868\u683c\u3001\u4e50\u8c31\u548c\u5316\u5b66\u7ed3\u6784\u3002 CMMMU \u4e13\u6ce8\u4e8e\u5728\u4e2d\u56fd\u80cc\u666f\u4e0b\u5229\u7528\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\u8fdb\u884c\u590d\u6742\u7684\u611f\u77e5\u548c\u63a8\u7406\u3002\u6211\u4eec\u8bc4\u4f30\u4e86 11 \u4e2a\u5f00\u6e90 LLM \u548c\u4e00\u4e2a\u4e13\u6709\u7684 GPT-4V(ision)\u3002\u5373\u4f7fGPT-4V\u4e5f\u53ea\u80fd\u8fbe\u523042%\u7684\u51c6\u786e\u7387\uff0c\u8fd9\u8868\u660e\u8fd8\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002 CMMMU \u5c06\u63a8\u52a8\u793e\u533a\u6784\u5efa\u9762\u5411\u4e13\u5bb6\u4eba\u5de5\u667a\u80fd\u7684\u4e0b\u4e00\u4ee3 LMM\uff0c\u5e76\u901a\u8fc7\u63d0\u4f9b\u591a\u6837\u5316\u7684\u8bed\u8a00\u73af\u5883\u6765\u4fc3\u8fdb LMM \u7684\u6c11\u4e3b\u5316\u3002|[2401.11944v1](http://arxiv.org/pdf/2401.11944v1)|null|\n"}, "\u751f\u6210\u6a21\u578b": {"2401.12175": "|**2024-01-22**|**Single-View 3D Human Digitalization with Large Reconstruction Models**|\u5177\u6709\u5927\u578b\u91cd\u5efa\u6a21\u578b\u7684\u5355\u89c6\u56fe 3D \u4eba\u4f53\u6570\u5b57\u5316|Zhenzhen Weng, Jingyuan Liu, Hao Tan, Zhan Xu, Yang Zhou, Serena Yeung-Levy, Jimei Yang|In this paper, we introduce Human-LRM, a single-stage feed-forward Large Reconstruction Model designed to predict human Neural Radiance Fields (NeRF) from a single image. Our approach demonstrates remarkable adaptability in training using extensive datasets containing 3D scans and multi-view capture. Furthermore, to enhance the model's applicability for in-the-wild scenarios especially with occlusions, we propose a novel strategy that distills multi-view reconstruction into single-view via a conditional triplane diffusion model. This generative extension addresses the inherent variations in human body shapes when observed from a single view, and makes it possible to reconstruct the full body human from an occluded image. Through extensive experiments, we show that Human-LRM surpasses previous methods by a significant margin on several benchmarks.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Human-LRM\uff0c\u8fd9\u662f\u4e00\u79cd\u5355\u7ea7\u524d\u9988\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff0c\u65e8\u5728\u4ece\u5355\u4e2a\u56fe\u50cf\u9884\u6d4b\u4eba\u7c7b\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4f7f\u7528\u5305\u542b 3D \u626b\u63cf\u548c\u591a\u89c6\u56fe\u6355\u83b7\u7684\u5e7f\u6cdb\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u65f6\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u9002\u5e94\u6027\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u589e\u5f3a\u6a21\u578b\u5728\u91ce\u5916\u573a\u666f\uff08\u5c24\u5176\u662f\u906e\u6321\u60c5\u51b5\u4e0b\uff09\u7684\u9002\u7528\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u6761\u4ef6\u4e09\u5e73\u9762\u6269\u6563\u6a21\u578b\u5c06\u591a\u89c6\u56fe\u91cd\u5efa\u63d0\u70bc\u4e3a\u5355\u89c6\u56fe\u3002\u8fd9\u79cd\u751f\u6210\u6269\u5c55\u89e3\u51b3\u4e86\u4ece\u5355\u4e00\u89c6\u56fe\u89c2\u5bdf\u65f6\u4eba\u4f53\u5f62\u72b6\u7684\u56fa\u6709\u53d8\u5316\uff0c\u5e76\u4e14\u4f7f\u5f97\u4ece\u906e\u6321\u56fe\u50cf\u91cd\u5efa\u4eba\u4f53\u5168\u8eab\u6210\u4e3a\u53ef\u80fd\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8868\u660e Human-LRM \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u7740\u8d85\u8d8a\u4e86\u4ee5\u524d\u7684\u65b9\u6cd5\u3002|[2401.12175v1](http://arxiv.org/pdf/2401.12175v1)|null|\n", "2401.11949": "|**2024-01-22**|**Feature Denoising Diffusion Model for Blind Image Quality Assessment**|\u7528\u4e8e\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u7279\u5f81\u53bb\u566a\u6269\u6563\u6a21\u578b|Xudong Li, Jingyuan Zheng, Runze Hu, Yan Zhang, Ke Li, Yunhang Shen, Xiawu Zheng, Yutao Liu, ShengChuan Zhang, Pingyang Dai, et.al.|Blind Image Quality Assessment (BIQA) aims to evaluate image quality in line with human perception, without reference benchmarks. Currently, deep learning BIQA methods typically depend on using features from high-level tasks for transfer learning. However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features. In this paper, we take an initial step towards exploring the diffusion model for feature denoising in BIQA, namely Perceptual Feature Diffusion for IQA (PFD-IQA), which aims to remove noise from quality-aware features. Specifically, (i) We propose a {Perceptual Prior Discovery and Aggregation module to establish two auxiliary tasks to discover potential low-level features in images that are used to aggregate perceptual text conditions for the diffusion model. (ii) We propose a Perceptual Prior-based Feature Refinement strategy, which matches noisy features to predefined denoising trajectories and then performs exact feature denoising based on text conditions. Extensive experiments on eight standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods, i.e., achieving the PLCC values of 0.935 ( vs. 0.905 in KADID) and 0.922 ( vs. 0.894 in LIVEC).|\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08BIQA\uff09\u65e8\u5728\u8bc4\u4f30\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u65e0\u9700\u53c2\u8003\u57fa\u51c6\u3002\u76ee\u524d\uff0c\u6df1\u5ea6\u5b66\u4e60 BIQA \u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4f7f\u7528\u9ad8\u7ea7\u4efb\u52a1\u7684\u7279\u5f81\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u3002\u7136\u800c\uff0cBIQA \u548c\u8fd9\u4e9b\u9ad8\u7ea7\u4efb\u52a1\u4e4b\u95f4\u7684\u56fa\u6709\u5dee\u5f02\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u7ed9\u8d28\u91cf\u611f\u77e5\u529f\u80fd\u5e26\u6765\u566a\u97f3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8fc8\u51fa\u4e86\u63a2\u7d22 BIQA \u4e2d\u7279\u5f81\u53bb\u566a\u7684\u6269\u6563\u6a21\u578b\u7684\u7b2c\u4e00\u6b65\uff0c\u5373 IQA \u611f\u77e5\u7279\u5f81\u6269\u6563\uff08PFD-IQA\uff09\uff0c\u5176\u76ee\u7684\u662f\u6d88\u9664\u8d28\u91cf\u611f\u77e5\u7279\u5f81\u4e2d\u7684\u566a\u58f0\u3002\u5177\u4f53\u6765\u8bf4\uff0c\uff08i\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u611f\u77e5\u5148\u9a8c\u53d1\u73b0\u548c\u805a\u5408\u6a21\u5757\u6765\u5efa\u7acb\u4e24\u4e2a\u8f85\u52a9\u4efb\u52a1\u6765\u53d1\u73b0\u56fe\u50cf\u4e2d\u6f5c\u5728\u7684\u4f4e\u7ea7\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u7528\u4e8e\u805a\u5408\u6269\u6563\u6a21\u578b\u7684\u611f\u77e5\u6587\u672c\u6761\u4ef6\u3002 \uff08ii\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u5148\u9a8c\u7684\u7279\u5f81\u7ec6\u5316\u7b56\u7565\uff0c\u5b83\u5c06\u566a\u58f0\u7279\u5f81\u4e0e\u9884\u5b9a\u4e49\u7684\u53bb\u566a\u8f68\u8ff9\u76f8\u5339\u914d\uff0c\u7136\u540e\u6839\u636e\u6587\u672c\u6761\u4ef6\u6267\u884c\u7cbe\u786e\u7684\u7279\u5f81\u53bb\u566a\u3002\u5bf9\u516b\u4e2a\u6807\u51c6 BIQA \u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 BIQA \u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5373\u5b9e\u73b0\u4e86 0.935\uff08\u76f8\u5bf9\u4e8e KADID \u4e2d\u7684 0.905\uff09\u548c 0.922\uff08\u76f8\u5bf9\u4e8e LIVEC \u4e2d\u7684 0.894\uff09\u7684 PLCC \u503c\u3002|[2401.11949v1](http://arxiv.org/pdf/2401.11949v1)|null|\n", "2401.11831": "|**2024-01-22**|**A Fair Evaluation of Various Deep Learning-Based Document Image Binarization Approaches**|\u5bf9\u5404\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6587\u6863\u56fe\u50cf\u4e8c\u503c\u5316\u65b9\u6cd5\u7684\u516c\u5e73\u8bc4\u4f30|Richin Sukesh, Mathias Seuret, Anguelos Nicolaou, Martin Mayr, Vincent Christlein|Binarization of document images is an important pre-processing step in the field of document analysis. Traditional image binarization techniques usually rely on histograms or local statistics to identify a valid threshold to differentiate between different aspects of the image. Deep learning techniques are able to generate binarized versions of the images by learning context-dependent features that are less error-prone to degradation typically occurring in document images. In recent years, many deep learning-based methods have been developed for document binarization. But which one to choose? There have been no studies that compare these methods rigorously. Therefore, this work focuses on the evaluation of different deep learning-based methods under the same evaluation protocol. We evaluate them on different Document Image Binarization Contest (DIBCO) datasets and obtain very heterogeneous results. We show that the DE-GAN model was able to perform better compared to other models when evaluated on the DIBCO2013 dataset while DP-LinkNet performed best on the DIBCO2017 dataset. The 2-StageGAN performed best on the DIBCO2018 dataset while SauvolaNet outperformed the others on the DIBCO2019 challenge. Finally, we make the code, all models and evaluation publicly available (https://github.com/RichSu95/Document_Binarization_Collection) to ensure reproducibility and simplify future binarization evaluations.|\u6587\u6863\u56fe\u50cf\u4e8c\u503c\u5316\u662f\u6587\u6863\u5206\u6790\u9886\u57df\u4e2d\u91cd\u8981\u7684\u9884\u5904\u7406\u6b65\u9aa4\u3002\u4f20\u7edf\u7684\u56fe\u50cf\u4e8c\u503c\u5316\u6280\u672f\u901a\u5e38\u4f9d\u8d56\u4e8e\u76f4\u65b9\u56fe\u6216\u5c40\u90e8\u7edf\u8ba1\u6765\u8bc6\u522b\u6709\u6548\u9608\u503c\u6765\u533a\u5206\u56fe\u50cf\u7684\u4e0d\u540c\u65b9\u9762\u3002\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u80fd\u591f\u901a\u8fc7\u5b66\u4e60\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7279\u5f81\u6765\u751f\u6210\u56fe\u50cf\u7684\u4e8c\u503c\u5316\u7248\u672c\uff0c\u8fd9\u4e9b\u7279\u5f81\u4e0d\u592a\u5bb9\u6613\u51fa\u73b0\u6587\u6863\u56fe\u50cf\u4e2d\u901a\u5e38\u53d1\u751f\u7684\u9000\u5316\u3002\u8fd1\u5e74\u6765\uff0c\u5df2\u7ecf\u5f00\u53d1\u4e86\u8bb8\u591a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u7528\u4e8e\u6587\u6863\u4e8c\u503c\u5316\u3002\u4f46\u8be5\u9009\u62e9\u54ea\u4e00\u4e2a\u5462\uff1f\u8fd8\u6ca1\u6709\u7814\u7a76\u4e25\u683c\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u8fd9\u9879\u5de5\u4f5c\u7684\u91cd\u70b9\u662f\u5728\u540c\u4e00\u8bc4\u4f30\u534f\u8bae\u4e0b\u8bc4\u4f30\u4e0d\u540c\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u5728\u4e0d\u540c\u7684\u6587\u6863\u56fe\u50cf\u4e8c\u503c\u5316\u7ade\u8d5b\uff08DIBCO\uff09\u6570\u636e\u96c6\u4e0a\u5bf9\u5b83\u4eec\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u83b7\u5f97\u4e86\u975e\u5e38\u5f02\u6784\u7684\u7ed3\u679c\u3002\u6211\u4eec\u8868\u660e\uff0c\u5728 DIBCO2013 \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u65f6\uff0cDE-GAN \u6a21\u578b\u80fd\u591f\u6bd4\u5176\u4ed6\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u800c DP-LinkNet \u5728 DIBCO2017 \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u597d\u3002 2-StageGAN \u5728 DIBCO2018 \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800c SauvolaNet \u5728 DIBCO2019 \u6311\u6218\u8d5b\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u4ee3\u7801\u3001\u6240\u6709\u6a21\u578b\u548c\u8bc4\u4f30\u516c\u5f00\uff08https://github.com/RichSu95/Document_Binarization_Collection\uff09\uff0c\u4ee5\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u5e76\u7b80\u5316\u672a\u6765\u7684\u4e8c\u503c\u5316\u8bc4\u4f30\u3002|[2401.11831v1](http://arxiv.org/pdf/2401.11831v1)|null|\n", "2401.11708": "|**2024-01-22**|**Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs**|\u638c\u63e1\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\uff1a\u4f7f\u7528\u591a\u6a21\u6001\u6cd5\u5b66\u7855\u58eb\u8fdb\u884c\u91cd\u8ff0\u3001\u89c4\u5212\u548c\u751f\u6210|Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui|Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: https://github.com/YangLing0818/RPG-DiffusionMaster|\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5904\u7406\u6d89\u53ca\u5177\u6709\u591a\u4e2a\u5c5e\u6027\u548c\u5173\u7cfb\u7684\u591a\u4e2a\u5bf9\u8c61\u7684\u590d\u6742\u6587\u672c\u63d0\u793a\u65f6\u7ecf\u5e38\u9762\u4e34\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u514d\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210/\u7f16\u8f91\u6846\u67b6\uff0c\u5373Recaption\u3001Plan\u548cGenerate\uff08RPG\uff09\uff0c\u5229\u7528\u591a\u6a21\u6001LLM\u5f3a\u5927\u7684\u601d\u60f3\u94fe\u63a8\u7406\u80fd\u529b\u6765\u589e\u5f3a\u6587\u672c\u7684\u7ec4\u5408\u6027\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528 MLLM \u4f5c\u4e3a\u5168\u5c40\u89c4\u5212\u5668\uff0c\u5c06\u751f\u6210\u590d\u6742\u56fe\u50cf\u7684\u8fc7\u7a0b\u5206\u89e3\u4e3a\u5b50\u533a\u57df\u5185\u591a\u4e2a\u66f4\u7b80\u5355\u7684\u751f\u6210\u4efb\u52a1\u3002\u6211\u4eec\u63d0\u51fa\u4e92\u8865\u7684\u533a\u57df\u6269\u6563\uff0c\u4ee5\u5b9e\u73b0\u533a\u57df\u6027\u7684\u5408\u6210\u751f\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4ee5\u95ed\u73af\u65b9\u5f0f\u5c06\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u96c6\u6210\u5230\u6240\u63d0\u51fa\u7684 RPG \u4e2d\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 RPG \u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u5305\u62ec DALL-E 3 \u548c SDXL\uff0c\u7279\u522b\u662f\u5728\u591a\u7c7b\u522b\u5bf9\u8c61\u7ec4\u5408\u548c\u6587\u672c\u56fe\u50cf\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684 RPG \u6846\u67b6\u8868\u73b0\u51fa\u4e0e\u5404\u79cd MLLM \u67b6\u6784\uff08\u4f8b\u5982 MiniGPT-4\uff09\u548c\u6269\u6563\u9aa8\u5e72\u7f51\uff08\u4f8b\u5982 ControlNet\uff09\u7684\u5e7f\u6cdb\u517c\u5bb9\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/YangLing0818/RPG-DiffusionMaster|[2401.11708v1](http://arxiv.org/pdf/2401.11708v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.12168": "|**2024-01-22**|**SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities**|SpatialVLM\uff1a\u8d4b\u4e88\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u63a8\u7406\u80fd\u529b|Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia|Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/|\u7406\u89e3\u548c\u63a8\u7406\u7a7a\u95f4\u5173\u7cfb\u662f\u89c6\u89c9\u95ee\u7b54 (VQA) \u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u57fa\u672c\u80fd\u529b\u3002\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u67d0\u4e9b VQA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u7f3a\u4e4f 3D \u7a7a\u95f4\u63a8\u7406\u529f\u80fd\uff0c\u4f8b\u5982\u8bc6\u522b\u7269\u7406\u5bf9\u8c61\u7684\u5b9a\u91cf\u5173\u7cfb\uff08\u4f8b\u5982\u8ddd\u79bb\u6216\u5927\u5c0f\u5dee\u5f02\uff09\u3002\u6211\u4eec\u5047\u8bbe VLM \u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\u662f\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u4e4f 3D \u7a7a\u95f4\u77e5\u8bc6\uff0c\u5e76\u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u7a7a\u95f4\u63a8\u7406\u6570\u636e\u8bad\u7ec3 VLM \u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6765\u4fc3\u8fdb\u8fd9\u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u9996\u5148\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8 3D \u7a7a\u95f4 VQA \u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u53ef\u5728 1000 \u4e07\u5f20\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e0a\u6269\u5c55\u81f3 20 \u4ebf\u4e2a VQA \u793a\u4f8b\u3002\u7136\u540e\uff0c\u6211\u4eec\u7814\u7a76\u8bad\u7ec3\u65b9\u6848\u4e2d\u7684\u5404\u79cd\u56e0\u7d20\uff0c\u5305\u62ec\u6570\u636e\u8d28\u91cf\u3001\u8bad\u7ec3\u7ba1\u9053\u548c VLM \u67b6\u6784\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u662f\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7b2c\u4e00\u4e2a\u4e92\u8054\u7f51\u89c4\u6a21\u7684 3D \u7a7a\u95f4\u63a8\u7406\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5728\u6b64\u7c7b\u6570\u636e\u4e0a\u8bad\u7ec3 VLM\uff0c\u6211\u4eec\u663e\u7740\u589e\u5f3a\u4e86\u5176\u5b9a\u6027\u548c\u5b9a\u91cf\u7a7a\u95f4 VQA \u7684\u80fd\u529b\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8be5 VLM \u7531\u4e8e\u5176\u5b9a\u91cf\u4f30\u8ba1\u80fd\u529b\uff0c\u5728\u601d\u60f3\u94fe\u7a7a\u95f4\u63a8\u7406\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u89e3\u9501\u4e86\u65b0\u9896\u7684\u4e0b\u6e38\u5e94\u7528\u3002\u9879\u76ee\u7f51\u7ad9\uff1ahttps://spatial-vlm.github.io/|[2401.12168v1](http://arxiv.org/pdf/2401.12168v1)|null|\n", "2401.11943": "|**2024-01-22**|**Benchmarking Large Multimodal Models against Common Corruptions**|\u9488\u5bf9\u5e38\u89c1\u8150\u8d25\u5bf9\u5927\u578b\u591a\u6a21\u5f0f\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5|Jiawei Zhang, Tianyu Pang, Chao Du, Yi Ren, Bo Li, Min Lin|This technical report aims to fill a deficiency in the assessment of large multimodal models (LMMs) by specifically examining the self-consistency of their outputs when subjected to common corruptions. We investigate the cross-modal interactions between text, image, and speech, encompassing four essential generation tasks: text-to-image, image-to-text, text-to-speech, and speech-to-text. We create a comprehensive benchmark, named MMCBench, that covers more than 100 popular LMMs (totally over 150 model checkpoints). A thorough evaluation under common corruptions is critical for practical deployment and facilitates a better understanding of the reliability of cutting-edge LMMs. The benchmarking code is available at https://github.com/sail-sg/MMCBench|\u672c\u6280\u672f\u62a5\u544a\u65e8\u5728\u901a\u8fc7\u4e13\u95e8\u68c0\u67e5\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u5728\u906d\u53d7\u5e38\u89c1\u8150\u8d25\u65f6\u8f93\u51fa\u7684\u81ea\u6d3d\u6027\u6765\u586b\u8865\u8bc4\u4f30\u7684\u7f3a\u9677\u3002\u6211\u4eec\u7814\u7a76\u6587\u672c\u3001\u56fe\u50cf\u548c\u8bed\u97f3\u4e4b\u95f4\u7684\u8de8\u6a21\u5f0f\u4ea4\u4e92\uff0c\u5305\u62ec\u56db\u4e2a\u57fa\u672c\u751f\u6210\u4efb\u52a1\uff1a\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u5230\u6587\u672c\u3001\u6587\u672c\u5230\u8bed\u97f3\u548c\u8bed\u97f3\u5230\u6587\u672c\u3002\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a MMCBench \u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6 100 \u591a\u4e2a\u6d41\u884c\u7684 LMM\uff08\u603b\u5171\u8d85\u8fc7 150 \u4e2a\u6a21\u578b\u68c0\u67e5\u70b9\uff09\u3002\u5bf9\u5e38\u89c1\u635f\u574f\u8fdb\u884c\u5f7b\u5e95\u8bc4\u4f30\u5bf9\u4e8e\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u4e86\u89e3\u5c16\u7aef LMM \u7684\u53ef\u9760\u6027\u3002\u57fa\u51c6\u6d4b\u8bd5\u4ee3\u7801\u53ef\u5728 https://github.com/sail-sg/MMCBench \u83b7\u53d6|[2401.11943v1](http://arxiv.org/pdf/2401.11943v1)|null|\n"}, "LLM": {"2401.12208": "|**2024-01-22**|**CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation**|CheXagent\uff1a\u5efa\u7acb\u80f8\u90e8 X \u5c04\u7ebf\u89e3\u8bfb\u7684\u57fa\u7840\u6a21\u578b|Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, et.al.|Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing \\emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present \\emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce \\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at \\url{https://stanford-aimi.github.io/chexagent.html}.|\u80f8\u90e8 X \u5149\u68c0\u67e5 (CXR) \u662f\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u6700\u5e38\u8fdb\u884c\u7684\u5f71\u50cf\u5b66\u68c0\u67e5\u3002\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b (FM) \u5f00\u53d1\u7684\u6700\u65b0\u8fdb\u5c55\u4f7f\u5f97\u6267\u884c\u81ea\u52a8 CXR \u89e3\u91ca\u6210\u4e3a\u53ef\u80fd\uff0c\u8fd9\u53ef\u4ee5\u5e2e\u52a9\u533b\u751f\u8fdb\u884c\u4e34\u5e8a\u51b3\u7b56\u5e76\u6539\u5584\u60a3\u8005\u7684\u6cbb\u7597\u7ed3\u679c\u3002\u7136\u800c\uff0c\u5f00\u53d1\u80fd\u591f\u51c6\u786e\u89e3\u91ca CXR \u7684 FM \u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a (1) \u533b\u5b66\u56fe\u50cf\u9886\u57df\u4e2d\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\u7684\u53ef\u7528\u6027\u6709\u9650\uff0c(2) \u7f3a\u4e4f\u80fd\u591f\u6355\u83b7\u533b\u5b66\u6570\u636e\u590d\u6742\u6027\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u7f16\u7801\u5668\uff0c(3) \u7f3a\u4e4f\u5bf9 FM \u7684 CXR \u89e3\u91ca\u80fd\u529b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u9996\u5148\u5f15\u5165 \\emph{CheXinstruct} \u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531 28 \u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u7ec4\u6210\u7684\u5927\u89c4\u6a21\u6307\u4ee4\u8c03\u6574\u6570\u636e\u96c6\u3002\u7136\u540e\u6211\u4eec\u63d0\u51fa \\emph{CheXagent} - \u4e00\u79cd\u80fd\u591f\u5206\u6790\u548c\u603b\u7ed3 CXR \u7684\u6307\u4ee4\u8c03\u6574 FM\u3002\u4e3a\u4e86\u6784\u5efa CheXagent\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7528\u4e8e\u89e3\u6790\u653e\u5c04\u5b66\u62a5\u544a\u7684\u4e34\u5e8a\u5927\u8bed\u8a00\u6a21\u578b (LLM)\u3001\u4e00\u4e2a\u7528\u4e8e\u8868\u793a CXR \u56fe\u50cf\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u4ee5\u53ca\u4e00\u4e2a\u8fde\u63a5\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u5f0f\u7684\u7f51\u7edc\u3002\u6700\u540e\uff0c\u6211\u4eec\u4ecb\u7ecd \\emph{CheXbench} - \u4e00\u79cd\u65b0\u9896\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u7cfb\u7edf\u5730\u8bc4\u4f30 8 \u4e2a\u4e34\u5e8a\u76f8\u5173 CXR \u89e3\u91ca\u4efb\u52a1\u4e2d\u7684 FM\u3002\u4e94\u4f4d\u653e\u5c04\u4e13\u5bb6\u4e13\u5bb6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9a\u91cf\u8bc4\u4f30\u548c\u5b9a\u6027\u5ba1\u67e5\u8868\u660e\uff0cCheXagent \u5728 CheXbench \u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4f18\u4e8e\u4e4b\u524d\u5f00\u53d1\u7684\u901a\u7528\u548c\u533b\u7597\u9886\u57df FM\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u900f\u660e\u5ea6\uff0c\u6211\u4eec\u5bf9\u6027\u522b\u3001\u79cd\u65cf\u548c\u5e74\u9f84\u56e0\u7d20\u8fdb\u884c\u516c\u5e73\u6027\u8bc4\u4f30\uff0c\u4ee5\u7a81\u51fa\u6f5c\u5728\u7684\u7ee9\u6548\u5dee\u5f02\u3002\u6211\u4eec\u7684\u9879\u76ee\u4f4d\u4e8e \\url{https://stanford-aimi.github.io/chexagent.html}\u3002|[2401.12208v1](http://arxiv.org/pdf/2401.12208v1)|null|\n"}, "Transformer": {"2401.12215": "|**2024-01-22**|**Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models**|\u8d8a\u5c11\u8d8a\u597d\uff1a\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u63a8\u8fdb\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b|Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, Liansheng Wang|Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at https://github.com/RL4M/MED-PEFT.|\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6700\u521d\u662f\u4e3a\u4e86\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u800c\u5f00\u53d1\u7684\uff0c\u6700\u8fd1\u5df2\u6210\u4e3a\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u6267\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0cPEFT \u5728\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\u4ecd\u4e0d\u6e05\u695a\uff0c\u6709\u5f85\u63a2\u7d22\u3002\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u6211\u4eec\u5bf9\u5c06 PEFT \u5e94\u7528\u4e8e\u80f8\u90e8\u653e\u5c04\u7ebf\u6444\u5f71\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u5b9e\u8bc1\u7814\u7a76\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86 LoRA\uff08\u4e00\u79cd\u4ee3\u8868\u6027\u7684 PEFT \u65b9\u6cd5\uff09\uff0c\u5e76\u5c06\u5176\u4e0e\u4e09\u4e2a\u6210\u719f\u7684\u80f8\u90e8 X \u5149\u6570\u636e\u96c6\u4e0a\u7684\u4e24\u4e2a\u81ea\u76d1\u7763 X \u7ebf\u6444\u5f71\u57fa\u7840\u6a21\u578b\u7684\u5168\u53c2\u6570\u5fae\u8c03 (FFT) \u8fdb\u884c\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5c11\u4e8e 1% \u7684\u53ef\u8c03\u53c2\u6570\uff0cLoRA \u5728 18 \u9879\u8fc1\u79fb\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684 13 \u9879\u4e2d\u6700\u591a\u4f18\u4e8e FFT 2.9%\u3002\u5c06 LoRA \u4e0e\u57fa\u7840\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u6211\u4eec\u5728\u4e00\u7cfb\u5217\u6570\u636e\u9ad8\u6548\u7684\u5b66\u4e60\u4efb\u52a1\u4e0a\u5efa\u7acb\u4e86\u6700\u5148\u8fdb\u7684\u6280\u672f\uff0c\u4f8b\u5982\u5728 NIH ChestX-ray14 \u4e0a\u4f7f\u7528 1% \u7684\u6807\u8bb0\u6570\u636e\uff0cAUROC \u5f97\u5206\u4e3a 80.6%\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u7814\u7a76\u80fd\u591f\u5f15\u8d77\u793e\u533a\u5bf9\u4f7f\u7528 PEFT \u8fdb\u884c\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60\u7684\u66f4\u591a\u5173\u6ce8\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 https://github.com/RL4M/MED-PEFT \u83b7\u53d6\u3002|[2401.12215v1](http://arxiv.org/pdf/2401.12215v1)|null|\n", "2401.11859": "|**2024-01-22**|**LKFormer: Large Kernel Transformer for Infrared Image Super-Resolution**|LKFormer\uff1a\u7528\u4e8e\u7ea2\u5916\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u5927\u578b\u5185\u6838\u53d8\u538b\u5668|Feiwei Qin, Kang Yan, Changmiao Wang, Ruiquan Ge, Yong Peng, Kai Zhang|Given the broad application of infrared technology across diverse fields, there is an increasing emphasis on investigating super-resolution techniques for infrared images within the realm of deep learning. Despite the impressive results of current Transformer-based methods in image super-resolution tasks, their reliance on the self-attentive mechanism intrinsic to the Transformer architecture results in images being treated as one-dimensional sequences, thereby neglecting their inherent two-dimensional structure. Moreover, infrared images exhibit a uniform pixel distribution and a limited gradient range, posing challenges for the model to capture effective feature information. Consequently, we suggest a potent Transformer model, termed Large Kernel Transformer (LKFormer), to address this issue. Specifically, we have designed a Large Kernel Residual Depth-wise Convolutional Attention (LKRDA) module with linear complexity. This mainly employs depth-wise convolution with large kernels to execute non-local feature modeling, thereby substituting the standard self-attentive layer. Additionally, we have devised a novel feed-forward network structure called Gated-Pixel Feed-Forward Network (GPFN) to augment the LKFormer's capacity to manage the information flow within the network. Comprehensive experimental results reveal that our method surpasses the most advanced techniques available, using fewer parameters and yielding considerably superior performance.|\u9274\u4e8e\u7ea2\u5916\u6280\u672f\u5728\u5404\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u91cd\u89c6\u5728\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7814\u7a76\u7ea2\u5916\u56fe\u50cf\u7684\u8d85\u5206\u8fa8\u7387\u6280\u672f\u3002\u5c3d\u7ba1\u5f53\u524d\u57fa\u4e8e Transformer \u7684\u65b9\u6cd5\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u5b83\u4eec\u5bf9 Transformer \u67b6\u6784\u56fa\u6709\u7684\u81ea\u6211\u5173\u6ce8\u673a\u5236\u7684\u4f9d\u8d56\u5bfc\u81f4\u56fe\u50cf\u88ab\u89c6\u4e3a\u4e00\u7ef4\u5e8f\u5217\uff0c\u4ece\u800c\u5ffd\u7565\u4e86\u5176\u56fa\u6709\u7684\u4e8c\u7ef4\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u7ea2\u5916\u56fe\u50cf\u5448\u73b0\u51fa\u5747\u5300\u7684\u50cf\u7d20\u5206\u5e03\u548c\u6709\u9650\u7684\u68af\u5ea6\u8303\u56f4\uff0c\u8fd9\u7ed9\u6a21\u578b\u6355\u83b7\u6709\u6548\u7684\u7279\u5f81\u4fe1\u606f\u5e26\u6765\u4e86\u6311\u6218\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u4e00\u79cd\u6709\u6548\u7684 Transformer \u6a21\u578b\uff0c\u79f0\u4e3a Large Kernel Transformer (LKFormer) \u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u5927\u5185\u6838\u6b8b\u5dee\u6df1\u5ea6\u5377\u79ef\u6ce8\u610f\u529b\uff08LKRDA\uff09\u6a21\u5757\u3002\u8fd9\u4e3b\u8981\u91c7\u7528\u5177\u6709\u5927\u5185\u6838\u7684\u6df1\u5ea6\u5377\u79ef\u6765\u6267\u884c\u975e\u5c40\u90e8\u7279\u5f81\u5efa\u6a21\uff0c\u4ece\u800c\u53d6\u4ee3\u6807\u51c6\u7684\u81ea\u6ce8\u610f\u529b\u5c42\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u524d\u9988\u7f51\u7edc\u7ed3\u6784\uff0c\u79f0\u4e3a\u95e8\u63a7\u50cf\u7d20\u524d\u9988\u7f51\u7edc (GPFN)\uff0c\u4ee5\u589e\u5f3a LKFormer \u7ba1\u7406\u7f51\u7edc\u5185\u4fe1\u606f\u6d41\u7684\u80fd\u529b\u3002\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6280\u672f\uff0c\u4f7f\u7528\u66f4\u5c11\u7684\u53c2\u6570\u5e76\u4ea7\u751f\u4e86\u76f8\u5f53\u4f18\u8d8a\u7684\u6027\u80fd\u3002|[2401.11859v1](http://arxiv.org/pdf/2401.11859v1)|null|\n", "2401.11711": "|**2024-01-22**|**HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs**|HG3-NeRF\uff1a\u7528\u4e8e\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u7684\u5206\u5c42\u51e0\u4f55\u3001\u8bed\u4e49\u548c\u5149\u5ea6\u5f15\u5bfc\u795e\u7ecf\u8f90\u5c04\u573a|Zelin Gao, Weichen Dai, Yu Zhang|Neural Radiance Fields (NeRF) have garnered considerable attention as a paradigm for novel view synthesis by learning scene representations from discrete observations. Nevertheless, NeRF exhibit pronounced performance degradation when confronted with sparse view inputs, consequently curtailing its further applicability. In this work, we introduce Hierarchical Geometric, Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can address the aforementioned limitation and enhance consistency of geometry, semantic content, and appearance across different views. We propose Hierarchical Geometric Guidance (HGG) to incorporate the attachment of Structure from Motion (SfM), namely sparse depth prior, into the scene representations. Different from direct depth supervision, HGG samples volume points from local-to-global geometric regions, mitigating the misalignment caused by inherent bias in the depth prior. Furthermore, we draw inspiration from notable variations in semantic consistency observed across images of different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn the coarse-to-fine semantic content, which corresponds to the coarse-to-fine scene representations. Experimental results demonstrate that HG3-NeRF can outperform other state-of-the-art methods on different standard benchmarks and achieve high-fidelity synthesis results for sparse view inputs.|\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u4f5c\u4e3a\u901a\u8fc7\u4ece\u79bb\u6563\u89c2\u5bdf\u4e2d\u5b66\u4e60\u573a\u666f\u8868\u793a\u6765\u5408\u6210\u65b0\u9896\u89c6\u56fe\u7684\u8303\u4f8b\uff0c\u5df2\u7ecf\u5f15\u8d77\u4e86\u76f8\u5f53\u5927\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5f53\u9762\u5bf9\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u65f6\uff0cNeRF \u8868\u73b0\u51fa\u660e\u663e\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u8fdb\u4e00\u6b65\u7684\u9002\u7528\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5206\u5c42\u51e0\u4f55\u3001\u8bed\u4e49\u548c\u5149\u5ea6\u5f15\u5bfc NeRF\uff08HG3-NeRF\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u89e3\u51b3\u4e0a\u8ff0\u9650\u5236\u5e76\u589e\u5f3a\u4e0d\u540c\u89c6\u56fe\u4e2d\u51e0\u4f55\u3001\u8bed\u4e49\u5185\u5bb9\u548c\u5916\u89c2\u7684\u4e00\u81f4\u6027\u3002\u6211\u4eec\u63d0\u51fa\u5206\u5c42\u51e0\u4f55\u5f15\u5bfc\uff08HGG\uff09\uff0c\u5c06\u8fd0\u52a8\u7ed3\u6784\uff08SfM\uff09\u7684\u9644\u4ef6\uff08\u5373\u7a00\u758f\u6df1\u5ea6\u5148\u9a8c\uff09\u5408\u5e76\u5230\u573a\u666f\u8868\u793a\u4e2d\u3002\u4e0e\u76f4\u63a5\u6df1\u5ea6\u76d1\u7763\u4e0d\u540c\uff0cHGG \u4ece\u5c40\u90e8\u5230\u5168\u5c40\u51e0\u4f55\u533a\u57df\u91c7\u6837\u4f53\u79ef\u70b9\uff0c\u51cf\u8f7b\u4e86\u6df1\u5ea6\u5148\u9a8c\u56fa\u6709\u504f\u5dee\u9020\u6210\u7684\u9519\u4f4d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4ece\u4e0d\u540c\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u89c2\u5bdf\u5230\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u663e\u7740\u53d8\u5316\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u5e76\u63d0\u51fa\u5c42\u6b21\u8bed\u4e49\u6307\u5bfc\uff08HSG\uff09\u6765\u5b66\u4e60\u4ece\u7c97\u5230\u7ec6\u7684\u8bed\u4e49\u5185\u5bb9\uff0c\u8fd9\u5bf9\u5e94\u4e8e\u4ece\u7c97\u5230\u7ec6\u7684\u573a\u666f\u8868\u793a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHG3-NeRF \u5728\u4e0d\u540c\u7684\u6807\u51c6\u57fa\u51c6\u4e0a\u53ef\u4ee5\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u9488\u5bf9\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5408\u6210\u7ed3\u679c\u3002|[2401.11711v1](http://arxiv.org/pdf/2401.11711v1)|null|\n", "2401.11687": "|**2024-01-22**|**TIM: An Efficient Temporal Interaction Module for Spiking Transformer**|TIM\uff1a\u5c16\u5cf0\u53d8\u538b\u5668\u7684\u9ad8\u6548\u65f6\u95f4\u4ea4\u4e92\u6a21\u5757|Sicheng Shen, Dongcheng Zhao, Guobin Shen, Yi Zeng|Spiking Neural Networks (SNNs), as the third generation of neural networks, have gained prominence for their biological plausibility and computational efficiency, especially in processing diverse datasets. The integration of attention mechanisms, inspired by advancements in neural network architectures, has led to the development of Spiking Transformers. These have shown promise in enhancing SNNs' capabilities, particularly in the realms of both static and neuromorphic datasets. Despite their progress, a discernible gap exists in these systems, specifically in the Spiking Self Attention (SSA) mechanism's effectiveness in leveraging the temporal processing potential of SNNs. To address this, we introduce the Temporal Interaction Module (TIM), a novel, convolution-based enhancement designed to augment the temporal data processing abilities within SNN architectures. TIM's integration into existing SNN frameworks is seamless and efficient, requiring minimal additional parameters while significantly boosting their temporal information handling capabilities. Through rigorous experimentation, TIM has demonstrated its effectiveness in exploiting temporal information, leading to state-of-the-art performance across various neuromorphic datasets.|\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u4f5c\u4e3a\u7b2c\u4e09\u4ee3\u795e\u7ecf\u7f51\u7edc\uff0c\u56e0\u5176\u751f\u7269\u5b66\u5408\u7406\u6027\u548c\u8ba1\u7b97\u6548\u7387\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u540c\u7684\u6570\u636e\u96c6\u65b9\u9762\u3002\u53d7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u6b65\u7684\u542f\u53d1\uff0c\u6ce8\u610f\u529b\u673a\u5236\u7684\u6574\u5408\u5bfc\u81f4\u4e86\u5c16\u5cf0\u53d8\u538b\u5668\u7684\u53d1\u5c55\u3002\u8fd9\u4e9b\u5728\u589e\u5f3a SNN \u529f\u80fd\u65b9\u9762\u663e\u793a\u51fa\u4e86\u5e0c\u671b\uff0c\u7279\u522b\u662f\u5728\u9759\u6001\u548c\u795e\u7ecf\u5f62\u6001\u6570\u636e\u96c6\u9886\u57df\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u8fd9\u4e9b\u7cfb\u7edf\u4e2d\u4ecd\u5b58\u5728\u660e\u663e\u7684\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5c16\u5cf0\u81ea\u6ce8\u610f\u529b (SSA) \u673a\u5236\u5728\u5229\u7528 SNN \u65f6\u95f4\u5904\u7406\u6f5c\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65f6\u6001\u4ea4\u4e92\u6a21\u5757 (TIM)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5377\u79ef\u7684\u589e\u5f3a\u529f\u80fd\uff0c\u65e8\u5728\u589e\u5f3a SNN \u67b6\u6784\u4e2d\u7684\u65f6\u6001\u6570\u636e\u5904\u7406\u80fd\u529b\u3002 TIM \u4e0e\u73b0\u6709 SNN \u6846\u67b6\u7684\u96c6\u6210\u662f\u65e0\u7f1d\u4e14\u9ad8\u6548\u7684\uff0c\u9700\u8981\u6700\u5c11\u7684\u989d\u5916\u53c2\u6570\uff0c\u540c\u65f6\u663e\u7740\u589e\u5f3a\u5176\u65f6\u6001\u4fe1\u606f\u5904\u7406\u80fd\u529b\u3002\u901a\u8fc7\u4e25\u683c\u7684\u5b9e\u9a8c\uff0cTIM \u8bc1\u660e\u4e86\u5176\u5728\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4ece\u800c\u5728\u5404\u79cd\u795e\u7ecf\u5f62\u6001\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2401.11687v1](http://arxiv.org/pdf/2401.11687v1)|null|\n", "2401.11673": "|**2024-01-22**|**MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo**|MVSFormer++\uff1a\u63ed\u793a Transformer \u591a\u89c6\u56fe\u7acb\u4f53\u7ec6\u8282\u4e2d\u7684\u9b54\u9b3c|Chenjie Cao, Xinlin Ren, Yanwei Fu|Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.|\u57fa\u4e8e\u5b66\u4e60\u7684\u591a\u89c6\u56fe\u7acb\u4f53\uff08MVS\uff09\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\u7a81\u51fa\u5730\u7a81\u51fa\u4e86\u5177\u6709\u6ce8\u610f\u673a\u5236\u7684\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5c1a\u672a\u5f7b\u5e95\u7814\u7a76\u53d8\u538b\u5668\u5bf9\u4e0d\u540c MVS \u6a21\u5757\u7684\u6df1\u8fdc\u5f71\u54cd\uff0c\u5bfc\u81f4\u6df1\u5ea6\u4f30\u8ba1\u80fd\u529b\u6709\u9650\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 MVSFormer++\uff0c\u8fd9\u200b\u200b\u662f\u4e00\u79cd\u8c28\u614e\u5730\u6700\u5927\u5316\u6ce8\u610f\u529b\u56fa\u6709\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a MVS \u7ba1\u9053\u7684\u5404\u4e2a\u7ec4\u4ef6\u3002\u5f62\u5f0f\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6d89\u53ca\u5c06\u4ea4\u53c9\u89c6\u56fe\u4fe1\u606f\u6ce8\u5165\u9884\u5148\u8bad\u7ec3\u7684 DINOv2 \u6a21\u578b\u4e2d\uff0c\u4ee5\u4fc3\u8fdb MVS \u5b66\u4e60\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5bf9\u7279\u5f81\u7f16\u7801\u5668\u548c\u6210\u672c\u91cf\u6b63\u5219\u5316\u91c7\u7528\u4e0d\u540c\u7684\u6ce8\u610f\u673a\u5236\uff0c\u5206\u522b\u5173\u6ce8\u7279\u5f81\u548c\u7a7a\u95f4\u805a\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u4e00\u4e9b\u8bbe\u8ba1\u7ec6\u8282\u4f1a\u6781\u5927\u5730\u5f71\u54cd MVS \u4e2d Transformer \u6a21\u5757\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5f52\u4e00\u5316 3D \u4f4d\u7f6e\u7f16\u7801\u3001\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u7f29\u653e\u548c\u5c42\u5f52\u4e00\u5316\u7684\u4f4d\u7f6e\u3002\u5728DTU\u3001Tanks-and-Temples\u3001BlishedMVS\u548cETH3D\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cMVSFormer++ \u5728\u5177\u6709\u6311\u6218\u6027\u7684 DTU \u548c Tanks-and-Temples \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2401.11673v1](http://arxiv.org/pdf/2401.11673v1)|null|\n", "2401.11652": "|**2024-01-22**|**OnDev-LCT: On-Device Lightweight Convolutional Transformers towards federated learning**|OnDev-LCT\uff1a\u9762\u5411\u8054\u90a6\u5b66\u4e60\u7684\u8bbe\u5907\u4e0a\u8f7b\u91cf\u7ea7\u5377\u79ef\u53d8\u538b\u5668|Chu Myaet Thwal, Minh N. H. Nguyen, Ye Lin Tun, Seong Tae Kim, My T. Thai, Choong Seon Hong|Federated learning (FL) has emerged as a promising approach to collaboratively train machine learning models across multiple edge devices while preserving privacy. The success of FL hinges on the efficiency of participating models and their ability to handle the unique challenges of distributed learning. While several variants of Vision Transformer (ViT) have shown great potential as alternatives to modern convolutional neural networks (CNNs) for centralized training, the unprecedented size and higher computational demands hinder their deployment on resource-constrained edge devices, challenging their widespread application in FL. Since client devices in FL typically have limited computing resources and communication bandwidth, models intended for such devices must strike a balance between model size, computational efficiency, and the ability to adapt to the diverse and non-IID data distributions encountered in FL. To address these challenges, we propose OnDev-LCT: Lightweight Convolutional Transformers for On-Device vision tasks with limited training data and resources. Our models incorporate image-specific inductive biases through the LCT tokenizer by leveraging efficient depthwise separable convolutions in residual linear bottleneck blocks to extract local features, while the multi-head self-attention (MHSA) mechanism in the LCT encoder implicitly facilitates capturing global representations of images. Extensive experiments on benchmark image datasets indicate that our models outperform existing lightweight vision models while having fewer parameters and lower computational demands, making them suitable for FL scenarios with data heterogeneity and communication bottlenecks.|\u8054\u90a6\u5b66\u4e60 (FL) \u5df2\u6210\u4e3a\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u8de8\u591a\u4e2a\u8fb9\u7f18\u8bbe\u5907\u534f\u4f5c\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3002 FL \u7684\u6210\u529f\u53d6\u51b3\u4e8e\u53c2\u4e0e\u6a21\u578b\u7684\u6548\u7387\u53ca\u5176\u5904\u7406\u5206\u5e03\u5f0f\u5b66\u4e60\u72ec\u7279\u6311\u6218\u7684\u80fd\u529b\u3002\u867d\u7136 Vision Transformer (ViT) \u7684\u51e0\u79cd\u53d8\u4f53\u663e\u793a\u51fa\u4f5c\u4e3a\u73b0\u4ee3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u96c6\u4e2d\u8bad\u7ec3\u66ff\u4ee3\u54c1\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u524d\u6240\u672a\u6709\u7684\u89c4\u6a21\u548c\u66f4\u9ad8\u7684\u8ba1\u7b97\u9700\u6c42\u963b\u788d\u4e86\u5b83\u4eec\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\uff0c\u4ece\u800c\u6311\u6218\u4e86\u5b83\u4eec\u5728 FL \u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u7531\u4e8e FL \u4e2d\u7684\u5ba2\u6237\u7aef\u8bbe\u5907\u901a\u5e38\u5177\u6709\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u901a\u4fe1\u5e26\u5bbd\uff0c\u56e0\u6b64\u7528\u4e8e\u6b64\u7c7b\u8bbe\u5907\u7684\u6a21\u578b\u5fc5\u987b\u5728\u6a21\u578b\u5927\u5c0f\u3001\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u9002\u5e94 FL \u4e2d\u9047\u5230\u7684\u591a\u6837\u5316\u548c\u975e IID \u6570\u636e\u5206\u5e03\u7684\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 OnDev-LCT\uff1a\u7528\u4e8e\u8bad\u7ec3\u6570\u636e\u548c\u8d44\u6e90\u6709\u9650\u7684\u8bbe\u5907\u4e0a\u89c6\u89c9\u4efb\u52a1\u7684\u8f7b\u91cf\u7ea7\u5377\u79ef\u53d8\u538b\u5668\u3002\u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7 LCT \u6807\u8bb0\u5668\u7ed3\u5408\u56fe\u50cf\u7279\u5b9a\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u5229\u7528\u6b8b\u4f59\u7ebf\u6027\u74f6\u9888\u5757\u4e2d\u7684\u9ad8\u6548\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u6765\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u800c LCT \u7f16\u7801\u5668\u4e2d\u7684\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08MHSA\uff09\u673a\u5236\u9690\u5f0f\u5730\u6709\u52a9\u4e8e\u6355\u83b7\u5168\u5c40\u8868\u793a\u56fe\u7247\u3002\u5bf9\u57fa\u51c6\u56fe\u50cf\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u7684\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b\uff0c\u540c\u65f6\u5177\u6709\u8f83\u5c11\u7684\u53c2\u6570\u548c\u8f83\u4f4e\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u4f7f\u5176\u9002\u5408\u5177\u6709\u6570\u636e\u5f02\u6784\u6027\u548c\u901a\u4fe1\u74f6\u9888\u7684 FL \u573a\u666f\u3002|[2401.11652v1](http://arxiv.org/pdf/2401.11652v1)|null|\n"}, "Nerf": {"2401.11985": "|**2024-01-22**|**Scaling Face Interaction Graph Networks to Real World Scenes**|\u5c06\u4eba\u8138\u4ea4\u4e92\u56fe\u7f51\u7edc\u6269\u5c55\u5230\u73b0\u5b9e\u4e16\u754c\u573a\u666f|Tatiana Lopez-Guevara, Yulia Rubanova, William F. Whitney, Tobias Pfaff, Kimberly Stachenfeld, Kelsey R. Allen|Accurately simulating real world object dynamics is essential for various applications such as robotics, engineering, graphics, and design. To better capture complex real dynamics such as contact and friction, learned simulators based on graph networks have recently shown great promise. However, applying these learned simulators to real scenes comes with two major challenges: first, scaling learned simulators to handle the complexity of real world scenes which can involve hundreds of objects each with complicated 3D shapes, and second, handling inputs from perception rather than 3D state information. Here we introduce a method which substantially reduces the memory required to run graph-based learned simulators. Based on this memory-efficient simulation model, we then present a perceptual interface in the form of editable NeRFs which can convert real-world scenes into a structured representation that can be processed by graph network simulator. We show that our method uses substantially less memory than previous graph-based simulators while retaining their accuracy, and that the simulators learned in synthetic environments can be applied to real world scenes captured from multiple camera angles. This paves the way for expanding the application of learned simulators to settings where only perceptual information is available at inference time.|\u51c6\u786e\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u7684\u7269\u4f53\u52a8\u529b\u5b66\u5bf9\u4e8e\u673a\u5668\u4eba\u3001\u5de5\u7a0b\u3001\u56fe\u5f62\u548c\u8bbe\u8ba1\u7b49\u5404\u79cd\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u6355\u6349\u63a5\u89e6\u548c\u6469\u64e6\u7b49\u590d\u6742\u7684\u771f\u5b9e\u52a8\u6001\uff0c\u57fa\u4e8e\u56fe\u7f51\u7edc\u7684\u5b66\u4e60\u6a21\u62df\u5668\u6700\u8fd1\u663e\u793a\u51fa\u4e86\u5de8\u5927\u7684\u524d\u666f\u3002\u7136\u800c\uff0c\u5c06\u8fd9\u4e9b\u5b66\u4e60\u7684\u6a21\u62df\u5668\u5e94\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u4f1a\u5e26\u6765\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u9996\u5148\uff0c\u6269\u5c55\u5b66\u4e60\u7684\u6a21\u62df\u5668\u4ee5\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u7684\u590d\u6742\u6027\uff0c\u5176\u4e2d\u53ef\u80fd\u6d89\u53ca\u6570\u767e\u4e2a\u5bf9\u8c61\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u90fd\u5177\u6709\u590d\u6742\u7684 3D \u5f62\u72b6\uff1b\u5176\u6b21\uff0c\u5904\u7406\u6765\u81ea\u611f\u77e5\u800c\u4e0d\u662f 3D \u7684\u8f93\u5165\u72b6\u6001\u4fe1\u606f\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e00\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5927\u5927\u51cf\u5c11\u8fd0\u884c\u57fa\u4e8e\u56fe\u7684\u5b66\u200b\u200b\u4e60\u6a21\u62df\u5668\u6240\u9700\u7684\u5185\u5b58\u3002\u57fa\u4e8e\u8fd9\u79cd\u8282\u7701\u5185\u5b58\u7684\u6a21\u62df\u6a21\u578b\uff0c\u6211\u4eec\u4ee5\u53ef\u7f16\u8f91 NeRF \u7684\u5f62\u5f0f\u5448\u73b0\u4e00\u4e2a\u611f\u77e5\u754c\u9762\uff0c\u5b83\u53ef\u4ee5\u5c06\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u8f6c\u6362\u4e3a\u53ef\u7531\u56fe\u7f51\u7edc\u6a21\u62df\u5668\u5904\u7406\u7684\u7ed3\u6784\u5316\u8868\u793a\u3002\u6211\u4eec\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u7684\u5185\u5b58\u6bd4\u4ee5\u524d\u57fa\u4e8e\u56fe\u5f62\u7684\u6a21\u62df\u5668\u8981\u5c11\u5f97\u591a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u5408\u6210\u73af\u5883\u4e2d\u5b66\u4e60\u7684\u6a21\u62df\u5668\u53ef\u4ee5\u5e94\u7528\u4e8e\u4ece\u591a\u4e2a\u6444\u50cf\u673a\u89d2\u5ea6\u6355\u83b7\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u3002\u8fd9\u4e3a\u5c06\u5b66\u4e60\u6a21\u62df\u5668\u7684\u5e94\u7528\u6269\u5c55\u5230\u63a8\u7406\u65f6\u4ec5\u63d0\u4f9b\u611f\u77e5\u4fe1\u606f\u7684\u8bbe\u7f6e\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2401.11985v1](http://arxiv.org/pdf/2401.11985v1)|null|\n"}, "3DGS": {}, "3D/CG": {"2401.12001": "|**2024-01-22**|**Modeling Stereo-Confidence Out of the End-to-End Stereo-Matching Network via Disparity Plane Sweep**|\u901a\u8fc7\u89c6\u5dee\u5e73\u9762\u626b\u63cf\u5bf9\u7aef\u5230\u7aef\u7acb\u4f53\u5339\u914d\u7f51\u7edc\u7684\u7acb\u4f53\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u5efa\u6a21|Jae Young Lee, Woonghyun Ka, Jaehyun Choi, Junmo Kim|We propose a novel stereo-confidence that can be measured externally to various stereo-matching networks, offering an alternative input modality choice of the cost volume for learning-based approaches, especially in safety-critical systems. Grounded in the foundational concepts of disparity definition and the disparity plane sweep, the proposed stereo-confidence method is built upon the idea that any shift in a stereo-image pair should be updated in a corresponding amount shift in the disparity map. Based on this idea, the proposed stereo-confidence method can be summarized in three folds. 1) Using the disparity plane sweep, multiple disparity maps can be obtained and treated as a 3-D volume (predicted disparity volume), like the cost volume is constructed. 2) One of these disparity maps serves as an anchor, allowing us to define a desirable (or ideal) disparity profile at every spatial point. 3) By comparing the desirable and predicted disparity profiles, we can quantify the level of matching ambiguity between left and right images for confidence measurement. Extensive experimental results using various stereo-matching networks and datasets demonstrate that the proposed stereo-confidence method not only shows competitive performance on its own but also consistent performance improvements when it is used as an input modality for learning-based stereo-confidence methods.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7acb\u4f53\u7f6e\u4fe1\u5ea6\uff0c\u53ef\u4ee5\u5728\u5404\u79cd\u7acb\u4f53\u5339\u914d\u7f51\u7edc\u7684\u5916\u90e8\u8fdb\u884c\u6d4b\u91cf\uff0c\u4e3a\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff08\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\uff09\u63d0\u4f9b\u6210\u672c\u91cf\u7684\u66ff\u4ee3\u8f93\u5165\u6a21\u5f0f\u9009\u62e9\u3002\u57fa\u4e8e\u89c6\u5dee\u5b9a\u4e49\u548c\u89c6\u5dee\u5e73\u9762\u626b\u63cf\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u6240\u63d0\u51fa\u7684\u7acb\u4f53\u7f6e\u4fe1\u65b9\u6cd5\u5efa\u7acb\u5728\u8fd9\u6837\u7684\u601d\u60f3\u4e4b\u4e0a\uff1a\u7acb\u4f53\u56fe\u50cf\u5bf9\u4e2d\u7684\u4efb\u4f55\u79fb\u4f4d\u90fd\u5e94\u8be5\u4ee5\u89c6\u5dee\u56fe\u4e2d\u76f8\u5e94\u7684\u79fb\u4f4d\u91cf\u8fdb\u884c\u66f4\u65b0\u3002\u57fa\u4e8e\u8fd9\u4e2a\u60f3\u6cd5\uff0c\u6240\u63d0\u51fa\u7684\u7acb\u4f53\u7f6e\u4fe1\u65b9\u6cd5\u53ef\u4ee5\u6982\u62ec\u4e3a\u4e09\u4e2a\u65b9\u9762\u3002 1\uff09\u4f7f\u7528\u89c6\u5dee\u5e73\u9762\u626b\u63cf\uff0c\u53ef\u4ee5\u83b7\u5f97\u591a\u4e2a\u89c6\u5dee\u56fe\u5e76\u5c06\u5176\u89c6\u4e3a3D\u4f53\u79ef\uff08\u9884\u6d4b\u89c6\u5dee\u4f53\u79ef\uff09\uff0c\u5c31\u50cf\u6784\u9020\u6210\u672c\u4f53\u79ef\u4e00\u6837\u3002 2\uff09\u8fd9\u4e9b\u89c6\u5dee\u56fe\u4e4b\u4e00\u5145\u5f53\u951a\u70b9\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u5728\u6bcf\u4e2a\u7a7a\u95f4\u70b9\u5b9a\u4e49\u7406\u60f3\u7684\uff08\u6216\u7406\u60f3\u7684\uff09\u89c6\u5dee\u8f6e\u5ed3\u3002 3\uff09\u901a\u8fc7\u6bd4\u8f83\u671f\u671b\u7684\u548c\u9884\u6d4b\u7684\u89c6\u5dee\u5206\u5e03\uff0c\u6211\u4eec\u53ef\u4ee5\u91cf\u5316\u5de6\u53f3\u56fe\u50cf\u4e4b\u95f4\u7684\u5339\u914d\u6a21\u7cca\u5ea6\u6c34\u5e73\u4ee5\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u3002\u4f7f\u7528\u5404\u79cd\u7acb\u4f53\u5339\u914d\u7f51\u7edc\u548c\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7acb\u4f53\u7f6e\u4fe1\u65b9\u6cd5\u4e0d\u4ec5\u672c\u8eab\u5177\u6709\u7ade\u4e89\u6027\u80fd\uff0c\u800c\u4e14\u5f53\u5176\u7528\u4f5c\u57fa\u4e8e\u5b66\u4e60\u7684\u7acb\u4f53\u7f6e\u4fe1\u65b9\u6cd5\u7684\u8f93\u5165\u6a21\u6001\u65f6\uff0c\u4e5f\u5177\u6709\u4e00\u81f4\u7684\u6027\u80fd\u6539\u8fdb\u3002|[2401.12001v1](http://arxiv.org/pdf/2401.12001v1)|null|\n", "2401.11960": "|**2024-01-22**|**Observation-Guided Meteorological Field Downscaling at Station Scale: A Benchmark and a New Method**|\u89c2\u6d4b\u5f15\u5bfc\u7684\u7ad9\u7ea7\u6c14\u8c61\u573a\u964d\u5c3a\u5ea6\uff1a\u57fa\u51c6\u548c\u65b0\u65b9\u6cd5|Zili Liu, Hao Chen, Lei Bai, Wenyuan Li, Keyan Chen, Zhengyi Wang, Wanli Ouyang, Zhengxia Zou, Zhenwei Shi|Downscaling (DS) of meteorological variables involves obtaining high-resolution states from low-resolution meteorological fields and is an important task in weather forecasting. Previous methods based on deep learning treat downscaling as a super-resolution task in computer vision and utilize high-resolution gridded meteorological fields as supervision to improve resolution at specific grid scales. However, this approach has struggled to align with the continuous distribution characteristics of meteorological fields, leading to an inherent systematic bias between the downscaled results and the actual observations at meteorological stations. In this paper, we extend meteorological downscaling to arbitrary scattered station scales, establish a brand new benchmark and dataset, and retrieve meteorological states at any given station location from a coarse-resolution meteorological field. Inspired by data assimilation techniques, we integrate observational data into the downscaling process, providing multi-scale observational priors. Building on this foundation, we propose a new downscaling model based on hypernetwork architecture, namely HyperDS, which efficiently integrates different observational information into the model training, achieving continuous scale modeling of the meteorological field. Through extensive experiments, our proposed method outperforms other specially designed baseline models on multiple surface variables. Notably, the mean squared error (MSE) for wind speed and surface pressure improved by 67% and 19.5% compared to other methods. We will release the dataset and code subsequently.|\u6c14\u8c61\u53d8\u91cf\u7684\u964d\u5c3a\u5ea6\uff08DS\uff09\u6d89\u53ca\u4ece\u4f4e\u5206\u8fa8\u7387\u6c14\u8c61\u573a\u83b7\u53d6\u9ad8\u5206\u8fa8\u7387\u72b6\u6001\uff0c\u662f\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u4e00\u9879\u91cd\u8981\u4efb\u52a1\u3002\u5148\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5c06\u964d\u5c3a\u5ea6\u89c6\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u6c14\u8c61\u573a\u4f5c\u4e3a\u76d1\u7763\u6765\u63d0\u9ad8\u7279\u5b9a\u7f51\u683c\u5c3a\u5ea6\u4e0b\u7684\u5206\u8fa8\u7387\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u6c14\u8c61\u573a\u7684\u8fde\u7eed\u5206\u5e03\u7279\u5f81\uff0c\u5bfc\u81f4\u964d\u5c3a\u5ea6\u7ed3\u679c\u4e0e\u6c14\u8c61\u7ad9\u7684\u5b9e\u9645\u89c2\u6d4b\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u7684\u7cfb\u7edf\u504f\u5dee\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u6c14\u8c61\u964d\u5c3a\u5ea6\u6269\u5c55\u5230\u4efb\u610f\u5206\u6563\u7684\u7ad9\u70b9\u5c3a\u5ea6\uff0c\u5efa\u7acb\u5168\u65b0\u7684\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff0c\u5e76\u4ece\u7c97\u5206\u8fa8\u7387\u6c14\u8c61\u573a\u4e2d\u68c0\u7d22\u4efb\u610f\u7ed9\u5b9a\u7ad9\u70b9\u4f4d\u7f6e\u7684\u6c14\u8c61\u72b6\u6001\u3002\u53d7\u6570\u636e\u540c\u5316\u6280\u672f\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5c06\u89c2\u6d4b\u6570\u636e\u6574\u5408\u5230\u964d\u5c3a\u5ea6\u8fc7\u7a0b\u4e2d\uff0c\u63d0\u4f9b\u591a\u5c3a\u5ea6\u89c2\u6d4b\u5148\u9a8c\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u67b6\u6784\u7684\u65b0\u578b\u964d\u5c3a\u5ea6\u6a21\u578b\uff0c\u5373HyperDS\uff0c\u5b83\u5c06\u4e0d\u540c\u7684\u89c2\u6d4b\u4fe1\u606f\u6709\u6548\u5730\u6574\u5408\u5230\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u5b9e\u73b0\u4e86\u6c14\u8c61\u9886\u57df\u7684\u8fde\u7eed\u5c3a\u5ea6\u5efa\u6a21\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u8868\u9762\u53d8\u91cf\u4e0a\u4f18\u4e8e\u5176\u4ed6\u4e13\u95e8\u8bbe\u8ba1\u7684\u57fa\u7ebf\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e0e\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u98ce\u901f\u548c\u8868\u9762\u538b\u529b\u7684\u5747\u65b9\u8bef\u5dee (MSE) \u5206\u522b\u63d0\u9ad8\u4e86 67% \u548c 19.5%\u3002\u6211\u4eec\u5c06\u968f\u540e\u53d1\u5e03\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002|[2401.11960v1](http://arxiv.org/pdf/2401.11960v1)|null|\n", "2401.11796": "|**2024-01-22**|**Local Agnostic Video Explanations: a Study on the Applicability of Removal-Based Explanations to Video**|\u5c40\u90e8\u4e0d\u53ef\u77e5\u89c6\u9891\u89e3\u91ca\uff1a\u57fa\u4e8e\u79fb\u9664\u7684\u89e3\u91ca\u5bf9\u89c6\u9891\u7684\u9002\u7528\u6027\u7814\u7a76|F. Xavier Gaya-Morey, Jose M. Buades-Rubio, Cristina Manresa-Yee|Explainable artificial intelligence techniques are becoming increasingly important with the rise of deep learning applications in various domains. These techniques aim to provide a better understanding of complex \"black box\" models and enhance user trust while maintaining high learning performance. While many studies have focused on explaining deep learning models in computer vision for image input, video explanations remain relatively unexplored due to the temporal dimension's complexity. In this paper, we present a unified framework for local agnostic explanations in the video domain. Our contributions include: (1) Extending a fine-grained explanation framework tailored for computer vision data, (2) Adapting six existing explanation techniques to work on video data by incorporating temporal information and enabling local explanations, and (3) Conducting an evaluation and comparison of the adapted explanation methods using different models and datasets. We discuss the possibilities and choices involved in the removal-based explanation process for visual data. The adaptation of six explanation methods for video is explained, with comparisons to existing approaches. We evaluate the performance of the methods using automated metrics and user-based evaluation, showing that 3D RISE, 3D LIME, and 3D Kernel SHAP outperform other methods. By decomposing the explanation process into manageable steps, we facilitate the study of each choice's impact and allow for further refinement of explanation methods to suit specific datasets and models.|\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u5728\u5404\u4e2a\u9886\u57df\u7684\u5174\u8d77\uff0c\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u8fd9\u4e9b\u6280\u672f\u65e8\u5728\u66f4\u597d\u5730\u7406\u89e3\u590d\u6742\u7684\u201c\u9ed1\u5323\u5b50\u201d\u6a21\u578b\u5e76\u589e\u5f3a\u7528\u6237\u4fe1\u4efb\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u5b66\u4e60\u6027\u80fd\u3002\u867d\u7136\u8bb8\u591a\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u89e3\u91ca\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7528\u4e8e\u56fe\u50cf\u8f93\u5165\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4f46\u7531\u4e8e\u65f6\u95f4\u7ef4\u5ea6\u7684\u590d\u6742\u6027\uff0c\u89c6\u9891\u89e3\u91ca\u4ecd\u7136\u76f8\u5bf9\u672a\u7ecf\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u89c6\u9891\u9886\u57df\u5c40\u90e8\u4e0d\u53ef\u77e5\u89e3\u91ca\u7684\u7edf\u4e00\u6846\u67b6\u3002\u6211\u4eec\u7684\u8d21\u732e\u5305\u62ec\uff1a\uff081\uff09\u6269\u5c55\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u91cf\u8eab\u5b9a\u5236\u7684\u7ec6\u7c92\u5ea6\u89e3\u91ca\u6846\u67b6\uff0c\uff082\uff09\u901a\u8fc7\u5408\u5e76\u65f6\u95f4\u4fe1\u606f\u5e76\u542f\u7528\u672c\u5730\u89e3\u91ca\uff0c\u91c7\u7528\u516d\u79cd\u73b0\u6709\u7684\u89e3\u91ca\u6280\u672f\u6765\u5904\u7406\u89c6\u9891\u6570\u636e\uff0c\u4ee5\u53ca\uff083\uff09\u8fdb\u884c\u8bc4\u4f30\u548c\u5206\u6790\u4f7f\u7528\u4e0d\u540c\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u9002\u5e94\u89e3\u91ca\u65b9\u6cd5\u7684\u6bd4\u8f83\u3002\u6211\u4eec\u8ba8\u8bba\u89c6\u89c9\u6570\u636e\u57fa\u4e8e\u79fb\u9664\u7684\u89e3\u91ca\u8fc7\u7a0b\u4e2d\u6d89\u53ca\u7684\u53ef\u80fd\u6027\u548c\u9009\u62e9\u3002\u89e3\u91ca\u4e86\u516d\u79cd\u89c6\u9891\u89e3\u91ca\u65b9\u6cd5\u7684\u9002\u5e94\uff0c\u5e76\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6211\u4eec\u4f7f\u7528\u81ea\u52a8\u5316\u6307\u6807\u548c\u57fa\u4e8e\u7528\u6237\u7684\u8bc4\u4f30\u6765\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660e 3D RISE\u30013D LIME \u548c 3D Kernel SHAP \u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u89e3\u91ca\u8fc7\u7a0b\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u6b65\u9aa4\uff0c\u6211\u4eec\u4fc3\u8fdb\u4e86\u5bf9\u6bcf\u4e2a\u9009\u62e9\u7684\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u5e76\u5141\u8bb8\u8fdb\u4e00\u6b65\u7ec6\u5316\u89e3\u91ca\u65b9\u6cd5\u4ee5\u9002\u5e94\u7279\u5b9a\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002|[2401.11796v1](http://arxiv.org/pdf/2401.11796v1)|null|\n", "2401.11783": "|**2024-01-22**|**Full-Body Motion Reconstruction with Sparse Sensing from Graph Perspective**|\u56fe\u89c6\u89d2\u7684\u7a00\u758f\u611f\u77e5\u5168\u8eab\u8fd0\u52a8\u91cd\u5efa|Feiyu Yao, Zongkai Wu, Li Yi|Estimating 3D full-body pose from sparse sensor data is a pivotal technique employed for the reconstruction of realistic human motions in Augmented Reality and Virtual Reality. However, translating sparse sensor signals into comprehensive human motion remains a challenge since the sparsely distributed sensors in common VR systems fail to capture the motion of full human body. In this paper, we use well-designed Body Pose Graph (BPG) to represent the human body and translate the challenge into a prediction problem of graph missing nodes. Then, we propose a novel full-body motion reconstruction framework based on BPG. To establish BPG, nodes are initially endowed with features extracted from sparse sensor signals. Features from identifiable joint nodes across diverse sensors are amalgamated and processed from both temporal and spatial perspectives. Temporal dynamics are captured using the Temporal Pyramid Structure, while spatial relations in joint movements inform the spatial attributes. The resultant features serve as the foundational elements of the BPG nodes. To further refine the BPG, node features are updated through a graph neural network that incorporates edge reflecting varying joint relations. Our method's effectiveness is evidenced by the attained state-of-the-art performance, particularly in lower body motion, outperforming other baseline methods. Additionally, an ablation study validates the efficacy of each module in our proposed framework.|\u4ece\u7a00\u758f\u4f20\u611f\u5668\u6570\u636e\u4f30\u8ba1 3D \u5168\u8eab\u59ff\u52bf\u662f\u589e\u5f3a\u73b0\u5b9e\u548c\u865a\u62df\u73b0\u5b9e\u4e2d\u91cd\u5efa\u771f\u5b9e\u4eba\u4f53\u8fd0\u52a8\u7684\u5173\u952e\u6280\u672f\u3002\u7136\u800c\uff0c\u5c06\u7a00\u758f\u7684\u4f20\u611f\u5668\u4fe1\u53f7\u8f6c\u6362\u4e3a\u5168\u9762\u7684\u4eba\u4f53\u8fd0\u52a8\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u5e38\u89c1 VR \u7cfb\u7edf\u4e2d\u7a00\u758f\u5206\u5e03\u7684\u4f20\u611f\u5668\u65e0\u6cd5\u6355\u83b7\u5b8c\u6574\u7684\u4eba\u4f53\u8fd0\u52a8\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8eab\u4f53\u59ff\u52bf\u56fe\uff08BPG\uff09\u6765\u8868\u793a\u4eba\u4f53\uff0c\u5e76\u5c06\u6311\u6218\u8f6c\u5316\u4e3a\u56fe\u7f3a\u5931\u8282\u70b9\u7684\u9884\u6d4b\u95ee\u9898\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e BPG \u7684\u65b0\u578b\u5168\u8eab\u8fd0\u52a8\u91cd\u5efa\u6846\u67b6\u3002\u4e3a\u4e86\u5efa\u7acb BPG\uff0c\u8282\u70b9\u6700\u521d\u88ab\u8d4b\u4e88\u4ece\u7a00\u758f\u4f20\u611f\u5668\u4fe1\u53f7\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u3002\u6765\u81ea\u4e0d\u540c\u4f20\u611f\u5668\u7684\u53ef\u8bc6\u522b\u5173\u8282\u8282\u70b9\u7684\u7279\u5f81\u4ece\u65f6\u95f4\u548c\u7a7a\u95f4\u89d2\u5ea6\u8fdb\u884c\u5408\u5e76\u548c\u5904\u7406\u3002\u4f7f\u7528\u65f6\u95f4\u91d1\u5b57\u5854\u7ed3\u6784\u6355\u83b7\u65f6\u95f4\u52a8\u6001\uff0c\u800c\u5173\u8282\u8fd0\u52a8\u7684\u7a7a\u95f4\u5173\u7cfb\u5219\u544a\u77e5\u7a7a\u95f4\u5c5e\u6027\u3002\u7531\u6b64\u4ea7\u751f\u7684\u7279\u5f81\u5145\u5f53 BPG \u8282\u70b9\u7684\u57fa\u672c\u5143\u7d20\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u7ec6\u5316 BPG\uff0c\u8282\u70b9\u7279\u5f81\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u66f4\u65b0\uff0c\u8be5\u7f51\u7edc\u5305\u542b\u53cd\u6620\u4e0d\u540c\u5173\u8282\u5173\u7cfb\u7684\u8fb9\u7f18\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u901a\u8fc7\u6240\u83b7\u5f97\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\u5f97\u5230\u4e86\u8bc1\u660e\uff0c\u7279\u522b\u662f\u5728\u4e0b\u534a\u8eab\u8fd0\u52a8\u65b9\u9762\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u4e2d\u6bcf\u4e2a\u6a21\u5757\u7684\u529f\u6548\u3002|[2401.11783v1](http://arxiv.org/pdf/2401.11783v1)|null|\n", "2401.11650": "|**2024-01-22**|**PointGL: A Simple Global-Local Framework for Efficient Point Cloud Analysis**|PointGL\uff1a\u7528\u4e8e\u9ad8\u6548\u70b9\u4e91\u5206\u6790\u7684\u7b80\u5355\u5168\u5c40\u5c40\u90e8\u6846\u67b6|Jianan Li, Jie Wang, Tingfa Xu|Efficient analysis of point clouds holds paramount significance in real-world 3D applications. Currently, prevailing point-based models adhere to the PointNet++ methodology, which involves embedding and abstracting point features within a sequence of spatially overlapping local point sets, resulting in noticeable computational redundancy. Drawing inspiration from the streamlined paradigm of pixel embedding followed by regional pooling in Convolutional Neural Networks (CNNs), we introduce a novel, uncomplicated yet potent architecture known as PointGL, crafted to facilitate efficient point cloud analysis. PointGL employs a hierarchical process of feature acquisition through two recursive steps. First, the Global Point Embedding leverages straightforward residual Multilayer Perceptrons (MLPs) to effectuate feature embedding for each individual point. Second, the novel Local Graph Pooling technique characterizes point-to-point relationships and abstracts regional representations through succinct local graphs. The harmonious fusion of one-time point embedding and parameter-free graph pooling contributes to PointGL's defining attributes of minimized model complexity and heightened efficiency. Our PointGL attains state-of-the-art accuracy on the ScanObjectNN dataset while exhibiting a runtime that is more than 5 times faster and utilizing only approximately 4% of the FLOPs and 30% of the parameters compared to the recent PointMLP model. The code for PointGL is available at https://github.com/Roywangj/PointGL.|\u70b9\u4e91\u7684\u6709\u6548\u5206\u6790\u5728\u73b0\u5b9e 3D \u5e94\u7528\u4e2d\u5177\u6709\u81f3\u5173\u91cd\u8981\u7684\u610f\u4e49\u3002\u76ee\u524d\uff0c\u6d41\u884c\u7684\u57fa\u4e8e\u70b9\u7684\u6a21\u578b\u9075\u5faa PointNet++ \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6d89\u53ca\u5728\u4e00\u7cfb\u5217\u7a7a\u95f4\u91cd\u53e0\u7684\u5c40\u90e8\u70b9\u96c6\u5185\u5d4c\u5165\u548c\u62bd\u8c61\u70b9\u7279\u5f81\uff0c\u4ece\u800c\u5bfc\u81f4\u660e\u663e\u7684\u8ba1\u7b97\u5197\u4f59\u3002\u53d7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u4e2d\u50cf\u7d20\u5d4c\u5165\u548c\u533a\u57df\u6c60\u5316\u7684\u7b80\u5316\u8303\u4f8b\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u7b80\u5355\u4f46\u6709\u6548\u7684\u67b6\u6784\uff0c\u79f0\u4e3a PointGL\uff0c\u65e8\u5728\u4fc3\u8fdb\u9ad8\u6548\u7684\u70b9\u4e91\u5206\u6790\u3002 PointGL \u901a\u8fc7\u4e24\u4e2a\u9012\u5f52\u6b65\u9aa4\u91c7\u7528\u5c42\u6b21\u5316\u7684\u7279\u5f81\u83b7\u53d6\u8fc7\u7a0b\u3002\u9996\u5148\uff0c\u5168\u5c40\u70b9\u5d4c\u5165\u5229\u7528\u7b80\u5355\u7684\u6b8b\u5dee\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u6765\u5b9e\u73b0\u6bcf\u4e2a\u5355\u72ec\u70b9\u7684\u7279\u5f81\u5d4c\u5165\u3002\u5176\u6b21\uff0c\u65b0\u9896\u7684\u5c40\u90e8\u56fe\u6c60\u5316\u6280\u672f\u63cf\u8ff0\u4e86\u70b9\u5bf9\u70b9\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u7b80\u6d01\u7684\u5c40\u90e8\u56fe\u62bd\u8c61\u51fa\u533a\u57df\u8868\u793a\u3002\u4e00\u6b21\u6027\u70b9\u5d4c\u5165\u548c\u65e0\u53c2\u6570\u56fe\u6c60\u7684\u548c\u8c10\u878d\u5408\u6709\u52a9\u4e8e PointGL \u6700\u5c0f\u5316\u6a21\u578b\u590d\u6742\u6027\u548c\u63d0\u9ad8\u6548\u7387\u7684\u5b9a\u4e49\u5c5e\u6027\u3002\u4e0e\u6700\u65b0\u7684 PointMLP \u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u4eec\u7684 PointGL \u5728 ScanObjectNN \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u8fd0\u884c\u65f6\u95f4\u5feb\u4e86 5 \u500d\u4ee5\u4e0a\uff0c\u5e76\u4e14\u4ec5\u5229\u7528\u4e86\u5927\u7ea6 4% \u7684 FLOP \u548c 30% \u7684\u53c2\u6570\u3002 PointGL \u7684\u4ee3\u7801\u53ef\u5728 https://github.com/Roywangj/PointGL \u83b7\u53d6\u3002|[2401.11650v1](http://arxiv.org/pdf/2401.11650v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "\u5176\u4ed6": {"2401.12033": "|**2024-01-22**|**Momentum-SAM: Sharpness Aware Minimization without Computational Overhead**|Momentum-SAM\uff1a\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\uff0c\u65e0\u9700\u8ba1\u7b97\u5f00\u9500|Marlon Becker, Frederick Altrock, Benjamin Risse|The recently proposed optimization algorithm for deep neural networks Sharpness Aware Minimization (SAM) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss. While significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making SAM unfeasible in case of limited computationally capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose Momentum-SAM (MSAM), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over SGD or Adam. We evaluate MSAM in detail and reveal insights on separable mechanisms of NAG, SAM and MSAM regarding training optimization and generalization. Code is available at https://github.com/MarlonBecker/MSAM.|\u6700\u8fd1\u63d0\u51fa\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\uff08SAM\uff09\u4f18\u5316\u7b97\u6cd5\u5efa\u8bae\u5728\u68af\u5ea6\u8ba1\u7b97\u4e4b\u524d\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u6b65\u9aa4\u6270\u52a8\u53c2\u6570\uff0c\u4ee5\u5f15\u5bfc\u4f18\u5316\u8fdb\u5165\u5e73\u5766\u635f\u5931\u7684\u53c2\u6570\u7a7a\u95f4\u533a\u57df\u3002\u867d\u7136\u53ef\u4ee5\u8bc1\u660e\u663e\u7740\u7684\u6cdb\u5316\u6539\u8fdb\uff0c\u4ece\u800c\u51cf\u5c11\u8fc7\u62df\u5408\uff0c\u4f46\u7531\u4e8e\u989d\u5916\u9700\u8981\u68af\u5ea6\u8ba1\u7b97\uff0c\u8ba1\u7b97\u6210\u672c\u52a0\u500d\uff0c\u4f7f\u5f97 SAM \u5728\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e0d\u53ef\u884c\u3002\u53d7 Nesterov \u52a0\u901f\u68af\u5ea6 (NAG) \u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Momentum-SAM (MSAM)\uff0c\u5b83\u4f1a\u6270\u52a8\u7d2f\u79ef\u52a8\u91cf\u5411\u91cf\u65b9\u5411\u4e0a\u7684\u53c2\u6570\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u9510\u5ea6\uff0c\u800c\u65e0\u9700\u6bd4 SGD \u6216 Adam \u663e\u7740\u7684\u8ba1\u7b97\u5f00\u9500\u6216\u5185\u5b58\u9700\u6c42\u3002\u6211\u4eec\u8be6\u7ec6\u8bc4\u4f30\u4e86 MSAM\uff0c\u5e76\u63ed\u793a\u4e86 NAG\u3001SAM \u548c MSAM \u5728\u8bad\u7ec3\u4f18\u5316\u548c\u6cdb\u5316\u65b9\u9762\u7684\u53ef\u5206\u79bb\u673a\u5236\u7684\u89c1\u89e3\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/MarlonBecker/MSAM \u83b7\u53d6\u3002|[2401.12033v1](http://arxiv.org/pdf/2401.12033v1)|null|\n", "2401.11902": "|**2024-01-22**|**A Training-Free Defense Framework for Robust Learned Image Compression**|\u7528\u4e8e\u9c81\u68d2\u5b66\u4e60\u56fe\u50cf\u538b\u7f29\u7684\u514d\u8bad\u7ec3\u9632\u5fa1\u6846\u67b6|Myungseo Song, Jinyoung Choi, Bohyung Han|We study the robustness of learned image compression models against adversarial attacks and present a training-free defense technique based on simple image transform functions. Recent learned image compression models are vulnerable to adversarial attacks that result in poor compression rate, low reconstruction quality, or weird artifacts. To address the limitations, we propose a simple but effective two-way compression algorithm with random input transforms, which is conveniently applicable to existing image compression models. Unlike the na\\\"ive approaches, our approach preserves the original rate-distortion performance of the models on clean images. Moreover, the proposed algorithm requires no additional training or modification of existing models, making it more practical. We demonstrate the effectiveness of the proposed techniques through extensive experiments under multiple compression models, evaluation metrics, and attack scenarios.|\u6211\u4eec\u7814\u7a76\u4e86\u5b66\u4e60\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u9488\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b80\u5355\u56fe\u50cf\u53d8\u6362\u51fd\u6570\u7684\u514d\u8bad\u7ec3\u9632\u5fa1\u6280\u672f\u3002\u6700\u8fd1\u5b66\u4e60\u7684\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u5f88\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u4ece\u800c\u5bfc\u81f4\u538b\u7f29\u7387\u4f4e\u3001\u91cd\u5efa\u8d28\u91cf\u4f4e\u6216\u5947\u602a\u7684\u4f2a\u5f71\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u5177\u6709\u968f\u673a\u8f93\u5165\u53d8\u6362\u7684\u53cc\u5411\u538b\u7f29\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u53ef\u4ee5\u65b9\u4fbf\u5730\u5e94\u7528\u4e8e\u73b0\u6709\u7684\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u3002\u4e0e\u539f\u59cb\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4fdd\u7559\u4e86\u6a21\u578b\u5728\u5e72\u51c0\u56fe\u50cf\u4e0a\u7684\u539f\u59cb\u7387\u5931\u771f\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u4e0d\u9700\u8981\u5bf9\u73b0\u6709\u6a21\u578b\u8fdb\u884c\u989d\u5916\u7684\u8bad\u7ec3\u6216\u4fee\u6539\uff0c\u4f7f\u5176\u66f4\u52a0\u5b9e\u7528\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u901a\u8fc7\u5728\u591a\u79cd\u538b\u7f29\u6a21\u578b\u3001\u8bc4\u4f30\u6307\u6807\u548c\u653b\u51fb\u573a\u666f\u4e0b\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u63d0\u51fa\u4e86\u6280\u672f\u3002|[2401.11902v1](http://arxiv.org/pdf/2401.11902v1)|null|\n", "2401.11844": "|**2024-01-22**|**Adaptive Fusion of Multi-view Remote Sensing data for Optimal Sub-field Crop Yield Prediction**|\u591a\u89c6\u56fe\u9065\u611f\u6570\u636e\u7684\u81ea\u9002\u5e94\u878d\u5408\u7528\u4e8e\u6700\u4f73\u5b50\u7530\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b|Francisco Mena, Deepak Pathak, Hiba Najjar, Cristhian Sanchez, Patrick Helber, Benjamin Bischke, Peter Habelitz, Miro Miranda, Jayanth Siddamsetty, Marlon Nuske, et.al.|Accurate crop yield prediction is of utmost importance for informed decision-making in agriculture, aiding farmers, and industry stakeholders. However, this task is complex and depends on multiple factors, such as environmental conditions, soil properties, and management practices. Combining heterogeneous data views poses a fusion challenge, like identifying the view-specific contribution to the predictive task. We present a novel multi-view learning approach to predict crop yield for different crops (soybean, wheat, rapeseed) and regions (Argentina, Uruguay, and Germany). Our multi-view input data includes multi-spectral optical images from Sentinel-2 satellites and weather data as dynamic features during the crop growing season, complemented by static features like soil properties and topographic information. To effectively fuse the data, we introduce a Multi-view Gated Fusion (MVGF) model, comprising dedicated view-encoders and a Gated Unit (GU) module. The view-encoders handle the heterogeneity of data sources with varying temporal resolutions by learning a view-specific representation. These representations are adaptively fused via a weighted sum. The fusion weights are computed for each sample by the GU using a concatenation of the view-representations. The MVGF model is trained at sub-field level with 10 m resolution pixels. Our evaluations show that the MVGF outperforms conventional models on the same task, achieving the best results by incorporating all the data sources, unlike the usual fusion results in the literature. For Argentina, the MVGF model achieves an R2 value of 0.68 at sub-field yield prediction, while at field level evaluation (comparing field averages), it reaches around 0.80 across different countries. The GU module learned different weights based on the country and crop-type, aligning with the variable significance of each data source to the prediction task.|\u51c6\u786e\u7684\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u5bf9\u4e8e\u519c\u4e1a\u660e\u667a\u51b3\u7b56\u3001\u5e2e\u52a9\u519c\u6c11\u548c\u884c\u4e1a\u5229\u76ca\u76f8\u5173\u8005\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8fd9\u9879\u4efb\u52a1\u5f88\u590d\u6742\uff0c\u53d6\u51b3\u4e8e\u591a\u79cd\u56e0\u7d20\uff0c\u4f8b\u5982\u73af\u5883\u6761\u4ef6\u3001\u571f\u58e4\u7279\u6027\u548c\u7ba1\u7406\u5b9e\u8df5\u3002\u7ec4\u5408\u5f02\u6784\u6570\u636e\u89c6\u56fe\u5e26\u6765\u4e86\u878d\u5408\u6311\u6218\uff0c\u4f8b\u5982\u8bc6\u522b\u89c6\u56fe\u7279\u5b9a\u5bf9\u9884\u6d4b\u4efb\u52a1\u7684\u8d21\u732e\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u89c6\u56fe\u5b66\u4e60\u65b9\u6cd5\u6765\u9884\u6d4b\u4e0d\u540c\u4f5c\u7269\uff08\u5927\u8c46\u3001\u5c0f\u9ea6\u3001\u6cb9\u83dc\u7c7d\uff09\u548c\u5730\u533a\uff08\u963f\u6839\u5ef7\u3001\u4e4c\u62c9\u572d\u548c\u5fb7\u56fd\uff09\u7684\u4f5c\u7269\u4ea7\u91cf\u3002\u6211\u4eec\u7684\u591a\u89c6\u56fe\u8f93\u5165\u6570\u636e\u5305\u62ec\u6765\u81ea Sentinel-2 \u536b\u661f\u7684\u591a\u5149\u8c31\u5149\u5b66\u56fe\u50cf\u548c\u5929\u6c14\u6570\u636e\uff0c\u4f5c\u4e3a\u4f5c\u7269\u751f\u957f\u5b63\u8282\u7684\u52a8\u6001\u7279\u5f81\uff0c\u5e76\u8f85\u4ee5\u571f\u58e4\u7279\u6027\u548c\u5730\u5f62\u4fe1\u606f\u7b49\u9759\u6001\u7279\u5f81\u3002\u4e3a\u4e86\u6709\u6548\u5730\u878d\u5408\u6570\u636e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u591a\u89c6\u56fe\u95e8\u63a7\u878d\u5408\uff08MVGF\uff09\u6a21\u578b\uff0c\u5305\u62ec\u4e13\u7528\u89c6\u56fe\u7f16\u7801\u5668\u548c\u95e8\u63a7\u5355\u5143\uff08GU\uff09\u6a21\u5757\u3002\u89c6\u56fe\u7f16\u7801\u5668\u901a\u8fc7\u5b66\u4e60\u7279\u5b9a\u4e8e\u89c6\u56fe\u7684\u8868\u793a\u6765\u5904\u7406\u5177\u6709\u4e0d\u540c\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u6570\u636e\u6e90\u7684\u5f02\u6784\u6027\u3002\u8fd9\u4e9b\u8868\u793a\u901a\u8fc7\u52a0\u6743\u548c\u81ea\u9002\u5e94\u5730\u878d\u5408\u3002 GU \u4f7f\u7528\u89c6\u56fe\u8868\u793a\u7684\u4e32\u8054\u6765\u8ba1\u7b97\u6bcf\u4e2a\u6837\u672c\u7684\u878d\u5408\u6743\u91cd\u3002 MVGF \u6a21\u578b\u5728\u5b50\u573a\u7ea7\u522b\u4ee5 10 m \u5206\u8fa8\u7387\u50cf\u7d20\u8fdb\u884c\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0cMVGF \u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u5e76\u6240\u6709\u6570\u636e\u6e90\u5b9e\u73b0\u4e86\u6700\u4f73\u7ed3\u679c\uff0c\u8fd9\u4e0e\u6587\u732e\u4e2d\u901a\u5e38\u7684\u878d\u5408\u7ed3\u679c\u4e0d\u540c\u3002\u5bf9\u4e8e\u963f\u6839\u5ef7\u6765\u8bf4\uff0cMVGF\u6a21\u578b\u5728\u5b50\u7530\u4ea7\u91cf\u9884\u6d4b\u4e2d\u7684R2\u503c\u4e3a0.68\uff0c\u800c\u5728\u7530\u95f4\u8bc4\u4f30\uff08\u6bd4\u8f83\u7530\u95f4\u5e73\u5747\u503c\uff09\u65f6\uff0c\u4e0d\u540c\u56fd\u5bb6\u7684R2\u503c\u8fbe\u52300.80\u5de6\u53f3\u3002 GU \u6a21\u5757\u6839\u636e\u56fd\u5bb6\u548c\u4f5c\u7269\u7c7b\u578b\u5b66\u4e60\u4e0d\u540c\u7684\u6743\u91cd\uff0c\u4e0e\u6bcf\u4e2a\u6570\u636e\u6e90\u5bf9\u9884\u6d4b\u4efb\u52a1\u7684\u53ef\u53d8\u91cd\u8981\u6027\u4fdd\u6301\u4e00\u81f4\u3002|[2401.11844v1](http://arxiv.org/pdf/2401.11844v1)|null|\n", "2401.11751": "|**2024-01-22**|**Boosting Multi-view Stereo with Late Cost Aggregation**|\u901a\u8fc7\u540e\u671f\u6210\u672c\u805a\u5408\u589e\u5f3a\u591a\u89c6\u56fe\u7acb\u4f53\u6548\u679c|Jiang Wu, Rui Li, Yu Zhu, Wenxun Zhao, Jinqiu Sun, Yanning Zhang|Pairwise matching cost aggregation is a crucial step for modern learning-based Multi-view Stereo (MVS). Prior works adopt an early aggregation scheme, which adds up pairwise costs into an intermediate cost. However, we analyze that this process can degrade informative pairwise matchings, thereby blocking the depth network from fully utilizing the original geometric matching cues.To address this challenge, we present a late aggregation approach that allows for aggregating pairwise costs throughout the network feed-forward process, achieving accurate estimations with only minor changes of the plain CasMVSNet.Instead of building an intermediate cost by weighted sum, late aggregation preserves all pairwise costs along a distinct view channel. This enables the succeeding depth network to fully utilize the crucial geometric cues without loss of cost fidelity. Grounded in the new aggregation scheme, we propose further techniques addressing view order dependence inside the preserved cost, handling flexible testing views, and improving the depth filtering process. Despite its technical simplicity, our method improves significantly upon the baseline cascade-based approach, achieving comparable results with state-of-the-art methods with favorable computation overhead.|\u6210\u5bf9\u5339\u914d\u6210\u672c\u805a\u5408\u662f\u73b0\u4ee3\u57fa\u4e8e\u5b66\u4e60\u7684\u591a\u89c6\u56fe\u7acb\u4f53\uff08MVS\uff09\u7684\u5173\u952e\u6b65\u9aa4\u3002\u5148\u524d\u7684\u5de5\u4f5c\u91c7\u7528\u65e9\u671f\u805a\u5408\u65b9\u6848\uff0c\u5c06\u6210\u5bf9\u6210\u672c\u52a0\u8d77\u6765\u4e3a\u4e2d\u95f4\u6210\u672c\u3002\u7136\u800c\uff0c\u6211\u4eec\u5206\u6790\u8ba4\u4e3a\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4f1a\u964d\u4f4e\u6210\u5bf9\u5339\u914d\u7684\u4fe1\u606f\u91cf\uff0c\u4ece\u800c\u963b\u6b62\u6df1\u5ea6\u7f51\u7edc\u5145\u5206\u5229\u7528\u539f\u59cb\u7684\u51e0\u4f55\u5339\u914d\u7ebf\u7d22\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u671f\u805a\u5408\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5141\u8bb8\u805a\u5408\u6574\u4e2a\u7f51\u7edc\u524d\u9988\u7684\u6210\u5bf9\u6210\u672c\u8fc7\u7a0b\u4e2d\uff0c\u53ea\u9700\u5bf9\u666e\u901a CasMVSNet \u8fdb\u884c\u5fae\u5c0f\u7684\u66f4\u6539\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u7684\u4f30\u8ba1\u3002\u540e\u671f\u805a\u5408\u4e0d\u662f\u901a\u8fc7\u52a0\u6743\u548c\u6784\u5efa\u4e2d\u95f4\u6210\u672c\uff0c\u800c\u662f\u4fdd\u7559\u6cbf\u4e0d\u540c\u89c6\u56fe\u901a\u9053\u7684\u6240\u6709\u6210\u5bf9\u6210\u672c\u3002\u8fd9\u4f7f\u5f97\u540e\u7eed\u7684\u6df1\u5ea6\u7f51\u7edc\u80fd\u591f\u5145\u5206\u5229\u7528\u5173\u952e\u7684\u51e0\u4f55\u7ebf\u7d22\uff0c\u800c\u4e0d\u4f1a\u635f\u5931\u6210\u672c\u4fdd\u771f\u5ea6\u3002\u57fa\u4e8e\u65b0\u7684\u805a\u5408\u65b9\u6848\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u7684\u6280\u672f\uff0c\u89e3\u51b3\u4fdd\u7559\u6210\u672c\u5185\u7684\u89c6\u56fe\u987a\u5e8f\u4f9d\u8d56\u6027\uff0c\u5904\u7406\u7075\u6d3b\u7684\u6d4b\u8bd5\u89c6\u56fe\uff0c\u5e76\u6539\u8fdb\u6df1\u5ea6\u8fc7\u6ee4\u8fc7\u7a0b\u3002\u5c3d\u7ba1\u6280\u672f\u7b80\u5355\uff0c\u4f46\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u57fa\u4e8e\u7ea7\u8054\u7684\u57fa\u7ebf\u65b9\u6cd5\u7684\u57fa\u7840\u4e0a\u6709\u4e86\u663e\u7740\u6539\u8fdb\uff0c\u4ee5\u6709\u5229\u7684\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\u3002|[2401.11751v1](http://arxiv.org/pdf/2401.11751v1)|null|\n", "2401.11740": "|**2024-01-22**|**Multi-level Cross-modal Alignment for Image Clustering**|\u56fe\u50cf\u805a\u7c7b\u7684\u591a\u7ea7\u8de8\u6a21\u6001\u5bf9\u9f50|Liping Qiu, Qin Zhang, Xiaojun Chen, Shaotian Cai|Recently, the cross-modal pretraining model has been employed to produce meaningful pseudo-labels to supervise the training of an image clustering model. However, numerous erroneous alignments in a cross-modal pre-training model could produce poor-quality pseudo-labels and degrade clustering performance. To solve the aforementioned issue, we propose a novel \\textbf{Multi-level Cross-modal Alignment} method to improve the alignments in a cross-modal pretraining model for downstream tasks, by building a smaller but better semantic space and aligning the images and texts in three levels, i.e., instance-level, prototype-level, and semantic-level. Theoretical results show that our proposed method converges, and suggests effective means to reduce the expected clustering risk of our method. Experimental results on five benchmark datasets clearly show the superiority of our new method.|\u6700\u8fd1\uff0c\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u88ab\u7528\u6765\u4ea7\u751f\u6709\u610f\u4e49\u7684\u4f2a\u6807\u7b7e\u6765\u76d1\u7763\u56fe\u50cf\u805a\u7c7b\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u8de8\u6a21\u5f0f\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u5927\u91cf\u9519\u8bef\u5bf9\u9f50\u53ef\u80fd\u4f1a\u4ea7\u751f\u8d28\u91cf\u5dee\u7684\u4f2a\u6807\u7b7e\u5e76\u964d\u4f4e\u805a\u7c7b\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 \\textbf{\u591a\u7ea7\u8de8\u6a21\u6001\u5bf9\u9f50} \u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u66f4\u5c0f\u4f46\u66f4\u597d\u7684\u8bed\u4e49\u7a7a\u95f4\u5e76\u5bf9\u9f50\u56fe\u50cf\u548c\u56fe\u50cf\u6765\u6539\u8fdb\u4e0b\u6e38\u4efb\u52a1\u7684\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u5bf9\u9f50\u3002\u6587\u672c\u5206\u4e3a\u4e09\u4e2a\u7ea7\u522b\uff0c\u5373\u5b9e\u4f8b\u7ea7\u522b\u3001\u539f\u578b\u7ea7\u522b\u548c\u8bed\u4e49\u7ea7\u522b\u3002\u7406\u8bba\u7ed3\u679c\u8868\u660e\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u6536\u655b\u7684\uff0c\u5e76\u63d0\u51fa\u4e86\u964d\u4f4e\u6211\u4eec\u65b9\u6cd5\u7684\u9884\u671f\u805a\u7c7b\u98ce\u9669\u7684\u6709\u6548\u65b9\u6cd5\u3002\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u6e05\u695a\u5730\u8868\u660e\u4e86\u6211\u4eec\u65b0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002|[2401.11740v1](http://arxiv.org/pdf/2401.11740v1)|null|\n"}}