{"\u751f\u6210\u6a21\u578b": {"2403.01248": "|**2024-03-02**|**SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code**|SceneCraft\uff1a\u7528\u4e8e\u5c06 3D \u573a\u666f\u5408\u6210\u4e3a Blender \u4ee3\u7801\u7684 LLM \u4ee3\u7406|Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi|This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.|\u672c\u6587\u4ecb\u7ecd\u4e86 SceneCraft\uff0c\u8fd9\u662f\u4e00\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4ee3\u7406\uff0c\u53ef\u5c06\u6587\u672c\u63cf\u8ff0\u8f6c\u6362\u4e3a Blender \u53ef\u6267\u884c\u7684 Python \u811a\u672c\uff0c\u4ece\u800c\u6e32\u67d3\u5177\u6709\u591a\u8fbe\u4e00\u767e\u4e2a 3D \u8d44\u6e90\u7684\u590d\u6742\u573a\u666f\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u9700\u8981\u590d\u6742\u7684\u7a7a\u95f4\u89c4\u5212\u548c\u5e03\u7f6e\u3002\u6211\u4eec\u901a\u8fc7\u7ed3\u5408\u5148\u8fdb\u7684\u62bd\u8c61\u3001\u6218\u7565\u89c4\u5212\u548c\u56fe\u4e66\u9986\u5b66\u4e60\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002 SceneCraft \u9996\u5148\u5c06\u573a\u666f\u56fe\u5efa\u6a21\u4e3a\u84dd\u56fe\uff0c\u8be6\u7ec6\u8bf4\u660e\u573a\u666f\u4e2d\u8d44\u6e90\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u3002\u7136\u540e\uff0cSceneCraft \u6839\u636e\u8be5\u56fe\u7f16\u5199 Python \u811a\u672c\uff0c\u5c06\u5173\u7cfb\u8f6c\u6362\u4e3a\u8d44\u4ea7\u5e03\u5c40\u7684\u6570\u5b57\u7ea6\u675f\u3002\u63a5\u4e0b\u6765\uff0cSceneCraft \u5229\u7528 GPT-V \u7b49\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u7684\u611f\u77e5\u4f18\u52bf\u6765\u5206\u6790\u6e32\u67d3\u56fe\u50cf\u5e76\u8fed\u4ee3\u5730\u7ec6\u5316\u573a\u666f\u3002\u5728\u6b64\u8fc7\u7a0b\u4e4b\u4e0a\uff0cSceneCraft \u5177\u6709\u5e93\u5b66\u4e60\u673a\u5236\uff0c\u53ef\u5c06\u5e38\u89c1\u811a\u672c\u51fd\u6570\u7f16\u8bd1\u4e3a\u53ef\u91cd\u7528\u5e93\uff0c\u4ece\u800c\u4fc3\u8fdb\u6301\u7eed\u81ea\u6211\u6539\u8fdb\uff0c\u800c\u65e0\u9700\u6602\u8d35\u7684 LLM \u53c2\u6570\u8c03\u6574\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0cSceneCraft \u5728\u6e32\u67d3\u590d\u6742\u573a\u666f\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u57fa\u4e8e LLM \u7684\u4ee3\u7406\uff0c\u8fd9\u4ece\u5b83\u9075\u5b88\u7ea6\u675f\u548c\u6709\u5229\u7684\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u53ef\u4ee5\u770b\u51fa\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u91cd\u5efa Sintel \u7535\u5f71\u4e2d\u7684\u8be6\u7ec6 3D \u573a\u666f\u5e76\u4ee5\u751f\u6210\u7684\u573a\u666f\u4f5c\u4e3a\u4e2d\u95f4\u63a7\u5236\u4fe1\u53f7\u6765\u6307\u5bfc\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u5c55\u793a\u4e86 SceneCraft \u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002|[2403.01248v1](http://arxiv.org/pdf/2403.01248v1)|null|\n", "2403.01226": "|**2024-03-02**|**DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction**|DiffSal\uff1a\u7528\u4e8e\u6269\u6563\u663e\u7740\u6027\u9884\u6d4b\u7684\u8054\u5408\u97f3\u9891\u548c\u89c6\u9891\u5b66\u4e60|Junwen Xiong, Peng Zhang, Tao You, Chuanyue Li, Wei Huang, Yufei Zha|Audio-visual saliency prediction can draw support from diverse modality complements, but further performance enhancement is still challenged by customized architectures as well as task-specific loss functions. In recent studies, denoising diffusion models have shown more promising in unifying task frameworks owing to their inherent ability of generalization. Following this motivation, a novel Diffusion architecture for generalized audio-visual Saliency prediction (DiffSal) is proposed in this work, which formulates the prediction problem as a conditional generative task of the saliency map by utilizing input audio and video as the conditions. Based on the spatio-temporal audio-visual features, an extra network Saliency-UNet is designed to perform multi-modal attention modulation for progressive refinement of the ground-truth saliency map from the noisy map. Extensive experiments demonstrate that the proposed DiffSal can achieve excellent performance across six challenging audio-visual benchmarks, with an average relative improvement of 6.3\\% over the previous state-of-the-art results by six metrics.|\u89c6\u542c\u663e\u7740\u6027\u9884\u6d4b\u53ef\u4ee5\u5f97\u5230\u591a\u79cd\u6a21\u6001\u8865\u5145\u7684\u652f\u6301\uff0c\u4f46\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u589e\u5f3a\u4ecd\u7136\u53d7\u5230\u5b9a\u5236\u67b6\u6784\u548c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u635f\u5931\u51fd\u6570\u7684\u6311\u6218\u3002\u5728\u6700\u8fd1\u7684\u7814\u7a76\u4e2d\uff0c\u53bb\u566a\u6269\u6563\u6a21\u578b\u7531\u4e8e\u5176\u56fa\u6709\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u7edf\u4e00\u4efb\u52a1\u6846\u67b6\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u66f4\u5927\u7684\u524d\u666f\u3002\u9075\u5faa\u8fd9\u4e00\u52a8\u673a\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5e7f\u4e49\u89c6\u542c\u663e\u7740\u6027\u9884\u6d4b\uff08DiffSal\uff09\u7684\u65b0\u9896\u6269\u6563\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528\u8f93\u5165\u97f3\u9891\u548c\u89c6\u9891\u4f5c\u4e3a\u6761\u4ef6\uff0c\u5c06\u9884\u6d4b\u95ee\u9898\u8868\u8ff0\u4e3a\u663e\u7740\u6027\u56fe\u7684\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u3002\u57fa\u4e8e\u65f6\u7a7a\u89c6\u542c\u7279\u5f81\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u989d\u5916\u7684\u7f51\u7edc Saliency-UNet \u6765\u6267\u884c\u591a\u6a21\u6001\u6ce8\u610f\u529b\u8c03\u5236\uff0c\u4ee5\u4ece\u566a\u58f0\u56fe\u4e2d\u9010\u6b65\u7ec6\u5316\u5730\u9762\u5b9e\u51b5\u663e\u7740\u56fe\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 DiffSal \u53ef\u4ee5\u5728\u516d\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u542c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u51fa\u8272\u7684\u6027\u80fd\uff0c\u4e0e\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u76f8\u6bd4\uff0c\u516d\u4e2a\u6307\u6807\u7684\u5e73\u5747\u76f8\u5bf9\u6539\u8fdb\u4e3a 6.3%\u3002|[2403.01226v1](http://arxiv.org/pdf/2403.01226v1)|null|\n", "2403.01212": "|**2024-03-02**|**TCIG: Two-Stage Controlled Image Generation with Quality Enhancement through Diffusion**|TCIG\uff1a\u4e24\u9636\u6bb5\u63a7\u5236\u56fe\u50cf\u751f\u6210\uff0c\u901a\u8fc7\u6269\u6563\u589e\u5f3a\u8d28\u91cf|Salaheldin Mohamed|In recent years, significant progress has been made in the development of text-to-image generation models. However, these models still face limitations when it comes to achieving full controllability during the generation process. Often, specific training or the use of limited models is required, and even then, they have certain restrictions. To address these challenges, A two-stage method that effectively combines controllability and high quality in the generation of images is proposed. This approach leverages the expertise of pre-trained models to achieve precise control over the generated images, while also harnessing the power of diffusion models to achieve state-of-the-art quality. By separating controllability from high quality, This method achieves outstanding results. It is compatible with both latent and image space diffusion models, ensuring versatility and flexibility. Moreover, This approach consistently produces comparable outcomes to the current state-of-the-art methods in the field. Overall, This proposed method represents a significant advancement in text-to-image generation, enabling improved controllability without compromising on the quality of the generated images.|\u8fd1\u5e74\u6765\uff0c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u73b0\u751f\u6210\u8fc7\u7a0b\u7684\u5b8c\u5168\u53ef\u63a7\u6027\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u5c40\u9650\u6027\u3002\u901a\u5e38\uff0c\u9700\u8981\u8fdb\u884c\u7279\u5b9a\u7684\u8bad\u7ec3\u6216\u4f7f\u7528\u6709\u9650\u7684\u6a21\u578b\uff0c\u5373\u4f7f\u8fd9\u6837\uff0c\u5b83\u4eec\u4e5f\u6709\u4e00\u5b9a\u7684\u9650\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7ed3\u5408\u56fe\u50cf\u751f\u6210\u7684\u53ef\u63a7\u6027\u548c\u9ad8\u8d28\u91cf\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e13\u4e1a\u77e5\u8bc6\u6765\u5b9e\u73b0\u200b\u200b\u5bf9\u751f\u6210\u56fe\u50cf\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u540c\u65f6\u8fd8\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u529b\u91cf\u6765\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8d28\u91cf\u3002\u901a\u8fc7\u5c06\u53ef\u63a7\u6027\u4e0e\u9ad8\u8d28\u91cf\u5206\u5f00\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u7ed3\u679c\u3002\u5b83\u4e0e\u6f5c\u5728\u6a21\u578b\u548c\u56fe\u50cf\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u517c\u5bb9\uff0c\u786e\u4fdd\u4e86\u591a\u529f\u80fd\u6027\u548c\u7075\u6d3b\u6027\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u65b9\u6cd5\u59cb\u7ec8\u80fd\u4ea7\u751f\u4e0e\u8be5\u9886\u57df\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4ee3\u8868\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u53ef\u63a7\u6027\u3002|[2403.01212v1](http://arxiv.org/pdf/2403.01212v1)|null|\n", "2403.01189": "|**2024-03-02**|**Training Unbiased Diffusion Models From Biased Dataset**|\u4ece\u6709\u504f\u6570\u636e\u96c6\u8bad\u7ec3\u65e0\u504f\u6269\u6563\u6a21\u578b|Yeongmin Kim, Byeonghu Na, Minsang Park, JoonHo Jang, Dongjun Kim, Wanmo Kang, Il-Chul Moon|With significant advancements in diffusion models, addressing the potential risks of dataset bias becomes increasingly important. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. This paper proposes time-dependent importance reweighting to mitigate the bias for the diffusion models. We demonstrate that the time-dependent density ratio becomes more precise than previous approaches, thereby minimizing error propagation in generative learning. While directly applying it to score-matching is intractable, we discover that using the time-dependent density ratio both for reweighting and score correction can lead to a tractable form of the objective function to regenerate the unbiased data density. Furthermore, we theoretically establish a connection with traditional score-matching, and we demonstrate its convergence to an unbiased distribution. The experimental evidence supports the usefulness of the proposed method, which outperforms baselines including time-independent importance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various bias settings. Our code is available at https://github.com/alsdudrla10/TIW-DSM.|\u968f\u7740\u6269\u6563\u6a21\u578b\u7684\u663e\u7740\u8fdb\u6b65\uff0c\u89e3\u51b3\u6570\u636e\u96c6\u504f\u5dee\u7684\u6f5c\u5728\u98ce\u9669\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7531\u4e8e\u751f\u6210\u7684\u8f93\u51fa\u76f4\u63a5\u53d7\u5230\u6570\u636e\u96c6\u504f\u5dee\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u51cf\u8f7b\u6f5c\u5728\u504f\u5dee\u6210\u4e3a\u63d0\u9ad8\u6837\u672c\u8d28\u91cf\u548c\u6bd4\u4f8b\u7684\u5173\u952e\u56e0\u7d20\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e0e\u65f6\u95f4\u76f8\u5173\u7684\u91cd\u8981\u6027\u91cd\u65b0\u52a0\u6743\uff0c\u4ee5\u51cf\u8f7b\u6269\u6563\u6a21\u578b\u7684\u504f\u5dee\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u4e0e\u65f6\u95f4\u76f8\u5173\u7684\u5bc6\u5ea6\u6bd4\u53d8\u5f97\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u66f4\u7cbe\u786e\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u751f\u6210\u5b66\u4e60\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u3002\u867d\u7136\u76f4\u63a5\u5c06\u5176\u5e94\u7528\u4e8e\u5206\u6570\u5339\u914d\u5f88\u56f0\u96be\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u4f7f\u7528\u65f6\u95f4\u76f8\u5173\u7684\u5bc6\u5ea6\u6bd4\u8fdb\u884c\u91cd\u65b0\u52a0\u6743\u548c\u5206\u6570\u6821\u6b63\u53ef\u4ee5\u4ea7\u751f\u76ee\u6807\u51fd\u6570\u7684\u6613\u4e8e\u5904\u7406\u7684\u5f62\u5f0f\uff0c\u4ee5\u91cd\u65b0\u751f\u6210\u65e0\u504f\u6570\u636e\u5bc6\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86\u4e0e\u4f20\u7edf\u5206\u6570\u5339\u914d\u7684\u8054\u7cfb\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u5230\u65e0\u504f\u5206\u5e03\u3002\u5b9e\u9a8c\u8bc1\u636e\u652f\u6301\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5305\u62ec\u5728\u5177\u6709\u5404\u79cd\u504f\u5dee\u8bbe\u7f6e\u7684 CIFAR-10\u3001CIFAR-100\u3001FFHQ \u548c CelebA \u4e0a\u8fdb\u884c\u4e0e\u65f6\u95f4\u65e0\u5173\u7684\u91cd\u8981\u6027\u91cd\u65b0\u52a0\u6743\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u4ece https://github.com/alsdudrla10/TIW-DSM \u83b7\u53d6\u3002|[2403.01189v1](http://arxiv.org/pdf/2403.01189v1)|**[link](https://github.com/alsdudrla10/tiw-dsm)**|\n", "2403.01129": "|**2024-03-02**|**Dynamic 3D Point Cloud Sequences as 2D Videos**|\u4f5c\u4e3a 2D \u89c6\u9891\u7684\u52a8\u6001 3D \u70b9\u4e91\u5e8f\u5217|Yiming Zeng, Junhui Hou, Qijian Zhang, Siyu Ren, Wenping Wang|Dynamic 3D point cloud sequences serve as one of the most common and practical representation modalities of dynamic real-world environments. However, their unstructured nature in both spatial and temporal domains poses significant challenges to effective and efficient processing. Existing deep point cloud sequence modeling approaches imitate the mature 2D video learning mechanisms by developing complex spatio-temporal point neighbor grouping and feature aggregation schemes, often resulting in methods lacking effectiveness, efficiency, and expressive power. In this paper, we propose a novel generic representation called \\textit{Structured Point Cloud Videos} (SPCVs). Intuitively, by leveraging the fact that 3D geometric shapes are essentially 2D manifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatial smoothness and temporal consistency, where the pixel values correspond to the 3D coordinates of points. The structured nature of our SPCV representation allows for the seamless adaptation of well-established 2D image/video techniques, enabling efficient and effective processing and analysis of 3D point cloud sequences. To achieve such re-organization, we design a self-supervised learning pipeline that is geometrically regularized and driven by self-reconstructive and deformation field learning objectives. Additionally, we construct SPCV-based frameworks for both low-level and high-level 3D point cloud sequence processing and analysis tasks, including action recognition, temporal interpolation, and compression. Extensive experiments demonstrate the versatility and superiority of the proposed SPCV, which has the potential to offer new possibilities for deep learning on unstructured 3D point cloud sequences. Code will be released at https://github.com/ZENGYIMING-EAMON/SPCV.|\u52a8\u6001 3D \u70b9\u4e91\u5e8f\u5217\u662f\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u6700\u5e38\u89c1\u3001\u6700\u5b9e\u7528\u7684\u8868\u793a\u65b9\u5f0f\u4e4b\u4e00\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u57df\u4e0a\u7684\u975e\u7ed3\u6784\u5316\u6027\u8d28\u5bf9\u6709\u6548\u548c\u9ad8\u6548\u7684\u5904\u7406\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u70b9\u4e91\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u901a\u8fc7\u5f00\u53d1\u590d\u6742\u7684\u65f6\u7a7a\u70b9\u90bb\u5c45\u5206\u7ec4\u548c\u7279\u5f81\u805a\u5408\u65b9\u6848\u6765\u6a21\u4eff\u6210\u719f\u76842D\u89c6\u9891\u5b66\u4e60\u673a\u5236\uff0c\u901a\u5e38\u5bfc\u81f4\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u8868\u8fbe\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u901a\u7528\u8868\u793a\u5f62\u5f0f\uff0c\u79f0\u4e3a \\textit{\u7ed3\u6784\u5316\u70b9\u4e91\u89c6\u9891} (SPCV)\u3002\u76f4\u89c2\u5730\u8bf4\uff0c\u5229\u7528 3D \u51e0\u4f55\u5f62\u72b6\u672c\u8d28\u4e0a\u662f 2D \u6d41\u5f62\u8fd9\u4e00\u4e8b\u5b9e\uff0cSPCV \u5c06\u70b9\u4e91\u5e8f\u5217\u91cd\u65b0\u7ec4\u7ec7\u4e3a\u5177\u6709\u7a7a\u95f4\u5e73\u6ed1\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684 2D \u89c6\u9891\uff0c\u5176\u4e2d\u50cf\u7d20\u503c\u5bf9\u5e94\u4e8e\u70b9\u7684 3D \u5750\u6807\u3002\u6211\u4eec\u7684 SPCV \u8868\u793a\u7684\u7ed3\u6784\u5316\u6027\u8d28\u5141\u8bb8\u65e0\u7f1d\u9002\u5e94\u6210\u719f\u7684 2D \u56fe\u50cf/\u89c6\u9891\u6280\u672f\uff0c\u4ece\u800c\u5b9e\u73b0 3D \u70b9\u4e91\u5e8f\u5217\u7684\u9ad8\u6548\u4e14\u6709\u6548\u7684\u5904\u7406\u548c\u5206\u6790\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u79cd\u91cd\u7ec4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u6211\u76d1\u7763\u7684\u5b66\u4e60\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u662f\u51e0\u4f55\u6b63\u5219\u5316\u7684\uff0c\u5e76\u7531\u81ea\u6211\u91cd\u5efa\u548c\u53d8\u5f62\u573a\u5b66\u4e60\u76ee\u6807\u9a71\u52a8\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4e3a\u4f4e\u7ea7\u548c\u9ad8\u7ea7 3D \u70b9\u4e91\u5e8f\u5217\u5904\u7406\u548c\u5206\u6790\u4efb\u52a1\u6784\u5efa\u4e86\u57fa\u4e8e SPCV \u7684\u6846\u67b6\uff0c\u5305\u62ec\u52a8\u4f5c\u8bc6\u522b\u3001\u65f6\u95f4\u63d2\u503c\u548c\u538b\u7f29\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 SPCV \u7684\u591a\u529f\u80fd\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u5b83\u6709\u53ef\u80fd\u4e3a\u975e\u7ed3\u6784\u5316 3D \u70b9\u4e91\u5e8f\u5217\u7684\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u65b0\u7684\u53ef\u80fd\u6027\u3002\u4ee3\u7801\u5c06\u5728 https://github.com/ZENGYIMING-EAMON/SPCV \u53d1\u5e03\u3002|[2403.01129v1](http://arxiv.org/pdf/2403.01129v1)|null|\n", "2403.01124": "|**2024-03-02**|**Text-guided Explorable Image Super-resolution**|\u6587\u672c\u5f15\u5bfc\u7684\u53ef\u63a2\u7d22\u56fe\u50cf\u8d85\u5206\u8fa8\u7387|Kanchana Vaishnavi Gandikota, Paramanand Chandramouli|In this paper, we introduce the problem of zero-shot text-guided exploration of the solutions to open-domain image super-resolution. Our goal is to allow users to explore diverse, semantically accurate reconstructions that preserve data consistency with the low-resolution inputs for different large downsampling factors without explicitly training for these specific degradations. We propose two approaches for zero-shot text-guided super-resolution - i) modifying the generative process of text-to-image \\textit{T2I} diffusion models to promote consistency with low-resolution inputs, and ii) incorporating language guidance into zero-shot diffusion-based restoration methods. We show that the proposed approaches result in diverse solutions that match the semantic meaning provided by the text prompt while preserving data consistency with the degraded inputs. We evaluate the proposed baselines for the task of extreme super-resolution and demonstrate advantages in terms of restoration quality, diversity, and explorability of solutions.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u5f00\u653e\u57df\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u89e3\u51b3\u65b9\u6848\u7684\u96f6\u6837\u672c\u6587\u672c\u5f15\u5bfc\u63a2\u7d22\u95ee\u9898\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u5141\u8bb8\u7528\u6237\u63a2\u7d22\u591a\u6837\u5316\u7684\u3001\u8bed\u4e49\u4e0a\u51c6\u786e\u7684\u91cd\u5efa\uff0c\u4ece\u800c\u4fdd\u6301\u6570\u636e\u4e0e\u4e0d\u540c\u5927\u4e0b\u91c7\u6837\u56e0\u5b50\u7684\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u7684\u4e00\u81f4\u6027\uff0c\u800c\u65e0\u9700\u9488\u5bf9\u8fd9\u4e9b\u7279\u5b9a\u7684\u9000\u5316\u8fdb\u884c\u660e\u786e\u7684\u8bad\u7ec3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u96f6\u6837\u672c\u6587\u672c\u5f15\u5bfc\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5 - i\uff09\u4fee\u6539\u6587\u672c\u5230\u56fe\u50cf \\textit{T2I} \u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u4ee5\u4fc3\u8fdb\u4e0e\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca ii\uff09\u5c06\u8bed\u8a00\u6307\u5bfc\u7eb3\u5165\u57fa\u4e8e\u96f6\u6837\u672c\u6269\u6563\u7684\u6062\u590d\u65b9\u6cd5\u3002\u6211\u4eec\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4ea7\u751f\u4e86\u591a\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u4e9b\u89e3\u51b3\u65b9\u6848\u4e0e\u6587\u672c\u63d0\u793a\u63d0\u4f9b\u7684\u8bed\u4e49\u76f8\u5339\u914d\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u4e0e\u964d\u7ea7\u8f93\u5165\u7684\u4e00\u81f4\u6027\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u6781\u7aef\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u6240\u63d0\u51fa\u7684\u57fa\u7ebf\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6062\u590d\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u63a2\u7d22\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002|[2403.01124v1](http://arxiv.org/pdf/2403.01124v1)|null|\n", "2403.01108": "|**2024-03-02**|**Face Swap via Diffusion Model**|\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9762\u90e8\u4ea4\u6362|Feifei Wang|This technical report presents a diffusion model based framework for face swapping between two portrait images. The basic framework consists of three components, i.e., IP-Adapter, ControlNet, and Stable Diffusion's inpainting pipeline, for face feature encoding, multi-conditional generation, and face inpainting respectively. Besides, I introduce facial guidance optimization and CodeFormer based blending to further improve the generation quality.   Specifically, we engage a recent light-weighted customization method (i.e., DreamBooth-LoRA), to guarantee the identity consistency by 1) using a rare identifier \"sks\" to represent the source identity, and 2) injecting the image features of source portrait into each cross-attention layer like the text features. Then I resort to the strong inpainting ability of Stable Diffusion, and utilize canny image and face detection annotation of the target portrait as the conditions, to guide ContorlNet's generation and align source portrait with the target portrait. To further correct face alignment, we add the facial guidance loss to optimize the text embedding during the sample generation.|\u8be5\u6280\u672f\u62a5\u544a\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e24\u4e2a\u8096\u50cf\u56fe\u50cf\u4e4b\u95f4\u7684\u9762\u90e8\u4ea4\u6362\u3002\u57fa\u672c\u6846\u67b6\u7531\u4e09\u4e2a\u7ec4\u4ef6\u7ec4\u6210\uff0c\u5373IP-Adapter\u3001ControlNet\u548cStable Diffusion\u7684\u4fee\u590d\u7ba1\u9053\uff0c\u5206\u522b\u7528\u4e8e\u4eba\u8138\u7279\u5f81\u7f16\u7801\u3001\u591a\u6761\u4ef6\u751f\u6210\u548c\u4eba\u8138\u4fee\u590d\u3002\u6b64\u5916\uff0c\u6211\u8fd8\u5f15\u5165\u4e86\u9762\u90e8\u5f15\u5bfc\u4f18\u5316\u548c\u57fa\u4e8e CodeFormer \u7684\u6df7\u5408\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u6700\u8fd1\u7684\u8f7b\u91cf\u7ea7\u5b9a\u5236\u65b9\u6cd5\uff08\u5373DreamBooth-LoRA\uff09\uff0c\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u4fdd\u8bc1\u8eab\u4efd\u4e00\u81f4\u6027\uff1a1\uff09\u4f7f\u7528\u7f55\u89c1\u7684\u6807\u8bc6\u7b26\u201csks\u201d\u6765\u8868\u793a\u6e90\u8eab\u4efd\uff0c2\uff09\u6ce8\u5165\u6e90\u8096\u50cf\u7684\u56fe\u50cf\u7279\u5f81\u50cf\u6587\u672c\u7279\u5f81\u4e00\u6837\u8fdb\u5165\u6bcf\u4e2a\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u3002\u7136\u540e\u501f\u52a9Stable Diffusion\u5f3a\u5927\u7684\u4fee\u590d\u80fd\u529b\uff0c\u4ee5\u76ee\u6807\u4eba\u50cf\u7684canny\u56fe\u50cf\u548c\u4eba\u8138\u68c0\u6d4b\u6807\u6ce8\u4e3a\u6761\u4ef6\uff0c\u6307\u5bfcContorlNet\u7684\u751f\u6210\uff0c\u5e76\u5c06\u6e90\u4eba\u50cf\u4e0e\u76ee\u6807\u4eba\u50cf\u5bf9\u9f50\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u7ea0\u6b63\u9762\u90e8\u5bf9\u9f50\uff0c\u6211\u4eec\u6dfb\u52a0\u9762\u90e8\u5f15\u5bfc\u635f\u5931\u4ee5\u4f18\u5316\u6837\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6587\u672c\u5d4c\u5165\u3002|[2403.01108v1](http://arxiv.org/pdf/2403.01108v1)|null|\n"}, "\u591a\u6a21\u6001": {"2403.01326": "|**2024-03-02**|**DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions**|DNA \u7cfb\u5217\uff1a\u901a\u8fc7\u5757\u7ea7\u76d1\u7763\u589e\u5f3a\u6743\u91cd\u5171\u4eab NAS|Guangrun Wang, Changlin Li, Liuchun Yuan, Jiefeng Peng, Xiaoyu Xian, Xiaodan Liang, Xiaojun Chang, Liang Lin|Neural Architecture Search (NAS), aiming at automatically designing neural architectures by machines, has been considered a key step toward automatic machine learning. One notable NAS branch is the weight-sharing NAS, which significantly improves search efficiency and allows NAS algorithms to run on ordinary computers. Despite receiving high expectations, this category of methods suffers from low search effectiveness. By employing a generalization boundedness tool, we demonstrate that the devil behind this drawback is the untrustworthy architecture rating with the oversized search space of the possible architectures. Addressing this problem, we modularize a large search space into blocks with small search spaces and develop a family of models with the distilling neural architecture (DNA) techniques. These proposed models, namely a DNA family, are capable of resolving multiple dilemmas of the weight-sharing NAS, such as scalability, efficiency, and multi-modal compatibility. Our proposed DNA models can rate all architecture candidates, as opposed to previous works that can only access a subsearch space using heuristic algorithms. Moreover, under a certain computational complexity constraint, our method can seek architectures with different depths and widths. Extensive experimental evaluations show that our models achieve state-of-the-art top-1 accuracy of 78.9% and 83.6% on ImageNet for a mobile convolutional network and a small vision transformer, respectively. Additionally, we provide in-depth empirical analysis and insights into neural architecture ratings. Codes available: \\url{https://github.com/changlin31/DNA}.|\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u65e8\u5728\u7531\u673a\u5668\u81ea\u52a8\u8bbe\u8ba1\u795e\u7ecf\u67b6\u6784\uff0c\u88ab\u8ba4\u4e3a\u662f\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u7684\u5173\u952e\u4e00\u6b65\u3002\u4e00\u4e2a\u503c\u5f97\u6ce8\u610f\u7684 NAS \u5206\u652f\u662f\u6743\u91cd\u5171\u4eab NAS\uff0c\u5b83\u663e\u7740\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\uff0c\u5e76\u5141\u8bb8 NAS \u7b97\u6cd5\u5728\u666e\u901a\u8ba1\u7b97\u673a\u4e0a\u8fd0\u884c\u3002\u5c3d\u7ba1\u53d7\u5230\u5f88\u9ad8\u7684\u671f\u671b\uff0c\u6b64\u7c7b\u65b9\u6cd5\u7684\u641c\u7d22\u6548\u7387\u8f83\u4f4e\u3002\u901a\u8fc7\u4f7f\u7528\u6cdb\u5316\u6709\u754c\u5de5\u5177\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u4e2a\u7f3a\u70b9\u80cc\u540e\u7684\u9b54\u9b3c\u662f\u4e0d\u53ef\u4fe1\u7684\u67b6\u6784\u8bc4\u7ea7\u4ee5\u53ca\u53ef\u80fd\u67b6\u6784\u7684\u8fc7\u5927\u641c\u7d22\u7a7a\u95f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5c06\u5927\u578b\u641c\u7d22\u7a7a\u95f4\u6a21\u5757\u5316\u4e3a\u5177\u6709\u5c0f\u578b\u641c\u7d22\u7a7a\u95f4\u7684\u5757\uff0c\u5e76\u4f7f\u7528\u84b8\u998f\u795e\u7ecf\u67b6\u6784\uff08DNA\uff09\u6280\u672f\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u6a21\u578b\u3002\u8fd9\u4e9b\u63d0\u51fa\u7684\u6a21\u578b\uff0c\u5373 DNA \u7cfb\u5217\uff0c\u80fd\u591f\u89e3\u51b3\u6743\u91cd\u5171\u4eab NAS \u7684\u591a\u79cd\u56f0\u5883\uff0c\u4f8b\u5982\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u591a\u6a21\u5f0f\u517c\u5bb9\u6027\u3002\u6211\u4eec\u63d0\u51fa\u7684 DNA \u6a21\u578b\u53ef\u4ee5\u5bf9\u6240\u6709\u5019\u9009\u67b6\u6784\u8fdb\u884c\u8bc4\u7ea7\uff0c\u800c\u4e4b\u524d\u7684\u5de5\u4f5c\u53ea\u80fd\u4f7f\u7528\u542f\u53d1\u5f0f\u7b97\u6cd5\u8bbf\u95ee\u5b50\u641c\u7d22\u7a7a\u95f4\u3002\u6b64\u5916\uff0c\u5728\u4e00\u5b9a\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u7ea6\u675f\u4e0b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5bfb\u627e\u5177\u6709\u4e0d\u540c\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u67b6\u6784\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728 ImageNet \u4e0a\u7684\u79fb\u52a8\u5377\u79ef\u7f51\u7edc\u548c\u5c0f\u578b\u89c6\u89c9\u53d8\u6362\u5668\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684 top-1 \u51c6\u786e\u7387 78.9% \u548c 83.6%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u5bf9\u795e\u7ecf\u67b6\u6784\u8bc4\u7ea7\u7684\u6df1\u5165\u5b9e\u8bc1\u5206\u6790\u548c\u89c1\u89e3\u3002\u53ef\u7528\u4ee3\u7801\uff1a\\url{https://github.com/changlin31/DNA}\u3002|[2403.01326v1](http://arxiv.org/pdf/2403.01326v1)|**[link](https://github.com/changlin31/DNA)**|\n", "2403.01316": "|**2024-03-02**|**TUMTraf V2X Cooperative Perception Dataset**|TUMTraf V2X \u534f\u4f5c\u611f\u77e5\u6570\u636e\u96c6|Walter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou, Rui Song, Alois C. Knoll|Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety. Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range. External sensors offer higher situational awareness for automated vehicles and prevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection and tracking task. Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors. It includes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, near-miss events, overtaking, and U-turns. Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model. Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website: https://tum-traffic-dataset.github.io/tumtraf-v2x.|\u5408\u4f5c\u611f\u77e5\u4e3a\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u80fd\u529b\u548c\u6539\u5584\u9053\u8def\u5b89\u5168\u63d0\u4f9b\u4e86\u591a\u79cd\u597d\u5904\u3002\u9664\u8f66\u8f7d\u4f20\u611f\u5668\u5916\uff0c\u4f7f\u7528\u8def\u8fb9\u4f20\u611f\u5668\u53ef\u63d0\u9ad8\u53ef\u9760\u6027\u5e76\u6269\u5c55\u4f20\u611f\u5668\u8303\u56f4\u3002\u5916\u90e8\u4f20\u611f\u5668\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u66f4\u9ad8\u7684\u6001\u52bf\u611f\u77e5\u5e76\u9632\u6b62\u906e\u6321\u3002\u6211\u4eec\u63d0\u51fa\u4e86 CoopDet3D\uff08\u4e00\u79cd\u534f\u4f5c\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\uff09\u548c TUMTraf-V2X\uff08\u611f\u77e5\u6570\u636e\u96c6\uff09\uff0c\u7528\u4e8e\u534f\u4f5c 3D \u5bf9\u8c61\u68c0\u6d4b\u548c\u8ddf\u8e2a\u4efb\u52a1\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u5305\u542b\u6765\u81ea\u4e94\u4e2a\u8def\u8fb9\u548c\u56db\u4e2a\u8f66\u8f7d\u4f20\u611f\u5668\u7684 2,000 \u4e2a\u6807\u8bb0\u70b9\u4e91\u548c 5,000 \u4e2a\u6807\u8bb0\u56fe\u50cf\u3002\u5b83\u5305\u62ec 30k \u4e2a 3D \u6846\uff0c\u5e26\u6709\u8f68\u9053 ID \u4ee5\u53ca\u7cbe\u786e\u7684 GPS \u548c IMU \u6570\u636e\u3002\u6211\u4eec\u6807\u8bb0\u4e86\u516b\u4e2a\u7c7b\u522b\uff0c\u5e76\u6db5\u76d6\u4e86\u5177\u6709\u6311\u6218\u6027\u9a7e\u9a76\u64cd\u4f5c\u7684\u906e\u6321\u573a\u666f\uff0c\u4f8b\u5982\u4ea4\u901a\u8fdd\u89c4\u3001\u672a\u9042\u4e8b\u4ef6\u3001\u8d85\u8f66\u548c\u6389\u5934\u3002\u901a\u8fc7\u591a\u6b21\u5b9e\u9a8c\uff0c\u6211\u4eec\u8868\u660e\uff0c\u4e0e\u8f66\u8f7d\u76f8\u673a-LiDAR \u878d\u5408\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u4eec\u7684 CoopDet3D \u76f8\u673a-LiDAR \u878d\u5408\u6a21\u578b\u5b9e\u73b0\u4e86+14.36 3D mAP \u7684\u589e\u52a0\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u6211\u4eec\u7684\u7f51\u7ad9\u4e0a\u516c\u5f00\u63d0\u4f9b\u6211\u4eec\u7684\u6570\u636e\u96c6\u3001\u6a21\u578b\u3001\u6807\u8bb0\u5de5\u5177\u548c\u5f00\u53d1\u5de5\u5177\u5305\uff1ahttps://tum-traffic-dataset.github.io/tumtraf-v2x\u3002|[2403.01316v1](http://arxiv.org/pdf/2403.01316v1)|null|\n", "2403.01306": "|**2024-03-02**|**ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation**|ICC\uff1a\u91cf\u5316\u591a\u6a21\u6001\u6570\u636e\u96c6\u7ba1\u7406\u7684\u56fe\u50cf\u63cf\u8ff0\u7684\u5177\u4f53\u6027|Moran Yanuka, Morris Alper, Hadar Averbuch-Elor, Raja Giryes|Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show that curation using ICC complements existing approaches: It succeeds in selecting the highest quality samples from multimodal web-scale datasets to allow for efficient training in resource-constrained settings.|\u5bf9\u6587\u672c-\u56fe\u50cf\u914d\u5bf9\u6570\u636e\u8fdb\u884c\u7f51\u7edc\u89c4\u6a21\u7684\u8bad\u7ec3\u5bf9\u4e8e\u591a\u6a21\u6001\u5b66\u4e60\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u53d7\u5230\u91ce\u5916\u6570\u636e\u96c6\u7684\u9ad8\u566a\u58f0\u6027\u8d28\u7684\u6311\u6218\u3002\u6807\u51c6\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\u6210\u529f\u5730\u5220\u9664\u4e86\u4e0d\u5339\u914d\u7684\u6587\u672c\u56fe\u50cf\u5bf9\uff0c\u4f46\u5141\u8bb8\u8bed\u4e49\u76f8\u5173\u4f46\u9ad8\u5ea6\u62bd\u8c61\u6216\u4e3b\u89c2\u7684\u6587\u672c\u3002\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u80fd\u529b\u6765\u9694\u79bb\u6700\u5177\u4f53\u7684\u6837\u672c\uff0c\u800c\u8fd9\u4e9b\u6837\u672c\u4e3a\u5728\u5608\u6742\u7684\u6570\u636e\u96c6\u4e2d\u63d0\u4f9b\u6700\u5f3a\u7684\u5b66\u4e60\u4fe1\u53f7\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6807\uff0c\u5373\u56fe\u50cf\u6807\u9898\u5177\u4f53\u6027\uff0c\u5b83\u53ef\u4ee5\u5728\u6ca1\u6709\u56fe\u50cf\u53c2\u8003\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u6807\u9898\u6587\u672c\uff0c\u4ee5\u8861\u91cf\u5176\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u4f7f\u7528\u7684\u5177\u4f53\u6027\u548c\u76f8\u5173\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u6765\u6d4b\u91cf\u591a\u6a21\u6001\u8868\u793a\u4e2d\u7684\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u5bf9\u5355\u8bcd\u548c\u53e5\u5b50\u7ea7\u6587\u672c\u7684\u5177\u4f53\u6027\u8bc4\u4f30\u5bc6\u5207\u76f8\u5173\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8868\u660e\u4f7f\u7528 ICC \u8fdb\u884c\u7ba1\u7406\u662f\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u8865\u5145\uff1a\u5b83\u6210\u529f\u5730\u4ece\u591a\u6a21\u5f0f\u7f51\u7edc\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u9009\u62e9\u6700\u9ad8\u8d28\u91cf\u7684\u6837\u672c\uff0c\u4ee5\u4fbf\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u6548\u7684\u57f9\u8bad\u3002|[2403.01306v1](http://arxiv.org/pdf/2403.01306v1)|null|\n", "2403.01229": "|**2024-03-02**|**REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild**|REWIND \u6570\u636e\u96c6\uff1a\u6839\u636e\u91ce\u5916\u591a\u6a21\u6001\u8eab\u4f53\u8fd0\u52a8\u4fe1\u53f7\u8fdb\u884c\u9690\u79c1\u4fdd\u62a4\u7684\u8bf4\u8bdd\u72b6\u6001\u5206\u5272|Jose Vargas Quiros, Chirag Raman, Stephanie Tan, Ekin Gedik, Laura Cabrera-Quiros, Hayley Hung|Recognizing speaking in humans is a central task towards understanding social interactions. Ideally, speaking would be detected from individual voice recordings, as done previously for meeting scenarios. However, individual voice recordings are hard to obtain in the wild, especially in crowded mingling scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize speech by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the speech signal. However, existing mingling datasets do not contain high quality audio recordings. Instead, speaking status annotations have often been inferred by human annotators from video, without validation of this approach against audio-based ground truth. In this paper we revisit no-audio speaking status estimation by presenting the first publicly available multimodal dataset with high-quality individual speech recordings of 33 subjects in a professional networking event. We present three baselines for no-audio speaking status segmentation: a) from video, b) from body acceleration (chest-worn accelerometer), c) from body pose tracks. In all cases we predict a 20Hz binary speaking status signal extracted from the audio, a time resolution not available in previous datasets. In addition to providing the signals and ground truth necessary to evaluate a wide range of speaking status detection methods, the availability of audio in REWIND makes it suitable for cross-modality studies not feasible with previous mingling datasets. Finally, our flexible data consent setup creates new challenges for multimodal systems under missing modalities.|\u8bc6\u522b\u4eba\u7c7b\u8bf4\u8bdd\u662f\u7406\u89e3\u793e\u4ea4\u4e92\u52a8\u7684\u6838\u5fc3\u4efb\u52a1\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4ece\u4e2a\u4eba\u5f55\u97f3\u4e2d\u68c0\u6d4b\u8bb2\u8bdd\uff0c\u5c31\u50cf\u4e4b\u524d\u5728\u4f1a\u8bae\u573a\u666f\u4e2d\u6240\u505a\u7684\u90a3\u6837\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6210\u672c\u3001\u7269\u6d41\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5728\u91ce\u5916\u5f88\u96be\u83b7\u5f97\u4e2a\u4eba\u8bed\u97f3\u8bb0\u5f55\uff0c\u5c24\u5176\u662f\u5728\u62e5\u6324\u7684\u6df7\u5408\u573a\u666f\u4e2d\u3002\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u57fa\u4e8e\u89c6\u9891\u548c\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6570\u636e\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0d\u5f15\u4eba\u6ce8\u76ee\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u65b9\u5f0f\u68c0\u6d4b\u76f8\u5173\u624b\u52bf\u6765\u8bc6\u522b\u8bed\u97f3\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u6a21\u578b\u672c\u8eab\u5e94\u8be5\u4f7f\u7528\u4ece\u8bed\u97f3\u4fe1\u53f7\u83b7\u5f97\u7684\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6df7\u5408\u6570\u636e\u96c6\u4e0d\u5305\u542b\u9ad8\u8d28\u91cf\u7684\u5f55\u97f3\u3002\u76f8\u53cd\uff0c\u8bf4\u8bdd\u72b6\u6001\u6ce8\u91ca\u901a\u5e38\u662f\u7531\u4eba\u7c7b\u6ce8\u91ca\u8005\u4ece\u89c6\u9891\u4e2d\u63a8\u65ad\u51fa\u6765\u7684\uff0c\u800c\u6ca1\u6709\u6839\u636e\u57fa\u4e8e\u97f3\u9891\u7684\u57fa\u672c\u4e8b\u5b9e\u9a8c\u8bc1\u8fd9\u79cd\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5c55\u793a\u7b2c\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4e13\u4e1a\u793e\u4ea4\u6d3b\u52a8\u4e2d 33 \u540d\u53d7\u8bd5\u8005\u7684\u9ad8\u8d28\u91cf\u4e2a\u4eba\u8bed\u97f3\u5f55\u97f3\uff0c\u91cd\u65b0\u5ba1\u89c6\u65e0\u97f3\u9891\u8bb2\u8bdd\u72b6\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u65e0\u97f3\u9891\u8bf4\u8bdd\u72b6\u6001\u5206\u5272\u7684\u4e09\u4e2a\u57fa\u7ebf\uff1aa\uff09\u6765\u81ea\u89c6\u9891\uff0cb\uff09\u6765\u81ea\u8eab\u4f53\u52a0\u901f\u5ea6\uff08\u80f8\u6234\u5f0f\u52a0\u901f\u5ea6\u8ba1\uff09\uff0cc\uff09\u6765\u81ea\u8eab\u4f53\u59ff\u52bf\u8f68\u8ff9\u3002\u5728\u6240\u6709\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u90fd\u4f1a\u9884\u6d4b\u4ece\u97f3\u9891\u4e2d\u63d0\u53d6\u7684 20Hz \u4e8c\u8fdb\u5236\u8bf4\u8bdd\u72b6\u6001\u4fe1\u53f7\uff0c\u8fd9\u662f\u4ee5\u524d\u6570\u636e\u96c6\u4e2d\u4e0d\u53ef\u7528\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u3002\u9664\u4e86\u63d0\u4f9b\u8bc4\u4f30\u5404\u79cd\u8bf4\u8bdd\u72b6\u6001\u68c0\u6d4b\u65b9\u6cd5\u6240\u9700\u7684\u4fe1\u53f7\u548c\u57fa\u672c\u4e8b\u5b9e\u4e4b\u5916\uff0cREWIND \u4e2d\u97f3\u9891\u7684\u53ef\u7528\u6027\u4f7f\u5176\u9002\u5408\u8de8\u6a21\u6001\u7814\u7a76\uff0c\u800c\u8fd9\u5bf9\u4e8e\u4ee5\u524d\u7684\u6df7\u5408\u6570\u636e\u96c6\u6765\u8bf4\u662f\u4e0d\u53ef\u884c\u7684\u3002\u6700\u540e\uff0c\u6211\u4eec\u7075\u6d3b\u7684\u6570\u636e\u540c\u610f\u8bbe\u7f6e\u4e3a\u7f3a\u5c11\u6a21\u5f0f\u7684\u591a\u6a21\u5f0f\u7cfb\u7edf\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002|[2403.01229v1](http://arxiv.org/pdf/2403.01229v1)|null|\n", "2403.01118": "|**2024-03-02**|**Adversarial Testing for Visual Grounding via Image-Aware Property Reduction**|\u901a\u8fc7\u56fe\u50cf\u611f\u77e5\u5c5e\u6027\u51cf\u5c11\u8fdb\u884c\u89c6\u89c9\u63a5\u5730\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5|Zhiyuan Chang, Mingyang Li, Junjie Wang, Cheng Li, Boyu Wu, Fanjiang Xu, Qing Wang|Due to the advantages of fusing information from various modalities, multimodal learning is gaining increasing attention. Being a fundamental task of multimodal learning, Visual Grounding (VG), aims to locate objects in images through natural language expressions. Ensuring the quality of VG models presents significant challenges due to the complex nature of the task. In the black box scenario, existing adversarial testing techniques often fail to fully exploit the potential of both modalities of information. They typically apply perturbations based solely on either the image or text information, disregarding the crucial correlation between the two modalities, which would lead to failures in test oracles or an inability to effectively challenge VG models. To this end, we propose PEELING, a text perturbation approach via image-aware property reduction for adversarial testing of the VG model. The core idea is to reduce the property-related information in the original expression meanwhile ensuring the reduced expression can still uniquely describe the original object in the image. To achieve this, PEELING first conducts the object and properties extraction and recombination to generate candidate property reduction expressions. It then selects the satisfied expressions that accurately describe the original object while ensuring no other objects in the image fulfill the expression, through querying the image with a visual understanding technique. We evaluate PEELING on the state-of-the-art VG model, i.e. OFA-VG, involving three commonly used datasets. Results show that the adversarial tests generated by PEELING achieves 21.4% in MultiModal Impact score (MMI), and outperforms state-of-the-art baselines for images and texts by 8.2%--15.1%.|\u7531\u4e8e\u878d\u5408\u6765\u81ea\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\u7684\u4f18\u52bf\uff0c\u591a\u6a21\u6001\u5b66\u4e60\u8d8a\u6765\u8d8a\u53d7\u5230\u5173\u6ce8\u3002\u89c6\u89c9\u63a5\u5730\uff08VG\uff09\u662f\u591a\u6a21\u6001\u5b66\u4e60\u7684\u4e00\u9879\u57fa\u672c\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u6765\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u3002\u7531\u4e8e\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u786e\u4fdd VG \u6a21\u578b\u7684\u8d28\u91cf\u9762\u4e34\u7740\u5de8\u5927\u7684\u6311\u6218\u3002\u5728\u9ed1\u76d2\u573a\u666f\u4e2d\uff0c\u73b0\u6709\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5\u6280\u672f\u901a\u5e38\u65e0\u6cd5\u5145\u5206\u5229\u7528\u8fd9\u4e24\u79cd\u4fe1\u606f\u6a21\u5f0f\u7684\u6f5c\u529b\u3002\u4ed6\u4eec\u901a\u5e38\u4ec5\u6839\u636e\u56fe\u50cf\u6216\u6587\u672c\u4fe1\u606f\u5e94\u7528\u6270\u52a8\uff0c\u800c\u5ffd\u7565\u4e24\u79cd\u6a21\u5f0f\u4e4b\u95f4\u7684\u5173\u952e\u76f8\u5173\u6027\uff0c\u8fd9\u5c06\u5bfc\u81f4\u6d4b\u8bd5\u9884\u8a00\u5931\u8d25\u6216\u65e0\u6cd5\u6709\u6548\u6311\u6218 VG \u6a21\u578b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 PEELING\uff0c\u4e00\u79cd\u901a\u8fc7\u56fe\u50cf\u611f\u77e5\u5c5e\u6027\u51cf\u5c11\u6765\u8fdb\u884c VG \u6a21\u578b\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5\u7684\u6587\u672c\u6270\u52a8\u65b9\u6cd5\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u51cf\u5c11\u539f\u59cb\u8868\u8fbe\u5f0f\u4e2d\u4e0e\u5c5e\u6027\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u8bc1\u51cf\u5c11\u540e\u7684\u8868\u8fbe\u5f0f\u4ecd\u7136\u80fd\u591f\u552f\u4e00\u5730\u63cf\u8ff0\u56fe\u50cf\u4e2d\u7684\u539f\u59cb\u5bf9\u8c61\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0cPEELING \u9996\u5148\u8fdb\u884c\u5bf9\u8c61\u548c\u5c5e\u6027\u63d0\u53d6\u548c\u91cd\u7ec4\uff0c\u4ee5\u751f\u6210\u5019\u9009\u5c5e\u6027\u7ea6\u7b80\u8868\u8fbe\u5f0f\u3002\u7136\u540e\uff0c\u901a\u8fc7\u4f7f\u7528\u89c6\u89c9\u7406\u89e3\u6280\u672f\u67e5\u8be2\u56fe\u50cf\uff0c\u9009\u62e9\u51c6\u786e\u63cf\u8ff0\u539f\u59cb\u5bf9\u8c61\u7684\u6ee1\u610f\u8868\u8fbe\u5f0f\uff0c\u540c\u65f6\u786e\u4fdd\u56fe\u50cf\u4e2d\u6ca1\u6709\u5176\u4ed6\u5bf9\u8c61\u6ee1\u8db3\u8be5\u8868\u8fbe\u5f0f\u3002\u6211\u4eec\u5728\u6700\u5148\u8fdb\u7684 VG \u6a21\u578b\uff08\u5373 OFA-VG\uff09\u4e0a\u8bc4\u4f30 PEELING\uff0c\u6d89\u53ca\u4e09\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u3002\u7ed3\u679c\u663e\u793a\uff0cPEELING \u751f\u6210\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5\u5728\u591a\u6a21\u5f0f\u5f71\u54cd\u8bc4\u5206 (MMI) \u4e2d\u8fbe\u5230\u4e86 21.4%\uff0c\u6bd4\u56fe\u50cf\u548c\u6587\u672c\u7684\u6700\u65b0\u57fa\u7ebf\u9ad8\u51fa 8.2%--15.1%\u3002|[2403.01118v1](http://arxiv.org/pdf/2403.01118v1)|null|\n"}, "Nerf": {"2403.01325": "|**2024-03-02**|**NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning**|NeRF-VPT\uff1a\u901a\u8fc7\u89c6\u56fe\u63d0\u793a\u8c03\u6574\u5b66\u4e60\u5177\u6709\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u65b0\u9896\u89c6\u56fe\u8868\u793a|Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr|Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis. Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge. While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement. In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality. NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods. By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods. Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis. The source code and dataset are available at \\url{https://github.com/Freedomcls/NeRF-VPT}.|\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u5728\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u529f\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u4e3a\u65b0\u9896\u7684\u89c6\u89d2\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u4efb\u52a1\u4ecd\u7136\u662f\u4e00\u9879\u4e25\u5cfb\u7684\u6311\u6218\u3002\u867d\u7136\u73b0\u6709\u7684\u52aa\u529b\u5df2\u7ecf\u53d6\u5f97\u4e86\u503c\u5f97\u79f0\u8d5e\u7684\u8fdb\u5c55\uff0c\u4f46\u6355\u83b7\u590d\u6742\u7684\u7ec6\u8282\u3001\u589e\u5f3a\u7eb9\u7406\u5e76\u5b9e\u73b0\u5353\u8d8a\u7684\u5cf0\u503c\u4fe1\u566a\u6bd4 (PSNR) \u6307\u6807\u503c\u5f97\u8fdb\u4e00\u6b65\u5173\u6ce8\u548c\u8fdb\u6b65\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 NeRF-VPT\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u521b\u65b0\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u7684 NeRF-VPT \u91c7\u7528\u7ea7\u8054\u89c6\u56fe\u63d0\u793a\u8c03\u6574\u8303\u4f8b\uff0c\u5176\u4e2d\u4ece\u5148\u524d\u6e32\u67d3\u7ed3\u679c\u83b7\u5f97\u7684 RGB \u4fe1\u606f\u4f5c\u4e3a\u540e\u7eed\u6e32\u67d3\u9636\u6bb5\u7684\u6307\u5bfc\u6027\u89c6\u89c9\u63d0\u793a\uff0c\u5e0c\u671b\u5d4c\u5165\u5728\u63d0\u793a\u4e2d\u7684\u5148\u9a8c\u77e5\u8bc6\u53ef\u4ee5\u4fc3\u8fdb\u6e32\u67d3\u56fe\u50cf\u7684\u9010\u6b65\u589e\u5f3a\u8d28\u91cf\u3002 NeRF-VPT \u53ea\u9700\u8981\u5728\u6bcf\u4e2a\u8bad\u7ec3\u9636\u6bb5\u4ece\u524d\u4e00\u9636\u6bb5\u6e32\u67d3\u4e2d\u91c7\u6837 RGB \u6570\u636e\u4f5c\u4e3a\u5148\u9a8c\uff0c\u65e0\u9700\u4f9d\u8d56\u989d\u5916\u7684\u6307\u5bfc\u6216\u590d\u6742\u7684\u6280\u672f\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684 NeRF-VPT \u662f\u5373\u63d2\u5373\u7528\u7684\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u65b9\u6cd5\u4e2d\u3002\u901a\u8fc7\u5bf9\u6211\u4eec\u7684 NeRF-VPT \u4e0e\u51e0\u79cd\u57fa\u4e8e NeRF \u7684\u65b9\u6cd5\u5728\u8981\u6c42\u4e25\u683c\u7684\u771f\u5b9e\u573a\u666f\u57fa\u51c6\u4e0a\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\uff0c\u4f8b\u5982 Realistic Synthetic 360\u3001Real Forward-Facing\u3001Replica \u6570\u636e\u96c6\u548c\u7528\u6237\u6355\u83b7\u7684\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684 NeRF-VPT\u4e0e\u6240\u6709\u6bd4\u8f83\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u57fa\u7ebf\u6027\u80fd\uff0c\u5e76\u80fd\u719f\u7ec3\u5730\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u65b0\u9896\u89c6\u56fe\u56fe\u50cf\u3002\u6b64\u5916\uff0cNeRF-VPT \u7684\u7ea7\u8054\u5b66\u4e60\u5f15\u5165\u4e86\u5bf9\u7a00\u758f\u8f93\u5165\u573a\u666f\u7684\u9002\u5e94\u6027\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u4e86\u7a00\u758f\u89c6\u56fe\u65b0\u9896\u89c6\u56fe\u5408\u6210\u7684\u51c6\u786e\u6027\u3002\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u5728 \\url{https://github.com/Freedomcls/NeRF-VPT} \u83b7\u53d6\u3002|[2403.01325v1](http://arxiv.org/pdf/2403.01325v1)|**[link](https://github.com/freedomcls/nerf-vpt)**|\n", "2403.01137": "|**2024-03-02**|**Neural radiance fields-based holography [Invited]**|\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u5168\u606f\u672f [\u9080\u8bf7]|Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba|This study presents a novel approach for generating holograms based on the neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data is difficult in hologram computation. NeRF is a state-of-the-art technique for 3D light-field reconstruction from 2D images based on volume rendering. The NeRF can rapidly predict new-view images that do not include a training dataset. In this study, we constructed a rendering pipeline directly from a 3D light field generated from 2D images by NeRF for hologram generation using deep neural networks within a reasonable time. The pipeline comprises three main components: the NeRF, a depth predictor, and a hologram generator, all constructed using deep neural networks. The pipeline does not include any physical calculations. The predicted holograms of a 3D scene viewed from any direction were computed using the proposed pipeline. The simulation and experimental results are presented.|\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u6280\u672f\u751f\u6210\u5168\u606f\u56fe\u7684\u65b0\u65b9\u6cd5\u3002\u5728\u5168\u606f\u56fe\u8ba1\u7b97\u4e2d\u751f\u6210\u4e09\u7ef4\uff083D\uff09\u6570\u636e\u5f88\u56f0\u96be\u3002 NeRF \u662f\u4e00\u79cd\u57fa\u4e8e\u4f53\u6e32\u67d3\u4ece 2D \u56fe\u50cf\u91cd\u5efa 3D \u5149\u573a\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002 NeRF \u53ef\u4ee5\u5feb\u901f\u9884\u6d4b\u4e0d\u5305\u542b\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u65b0\u89c6\u56fe\u56fe\u50cf\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u76f4\u63a5\u6839\u636e NeRF \u4ece 2D \u56fe\u50cf\u751f\u6210\u7684 3D \u5149\u573a\u6784\u5efa\u4e86\u6e32\u67d3\u7ba1\u9053\uff0c\u4ee5\u4fbf\u5728\u5408\u7406\u7684\u65f6\u95f4\u5185\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u5168\u606f\u56fe\u3002\u8be5\u7ba1\u9053\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1aNeRF\u3001\u6df1\u5ea6\u9884\u6d4b\u5668\u548c\u5168\u606f\u56fe\u751f\u6210\u5668\uff0c\u6240\u6709\u7ec4\u4ef6\u5747\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u3002\u8be5\u7ba1\u9053\u4e0d\u5305\u62ec\u4efb\u4f55\u7269\u7406\u8ba1\u7b97\u3002\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u7ba1\u9053\u8ba1\u7b97\u4ece\u4efb\u4f55\u65b9\u5411\u89c2\u770b\u7684 3D \u573a\u666f\u7684\u9884\u6d4b\u5168\u606f\u56fe\u3002\u7ed9\u51fa\u4e86\u6a21\u62df\u548c\u5b9e\u9a8c\u7ed3\u679c\u3002|[2403.01137v1](http://arxiv.org/pdf/2403.01137v1)|null|\n", "2403.01058": "|**2024-03-02**|**Neural Field Classifiers via Target Encoding and Classification Loss**|\u901a\u8fc7\u76ee\u6807\u7f16\u7801\u548c\u5206\u7c7b\u635f\u5931\u7684\u795e\u7ecf\u573a\u5206\u7c7b\u5668|Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun|Neural field methods have seen great progress in various long-standing tasks in computer vision and computer graphics, including novel view synthesis and geometry reconstruction. As existing neural field methods try to predict some coordinate-based continuous target values, such as RGB for Neural Radiance Field (NeRF), all of these methods are regression models and are optimized by some regression loss. However, are regression models really better than classification models for neural field methods? In this work, we try to visit this very fundamental but overlooked question for neural fields from a machine learning perspective. We successfully propose a novel Neural Field Classifier (NFC) framework which formulates existing neural field methods as classification tasks rather than regression tasks. The proposed NFC can easily transform arbitrary Neural Field Regressor (NFR) into its classification variant via employing a novel Target Encoding module and optimizing a classification loss. By encoding a continuous regression target into a high-dimensional discrete encoding, we naturally formulate a multi-label classification task. Extensive experiments demonstrate the impressive effectiveness of NFC at the nearly free extra computational costs. Moreover, NFC also shows robustness to sparse inputs, corrupted images, and dynamic scenes.|\u795e\u7ecf\u573a\u65b9\u6cd5\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u7684\u5404\u79cd\u957f\u671f\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\uff0c\u5305\u62ec\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u548c\u51e0\u4f55\u91cd\u5efa\u3002\u7531\u4e8e\u73b0\u6709\u7684\u795e\u7ecf\u573a\u65b9\u6cd5\u8bd5\u56fe\u9884\u6d4b\u4e00\u4e9b\u57fa\u4e8e\u5750\u6807\u7684\u8fde\u7eed\u76ee\u6807\u503c\uff0c\u4f8b\u5982\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684RGB\uff0c\u6240\u6709\u8fd9\u4e9b\u65b9\u6cd5\u90fd\u662f\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4e00\u4e9b\u56de\u5f52\u635f\u5931\u8fdb\u884c\u4f18\u5316\u3002\u7136\u800c\uff0c\u56de\u5f52\u6a21\u578b\u771f\u7684\u6bd4\u795e\u7ecf\u573a\u65b9\u6cd5\u7684\u5206\u7c7b\u6a21\u578b\u66f4\u597d\u5417\uff1f\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c1d\u8bd5\u4ece\u673a\u5668\u5b66\u4e60\u7684\u89d2\u5ea6\u63a2\u8ba8\u795e\u7ecf\u9886\u57df\u8fd9\u4e2a\u975e\u5e38\u57fa\u672c\u4f46\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\u3002\u6211\u4eec\u6210\u529f\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u573a\u5206\u7c7b\u5668\uff08NFC\uff09\u6846\u67b6\uff0c\u5b83\u5c06\u73b0\u6709\u7684\u795e\u7ecf\u573a\u65b9\u6cd5\u5236\u5b9a\u4e3a\u5206\u7c7b\u4efb\u52a1\u800c\u4e0d\u662f\u56de\u5f52\u4efb\u52a1\u3002\u901a\u8fc7\u91c7\u7528\u65b0\u9896\u7684\u76ee\u6807\u7f16\u7801\u6a21\u5757\u5e76\u4f18\u5316\u5206\u7c7b\u635f\u5931\uff0c\u6240\u63d0\u51fa\u7684 NFC \u53ef\u4ee5\u8f7b\u677e\u5730\u5c06\u4efb\u610f\u795e\u7ecf\u573a\u56de\u5f52\u5668 (NFR) \u8f6c\u6362\u4e3a\u5176\u5206\u7c7b\u53d8\u4f53\u3002\u901a\u8fc7\u5c06\u8fde\u7eed\u56de\u5f52\u76ee\u6807\u7f16\u7801\u4e3a\u9ad8\u7ef4\u79bb\u6563\u7f16\u7801\uff0c\u6211\u4eec\u81ea\u7136\u5730\u5236\u5b9a\u4e86\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86 NFC \u7684\u6709\u6548\u6027\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u5e76\u4e14\u51e0\u4e4e\u4e0d\u9700\u8981\u989d\u5916\u7684\u8ba1\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0cNFC \u8fd8\u8868\u73b0\u51fa\u5bf9\u7a00\u758f\u8f93\u5165\u3001\u635f\u574f\u56fe\u50cf\u548c\u52a8\u6001\u573a\u666f\u7684\u9c81\u68d2\u6027\u3002|[2403.01058v1](http://arxiv.org/pdf/2403.01058v1)|null|\n"}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2403.01329": "|**2024-03-02**|**Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models**|\u7528\u4e8e\u6269\u6563\u548c\u6d41\u52a8\u6a21\u578b\u5feb\u901f\u91c7\u6837\u7684\u5b9a\u5236\u975e\u5e73\u7a33\u6c42\u89e3\u5668|Neta Shaul, Uriel Singer, Ricky T. Q. Chen, Matthew Le, Ali Thabet, Albert Pumarola, Yaron Lipman|This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver distillation approach to improve sample efficiency of Diffusion and Flow models. BNS solvers are based on a family of non-stationary solvers that provably subsumes existing numerical ODE solvers and consequently demonstrate considerable improvement in sample approximation (PSNR) over these baselines. Compared to model distillation, BNS solvers benefit from a tiny parameter space ($<$200 parameters), fast optimization (two orders of magnitude faster), maintain diversity of samples, and in contrast to previous solver distillation approaches nearly close the gap from standard distillation methods such as Progressive Distillation in the low-medium NFE regime. For example, BNS solver achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We experimented with BNS solvers for conditional image generation, text-to-image generation, and text-2-audio generation showing significant improvement in sample approximation (PSNR) in all.|\u672c\u6587\u4ecb\u7ecd\u4e86\u5b9a\u5236\u975e\u5e73\u7a33 (BNS) \u6c42\u89e3\u5668\uff0c\u8fd9\u662f\u4e00\u79cd\u6c42\u89e3\u5668\u84b8\u998f\u65b9\u6cd5\uff0c\u53ef\u63d0\u9ad8\u6269\u6563\u548c\u6d41\u52a8\u6a21\u578b\u7684\u6837\u672c\u6548\u7387\u3002 BNS \u6c42\u89e3\u5668\u57fa\u4e8e\u4e00\u7cfb\u5217\u975e\u5e73\u7a33\u6c42\u89e3\u5668\uff0c\u8fd9\u4e9b\u6c42\u89e3\u5668\u53ef\u8bc1\u660e\u5305\u542b\u73b0\u6709\u7684\u6570\u503c ODE \u6c42\u89e3\u5668\uff0c\u56e0\u6b64\u5728\u6837\u672c\u8fd1\u4f3c (PSNR) \u65b9\u9762\u6bd4\u8fd9\u4e9b\u57fa\u7ebf\u8868\u73b0\u51fa\u663e\u7740\u6539\u8fdb\u3002\u4e0e\u6a21\u578b\u84b8\u998f\u76f8\u6bd4\uff0cBNS \u6c42\u89e3\u5668\u53d7\u76ca\u4e8e\u5fae\u5c0f\u7684\u53c2\u6570\u7a7a\u95f4\uff08$<200 \u53c2\u6570\uff09\u3001\u5feb\u901f\u4f18\u5316\uff08\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\uff09\u3001\u4fdd\u6301\u6837\u672c\u7684\u591a\u6837\u6027\uff0c\u5e76\u4e14\u4e0e\u4e4b\u524d\u7684\u6c42\u89e3\u5668\u84b8\u998f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u51e0\u4e4e\u7f29\u5c0f\u4e86\u4e0e\u6807\u51c6\u84b8\u998f\u7684\u5dee\u8ddd\u65b9\u6cd5\uff0c\u4f8b\u5982\u4f4e-\u4e2d NFE \u4f53\u7cfb\u4e2d\u7684\u6e10\u8fdb\u84b8\u998f\u3002\u4f8b\u5982\uff0cBNS \u6c42\u89e3\u5668\u5728\u7c7b\u6761\u4ef6 ImageNet-64 \u4e2d\u4f7f\u7528 16 NFE \u5b9e\u73b0 45 PSNR / 1.76 FID\u3002\u6211\u4eec\u5c1d\u8bd5\u4f7f\u7528 BNS \u6c42\u89e3\u5668\u8fdb\u884c\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u3001\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u6587\u672c 2 \u97f3\u9891\u751f\u6210\uff0c\u7ed3\u679c\u663e\u793a\u6837\u672c\u8fd1\u4f3c (PSNR) \u603b\u4f53\u4e0a\u6709\u663e\u7740\u6539\u5584\u3002|[2403.01329v1](http://arxiv.org/pdf/2403.01329v1)|null|\n", "2403.01238": "|**2024-03-02**|**On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving**|\u8d70\u5411\u4fbf\u643a\u6027\uff1a\u538b\u7f29\u81ea\u52a8\u9a7e\u9a76\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u5668|Kaituo Feng, Changsheng Li, Dongchun Ren, Ye Yuan, Guoren Wang|End-to-end motion planning models equipped with deep neural networks have shown great potential for enabling full autonomous driving. However, the oversized neural networks render them impractical for deployment on resource-constrained systems, which unavoidably requires more computational time and resources during reference.To handle this, knowledge distillation offers a promising approach that compresses models by enabling a smaller student model to learn from a larger teacher model. Nevertheless, how to apply knowledge distillation to compress motion planners has not been explored so far. In this paper, we propose PlanKD, the first knowledge distillation framework tailored for compressing end-to-end motion planners. First, considering that driving scenes are inherently complex, often containing planning-irrelevant or even noisy information, transferring such information is not beneficial for the student planner. Thus, we design an information bottleneck based strategy to only distill planning-relevant information, rather than transfer all information indiscriminately. Second, different waypoints in an output planned trajectory may hold varying degrees of importance for motion planning, where a slight deviation in certain crucial waypoints might lead to a collision. Therefore, we devise a safety-aware waypoint-attentive distillation module that assigns adaptive weights to different waypoints based on the importance, to encourage the student to accurately mimic more crucial waypoints, thereby improving overall safety. Experiments demonstrate that our PlanKD can boost the performance of smaller planners by a large margin, and significantly reduce their reference time.|\u914d\u5907\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u6a21\u578b\u5df2\u663e\u793a\u51fa\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\u7684\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u8fc7\u5927\u7684\u795e\u7ecf\u7f51\u7edc\u4f7f\u5f97\u5b83\u4eec\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7cfb\u7edf\u4e0a\u90e8\u7f72\u4e0d\u5207\u5b9e\u9645\uff0c\u8fd9\u5728\u53c2\u8003\u8fc7\u7a0b\u4e2d\u4e0d\u53ef\u907f\u514d\u5730\u9700\u8981\u66f4\u591a\u7684\u8ba1\u7b97\u65f6\u95f4\u548c\u8d44\u6e90\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u8f83\u5c0f\u7684\u5b66\u751f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u6765\u538b\u7f29\u6a21\u578b\u66f4\u5927\u7684\u6559\u5e08\u6a21\u578b\u3002\u7136\u800c\uff0c\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u5982\u4f55\u5e94\u7528\u77e5\u8bc6\u84b8\u998f\u6765\u538b\u7f29\u8fd0\u52a8\u89c4\u5212\u5668\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 PlanKD\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e3a\u538b\u7f29\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u5668\u800c\u5b9a\u5236\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u3002\u9996\u5148\uff0c\u8003\u8651\u5230\u9a7e\u9a76\u573a\u666f\u672c\u8d28\u4e0a\u5f88\u590d\u6742\uff0c\u901a\u5e38\u5305\u542b\u4e0e\u89c4\u5212\u65e0\u5173\u751a\u81f3\u5608\u6742\u7684\u4fe1\u606f\uff0c\u4f20\u8f93\u6b64\u7c7b\u4fe1\u606f\u5bf9\u5b66\u751f\u89c4\u5212\u8005\u6765\u8bf4\u5e76\u6ca1\u6709\u597d\u5904\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7684\u7b56\u7565\uff0c\u4ec5\u63d0\u53d6\u4e0e\u89c4\u5212\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u800c\u4e0d\u662f\u4e0d\u52a0\u533a\u522b\u5730\u4f20\u8f93\u6240\u6709\u4fe1\u606f\u3002\u5176\u6b21\uff0c\u8f93\u51fa\u89c4\u5212\u8f68\u8ff9\u4e2d\u7684\u4e0d\u540c\u822a\u8def\u70b9\u5bf9\u4e8e\u8fd0\u52a8\u89c4\u5212\u53ef\u80fd\u5177\u6709\u4e0d\u540c\u7a0b\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u5176\u4e2d\u67d0\u4e9b\u5173\u952e\u822a\u8def\u70b9\u7684\u8f7b\u5fae\u504f\u5dee\u53ef\u80fd\u4f1a\u5bfc\u81f4\u78b0\u649e\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b89\u5168\u610f\u8bc6\u822a\u8def\u70b9\u5173\u6ce8\u84b8\u998f\u6a21\u5757\uff0c\u6839\u636e\u91cd\u8981\u6027\u4e3a\u4e0d\u540c\u822a\u8def\u70b9\u5206\u914d\u81ea\u9002\u5e94\u6743\u91cd\uff0c\u4ee5\u9f13\u52b1\u5b66\u751f\u51c6\u786e\u6a21\u4eff\u66f4\u5173\u952e\u7684\u822a\u8def\u70b9\uff0c\u4ece\u800c\u63d0\u9ad8\u6574\u4f53\u5b89\u5168\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 PlanKD \u53ef\u4ee5\u5927\u5e45\u63d0\u9ad8\u5c0f\u578b\u89c4\u5212\u5668\u7684\u6027\u80fd\uff0c\u5e76\u663e\u7740\u51cf\u5c11\u5176\u53c2\u8003\u65f6\u95f4\u3002|[2403.01238v1](http://arxiv.org/pdf/2403.01238v1)|null|\n", "2403.01076": "|**2024-03-02**|**Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification for OOD Detection**|\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4ece\u91cf\u5316\u7f51\u7edc\u4e2d\u63d0\u53d6\u53ef\u7528\u9884\u6d4b\u4ee5\u8fdb\u884c OOD \u68c0\u6d4b|Rishi Singhal, Srinath Srinivasan|OOD detection has become more pertinent with advances in network design and increased task complexity. Identifying which parts of the data a given network is misclassifying has become as valuable as the network's overall performance. We can compress the model with quantization, but it suffers minor performance loss. The loss of performance further necessitates the need to derive the confidence estimate of the network's predictions. In line with this thinking, we introduce an Uncertainty Quantification(UQ) technique to quantify the uncertainty in the predictions from a pre-trained vision model. We subsequently leverage this information to extract valuable predictions while ignoring the non-confident predictions. We observe that our technique saves up to 80% of ignored samples from being misclassified. The code for the same is available here.|\u968f\u7740\u7f51\u7edc\u8bbe\u8ba1\u7684\u8fdb\u6b65\u548c\u200b\u200b\u4efb\u52a1\u590d\u6742\u6027\u7684\u589e\u52a0\uff0cOOD \u68c0\u6d4b\u53d8\u5f97\u66f4\u52a0\u76f8\u5173\u3002\u8bc6\u522b\u7ed9\u5b9a\u7f51\u7edc\u7684\u6570\u636e\u7684\u54ea\u4e9b\u90e8\u5206\u88ab\u9519\u8bef\u5206\u7c7b\u5df2\u7ecf\u53d8\u5f97\u4e0e\u7f51\u7edc\u7684\u6574\u4f53\u6027\u80fd\u4e00\u6837\u6709\u4ef7\u503c\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u91cf\u5316\u6765\u538b\u7f29\u6a21\u578b\uff0c\u4f46\u5b83\u7684\u6027\u80fd\u635f\u5931\u5f88\u5c0f\u3002\u6027\u80fd\u635f\u5931\u8fdb\u4e00\u6b65\u9700\u8981\u5bfc\u51fa\u7f51\u7edc\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002\u6839\u636e\u8fd9\u79cd\u60f3\u6cd5\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u6280\u672f\u6765\u91cf\u5316\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u968f\u540e\uff0c\u6211\u4eec\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6765\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u5ffd\u7565\u4e0d\u53ef\u4fe1\u7684\u9884\u6d4b\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u6211\u4eec\u7684\u6280\u672f\u53ef\u4ee5\u907f\u514d\u9ad8\u8fbe 80% \u7684\u88ab\u5ffd\u7565\u6837\u672c\u88ab\u9519\u8bef\u5206\u7c7b\u3002\u6b64\u5904\u63d0\u4f9b\u4e86\u76f8\u540c\u7684\u4ee3\u7801\u3002|[2403.01076v1](http://arxiv.org/pdf/2403.01076v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2403.01310": "|**2024-03-02**|**Image-Based Dietary Assessment: A Healthy Eating Plate Estimation System**|\u57fa\u4e8e\u56fe\u50cf\u7684\u996e\u98df\u8bc4\u4f30\uff1a\u5065\u5eb7\u996e\u98df\u9910\u76d8\u4f30\u8ba1\u7cfb\u7edf|Assylzhan Izbassar, Pakizar Shamoi|The nutritional quality of diets has significantly deteriorated over the past two to three decades, a decline often underestimated by the people. This deterioration, coupled with a hectic lifestyle, has contributed to escalating health concerns. Recognizing this issue, researchers at Harvard have advocated for a balanced nutritional plate model to promote health. Inspired by this research, our paper introduces an innovative Image-Based Dietary Assessment system aimed at evaluating the healthiness of meals through image analysis. Our system employs advanced image segmentation and classification techniques to analyze food items on a plate, assess their proportions, and calculate meal adherence to Harvard's healthy eating recommendations. This approach leverages machine learning and nutritional science to empower individuals with actionable insights for healthier eating choices. Our four-step framework involves segmenting the image, classifying the items, conducting a nutritional assessment based on the Harvard Healthy Eating Plate research, and offering tailored recommendations. The prototype system has shown promising results in promoting healthier eating habits by providing an accessible, evidence-based tool for dietary assessment.|\u5728\u8fc7\u53bb\u7684\u4e24\u5230\u4e09\u5341\u5e74\u91cc\uff0c\u996e\u98df\u7684\u8425\u517b\u8d28\u91cf\u663e\u7740\u6076\u5316\uff0c\u4eba\u4eec\u5e38\u5e38\u4f4e\u4f30\u4e86\u8fd9\u79cd\u4e0b\u964d\u3002\u8fd9\u79cd\u6076\u5316\uff0c\u52a0\u4e0a\u5fd9\u788c\u7684\u751f\u6d3b\u65b9\u5f0f\uff0c\u5bfc\u81f4\u5065\u5eb7\u95ee\u9898\u4e0d\u65ad\u5347\u7ea7\u3002\u8ba4\u8bc6\u5230\u8fd9\u4e2a\u95ee\u9898\uff0c\u54c8\u4f5b\u5927\u5b66\u7684\u7814\u7a76\u4eba\u5458\u63d0\u5021\u91c7\u7528\u5747\u8861\u8425\u517b\u9910\u76d8\u6a21\u5f0f\u6765\u4fc3\u8fdb\u5065\u5eb7\u3002\u53d7\u8fd9\u9879\u7814\u7a76\u7684\u542f\u53d1\uff0c\u6211\u4eec\u7684\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u996e\u98df\u8bc4\u4f30\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u56fe\u50cf\u5206\u6790\u8bc4\u4f30\u81b3\u98df\u7684\u5065\u5eb7\u7a0b\u5ea6\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u91c7\u7528\u5148\u8fdb\u7684\u56fe\u50cf\u5206\u5272\u548c\u5206\u7c7b\u6280\u672f\u6765\u5206\u6790\u76d8\u5b50\u4e0a\u7684\u98df\u7269\uff0c\u8bc4\u4f30\u5176\u6bd4\u4f8b\uff0c\u5e76\u8ba1\u7b97\u81b3\u98df\u662f\u5426\u7b26\u5408\u54c8\u4f5b\u5927\u5b66\u7684\u5065\u5eb7\u996e\u98df\u5efa\u8bae\u3002\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u8425\u517b\u79d1\u5b66\uff0c\u4e3a\u4e2a\u4eba\u63d0\u4f9b\u53ef\u884c\u7684\u89c1\u89e3\uff0c\u4ee5\u505a\u51fa\u66f4\u5065\u5eb7\u7684\u996e\u98df\u9009\u62e9\u3002\u6211\u4eec\u7684\u56db\u6b65\u6846\u67b6\u5305\u62ec\u5206\u5272\u56fe\u50cf\u3001\u5bf9\u9879\u76ee\u8fdb\u884c\u5206\u7c7b\u3001\u6839\u636e\u54c8\u4f5b\u5065\u5eb7\u996e\u98df\u9910\u76d8\u7814\u7a76\u8fdb\u884c\u8425\u517b\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u91cf\u8eab\u5b9a\u5236\u7684\u5efa\u8bae\u3002\u8be5\u539f\u578b\u7cfb\u7edf\u901a\u8fc7\u63d0\u4f9b\u4e00\u79cd\u6613\u4e8e\u4f7f\u7528\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u996e\u98df\u8bc4\u4f30\u5de5\u5177\uff0c\u5728\u4fc3\u8fdb\u66f4\u5065\u5eb7\u7684\u996e\u98df\u4e60\u60ef\u65b9\u9762\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6210\u679c\u3002|[2403.01310v1](http://arxiv.org/pdf/2403.01310v1)|null|\n", "2403.01300": "|**2024-03-02**|**Causal Mode Multiplexer: A Novel Framework for Unbiased Multispectral Pedestrian Detection**|\u56e0\u679c\u6a21\u5f0f\u591a\u8def\u590d\u7528\u5668\uff1a\u65e0\u504f\u591a\u5149\u8c31\u884c\u4eba\u68c0\u6d4b\u7684\u65b0\u9896\u6846\u67b6|Taeheon Kim, Sebin Shin, Youngjoon Yu, Hak Gu Kim, Yong Man Ro|RGBT multispectral pedestrian detection has emerged as a promising solution for safety-critical applications that require day/night operations. However, the modality bias problem remains unsolved as multispectral pedestrian detectors learn the statistical bias in datasets. Specifically, datasets in multispectral pedestrian detection mainly distribute between ROTO (day) and RXTO (night) data; the majority of the pedestrian labels statistically co-occur with their thermal features. As a result, multispectral pedestrian detectors show poor generalization ability on examples beyond this statistical correlation, such as ROTX data. To address this problem, we propose a novel Causal Mode Multiplexer (CMM) framework that effectively learns the causalities between multispectral inputs and predictions. Moreover, we construct a new dataset (ROTX-MP) to evaluate modality bias in multispectral pedestrian detection. ROTX-MP mainly includes ROTX examples not presented in previous datasets. Extensive experiments demonstrate that our proposed CMM framework generalizes well on existing datasets (KAIST, CVC-14, FLIR) and the new ROTX-MP. We will release our new dataset to the public for future research.|RGBT \u591a\u5149\u8c31\u884c\u4eba\u68c0\u6d4b\u5df2\u6210\u4e3a\u9700\u8981\u767d\u5929/\u591c\u95f4\u64cd\u4f5c\u7684\u5b89\u5168\u5173\u952e\u578b\u5e94\u7528\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u968f\u7740\u591a\u5149\u8c31\u884c\u4eba\u68c0\u6d4b\u5668\u5b66\u4e60\u6570\u636e\u96c6\u4e2d\u7684\u7edf\u8ba1\u504f\u5dee\uff0c\u6a21\u6001\u504f\u5dee\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u591a\u5149\u8c31\u884c\u4eba\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u96c6\u4e3b\u8981\u5206\u5e03\u5728ROTO\uff08\u767d\u5929\uff09\u548cRXTO\uff08\u591c\u95f4\uff09\u6570\u636e\u4e4b\u95f4\uff1b\u5927\u591a\u6570\u884c\u4eba\u6807\u7b7e\u5728\u7edf\u8ba1\u4e0a\u4e0e\u5176\u70ed\u7279\u5f81\u540c\u65f6\u51fa\u73b0\u3002\u56e0\u6b64\uff0c\u591a\u5149\u8c31\u884c\u4eba\u68c0\u6d4b\u5668\u5728\u8d85\u51fa\u8fd9\u79cd\u7edf\u8ba1\u76f8\u5173\u6027\u7684\u793a\u4f8b\uff08\u4f8b\u5982 ROTX \u6570\u636e\uff09\u4e0a\u8868\u73b0\u51fa\u8f83\u5dee\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56e0\u679c\u6a21\u5f0f\u590d\u7528\u5668\uff08CMM\uff09\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5b66\u4e60\u591a\u5149\u8c31\u8f93\u5165\u548c\u9884\u6d4b\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff08ROTX-MP\uff09\u6765\u8bc4\u4f30\u591a\u5149\u8c31\u884c\u4eba\u68c0\u6d4b\u4e2d\u7684\u6a21\u6001\u504f\u5dee\u3002 ROTX-MP \u4e3b\u8981\u5305\u62ec\u4ee5\u524d\u6570\u636e\u96c6\u4e2d\u672a\u51fa\u73b0\u7684 ROTX \u793a\u4f8b\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 CMM \u6846\u67b6\u53ef\u4ee5\u5f88\u597d\u5730\u63a8\u5e7f\u73b0\u6709\u6570\u636e\u96c6\uff08KAIST\u3001CVC-14\u3001FLIR\uff09\u548c\u65b0\u7684 ROTX-MP\u3002\u6211\u4eec\u5c06\u5411\u516c\u4f17\u53d1\u5e03\u6211\u4eec\u7684\u65b0\u6570\u636e\u96c6\u4ee5\u4f9b\u672a\u6765\u7814\u7a76\u3002|[2403.01300v1](http://arxiv.org/pdf/2403.01300v1)|null|\n", "2403.01281": "|**2024-03-02**|**Fast Low-parameter Video Activity Localization in Collaborative Learning Environments**|\u534f\u4f5c\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u5feb\u901f\u4f4e\u53c2\u6570\u89c6\u9891\u6d3b\u52a8\u5b9a\u4f4d|Venkatesh Jatla, Sravani Teeparthi, Ugesh Egala, Sylvia Celedon Pattichis, Marios S. Patticis|Research on video activity detection has primarily focused on identifying well-defined human activities in short video segments. The majority of the research on video activity recognition is focused on the development of large parameter systems that require training on large video datasets. This paper develops a low-parameter, modular system with rapid inferencing capabilities that can be trained entirely on limited datasets without requiring transfer learning from large-parameter systems. The system can accurately detect and associate specific activities with the students who perform the activities in real-life classroom videos. Additionally, the paper develops an interactive web-based application to visualize human activity maps over long real-life classroom videos.|\u89c6\u9891\u6d3b\u52a8\u68c0\u6d4b\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8bc6\u522b\u77ed\u89c6\u9891\u7247\u6bb5\u4e2d\u660e\u786e\u5b9a\u4e49\u7684\u4eba\u7c7b\u6d3b\u52a8\u3002\u89c6\u9891\u6d3b\u52a8\u8bc6\u522b\u7684\u5927\u90e8\u5206\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u9700\u8981\u5bf9\u5927\u578b\u89c6\u9891\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u7684\u5927\u53c2\u6570\u7cfb\u7edf\u7684\u5f00\u53d1\u4e0a\u3002\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u5177\u6709\u5feb\u901f\u63a8\u7406\u80fd\u529b\u7684\u4f4e\u53c2\u6570\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5b8c\u5168\u5728\u6709\u9650\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u4e0d\u9700\u8981\u4ece\u5927\u53c2\u6570\u7cfb\u7edf\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u3002\u8be5\u7cfb\u7edf\u53ef\u4ee5\u51c6\u786e\u5730\u68c0\u6d4b\u7279\u5b9a\u6d3b\u52a8\u5e76\u5c06\u5176\u4e0e\u73b0\u5b9e\u8bfe\u5802\u89c6\u9891\u4e2d\u6267\u884c\u6d3b\u52a8\u7684\u5b66\u751f\u5173\u8054\u8d77\u6765\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u7684\u4ea4\u4e92\u5f0f\u5e94\u7528\u7a0b\u5e8f\uff0c\u53ef\u4ee5\u901a\u8fc7\u957f\u957f\u7684\u73b0\u5b9e\u8bfe\u5802\u89c6\u9891\u53ef\u89c6\u5316\u4eba\u7c7b\u6d3b\u52a8\u5730\u56fe\u3002|[2403.01281v1](http://arxiv.org/pdf/2403.01281v1)|null|\n", "2403.01231": "|**2024-03-02**|**Benchmarking Segmentation Models with Mask-Preserved Attribute Editing**|\u4f7f\u7528\u63a9\u6a21\u4fdd\u7559\u5c5e\u6027\u7f16\u8f91\u5bf9\u5206\u5272\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5|Zijin Yin, Kongming Liang, Bing Li, Zhanyu Ma, Jun Guo|When deploying segmentation models in practice, it is critical to evaluate their behaviors in varied and complex scenes. Different from the previous evaluation paradigms only in consideration of global attribute variations (e.g. adverse weather), we investigate both local and global attribute variations for robustness evaluation. To achieve this, we construct a mask-preserved attribute editing pipeline to edit visual attributes of real images with precise control of structural information. Therefore, the original segmentation labels can be reused for the edited images. Using our pipeline, we construct a benchmark covering both object and image attributes (e.g. color, material, pattern, style). We evaluate a broad variety of semantic segmentation models, spanning from conventional close-set models to recent open-vocabulary large models on their robustness to different types of variations. We find that both local and global attribute variations affect segmentation performances, and the sensitivity of models diverges across different variation types. We argue that local attributes have the same importance as global attributes, and should be considered in the robustness evaluation of segmentation models. Code: https://github.com/PRIS-CV/Pascal-EA.|\u5728\u5b9e\u8df5\u4e2d\u90e8\u7f72\u5206\u5272\u6a21\u578b\u65f6\uff0c\u8bc4\u4f30\u5176\u5728\u5404\u79cd\u590d\u6742\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002\u4e0e\u4e4b\u524d\u4ec5\u8003\u8651\u5168\u5c40\u5c5e\u6027\u53d8\u5316\uff08\u4f8b\u5982\u6076\u52a3\u5929\u6c14\uff09\u7684\u8bc4\u4f30\u8303\u5f0f\u4e0d\u540c\uff0c\u6211\u4eec\u540c\u65f6\u7814\u7a76\u5c40\u90e8\u548c\u5168\u5c40\u5c5e\u6027\u53d8\u5316\u4ee5\u8fdb\u884c\u9c81\u68d2\u6027\u8bc4\u4f30\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u4fdd\u7559\u63a9\u6a21\u7684\u5c5e\u6027\u7f16\u8f91\u7ba1\u9053\uff0c\u901a\u8fc7\u7cbe\u786e\u63a7\u5236\u7ed3\u6784\u4fe1\u606f\u6765\u7f16\u8f91\u771f\u5b9e\u56fe\u50cf\u7684\u89c6\u89c9\u5c5e\u6027\u3002\u56e0\u6b64\uff0c\u539f\u59cb\u5206\u5272\u6807\u7b7e\u53ef\u4ee5\u91cd\u65b0\u7528\u4e8e\u7f16\u8f91\u540e\u7684\u56fe\u50cf\u3002\u4f7f\u7528\u6211\u4eec\u7684\u7ba1\u9053\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d6\u5bf9\u8c61\u548c\u56fe\u50cf\u5c5e\u6027\uff08\u4f8b\u5982\u989c\u8272\u3001\u6750\u6599\u3001\u56fe\u6848\u3001\u98ce\u683c\uff09\u7684\u57fa\u51c6\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5404\u79cd\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u4ece\u4f20\u7edf\u7684\u5c01\u95ed\u96c6\u6a21\u578b\u5230\u6700\u8fd1\u7684\u5f00\u653e\u8bcd\u6c47\u5927\u578b\u6a21\u578b\uff0c\u8bc4\u4f30\u5b83\u4eec\u5bf9\u4e0d\u540c\u7c7b\u578b\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u53d1\u73b0\u5c40\u90e8\u548c\u5168\u5c40\u5c5e\u6027\u53d8\u5316\u90fd\u4f1a\u5f71\u54cd\u5206\u5272\u6027\u80fd\uff0c\u5e76\u4e14\u6a21\u578b\u7684\u654f\u611f\u6027\u5728\u4e0d\u540c\u53d8\u5316\u7c7b\u578b\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u3002\u6211\u4eec\u8ba4\u4e3a\u5c40\u90e8\u5c5e\u6027\u4e0e\u5168\u5c40\u5c5e\u6027\u5177\u6709\u76f8\u540c\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e14\u5e94\u8be5\u5728\u5206\u5272\u6a21\u578b\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u4e2d\u8003\u8651\u3002\u4ee3\u7801\uff1ahttps://github.com/PRIS-CV/Pascal-EA\u3002|[2403.01231v1](http://arxiv.org/pdf/2403.01231v1)|**[link](https://github.com/pris-cv/pascal-ea)**|\n", "2403.01214": "|**2024-03-02**|**Boosting Box-supervised Instance Segmentation with Pseudo Depth**|\u4f7f\u7528\u4f2a\u6df1\u5ea6\u589e\u5f3a\u76d2\u76d1\u7763\u5b9e\u4f8b\u5206\u5272|Xinyi Yu, Ling Yan, Pengtao Jiang, Hao Chen, Bo Li, Lin Yuanbo Wu, Linlin Ou|The realm of Weakly Supervised Instance Segmentation (WSIS) under box supervision has garnered substantial attention, showcasing remarkable advancements in recent years. However, the limitations of box supervision become apparent in its inability to furnish effective information for distinguishing foreground from background within the specified target box. This research addresses this challenge by introducing pseudo-depth maps into the training process of the instance segmentation network, thereby boosting its performance by capturing depth differences between instances. These pseudo-depth maps are generated using a readily available depth predictor and are not necessary during the inference stage. To enable the network to discern depth features when predicting masks, we integrate a depth prediction layer into the mask prediction head. This innovative approach empowers the network to simultaneously predict masks and depth, enhancing its ability to capture nuanced depth-related information during the instance segmentation process. We further utilize the mask generated in the training process as supervision to distinguish the foreground from the background. When selecting the best mask for each box through the Hungarian algorithm, we use depth consistency as one calculation cost item. The proposed method achieves significant improvements on Cityscapes and COCO dataset.|\u76d2\u5b50\u76d1\u7763\u4e0b\u7684\u5f31\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\uff08WSIS\uff09\u9886\u57df\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u8fd1\u5e74\u6765\u53d6\u5f97\u4e86\u663e\u7740\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u6846\u76d1\u7763\u7684\u5c40\u9650\u6027\u53d8\u5f97\u663e\u800c\u6613\u89c1\uff0c\u56e0\u4e3a\u5b83\u65e0\u6cd5\u63d0\u4f9b\u6709\u6548\u7684\u4fe1\u606f\u6765\u533a\u5206\u6307\u5b9a\u76ee\u6807\u6846\u4e2d\u7684\u524d\u666f\u548c\u80cc\u666f\u3002\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5c06\u4f2a\u6df1\u5ea6\u56fe\u5f15\u5165\u5b9e\u4f8b\u5206\u5272\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4ece\u800c\u901a\u8fc7\u6355\u83b7\u5b9e\u4f8b\u4e4b\u95f4\u7684\u6df1\u5ea6\u5dee\u5f02\u6765\u63d0\u9ad8\u5176\u6027\u80fd\u3002\u8fd9\u4e9b\u4f2a\u6df1\u5ea6\u56fe\u662f\u4f7f\u7528\u73b0\u6210\u7684\u6df1\u5ea6\u9884\u6d4b\u5668\u751f\u6210\u7684\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u9636\u6bb5\u4e0d\u662f\u5fc5\u9700\u7684\u3002\u4e3a\u4e86\u4f7f\u7f51\u7edc\u5728\u9884\u6d4b\u63a9\u6a21\u65f6\u80fd\u591f\u8bc6\u522b\u6df1\u5ea6\u7279\u5f81\uff0c\u6211\u4eec\u5c06\u6df1\u5ea6\u9884\u6d4b\u5c42\u96c6\u6210\u5230\u63a9\u6a21\u9884\u6d4b\u5934\u4e2d\u3002\u8fd9\u79cd\u521b\u65b0\u65b9\u6cd5\u4f7f\u7f51\u7edc\u80fd\u591f\u540c\u65f6\u9884\u6d4b\u63a9\u6a21\u548c\u6df1\u5ea6\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u5728\u5b9e\u4f8b\u5206\u5272\u8fc7\u7a0b\u4e2d\u6355\u83b7\u4e0e\u6df1\u5ea6\u76f8\u5173\u7684\u7ec6\u5fae\u4fe1\u606f\u7684\u80fd\u529b\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5229\u7528\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u63a9\u6a21\u4f5c\u4e3a\u76d1\u7763\u6765\u533a\u5206\u524d\u666f\u548c\u80cc\u666f\u3002\u5728\u901a\u8fc7\u5308\u7259\u5229\u7b97\u6cd5\u4e3a\u6bcf\u4e2a\u6846\u9009\u62e9\u6700\u4f73\u63a9\u6a21\u65f6\uff0c\u6211\u4eec\u4f7f\u7528\u6df1\u5ea6\u4e00\u81f4\u6027\u4f5c\u4e3a\u4e00\u9879\u8ba1\u7b97\u6210\u672c\u9879\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 Cityscapes \u548c COCO \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6539\u8fdb\u3002|[2403.01214v1](http://arxiv.org/pdf/2403.01214v1)|null|\n", "2403.01210": "|**2024-03-02**|**SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with Target Scattering Feature Parameters**|SAR-AE-SFP\uff1a\u5177\u6709\u76ee\u6807\u6563\u5c04\u7279\u5f81\u53c2\u6570\u7684\u771f\u5b9e\u7269\u7406\u57df\u4e2d\u7684 SAR \u56fe\u50cf\u5bf9\u6297\u793a\u4f8b|Jiahao Cui, Jiale Duan, Binyan Luo, Hang Cao, Wang Guo, Haifeng Li|Deep neural network-based Synthetic Aperture Radar (SAR) target recognition models are susceptible to adversarial examples. Current adversarial example generation methods for SAR imagery primarily operate in the 2D digital domain, known as image adversarial examples. Recent work, while considering SAR imaging scatter mechanisms, fails to account for the actual imaging process, rendering attacks in the three-dimensional physical domain infeasible, termed pseudo physics adversarial examples. To address these challenges, this paper proposes SAR-AE-SFP-Attack, a method to generate real physics adversarial examples by altering the scattering feature parameters of target objects. Specifically, we iteratively optimize the coherent energy accumulation of the target echo by perturbing the reflection coefficient and scattering coefficient in the scattering feature parameters of the three-dimensional target object, and obtain the adversarial example after echo signal processing and imaging processing in the RaySAR simulator. Experimental results show that compared to digital adversarial attack methods, SAR-AE-SFP Attack significantly improves attack efficiency on CNN-based models (over 30\\%) and Transformer-based models (over 13\\%), demonstrating significant transferability of attack effects across different models and perspectives.|\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5408\u6210\u5b54\u5f84\u96f7\u8fbe (SAR) \u76ee\u6807\u8bc6\u522b\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u5f71\u54cd\u3002\u5f53\u524d SAR \u56fe\u50cf\u7684\u5bf9\u6297\u6027\u793a\u4f8b\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5728 2D \u6570\u5b57\u57df\u4e2d\u8fd0\u884c\uff0c\u79f0\u4e3a\u56fe\u50cf\u5bf9\u6297\u6027\u793a\u4f8b\u3002\u6700\u8fd1\u7684\u5de5\u4f5c\u867d\u7136\u8003\u8651\u4e86 SAR \u6210\u50cf\u6563\u5c04\u673a\u5236\uff0c\u4f46\u672a\u80fd\u8003\u8651\u5b9e\u9645\u7684\u6210\u50cf\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u4e09\u7ef4\u7269\u7406\u57df\u4e2d\u7684\u653b\u51fb\u4e0d\u53ef\u884c\uff0c\u79f0\u4e3a\u4f2a\u7269\u7406\u5bf9\u6297\u793a\u4f8b\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86 SAR-AE-SFP-Attack\uff0c\u4e00\u79cd\u901a\u8fc7\u6539\u53d8\u76ee\u6807\u7269\u4f53\u7684\u6563\u5c04\u7279\u5f81\u53c2\u6570\u6765\u751f\u6210\u771f\u5b9e\u7269\u7406\u5bf9\u6297\u793a\u4f8b\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u901a\u8fc7\u6270\u52a8\u4e09\u7ef4\u76ee\u6807\u7269\u4f53\u6563\u5c04\u7279\u5f81\u53c2\u6570\u4e2d\u7684\u53cd\u5c04\u7cfb\u6570\u548c\u6563\u5c04\u7cfb\u6570\u6765\u8fed\u4ee3\u4f18\u5316\u76ee\u6807\u56de\u6ce2\u7684\u76f8\u5e72\u80fd\u91cf\u79ef\u7d2f\uff0c\u5e76\u5728RaySAR\u6a21\u62df\u5668\u4e2d\u7ecf\u8fc7\u56de\u6ce2\u4fe1\u53f7\u5904\u7406\u548c\u6210\u50cf\u5904\u7406\u540e\u5f97\u5230\u5bf9\u6297\u6837\u672c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6570\u5b57\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u76f8\u6bd4\uff0cSAR-AE-SFP Attack\u663e\u7740\u63d0\u9ad8\u4e86\u57fa\u4e8eCNN\u7684\u6a21\u578b\uff08\u8d85\u8fc730%\uff09\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08\u8d85\u8fc713%\uff09\u7684\u653b\u51fb\u6548\u7387\uff0c\u8868\u73b0\u51fa\u653b\u51fb\u6548\u679c\u7684\u663e\u7740\u53ef\u8f6c\u79fb\u6027\u8de8\u8d8a\u4e0d\u540c\u7684\u6a21\u578b\u548c\u89c6\u89d2\u3002|[2403.01210v1](http://arxiv.org/pdf/2403.01210v1)|null|\n", "2403.01209": "|**2024-03-02**|**Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning**|\u901a\u8fc7 LLM \u652f\u6301\u7684\u5373\u65f6\u8c03\u6574\u8fdb\u884c\u65e0\u6570\u636e\u591a\u6807\u7b7e\u56fe\u50cf\u8bc6\u522b|Shuo Yang, Zirui Shang, Yongqi Wang, Derong Deng, Hongwei Chen, Qiyuan Cheng, Xinxiao Wu|This paper proposes a novel framework for multi-label image recognition without any training data, called data-free framework, which uses knowledge of pre-trained Large Language Model (LLM) to learn prompts to adapt pretrained Vision-Language Model (VLM) like CLIP to multilabel classification. Through asking LLM by well-designed questions, we acquire comprehensive knowledge about characteristics and contexts of objects, which provides valuable text descriptions for learning prompts. Then we propose a hierarchical prompt learning method by taking the multi-label dependency into consideration, wherein a subset of category-specific prompt tokens are shared when the corresponding objects exhibit similar attributes or are more likely to co-occur. Benefiting from the remarkable alignment between visual and linguistic semantics of CLIP, the hierarchical prompts learned from text descriptions are applied to perform classification of images during inference. Our framework presents a new way to explore the synergies between multiple pre-trained models for novel category recognition. Extensive experiments on three public datasets (MS-COCO, VOC2007, and NUS-WIDE) demonstrate that our method achieves better results than the state-of-the-art methods, especially outperforming the zero-shot multi-label recognition methods by 4.7% in mAP on MS-COCO.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6807\u7b7e\u56fe\u50cf\u8bc6\u522b\u65b0\u6846\u67b6\uff0c\u79f0\u4e3a\u65e0\u6570\u636e\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u77e5\u8bc6\u6765\u5b66\u4e60\u63d0\u793a\u4ee5\u9002\u5e94\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u4f8b\u5982CLIP \u5230\u591a\u6807\u7b7e\u5206\u7c7b\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u95ee\u9898\u5411LLM\u63d0\u51fa\u95ee\u9898\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u6709\u5173\u7269\u4f53\u7279\u5f81\u548c\u4e0a\u4e0b\u6587\u7684\u5168\u9762\u77e5\u8bc6\uff0c\u8fd9\u4e3a\u5b66\u4e60\u63d0\u793a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6587\u672c\u63cf\u8ff0\u3002\u7136\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u8003\u8651\u591a\u6807\u7b7e\u4f9d\u8d56\u6027\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5176\u4e2d\u5f53\u76f8\u5e94\u7684\u5bf9\u8c61\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u5c5e\u6027\u6216\u66f4\u53ef\u80fd\u540c\u65f6\u51fa\u73b0\u65f6\uff0c\u5171\u4eab\u7279\u5b9a\u4e8e\u7c7b\u522b\u7684\u63d0\u793a\u6807\u8bb0\u7684\u5b50\u96c6\u3002\u53d7\u76ca\u4e8e CLIP \u7684\u89c6\u89c9\u548c\u8bed\u8a00\u8bed\u4e49\u4e4b\u95f4\u7684\u663e\u7740\u4e00\u81f4\u6027\uff0c\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u5b66\u4e60\u5230\u7684\u5206\u5c42\u63d0\u793a\u53ef\u7528\u4e8e\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5bf9\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u3002\u6211\u4eec\u7684\u6846\u67b6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63a2\u7d22\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u4ee5\u8fdb\u884c\u65b0\u7c7b\u522b\u8bc6\u522b\u3002\u5bf9\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff08MS-COCO\u3001VOC2007 \u548c NUS-WIDE\uff09\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u7279\u522b\u662f\u6bd4\u96f6\u6837\u672c\u591a\u6807\u7b7e\u8bc6\u522b\u65b9\u6cd5\u9ad8\u51fa 4.7%\u5728 MS-COCO \u4e0a\u7684 mAP \u4e2d\u3002|[2403.01209v1](http://arxiv.org/pdf/2403.01209v1)|null|\n", "2403.01183": "|**2024-03-02**|**Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery**|\u5229\u7528\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u513f\u7ae5\u6027\u8650\u5f85\u56fe\u50cf\u7684\u573a\u666f\u8bc6\u522b|Pedro H. V. Valois, Jo\u00e3o Macedo, Leo S. F. Ribeiro, Jefersson A. dos Santos, Sandra Avila|Crime in the 21st century is split into a virtual and real world. However, the former has become a global menace to people's well-being and security in the latter. The challenges it presents must be faced with unified global cooperation, and we must rely more than ever on automated yet trustworthy tools to combat the ever-growing nature of online offenses. Over 10 million child sexual abuse reports are submitted to the US National Center for Missing & Exploited Children every year, and over 80% originated from online sources. Therefore, investigation centers and clearinghouses cannot manually process and correctly investigate all imagery. In light of that, reliable automated tools that can securely and efficiently deal with this data are paramount. In this sense, the scene recognition task looks for contextual cues in the environment, being able to group and classify child sexual abuse data without requiring to be trained on sensitive material. The scarcity and limitations of working with child sexual abuse images lead to self-supervised learning, a machine-learning methodology that leverages unlabeled data to produce powerful representations that can be more easily transferred to target tasks. This work shows that self-supervised deep learning models pre-trained on scene-centric data can reach 71.6% balanced accuracy on our indoor scene classification task and, on average, 2.2 percentage points better performance than a fully supervised version. We cooperate with Brazilian Federal Police experts to evaluate our indoor classification model on actual child abuse material. The results demonstrate a notable discrepancy between the features observed in widely used scene datasets and those depicted on sensitive materials.|21\u4e16\u7eaa\u7684\u72af\u7f6a\u5206\u4e3a\u865a\u62df\u4e16\u754c\u548c\u73b0\u5b9e\u4e16\u754c\u3002\u7136\u800c\uff0c\u524d\u8005\u5df2\u6210\u4e3a\u5bf9\u540e\u8005\u4eba\u6c11\u798f\u7949\u548c\u5b89\u5168\u7684\u5168\u7403\u6027\u5a01\u80c1\u3002\u5fc5\u987b\u901a\u8fc7\u5168\u7403\u7edf\u4e00\u5408\u4f5c\u6765\u5e94\u5bf9\u5b83\u5e26\u6765\u7684\u6311\u6218\uff0c\u6211\u4eec\u5fc5\u987b\u6bd4\u4ee5\u5f80\u4efb\u4f55\u65f6\u5019\u90fd\u66f4\u52a0\u4f9d\u8d56\u81ea\u52a8\u5316\u4f46\u503c\u5f97\u4fe1\u8d56\u7684\u5de5\u5177\u6765\u6253\u51fb\u65e5\u76ca\u589e\u957f\u7684\u7f51\u7edc\u72af\u7f6a\u3002\u6bcf\u5e74\u6709\u8d85\u8fc7 1000 \u4e07\u4efd\u513f\u7ae5\u6027\u8650\u5f85\u62a5\u544a\u63d0\u4ea4\u7ed9\u7f8e\u56fd\u56fd\u5bb6\u5931\u8e2a\u4e0e\u53d7\u8650\u513f\u7ae5\u4e2d\u5fc3\uff0c\u5176\u4e2d\u8d85\u8fc7 80% \u6765\u81ea\u7f51\u7edc\u6765\u6e90\u3002\u56e0\u6b64\uff0c\u8c03\u67e5\u4e2d\u5fc3\u548c\u4fe1\u606f\u4ea4\u6362\u6240\u65e0\u6cd5\u624b\u52a8\u5904\u7406\u548c\u6b63\u786e\u8c03\u67e5\u6240\u6709\u56fe\u50cf\u3002\u6709\u9274\u4e8e\u6b64\uff0c\u80fd\u591f\u5b89\u5168\u6709\u6548\u5730\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u7684\u53ef\u9760\u81ea\u52a8\u5316\u5de5\u5177\u81f3\u5173\u91cd\u8981\u3002\u4ece\u8fd9\u4e2a\u610f\u4e49\u4e0a\u8bf4\uff0c\u573a\u666f\u8bc6\u522b\u4efb\u52a1\u5bfb\u627e\u73af\u5883\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u80fd\u591f\u5bf9\u513f\u7ae5\u6027\u8650\u5f85\u6570\u636e\u8fdb\u884c\u5206\u7ec4\u548c\u5206\u7c7b\uff0c\u800c\u65e0\u9700\u63a5\u53d7\u654f\u611f\u6750\u6599\u7684\u57f9\u8bad\u3002\u5904\u7406\u513f\u7ae5\u6027\u8650\u5f85\u56fe\u50cf\u7684\u7a00\u7f3a\u6027\u548c\u5c40\u9650\u6027\u5bfc\u81f4\u4e86\u81ea\u6211\u76d1\u7763\u5b66\u4e60\uff0c\u8fd9\u662f\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u672a\u6807\u8bb0\u7684\u6570\u636e\u6765\u4ea7\u751f\u5f3a\u5927\u7684\u8868\u793a\uff0c\u53ef\u4ee5\u66f4\u5bb9\u6613\u5730\u8f6c\u79fb\u5230\u76ee\u6807\u4efb\u52a1\u3002\u8fd9\u9879\u5de5\u4f5c\u8868\u660e\uff0c\u5728\u4ee5\u573a\u666f\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6211\u4eec\u7684\u5ba4\u5185\u573a\u666f\u5206\u7c7b\u4efb\u52a1\u4e0a\u53ef\u4ee5\u8fbe\u5230 71.6% \u7684\u5e73\u8861\u7cbe\u5ea6\uff0c\u5e73\u5747\u6bd4\u5b8c\u5168\u76d1\u7763\u7248\u672c\u7684\u6027\u80fd\u63d0\u9ad8 2.2 \u4e2a\u767e\u5206\u70b9\u3002\u6211\u4eec\u4e0e\u5df4\u897f\u8054\u90a6\u8b66\u5bdf\u4e13\u5bb6\u5408\u4f5c\uff0c\u8bc4\u4f30\u6211\u4eec\u9488\u5bf9\u5b9e\u9645\u8650\u5f85\u513f\u7ae5\u6750\u6599\u7684\u5ba4\u5185\u5206\u7c7b\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u573a\u666f\u6570\u636e\u96c6\u4e2d\u89c2\u5bdf\u5230\u7684\u7279\u5f81\u4e0e\u654f\u611f\u6750\u6599\u4e0a\u63cf\u7ed8\u7684\u7279\u5f81\u4e4b\u95f4\u5b58\u5728\u663e\u7740\u5dee\u5f02\u3002|[2403.01183v1](http://arxiv.org/pdf/2403.01183v1)|null|\n", "2403.01172": "|**2024-03-02**|**Run-time Introspection of 2D Object Detection in Automated Driving Systems Using Learning Representations**|\u4f7f\u7528\u5b66\u4e60\u8868\u793a\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684 2D \u5bf9\u8c61\u68c0\u6d4b\u8fdb\u884c\u8fd0\u884c\u65f6\u81ea\u7701|Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman|Reliable detection of various objects and road users in the surrounding environment is crucial for the safe operation of automated driving systems (ADS). Despite recent progresses in developing highly accurate object detectors based on Deep Neural Networks (DNNs), they still remain prone to detection errors, which can lead to fatal consequences in safety-critical applications such as ADS. An effective remedy to this problem is to equip the system with run-time monitoring, named as introspection in the context of autonomous systems. Motivated by this, we introduce a novel introspection solution, which operates at the frame level for DNN-based 2D object detection and leverages neural network activation patterns. The proposed approach pre-processes the neural activation patterns of the object detector's backbone using several different modes. To provide extensive comparative analysis and fair comparison, we also adapt and implement several state-of-the-art (SOTA) introspection mechanisms for error detection in 2D object detection, using one-stage and two-stage object detectors evaluated on KITTI and BDD datasets. We compare the performance of the proposed solution in terms of error detection, adaptability to dataset shift, and, computational and memory resource requirements. Our performance evaluation shows that the proposed introspection solution outperforms SOTA methods, achieving an absolute reduction in the missed error ratio of 9% to 17% in the BDD dataset.|\u53ef\u9760\u5730\u68c0\u6d4b\u5468\u56f4\u73af\u5883\u4e2d\u7684\u5404\u79cd\u7269\u4f53\u548c\u9053\u8def\u4f7f\u7528\u8005\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u7684\u5b89\u5168\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u6700\u8fd1\u5728\u5f00\u53d1\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u7684\u9ad8\u7cbe\u5ea6\u7269\u4f53\u68c0\u6d4b\u5668\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u51fa\u73b0\u68c0\u6d4b\u9519\u8bef\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5728 ADS \u7b49\u5b89\u5168\u5173\u952e\u578b\u5e94\u7528\u4e2d\u5bfc\u81f4\u81f4\u547d\u540e\u679c\u3002\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\u662f\u4e3a\u7cfb\u7edf\u914d\u5907\u8fd0\u884c\u65f6\u76d1\u63a7\uff0c\u5728\u81ea\u6cbb\u7cfb\u7edf\u7684\u80cc\u666f\u4e0b\u79f0\u4e3a\u5185\u7701\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u7701\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u5728\u5e27\u7ea7\u522b\u8fd0\u884c\uff0c\u7528\u4e8e\u57fa\u4e8e DNN \u7684 2D \u5bf9\u8c61\u68c0\u6d4b\uff0c\u5e76\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6fc0\u6d3b\u6a21\u5f0f\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u7528\u51e0\u79cd\u4e0d\u540c\u7684\u6a21\u5f0f\u9884\u5904\u7406\u5bf9\u8c61\u68c0\u6d4b\u5668\u4e3b\u5e72\u7684\u795e\u7ecf\u6fc0\u6d3b\u6a21\u5f0f\u3002\u4e3a\u4e86\u63d0\u4f9b\u5e7f\u6cdb\u7684\u6bd4\u8f83\u5206\u6790\u548c\u516c\u5e73\u6bd4\u8f83\uff0c\u6211\u4eec\u8fd8\u4f7f\u7528\u5728 KITTI \u548c BDD \u4e0a\u8bc4\u4f30\u7684\u4e00\u7ea7\u548c\u4e24\u7ea7\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u91c7\u7528\u5e76\u5b9e\u73b0\u4e86\u51e0\u79cd\u6700\u5148\u8fdb\u7684 (SOTA) \u5185\u7701\u673a\u5236\uff0c\u7528\u4e8e 2D \u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u9519\u8bef\u68c0\u6d4b\u6570\u636e\u96c6\u3002\u6211\u4eec\u6bd4\u8f83\u4e86\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u5728\u9519\u8bef\u68c0\u6d4b\u3001\u6570\u636e\u96c6\u8f6c\u6362\u7684\u9002\u5e94\u6027\u4ee5\u53ca\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u8981\u6c42\u65b9\u9762\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u6027\u80fd\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5185\u7701\u89e3\u51b3\u65b9\u6848\u4f18\u4e8e SOTA \u65b9\u6cd5\uff0c\u5728 BDD \u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u4e86 9% \u81f3 17% \u7684\u9057\u6f0f\u9519\u8bef\u7387\u7684\u7edd\u5bf9\u51cf\u5c11\u3002|[2403.01172v1](http://arxiv.org/pdf/2403.01172v1)|null|\n", "2403.01169": "|**2024-03-02**|**Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection**|\u4ece\u4e8b\u4ef6\u63d0\u793a\u4e2d\u4e86\u89e3\u53ef\u7591\u5f02\u5e38\u60c5\u51b5\u4ee5\u8fdb\u884c\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b|Chenchen Tao, Chong Wang, Yuexian Zou, Xiaohao Peng, Jiafei Wu, Jiangbo Qian|Most models for weakly supervised video anomaly detection (WS-VAD) rely on multiple instance learning, aiming to distinguish normal and abnormal snippets without specifying the type of anomaly. The ambiguous nature of anomaly definitions across contexts introduces bias in detecting abnormal and normal snippets within the abnormal bag. Taking the first step to show the model why it is anomalous, a novel framework is proposed to guide the learning of suspected anomalies from event prompts. Given a textual prompt dictionary of potential anomaly events and the captions generated from anomaly videos, the semantic anomaly similarity between them could be calculated to identify the suspected anomalous events for each video snippet. It enables a new multi-prompt learning process to constrain the visual-semantic features across all videos, as well as provides a new way to label pseudo anomalies for self-training. To demonstrate effectiveness, comprehensive experiments and detailed ablation studies are conducted on four datasets, namely XD-Violence, UCF-Crime, TAD, and ShanghaiTech. Our proposed model outperforms most state-of-the-art methods in terms of AP or AUC (82.6\\%, 87.7\\%, 93.1\\%, and 97.4\\%). Furthermore, it shows promising performance in open-set and cross-dataset cases.|\u5927\u591a\u6570\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08WS-VAD\uff09\u6a21\u578b\u90fd\u4f9d\u8d56\u4e8e\u591a\u5b9e\u4f8b\u5b66\u4e60\uff0c\u65e8\u5728\u533a\u5206\u6b63\u5e38\u548c\u5f02\u5e38\u7247\u6bb5\uff0c\u800c\u4e0d\u6307\u5b9a\u5f02\u5e38\u7c7b\u578b\u3002\u8de8\u4e0a\u4e0b\u6587\u7684\u5f02\u5e38\u5b9a\u4e49\u7684\u6a21\u7cca\u6027\u4f1a\u5728\u68c0\u6d4b\u5f02\u5e38\u5305\u5185\u7684\u5f02\u5e38\u548c\u6b63\u5e38\u7247\u6bb5\u65f6\u5f15\u5165\u504f\u5dee\u3002\u7b2c\u4e00\u6b65\u5411\u6a21\u578b\u5c55\u793a\u5176\u5f02\u5e38\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\u6765\u6307\u5bfc\u4ece\u4e8b\u4ef6\u63d0\u793a\u4e2d\u5b66\u4e60\u53ef\u7591\u5f02\u5e38\u3002\u7ed9\u5b9a\u6f5c\u5728\u5f02\u5e38\u4e8b\u4ef6\u7684\u6587\u672c\u63d0\u793a\u5b57\u5178\u548c\u5f02\u5e38\u89c6\u9891\u751f\u6210\u7684\u5b57\u5e55\uff0c\u53ef\u4ee5\u8ba1\u7b97\u5b83\u4eec\u4e4b\u95f4\u7684\u8bed\u4e49\u5f02\u5e38\u76f8\u4f3c\u5ea6\uff0c\u4ee5\u8bc6\u522b\u6bcf\u4e2a\u89c6\u9891\u7247\u6bb5\u7684\u53ef\u7591\u5f02\u5e38\u4e8b\u4ef6\u3002\u5b83\u652f\u6301\u65b0\u7684\u591a\u63d0\u793a\u5b66\u4e60\u8fc7\u7a0b\u6765\u7ea6\u675f\u6240\u6709\u89c6\u9891\u7684\u89c6\u89c9\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u6807\u8bb0\u4f2a\u5f02\u5e38\u4ee5\u8fdb\u884c\u81ea\u6211\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\u3002\u4e3a\u4e86\u8bc1\u660e\u6709\u6548\u6027\uff0c\u5728 XD-Violence\u3001UCF-Crime\u3001TAD \u548c ShanghaiTech \u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\u548c\u8be6\u7ec6\u7684\u6d88\u878d\u7814\u7a76\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u5728 AP \u6216 AUC \u65b9\u9762\u4f18\u4e8e\u5927\u591a\u6570\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0882.6%\u300187.7%\u300193.1% \u548c 97.4%\uff09\u3002\u6b64\u5916\uff0c\u5b83\u5728\u5f00\u653e\u96c6\u548c\u8de8\u6570\u636e\u96c6\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002|[2403.01169v1](http://arxiv.org/pdf/2403.01169v1)|null|\n", "2403.01156": "|**2024-03-02**|**Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised Semantic Segmentation**|\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u53cc\u4eb2\u548c\u529b\u5b66\u4e60|Lian Xu, Mohammed Bennamoun, Farid Boussaid, Wanli Ouyang, Ferdous Sohel, Dan Xu|Most existing weakly supervised semantic segmentation (WSSS) methods rely on Class Activation Mapping (CAM) to extract coarse class-specific localization maps using image-level labels. Prior works have commonly used an off-line heuristic thresholding process that combines the CAM maps with off-the-shelf saliency maps produced by a general pre-trained saliency model to produce more accurate pseudo-segmentation labels. We propose AuxSegNet+, a weakly supervised auxiliary learning framework to explore the rich information from these saliency maps and the significant inter-task correlation between saliency detection and semantic segmentation. In the proposed AuxSegNet+, saliency detection and multi-label image classification are used as auxiliary tasks to improve the primary task of semantic segmentation with only image-level ground-truth labels. We also propose a cross-task affinity learning mechanism to learn pixel-level affinities from the saliency and segmentation feature maps. In particular, we propose a cross-task dual-affinity learning module to learn both pairwise and unary affinities, which are used to enhance the task-specific features and predictions by aggregating both query-dependent and query-independent global context for both saliency detection and semantic segmentation. The learned cross-task pairwise affinity can also be used to refine and propagate CAM maps to provide better pseudo labels for both tasks. Iterative improvement of segmentation performance is enabled by cross-task affinity learning and pseudo-label updating. Extensive experiments demonstrate the effectiveness of the proposed approach with new state-of-the-art WSSS results on the challenging PASCAL VOC and MS COCO benchmarks.|\u5927\u591a\u6570\u73b0\u6709\u7684\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\uff08WSSS\uff09\u65b9\u6cd5\u4f9d\u8d56\u7c7b\u6fc0\u6d3b\u6620\u5c04\uff08CAM\uff09\u6765\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u63d0\u53d6\u7c97\u7565\u7684\u7c7b\u7279\u5b9a\u5b9a\u4f4d\u56fe\u3002\u5148\u524d\u7684\u5de5\u4f5c\u901a\u5e38\u4f7f\u7528\u79bb\u7ebf\u542f\u53d1\u5f0f\u9608\u503c\u5904\u7406\uff0c\u5c06 CAM \u5730\u56fe\u4e0e\u7531\u901a\u7528\u9884\u8bad\u7ec3\u663e\u7740\u6027\u6a21\u578b\u751f\u6210\u7684\u73b0\u6210\u663e\u7740\u6027\u5730\u56fe\u76f8\u7ed3\u5408\uff0c\u4ee5\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u4f2a\u5206\u5272\u6807\u7b7e\u3002\u6211\u4eec\u63d0\u51fa\u4e86 AuxSegNet+\uff0c\u4e00\u4e2a\u5f31\u76d1\u7763\u7684\u8f85\u52a9\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u7d22\u8fd9\u4e9b\u663e\u7740\u6027\u56fe\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\u4ee5\u53ca\u663e\u7740\u6027\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u4e4b\u95f4\u7684\u663e\u7740\u4efb\u52a1\u95f4\u76f8\u5173\u6027\u3002\u5728\u63d0\u51fa\u7684 AuxSegNet+ \u4e2d\uff0c\u663e\u7740\u6027\u68c0\u6d4b\u548c\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u88ab\u7528\u4f5c\u8f85\u52a9\u4efb\u52a1\uff0c\u4ee5\u6539\u8fdb\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u771f\u5b9e\u6807\u7b7e\u7684\u8bed\u4e49\u5206\u5272\u7684\u4e3b\u8981\u4efb\u52a1\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u4efb\u52a1\u4eb2\u548c\u529b\u5b66\u4e60\u673a\u5236\uff0c\u7528\u4e8e\u4ece\u663e\u7740\u6027\u548c\u5206\u5272\u7279\u5f81\u56fe\u4e2d\u5b66\u4e60\u50cf\u7d20\u7ea7\u4eb2\u548c\u529b\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u4efb\u52a1\u53cc\u4eb2\u548c\u529b\u5b66\u4e60\u6a21\u5757\u6765\u5b66\u4e60\u6210\u5bf9\u548c\u4e00\u5143\u4eb2\u548c\u529b\uff0c\u5b83\u4eec\u7528\u4e8e\u901a\u8fc7\u805a\u5408\u67e5\u8be2\u76f8\u5173\u548c\u67e5\u8be2\u65e0\u5173\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u6765\u589e\u5f3a\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u7279\u5f81\u548c\u9884\u6d4b\u4ee5\u8fdb\u884c\u663e\u7740\u6027\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u3002\u5b66\u4e60\u5230\u7684\u8de8\u4efb\u52a1\u6210\u5bf9\u4eb2\u548c\u529b\u8fd8\u53ef\u4ee5\u7528\u4e8e\u7ec6\u5316\u548c\u4f20\u64ad CAM \u6620\u5c04\uff0c\u4ee5\u4e3a\u8fd9\u4e24\u4e2a\u4efb\u52a1\u63d0\u4f9b\u66f4\u597d\u7684\u4f2a\u6807\u7b7e\u3002\u901a\u8fc7\u8de8\u4efb\u52a1\u4eb2\u548c\u6027\u5b66\u4e60\u548c\u4f2a\u6807\u7b7e\u66f4\u65b0\u6765\u5b9e\u73b0\u5206\u5272\u6027\u80fd\u7684\u8fed\u4ee3\u6539\u8fdb\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5177\u6709\u6311\u6218\u6027\u7684 PASCAL VOC \u548c MS COCO \u57fa\u51c6\u4e0a\u83b7\u5f97\u4e86\u6700\u5148\u8fdb\u7684 WSSS \u7ed3\u679c\u3002|[2403.01156v1](http://arxiv.org/pdf/2403.01156v1)|null|\n", "2403.01123": "|**2024-03-02**|**ELA: Efficient Local Attention for Deep Convolutional Neural Networks**|ELA\uff1a\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u5c40\u90e8\u6ce8\u610f\u529b|Wei Xu, Yi Wan|The attention mechanism has gained significant recognition in the field of computer vision due to its ability to effectively enhance the performance of deep neural networks. However, existing methods often struggle to effectively utilize spatial information or, if they do, they come at the cost of reducing channel dimensions or increasing the complexity of neural networks. In order to address these limitations, this paper introduces an Efficient Local Attention (ELA) method that achieves substantial performance improvements with a simple structure. By analyzing the limitations of the Coordinate Attention method, we identify the lack of generalization ability in Batch Normalization, the adverse effects of dimension reduction on channel attention, and the complexity of attention generation process. To overcome these challenges, we propose the incorporation of 1D convolution and Group Normalization feature enhancement techniques. This approach enables accurate localization of regions of interest by efficiently encoding two 1D positional feature maps without the need for dimension reduction, while allowing for a lightweight implementation. We carefully design three hyperparameters in ELA, resulting in four different versions: ELA-T, ELA-B, ELA-S, and ELA-L, to cater to the specific requirements of different visual tasks such as image classification, object detection and sementic segmentation. ELA can be seamlessly integrated into deep CNN networks such as ResNet, MobileNet, and DeepLab. Extensive evaluations on the ImageNet, MSCOCO, and Pascal VOC datasets demonstrate the superiority of the proposed ELA module over current state-of-the-art methods in all three aforementioned visual tasks.|\u6ce8\u610f\u529b\u673a\u5236\u7531\u4e8e\u80fd\u591f\u6709\u6548\u589e\u5f3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u800c\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u83b7\u5f97\u4e86\u663e\u7740\u7684\u8ba4\u53ef\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u6709\u6548\u5730\u5229\u7528\u7a7a\u95f4\u4fe1\u606f\uff0c\u6216\u8005\u5373\u4f7f\u6709\u6548\u5229\u7528\u7a7a\u95f4\u4fe1\u606f\uff0c\u4e5f\u4f1a\u4ee5\u51cf\u5c11\u901a\u9053\u7ef4\u5ea6\u6216\u589e\u52a0\u795e\u7ecf\u7f51\u7edc\u7684\u590d\u6742\u6027\u4e3a\u4ee3\u4ef7\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u9ad8\u6548\u5c40\u90e8\u6ce8\u610f\u529b\uff08ELA\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ee5\u7b80\u5355\u7684\u7ed3\u6784\u5b9e\u73b0\u4e86\u663e\u7740\u7684\u6027\u80fd\u63d0\u5347\u3002\u901a\u8fc7\u5206\u6790\u5750\u6807\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u53d1\u73b0\u4e86Batch Normalization\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3001\u964d\u7ef4\u5bf9\u901a\u9053\u6ce8\u610f\u529b\u7684\u4e0d\u5229\u5f71\u54cd\u4ee5\u53ca\u6ce8\u610f\u529b\u751f\u6210\u8fc7\u7a0b\u7684\u590d\u6742\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5efa\u8bae\u7ed3\u5408\u4e00\u7ef4\u5377\u79ef\u548c\u7ec4\u5f52\u4e00\u5316\u7279\u5f81\u589e\u5f3a\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5730\u7f16\u7801\u4e24\u4e2a\u4e00\u7ef4\u4f4d\u7f6e\u7279\u5f81\u56fe\u6765\u5b9e\u73b0\u611f\u5174\u8da3\u533a\u57df\u7684\u7cbe\u786e\u5b9a\u4f4d\uff0c\u65e0\u9700\u964d\u7ef4\uff0c\u540c\u65f6\u5141\u8bb8\u8f7b\u91cf\u7ea7\u5b9e\u73b0\u3002\u6211\u4eec\u5728ELA\u4e2d\u7cbe\u5fc3\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u8d85\u53c2\u6570\uff0c\u4ea7\u751f\u4e86\u56db\u4e2a\u4e0d\u540c\u7684\u7248\u672c\uff1aELA-T\u3001ELA-B\u3001ELA-S\u548cELA-L\uff0c\u4ee5\u6ee1\u8db3\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u7b49\u4e0d\u540c\u89c6\u89c9\u4efb\u52a1\u7684\u5177\u4f53\u8981\u6c42\u3002\u5206\u5272\u3002 ELA\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230ResNet\u3001MobileNet\u548cDeepLab\u7b49\u6df1\u5ea6CNN\u7f51\u7edc\u4e2d\u3002\u5bf9 ImageNet\u3001MSCOCO \u548c Pascal VOC \u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 ELA \u6a21\u5757\u5728\u4e0a\u8ff0\u6240\u6709\u4e09\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e2d\u76f8\u5bf9\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002|[2403.01123v1](http://arxiv.org/pdf/2403.01123v1)|null|\n", "2403.01083": "|**2024-03-02**|**Beyond Night Visibility: Adaptive Multi-Scale Fusion of Infrared and Visible Images**|\u8d85\u8d8a\u591c\u95f4\u80fd\u89c1\u5ea6\uff1a\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u56fe\u50cf\u7684\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u878d\u5408|Shufan Pei, Junhong Lin, Wenxi Liu, Tiesong Zhao, Chia-Wen Lin|In addition to low light, night images suffer degradation from light effects (e.g., glare, floodlight, etc). However, existing nighttime visibility enhancement methods generally focus on low-light regions, which neglects, or even amplifies the light effects. To address this issue, we propose an Adaptive Multi-scale Fusion network (AMFusion) with infrared and visible images, which designs fusion rules according to different illumination regions. First, we separately fuse spatial and semantic features from infrared and visible images, where the former are used for the adjustment of light distribution and the latter are used for the improvement of detection accuracy. Thereby, we obtain an image free of low light and light effects, which improves the performance of nighttime object detection. Second, we utilize detection features extracted by a pre-trained backbone that guide the fusion of semantic features. Hereby, we design a Detection-guided Semantic Fusion Module (DSFM) to bridge the domain gap between detection and semantic features. Third, we propose a new illumination loss to constrain fusion image with normal light intensity. Experimental results demonstrate the superiority of AMFusion with better visual quality and detection accuracy. The source code will be released after the peer review process.|\u9664\u4e86\u5f31\u5149\u4e4b\u5916\uff0c\u591c\u95f4\u56fe\u50cf\u8fd8\u4f1a\u56e0\u5149\u6548\u5e94\uff08\u4f8b\u5982\u7729\u5149\u3001\u6cdb\u5149\u706f\u7b49\uff09\u800c\u964d\u4f4e\u8d28\u91cf\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u591c\u95f4\u80fd\u89c1\u5ea6\u589e\u5f3a\u65b9\u6cd5\u666e\u904d\u5173\u6ce8\u5f31\u5149\u533a\u57df\uff0c\u5ffd\u7565\u751a\u81f3\u653e\u5927\u4e86\u5149\u6548\u5e94\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u56fe\u50cf\u7684\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u878d\u5408\u7f51\u7edc\uff08AMFusion\uff09\uff0c\u5b83\u6839\u636e\u4e0d\u540c\u7684\u7167\u660e\u533a\u57df\u8bbe\u8ba1\u878d\u5408\u89c4\u5219\u3002\u9996\u5148\uff0c\u6211\u4eec\u5206\u522b\u878d\u5408\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u56fe\u50cf\u7684\u7a7a\u95f4\u7279\u5f81\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u524d\u8005\u7528\u4e8e\u8c03\u6574\u5149\u5206\u5e03\uff0c\u540e\u8005\u7528\u4e8e\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002\u7531\u6b64\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u6ca1\u6709\u5f31\u5149\u548c\u5149\u6548\u5e94\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u591c\u95f4\u7269\u4f53\u68c0\u6d4b\u7684\u6027\u80fd\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u4e3b\u5e72\u63d0\u53d6\u7684\u68c0\u6d4b\u7279\u5f81\u6765\u6307\u5bfc\u8bed\u4e49\u7279\u5f81\u7684\u878d\u5408\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u68c0\u6d4b\u5f15\u5bfc\u7684\u8bed\u4e49\u878d\u5408\u6a21\u5757\uff08DSFM\uff09\u6765\u5f25\u5408\u68c0\u6d4b\u548c\u8bed\u4e49\u7279\u5f81\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7167\u660e\u635f\u5931\u6765\u7ea6\u675f\u5177\u6709\u6b63\u5e38\u5149\u5f3a\u5ea6\u7684\u878d\u5408\u56fe\u50cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86 AMFusion \u7684\u4f18\u8d8a\u6027\uff0c\u5177\u6709\u66f4\u597d\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u68c0\u6d4b\u7cbe\u5ea6\u3002\u6e90\u4ee3\u7801\u5c06\u5728\u540c\u884c\u8bc4\u5ba1\u8fc7\u7a0b\u540e\u53d1\u5e03\u3002|[2403.01083v1](http://arxiv.org/pdf/2403.01083v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2403.01105": "|**2024-03-02**|**Depth Information Assisted Collaborative Mutual Promotion Network for Single Image Dehazing**|\u6df1\u5ea6\u4fe1\u606f\u8f85\u52a9\u5355\u5e45\u56fe\u50cf\u53bb\u96fe\u534f\u4f5c\u4e92\u4fc3\u7f51\u7edc|Yafei Zhang, Shen Zhou, Huafeng Li|Recovering a clear image from a single hazy image is an open inverse problem. Although significant research progress has been made, most existing methods ignore the effect that downstream tasks play in promoting upstream dehazing. From the perspective of the haze generation mechanism, there is a potential relationship between the depth information of the scene and the hazy image. Based on this, we propose a dual-task collaborative mutual promotion framework to achieve the dehazing of a single image. This framework integrates depth estimation and dehazing by a dual-task interaction mechanism and achieves mutual enhancement of their performance. To realize the joint optimization of the two tasks, an alternative implementation mechanism with the difference perception is developed. On the one hand, the difference perception between the depth maps of the dehazing result and the ideal image is proposed to promote the dehazing network to pay attention to the non-ideal areas of the dehazing. On the other hand, by improving the depth estimation performance in the difficult-to-recover areas of the hazy image, the dehazing network can explicitly use the depth information of the hazy image to assist the clear image recovery. To promote the depth estimation, we propose to use the difference between the dehazed image and the ground truth to guide the depth estimation network to focus on the dehazed unideal areas. It allows dehazing and depth estimation to leverage their strengths in a mutually reinforcing manner. Experimental results show that the proposed method can achieve better performance than that of the state-of-the-art approaches.|\u4ece\u5355\u4e2a\u6a21\u7cca\u56fe\u50cf\u4e2d\u6062\u590d\u6e05\u6670\u56fe\u50cf\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u9006\u95ee\u9898\u3002\u5c3d\u7ba1\u5df2\u7ecf\u53d6\u5f97\u4e86\u91cd\u5927\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u4f46\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e0b\u6e38\u4efb\u52a1\u5728\u4fc3\u8fdb\u4e0a\u6e38\u53bb\u96fe\u4e2d\u6240\u53d1\u6325\u7684\u4f5c\u7528\u3002\u4ece\u96fe\u973e\u751f\u6210\u673a\u5236\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u573a\u666f\u7684\u6df1\u5ea6\u4fe1\u606f\u4e0e\u96fe\u973e\u56fe\u50cf\u4e4b\u95f4\u5b58\u5728\u6f5c\u5728\u7684\u5173\u7cfb\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u4efb\u52a1\u534f\u540c\u76f8\u4e92\u4fc3\u8fdb\u6846\u67b6\u6765\u5b9e\u73b0\u5355\u4e2a\u56fe\u50cf\u7684\u53bb\u96fe\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u53cc\u4efb\u52a1\u4ea4\u4e92\u673a\u5236\u5c06\u6df1\u5ea6\u4f30\u8ba1\u548c\u53bb\u96fe\u96c6\u6210\u5728\u4e00\u8d77\uff0c\u5b9e\u73b0\u4e86\u4e24\u8005\u6027\u80fd\u7684\u76f8\u4e92\u589e\u5f3a\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e24\u4e2a\u4efb\u52a1\u7684\u8054\u5408\u4f18\u5316\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5177\u6709\u5dee\u5f02\u611f\u77e5\u7684\u66ff\u4ee3\u6267\u884c\u673a\u5236\u3002\u4e00\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u53bb\u96fe\u7ed3\u679c\u7684\u6df1\u5ea6\u56fe\u4e0e\u7406\u60f3\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u611f\u77e5\uff0c\u4ee5\u4fc3\u8fdb\u53bb\u96fe\u7f51\u7edc\u5173\u6ce8\u53bb\u96fe\u7684\u975e\u7406\u60f3\u533a\u57df\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u901a\u8fc7\u63d0\u9ad8\u6709\u96fe\u56fe\u50cf\u96be\u4ee5\u6062\u590d\u533a\u57df\u7684\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u53bb\u96fe\u7f51\u7edc\u53ef\u4ee5\u660e\u786e\u5730\u4f7f\u7528\u6709\u96fe\u56fe\u50cf\u7684\u6df1\u5ea6\u4fe1\u606f\u6765\u8f85\u52a9\u6e05\u6670\u56fe\u50cf\u6062\u590d\u3002\u4e3a\u4e86\u4fc3\u8fdb\u6df1\u5ea6\u4f30\u8ba1\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528\u53bb\u96fe\u56fe\u50cf\u4e0e\u5730\u9762\u5b9e\u51b5\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u5f15\u5bfc\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\u805a\u7126\u4e8e\u53bb\u96fe\u7684\u4e0d\u7406\u60f3\u533a\u57df\u3002\u5b83\u5141\u8bb8\u53bb\u96fe\u548c\u6df1\u5ea6\u4f30\u8ba1\u4ee5\u76f8\u8f85\u76f8\u6210\u7684\u65b9\u5f0f\u53d1\u6325\u5404\u81ea\u7684\u4f18\u52bf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002|[2403.01105v1](http://arxiv.org/pdf/2403.01105v1)|null|\n"}, "LLM": {}, "Transformer": {"2403.01246": "|**2024-03-02**|**Dual Graph Attention based Disentanglement Multiple Instance Learning for Brain Age Estimation**|\u57fa\u4e8e\u53cc\u56fe\u6ce8\u610f\u529b\u7684\u89e3\u7f20\u591a\u5b9e\u4f8b\u5b66\u4e60\u7528\u4e8e\u8111\u5e74\u9f84\u4f30\u8ba1|Fanzhe Yan, Gang Yang, Yu Li, Aiping Liu, Xun Chen|Deep learning techniques have demonstrated great potential for accurately estimating brain age by analyzing Magnetic Resonance Imaging (MRI) data from healthy individuals. However, current methods for brain age estimation often directly utilize whole input images, overlooking two important considerations: 1) the heterogeneous nature of brain aging, where different brain regions may degenerate at different rates, and 2) the existence of age-independent redundancies in brain structure. To overcome these limitations, we propose a Dual Graph Attention based Disentanglement Multi-instance Learning (DGA-DMIL) framework for improving brain age estimation. Specifically, the 3D MRI data, treated as a bag of instances, is fed into a 2D convolutional neural network backbone, to capture the unique aging patterns in MRI. A dual graph attention aggregator is then proposed to learn the backbone features by exploiting the intra- and inter-instance relationships. Furthermore, a disentanglement branch is introduced to separate age-related features from age-independent structural representations to ameliorate the interference of redundant information on age prediction. To verify the effectiveness of the proposed framework, we evaluate it on two datasets, UK Biobank and ADNI, containing a total of 35,388 healthy individuals. Our proposed model demonstrates exceptional accuracy in estimating brain age, achieving a remarkable mean absolute error of 2.12 years in the UK Biobank. The results establish our approach as state-of-the-art compared to other competing brain age estimation models. In addition, the instance contribution scores identify the varied importance of brain areas for aging prediction, which provides deeper insights into the understanding of brain aging.|\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5df2\u663e\u793a\u51fa\u901a\u8fc7\u5206\u6790\u5065\u5eb7\u4e2a\u4f53\u7684\u78c1\u5171\u632f\u6210\u50cf (MRI) \u6570\u636e\u6765\u51c6\u786e\u4f30\u8ba1\u5927\u8111\u5e74\u9f84\u7684\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u5927\u8111\u5e74\u9f84\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u5229\u7528\u6574\u4e2a\u8f93\u5165\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u4e24\u4e2a\u91cd\u8981\u7684\u8003\u8651\u56e0\u7d20\uff1a1\uff09\u5927\u8111\u8870\u8001\u7684\u5f02\u8d28\u6027\uff0c\u5176\u4e2d\u4e0d\u540c\u7684\u5927\u8111\u533a\u57df\u53ef\u80fd\u4ee5\u4e0d\u540c\u7684\u901f\u5ea6\u9000\u5316\uff0c2\uff09\u5927\u8111\u8001\u5316\u4e2d\u5b58\u5728\u4e0e\u5e74\u9f84\u65e0\u5173\u7684\u5197\u4f59\u3002\u5927\u8111\u7ed3\u6784\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u56fe\u6ce8\u610f\u529b\u7684\u89e3\u7f20\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08DGA-DMIL\uff09\u6846\u67b6\u6765\u6539\u8fdb\u5927\u8111\u5e74\u9f84\u4f30\u8ba1\u3002\u5177\u4f53\u6765\u8bf4\uff0c3D MRI \u6570\u636e\u88ab\u89c6\u4e3a\u5b9e\u4f8b\u5305\uff0c\u88ab\u8f93\u5165\u5230 2D \u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e3b\u5e72\u4e2d\uff0c\u4ee5\u6355\u83b7 MRI \u4e2d\u72ec\u7279\u7684\u8001\u5316\u6a21\u5f0f\u3002\u7136\u540e\u63d0\u51fa\u4e86\u53cc\u56fe\u6ce8\u610f\u529b\u805a\u5408\u5668\uff0c\u901a\u8fc7\u5229\u7528\u5b9e\u4f8b\u5185\u548c\u5b9e\u4f8b\u95f4\u5173\u7cfb\u6765\u5b66\u4e60\u4e3b\u5e72\u7279\u5f81\u3002\u6b64\u5916\uff0c\u5f15\u5165\u89e3\u7f20\u7ed3\u5206\u652f\u5c06\u4e0e\u5e74\u9f84\u76f8\u5173\u7684\u7279\u5f81\u4e0e\u4e0e\u5e74\u9f84\u65e0\u5173\u7684\u7ed3\u6784\u8868\u793a\u5206\u79bb\uff0c\u4ee5\u6539\u5584\u5197\u4f59\u4fe1\u606f\u5bf9\u5e74\u9f84\u9884\u6d4b\u7684\u5e72\u6270\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728 UK Biobank \u548c ADNI \u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u603b\u5171\u5305\u542b 35,388 \u540d\u5065\u5eb7\u4e2a\u4f53\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4f30\u8ba1\u5927\u8111\u5e74\u9f84\u65b9\u9762\u8868\u73b0\u51fa\u6781\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5728\u82f1\u56fd\u751f\u7269\u94f6\u884c\u4e2d\u5b9e\u73b0\u4e86 2.12 \u5e74\u7684\u663e\u7740\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u7ade\u4e89\u6027\u5927\u8111\u5e74\u9f84\u4f30\u8ba1\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u6700\u5148\u8fdb\u7684\u3002\u6b64\u5916\uff0c\u5b9e\u4f8b\u8d21\u732e\u5206\u6570\u786e\u5b9a\u4e86\u5927\u8111\u533a\u57df\u5bf9\u4e8e\u8870\u8001\u9884\u6d4b\u7684\u4e0d\u540c\u91cd\u8981\u6027\uff0c\u8fd9\u4e3a\u7406\u89e3\u5927\u8111\u8870\u8001\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002|[2403.01246v1](http://arxiv.org/pdf/2403.01246v1)|null|\n"}, "3D/CG": {"2403.01053": "|**2024-03-05**|**Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling**|\u770b\u5230\u770b\u4e0d\u89c1\u7684\u4e1c\u897f\uff1a\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u6982\u7387\u5efa\u6a21\u53d1\u73b0\u65b0\u7684\u751f\u7269\u533b\u5b66\u6982\u5ff5|Jianan Fan, Dongnan Liu, Hang Chang, Heng Huang, Mei Chen, Weidong Cai|Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a geometry-constrained probabilistic modeling treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the interference of distributional latent bias. Then, we incorporate a suite of critical geometric properties to impose proper constraints on the layout of constructed embedding space, which in turn minimizes the uncontrollable risk for unknown class learning and structuring. Furthermore, a spectral graph-theoretic method is devised to estimate the number of potential novel classes. It inherits two intriguing merits compared to existent approaches, namely high computational efficiency and flexibility for taxonomy-adaptive estimation. Extensive experiments across various biomedical scenarios substantiate the effectiveness and general applicability of our method.|\u673a\u5668\u5b66\u4e60\u51ed\u501f\u5176\u6570\u636e\u9a71\u52a8\u7684\u6027\u8d28\uff0c\u4e3a\u6539\u53d8\u79d1\u5b66\u53d1\u73b0\u7684\u57fa\u672c\u5b9e\u8df5\u5e26\u6765\u4e86\u5de8\u5927\u7684\u5e0c\u671b\u3002\u968f\u7740\u7814\u7a76\u6570\u636e\u6536\u96c6\u7684\u4e0d\u65ad\u589e\u52a0\uff0c\u4ece\u89c2\u5bdf\u6570\u636e\u4e2d\u81ea\u4e3b\u63a2\u7d22\u6a21\u5f0f\u548c\u89c1\u89e3\u4ee5\u53d1\u73b0\u65b0\u7684\u8868\u578b\u548c\u6982\u5ff5\u7c7b\u522b\u5c06\u5f88\u6709\u5438\u5f15\u529b\u3002\u7136\u800c\uff0c\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\uff0c\u7d2f\u79ef\u6570\u636e\u672c\u8eab\u5b58\u5728\u4e00\u4e9b\u6311\u6218\uff0c\u963b\u788d\u4e86\u65b0\u7c7b\u522b\u53d1\u73b0\u7684\u8fdb\u5c55\u3002\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5206\u5e03\u4f34\u968f\u7740\u4e0d\u540c\u7c7b\u7ec4\u4e4b\u95f4\u7684\u4e25\u91cd\u4e0d\u5e73\u8861\uff0c\u672c\u8d28\u4e0a\u5bfc\u81f4\u4e86\u8bed\u4e49\u8868\u793a\u7684\u6a21\u7cca\u6027\u548c\u504f\u5dee\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u7ea6\u675f\u7684\u6982\u7387\u5efa\u6a21\u5904\u7406\u6765\u89e3\u51b3\u5df2\u8bc6\u522b\u7684\u95ee\u9898\u3002\u9996\u5148\uff0c\u6211\u4eec\u5efa\u8bae\u5c06\u5b9e\u4f8b\u5d4c\u5165\u7684\u8fd1\u4f3c\u540e\u9a8c\u53c2\u6570\u5316\u4e3a\u8fb9\u9645 von MisesFisher \u5206\u5e03\uff0c\u4ee5\u8003\u8651\u5206\u5e03\u6f5c\u5728\u504f\u5dee\u7684\u5e72\u6270\u3002\u7136\u540e\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u4e00\u5957\u5173\u952e\u7684\u51e0\u4f55\u5c5e\u6027\uff0c\u5bf9\u6784\u5efa\u7684\u5d4c\u5165\u7a7a\u95f4\u7684\u5e03\u5c40\u65bd\u52a0\u9002\u5f53\u7684\u7ea6\u675f\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u672a\u77e5\u7c7b\u5b66\u4e60\u548c\u7ed3\u6784\u5316\u7684\u4e0d\u53ef\u63a7\u98ce\u9669\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u8c31\u56fe\u7406\u8bba\u65b9\u6cd5\u6765\u4f30\u8ba1\u6f5c\u5728\u65b0\u7c7b\u522b\u7684\u6570\u91cf\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u7ee7\u627f\u4e86\u4e24\u4e2a\u6709\u8da3\u7684\u4f18\u70b9\uff0c\u5373\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u5206\u7c7b\u81ea\u9002\u5e94\u4f30\u8ba1\u7684\u7075\u6d3b\u6027\u3002\u8de8\u5404\u79cd\u751f\u7269\u533b\u5b66\u573a\u666f\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u666e\u904d\u9002\u7528\u6027\u3002|[2403.01053v2](http://arxiv.org/pdf/2403.01053v2)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {}, "\u5176\u4ed6": {"2403.01345": "|**2024-03-02**|**ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation**|ShapeBoost\uff1a\u901a\u8fc7\u57fa\u4e8e\u90e8\u4f4d\u7684\u53c2\u6570\u5316\u548c\u670d\u88c5\u4fdd\u7559\u589e\u5f3a\u6765\u4fc3\u8fdb\u4eba\u4f53\u5f62\u72b6\u4f30\u8ba1|Siyuan Bian, Jiefeng Li, Jiasheng Tang, Cewu Lu|Accurate human shape recovery from a monocular RGB image is a challenging task because humans come in different shapes and sizes and wear different clothes. In this paper, we propose ShapeBoost, a new human shape recovery framework that achieves pixel-level alignment even for rare body shapes and high accuracy for people wearing different types of clothes. Unlike previous approaches that rely on the use of PCA-based shape coefficients, we adopt a new human shape parameterization that decomposes the human shape into bone lengths and the mean width of each part slice. This part-based parameterization technique achieves a balance between flexibility and validity using a semi-analytical shape reconstruction algorithm. Based on this new parameterization, a clothing-preserving data augmentation module is proposed to generate realistic images with diverse body shapes and accurate annotations. Experimental results show that our method outperforms other state-of-the-art methods in diverse body shape situations as well as in varied clothing situations.|\u4ece\u5355\u76ee RGB \u56fe\u50cf\u4e2d\u51c6\u786e\u6062\u590d\u4eba\u4f53\u5f62\u72b6\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u4eba\u7c7b\u5177\u6709\u4e0d\u540c\u7684\u5f62\u72b6\u548c\u5927\u5c0f\u5e76\u4e14\u7a7f\u7740\u4e0d\u540c\u7684\u8863\u670d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ShapeBoost\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u4eba\u4f53\u5f62\u72b6\u6062\u590d\u6846\u67b6\uff0c\u5373\u4f7f\u5bf9\u4e8e\u7f55\u89c1\u7684\u4f53\u5f62\u4e5f\u80fd\u5b9e\u73b0\u50cf\u7d20\u7ea7\u5bf9\u9f50\uff0c\u5e76\u4e14\u5bf9\u4e8e\u7a7f\u7740\u4e0d\u540c\u7c7b\u578b\u8863\u670d\u7684\u4eba\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002\u4e0e\u4e4b\u524d\u4f9d\u8d56\u4f7f\u7528\u57fa\u4e8e PCA \u7684\u5f62\u72b6\u7cfb\u6570\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u4f53\u5f62\u72b6\u53c2\u6570\u5316\uff0c\u5c06\u4eba\u4f53\u5f62\u72b6\u5206\u89e3\u4e3a\u9aa8\u9abc\u957f\u5ea6\u548c\u6bcf\u4e2a\u90e8\u5206\u5207\u7247\u7684\u5e73\u5747\u5bbd\u5ea6\u3002\u8fd9\u79cd\u57fa\u4e8e\u96f6\u4ef6\u7684\u53c2\u6570\u5316\u6280\u672f\u4f7f\u7528\u534a\u89e3\u6790\u5f62\u72b6\u91cd\u5efa\u7b97\u6cd5\u5b9e\u73b0\u4e86\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u57fa\u4e8e\u8fd9\u79cd\u65b0\u7684\u53c2\u6570\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u670d\u88c5\u4fdd\u62a4\u6570\u636e\u589e\u5f3a\u6a21\u5757\uff0c\u4ee5\u751f\u6210\u5177\u6709\u4e0d\u540c\u8eab\u4f53\u5f62\u72b6\u548c\u51c6\u786e\u6ce8\u91ca\u7684\u903c\u771f\u56fe\u50cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u4f53\u578b\u60c5\u51b5\u4ee5\u53ca\u4e0d\u540c\u7684\u670d\u88c5\u60c5\u51b5\u4e0b\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2403.01345v1](http://arxiv.org/pdf/2403.01345v1)|null|\n", "2403.01344": "|**2024-03-02**|**Mitigating the Bias in the Model for Continual Test-Time Adaptation**|\u51cf\u8f7b\u6301\u7eed\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u6a21\u578b\u4e2d\u7684\u504f\u5dee|Inseop Chung, Kyomin Hwang, Jayeon Yoo, Nojun Kwak|Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt a source pre-trained model to continually changing target domains. In the CTA setting, a model does not know when the target domain changes, thus facing a drastic change in the distribution of streaming inputs during the test-time. The key challenge is to keep adapting the model to the continually changing target domains in an online manner. We find that a model shows highly biased predictions as it constantly adapts to the chaining distribution of the target data. It predicts certain classes more often than other classes, making inaccurate over-confident predictions. This paper mitigates this issue to improve performance in the CTA scenario. To alleviate the bias issue, we make class-wise exponential moving average target prototypes with reliable target samples and exploit them to cluster the target features class-wisely. Moreover, we aim to align the target distributions to the source distribution by anchoring the target feature to its corresponding source prototype. With extensive experiments, our proposed method achieves noteworthy performance gain when applied on top of existing CTA methods without substantial adaptation time overhead.|\u6301\u7eed\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94 (CTA) \u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u65e8\u5728\u4f7f\u6e90\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u76ee\u6807\u9886\u57df\u3002\u5728 CTA \u8bbe\u7f6e\u4e2d\uff0c\u6a21\u578b\u4e0d\u77e5\u9053\u76ee\u6807\u57df\u4f55\u65f6\u53d1\u751f\u53d8\u5316\uff0c\u56e0\u6b64\u5728\u6d4b\u8bd5\u671f\u95f4\u9762\u4e34\u6d41\u8f93\u5165\u5206\u5e03\u7684\u5de8\u5927\u53d8\u5316\u3002\u5173\u952e\u7684\u6311\u6218\u662f\u4ee5\u5728\u7ebf\u65b9\u5f0f\u4e0d\u65ad\u8c03\u6574\u6a21\u578b\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u76ee\u6807\u9886\u57df\u3002\u6211\u4eec\u53d1\u73b0\u6a21\u578b\u663e\u793a\u51fa\u9ad8\u5ea6\u504f\u5dee\u7684\u9884\u6d4b\uff0c\u56e0\u4e3a\u5b83\u4e0d\u65ad\u9002\u5e94\u76ee\u6807\u6570\u636e\u7684\u94fe\u63a5\u5206\u5e03\u3002\u5b83\u6bd4\u5176\u4ed6\u7c7b\u522b\u66f4\u9891\u7e41\u5730\u9884\u6d4b\u67d0\u4e9b\u7c7b\u522b\uff0c\u4ece\u800c\u505a\u51fa\u4e0d\u51c6\u786e\u7684\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\u3002\u672c\u6587\u7f13\u89e3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8 CTA \u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u7f13\u89e3\u504f\u5dee\u95ee\u9898\uff0c\u6211\u4eec\u4f7f\u7528\u53ef\u9760\u7684\u76ee\u6807\u6837\u672c\u5236\u4f5c\u4e86\u7c7b\u522b\u6307\u6570\u79fb\u52a8\u5e73\u5747\u76ee\u6807\u539f\u578b\uff0c\u5e76\u5229\u7528\u5b83\u4eec\u5bf9\u76ee\u6807\u7279\u5f81\u8fdb\u884c\u7c7b\u522b\u805a\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u5c06\u76ee\u6807\u7279\u5f81\u951a\u5b9a\u5230\u5176\u76f8\u5e94\u7684\u6e90\u539f\u578b\u6765\u5c06\u76ee\u6807\u5206\u5e03\u4e0e\u6e90\u5206\u5e03\u5bf9\u9f50\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u73b0\u6709 CTA \u65b9\u6cd5\u4e4b\u4e0a\u65f6\u5b9e\u73b0\u4e86\u663e\u7740\u7684\u6027\u80fd\u589e\u76ca\uff0c\u800c\u65e0\u9700\u5927\u91cf\u7684\u9002\u5e94\u65f6\u95f4\u5f00\u9500\u3002|[2403.01344v1](http://arxiv.org/pdf/2403.01344v1)|null|\n", "2403.01263": "|**2024-03-02**|**Single-image camera calibration with model-free distortion correction**|\u5177\u6709\u65e0\u6a21\u578b\u7578\u53d8\u6821\u6b63\u7684\u5355\u56fe\u50cf\u76f8\u673a\u6821\u51c6|Katia Genovese|Camera calibration is a process of paramount importance in computer vision applications that require accurate quantitative measurements. The popular method developed by Zhang relies on the use of a large number of images of a planar grid of fiducial points captured in multiple poses. Although flexible and easy to implement, Zhang's method has some limitations. The simultaneous optimization of the entire parameter set, including the coefficients of a predefined distortion model, may result in poor distortion correction at the image boundaries or in miscalculation of the intrinsic parameters, even with a reasonably small reprojection error. Indeed, applications involving image stitching (e.g. multi-camera systems) require accurate mapping of distortion up to the outermost regions of the image. Moreover, intrinsic parameters affect the accuracy of camera pose estimation, which is fundamental for applications such as vision servoing in robot navigation and automated assembly. This paper proposes a method for estimating the complete set of calibration parameters from a single image of a planar speckle pattern covering the entire sensor. The correspondence between image points and physical points on the calibration target is obtained using Digital Image Correlation. The effective focal length and the extrinsic parameters are calculated separately after a prior evaluation of the principal point. At the end of the procedure, a dense and uniform model-free distortion map is obtained over the entire image. Synthetic data with different noise levels were used to test the feasibility of the proposed method and to compare its metrological performance with Zhang's method. Real-world tests demonstrate the potential of the developed method to reveal aspects of the image formation that are hidden by averaging over multiple images.|\u76f8\u673a\u6821\u51c6\u662f\u9700\u8981\u7cbe\u786e\u5b9a\u91cf\u6d4b\u91cf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u8fc7\u7a0b\u3002\u5f20\u5f00\u53d1\u7684\u6d41\u884c\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4f7f\u7528\u4ee5\u591a\u79cd\u59ff\u52bf\u6355\u83b7\u7684\u57fa\u51c6\u70b9\u5e73\u9762\u7f51\u683c\u7684\u5927\u91cf\u56fe\u50cf\u3002\u867d\u7136\u7075\u6d3b\u4e14\u6613\u4e8e\u5b9e\u65bd\uff0c\u4f46\u5f20\u7684\u65b9\u6cd5\u6709\u4e00\u4e9b\u5c40\u9650\u6027\u3002\u5373\u4f7f\u91cd\u6295\u5f71\u8bef\u5dee\u76f8\u5f53\u5c0f\uff0c\u540c\u65f6\u4f18\u5316\u6574\u4e2a\u53c2\u6570\u96c6\uff08\u5305\u62ec\u9884\u5b9a\u4e49\u5931\u771f\u6a21\u578b\u7684\u7cfb\u6570\uff09\u4e5f\u53ef\u80fd\u5bfc\u81f4\u56fe\u50cf\u8fb9\u754c\u5904\u7684\u5931\u771f\u6821\u6b63\u6548\u679c\u4e0d\u4f73\u6216\u5185\u5728\u53c2\u6570\u8ba1\u7b97\u9519\u8bef\u3002\u4e8b\u5b9e\u4e0a\uff0c\u6d89\u53ca\u56fe\u50cf\u62fc\u63a5\u7684\u5e94\u7528\uff08\u4f8b\u5982\u591a\u6444\u50cf\u5934\u7cfb\u7edf\uff09\u9700\u8981\u5c06\u7578\u53d8\u7cbe\u786e\u6620\u5c04\u5230\u56fe\u50cf\u7684\u6700\u5916\u5c42\u533a\u57df\u3002\u6b64\u5916\uff0c\u5185\u5728\u53c2\u6570\u4f1a\u5f71\u54cd\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u5bf9\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\u548c\u81ea\u52a8\u5316\u88c5\u914d\u4e2d\u7684\u89c6\u89c9\u4f3a\u670d\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u8986\u76d6\u6574\u4e2a\u4f20\u611f\u5668\u7684\u5e73\u9762\u6563\u6591\u56fe\u6848\u7684\u5355\u4e2a\u56fe\u50cf\u4f30\u8ba1\u6574\u5957\u6821\u51c6\u53c2\u6570\u7684\u65b9\u6cd5\u3002\u4f7f\u7528\u6570\u5b57\u56fe\u50cf\u76f8\u5173\u83b7\u5f97\u56fe\u50cf\u70b9\u548c\u6821\u51c6\u76ee\u6807\u4e0a\u7684\u7269\u7406\u70b9\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u5728\u9884\u5148\u8bc4\u4f30\u4e3b\u70b9\u540e\uff0c\u5206\u522b\u8ba1\u7b97\u6709\u6548\u7126\u8ddd\u548c\u5916\u5728\u53c2\u6570\u3002\u5728\u8be5\u8fc7\u7a0b\u7ed3\u675f\u65f6\uff0c\u5728\u6574\u4e2a\u56fe\u50cf\u4e0a\u83b7\u5f97\u5bc6\u96c6\u4e14\u5747\u5300\u7684\u65e0\u6a21\u578b\u7578\u53d8\u56fe\u3002\u4f7f\u7528\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u7684\u5408\u6210\u6570\u636e\u6765\u6d4b\u8bd5\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c06\u5176\u8ba1\u91cf\u6027\u80fd\u4e0e\u5f20\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u73b0\u5b9e\u4e16\u754c\u7684\u6d4b\u8bd5\u8bc1\u660e\u4e86\u6240\u5f00\u53d1\u7684\u65b9\u6cd5\u6709\u6f5c\u529b\u63ed\u793a\u901a\u8fc7\u5bf9\u591a\u4e2a\u56fe\u50cf\u8fdb\u884c\u5e73\u5747\u800c\u9690\u85cf\u7684\u56fe\u50cf\u5f62\u6210\u7684\u5404\u4e2a\u65b9\u9762\u3002|[2403.01263v1](http://arxiv.org/pdf/2403.01263v1)|null|\n", "2403.01174": "|**2024-03-02**|**Consistent and Asymptotically Statistically-Efficient Solution to Camera Motion Estimation**|\u76f8\u673a\u8fd0\u52a8\u4f30\u8ba1\u7684\u4e00\u81f4\u4e14\u6e10\u8fd1\u7edf\u8ba1\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848|Guangyang Zeng, Qingcheng Zeng, Xinghan Li, Biqiang Mu, Jiming Chen, Ling Shi, Junfeng Wu|Given 2D point correspondences between an image pair, inferring the camera motion is a fundamental issue in the computer vision community. The existing works generally set out from the epipolar constraint and estimate the essential matrix, which is not optimal in the maximum likelihood (ML) sense. In this paper, we dive into the original measurement model with respect to the rotation matrix and normalized translation vector and formulate the ML problem. We then propose a two-step algorithm to solve it: In the first step, we estimate the variance of measurement noises and devise a consistent estimator based on bias elimination; In the second step, we execute a one-step Gauss-Newton iteration on manifold to refine the consistent estimate. We prove that the proposed estimate owns the same asymptotic statistical properties as the ML estimate: The first is consistency, i.e., the estimate converges to the ground truth as the point number increases; The second is asymptotic efficiency, i.e., the mean squared error of the estimate converges to the theoretical lower bound -- Cramer-Rao bound. In addition, we show that our algorithm has linear time complexity. These appealing characteristics endow our estimator with a great advantage in the case of dense point correspondences. Experiments on both synthetic data and real images demonstrate that when the point number reaches the order of hundreds, our estimator outperforms the state-of-the-art ones in terms of estimation accuracy and CPU time.|\u7ed9\u5b9a\u56fe\u50cf\u5bf9\u4e4b\u95f4\u7684 2D \u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u63a8\u65ad\u76f8\u673a\u8fd0\u52a8\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u793e\u533a\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u73b0\u6709\u7684\u5de5\u4f5c\u4e00\u822c\u4ece\u6781\u7ebf\u7ea6\u675f\u51fa\u53d1\uff0c\u4f30\u8ba1\u672c\u8d28\u77e9\u9635\uff0c\u8fd9\u5728\u6700\u5927\u4f3c\u7136\uff08ML\uff09\u610f\u4e49\u4e0a\u5e76\u4e0d\u662f\u6700\u4f18\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86\u5173\u4e8e\u65cb\u8f6c\u77e9\u9635\u548c\u5f52\u4e00\u5316\u5e73\u79fb\u5411\u91cf\u7684\u539f\u59cb\u6d4b\u91cf\u6a21\u578b\uff0c\u5e76\u5236\u5b9a\u4e86\u673a\u5668\u5b66\u4e60\u95ee\u9898\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u6b65\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1a\u7b2c\u4e00\u6b65\uff0c\u6211\u4eec\u4f30\u8ba1\u6d4b\u91cf\u566a\u58f0\u7684\u65b9\u5dee\uff0c\u5e76\u8bbe\u8ba1\u4e00\u4e2a\u57fa\u4e8e\u504f\u5dee\u6d88\u9664\u7684\u4e00\u81f4\u4f30\u8ba1\u5668\uff1b\u5728\u7b2c\u4e8c\u6b65\u4e2d\uff0c\u6211\u4eec\u5728\u6d41\u5f62\u4e0a\u6267\u884c\u4e00\u6b65\u9ad8\u65af-\u725b\u987f\u8fed\u4ee3\u4ee5\u7ec6\u5316\u4e00\u81f4\u7684\u4f30\u8ba1\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u4f30\u8ba1\u5177\u6709\u4e0e ML \u4f30\u8ba1\u76f8\u540c\u7684\u6e10\u8fd1\u7edf\u8ba1\u7279\u6027\uff1a\u9996\u5148\u662f\u4e00\u81f4\u6027\uff0c\u5373\u968f\u7740\u70b9\u6570\u7684\u589e\u52a0\uff0c\u4f30\u8ba1\u6536\u655b\u5230\u771f\u5b9e\u503c\uff1b\u7b2c\u4e8c\u4e2a\u662f\u6e10\u8fd1\u6548\u7387\uff0c\u5373\u4f30\u8ba1\u7684\u5747\u65b9\u8bef\u5dee\u6536\u655b\u5230\u7406\u8bba\u4e0b\u754c\u2014\u2014Cramer-Rao \u754c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8868\u660e\u6211\u4eec\u7684\u7b97\u6cd5\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u8fd9\u4e9b\u5438\u5f15\u4eba\u7684\u7279\u5f81\u4f7f\u6211\u4eec\u7684\u4f30\u8ba1\u5668\u5728\u5bc6\u96c6\u70b9\u5bf9\u5e94\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u5f88\u5927\u7684\u4f18\u52bf\u3002\u5bf9\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u56fe\u50cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u70b9\u6570\u8fbe\u5230\u6570\u767e\u4e2a\u6570\u91cf\u7ea7\u65f6\uff0c\u6211\u4eec\u7684\u4f30\u8ba1\u5668\u5728\u4f30\u8ba1\u7cbe\u5ea6\u548c CPU \u65f6\u95f4\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4f30\u8ba1\u5668\u3002|[2403.01174v1](http://arxiv.org/pdf/2403.01174v1)|null|\n", "2403.01142": "|**2024-03-02**|**Edge-guided Low-light Image Enhancement with Inertial Bregman Alternating Linearized Minimization**|\u91c7\u7528\u60ef\u6027 Bregman \u4ea4\u66ff\u7ebf\u6027\u5316\u6700\u5c0f\u5316\u7684\u8fb9\u7f18\u5f15\u5bfc\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a|Chaoyan Huang, Zhongming Wu, Tieyong Zeng|Prior-based methods for low-light image enhancement often face challenges in extracting available prior information from dim images. To overcome this limitation, we introduce a simple yet effective Retinex model with the proposed edge extraction prior. More specifically, we design an edge extraction network to capture the fine edge features from the low-light image directly. Building upon the Retinex theory, we decompose the low-light image into its illumination and reflectance components and introduce an edge-guided Retinex model for enhancing low-light images. To solve the proposed model, we propose a novel inertial Bregman alternating linearized minimization algorithm. This algorithm addresses the optimization problem associated with the edge-guided Retinex model, enabling effective enhancement of low-light images. Through rigorous theoretical analysis, we establish the convergence properties of the algorithm. Besides, we prove that the proposed algorithm converges to a stationary point of the problem through nonconvex optimization theory. Furthermore, extensive experiments are conducted on multiple real-world low-light image datasets to demonstrate the efficiency and superiority of the proposed scheme.|\u57fa\u4e8e\u5148\u9a8c\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u9762\u4e34\u7740\u4ece\u6697\u6de1\u56fe\u50cf\u4e2d\u63d0\u53d6\u53ef\u7528\u5148\u9a8c\u4fe1\u606f\u7684\u6311\u6218\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684 Retinex \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u6240\u63d0\u51fa\u7684\u8fb9\u7f18\u63d0\u53d6\u5148\u9a8c\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fb9\u7f18\u63d0\u53d6\u7f51\u7edc\u6765\u76f4\u63a5\u4ece\u4f4e\u5149\u56fe\u50cf\u4e2d\u6355\u83b7\u7cbe\u7ec6\u8fb9\u7f18\u7279\u5f81\u3002\u57fa\u4e8e Retinex \u7406\u8bba\uff0c\u6211\u4eec\u5c06\u4f4e\u5149\u56fe\u50cf\u5206\u89e3\u4e3a\u5176\u7167\u660e\u548c\u53cd\u5c04\u5206\u91cf\uff0c\u5e76\u5f15\u5165\u8fb9\u7f18\u5f15\u5bfc\u7684 Retinex \u6a21\u578b\u6765\u589e\u5f3a\u4f4e\u5149\u56fe\u50cf\u3002\u4e3a\u4e86\u6c42\u89e3\u6240\u63d0\u51fa\u7684\u6a21\u578b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u60ef\u6027 Bregman \u4ea4\u66ff\u7ebf\u6027\u5316\u6700\u5c0f\u5316\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u89e3\u51b3\u4e86\u4e0e\u8fb9\u7f18\u5f15\u5bfc Retinex \u6a21\u578b\u76f8\u5173\u7684\u4f18\u5316\u95ee\u9898\uff0c\u80fd\u591f\u6709\u6548\u589e\u5f3a\u4f4e\u5149\u56fe\u50cf\u3002\u901a\u8fc7\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u7279\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u975e\u51f8\u4f18\u5316\u7406\u8bba\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u6536\u655b\u5230\u95ee\u9898\u7684\u9a7b\u70b9\u3002\u6b64\u5916\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u4f4e\u5149\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u8bc1\u660e\u8be5\u65b9\u6848\u7684\u6548\u7387\u548c\u4f18\u8d8a\u6027\u3002|[2403.01142v1](http://arxiv.org/pdf/2403.01142v1)|null|\n", "2403.01087": "|**2024-03-02**|**Towards Accurate Lip-to-Speech Synthesis in-the-Wild**|\u5b9e\u73b0\u91ce\u5916\u51c6\u786e\u7684\u5507\u8bed\u5408\u6210|Sindhu Hegde, Rudrabha Mukhopadhyay, C. V. Jawahar, Vinay Namboodiri|In this paper, we introduce a novel approach to address the task of synthesizing speech from silent videos of any in-the-wild speaker solely based on lip movements. The traditional approach of directly generating speech from lip videos faces the challenge of not being able to learn a robust language model from speech alone, resulting in unsatisfactory outcomes. To overcome this issue, we propose incorporating noisy text supervision using a state-of-the-art lip-to-text network that instills language information into our model. The noisy text is generated using a pre-trained lip-to-text model, enabling our approach to work without text annotations during inference. We design a visual text-to-speech network that utilizes the visual stream to generate accurate speech, which is in-sync with the silent input video. We perform extensive experiments and ablation studies, demonstrating our approach's superiority over the current state-of-the-art methods on various benchmark datasets. Further, we demonstrate an essential practical application of our method in assistive technology by generating speech for an ALS patient who has lost the voice but can make mouth movements. Our demo video, code, and additional details can be found at \\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw}.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u4ec5\u57fa\u4e8e\u5634\u5507\u8fd0\u52a8\u4ece\u4efb\u4f55\u91ce\u5916\u8bf4\u8bdd\u8005\u7684\u65e0\u58f0\u89c6\u9891\u4e2d\u5408\u6210\u8bed\u97f3\u7684\u4efb\u52a1\u3002\u76f4\u63a5\u4ece\u5507\u5f62\u89c6\u9891\u751f\u6210\u8bed\u97f3\u7684\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u7740\u65e0\u6cd5\u4ec5\u4ece\u8bed\u97f3\u4e2d\u5b66\u4e60\u9c81\u68d2\u8bed\u8a00\u6a21\u578b\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4ee4\u4eba\u6ee1\u610f\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5507\u8bed\u5230\u6587\u672c\u7f51\u7edc\u5c06\u566a\u58f0\u6587\u672c\u76d1\u7763\u878d\u5165\u5230\u6211\u4eec\u7684\u6a21\u578b\u4e2d\u3002\u5608\u6742\u7684\u6587\u672c\u662f\u4f7f\u7528\u9884\u5148\u8bad\u7ec3\u7684\u5507\u8bed\u5230\u6587\u672c\u6a21\u578b\u751f\u6210\u7684\uff0c\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u65e0\u9700\u6587\u672c\u6ce8\u91ca\u5373\u53ef\u5de5\u4f5c\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u89c6\u89c9\u6587\u672c\u5230\u8bed\u97f3\u7f51\u7edc\uff0c\u5229\u7528\u89c6\u89c9\u6d41\u751f\u6210\u51c6\u786e\u7684\u8bed\u97f3\uff0c\u8be5\u8bed\u97f3\u4e0e\u65e0\u58f0\u8f93\u5165\u89c6\u9891\u540c\u6b65\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u4e3a\u5931\u53bb\u58f0\u97f3\u4f46\u53ef\u4ee5\u8fdb\u884c\u5634\u5df4\u8fd0\u52a8\u7684 ALS \u60a3\u8005\u751f\u6210\u8bed\u97f3\u6765\u5c55\u793a\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8f85\u52a9\u6280\u672f\u4e2d\u7684\u91cd\u8981\u5b9e\u9645\u5e94\u7528\u3002\u6211\u4eec\u7684\u6f14\u793a\u89c6\u9891\u3001\u4ee3\u7801\u548c\u5176\u4ed6\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728 \\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw} \u4e2d\u627e\u5230\u3002|[2403.01087v1](http://arxiv.org/pdf/2403.01087v1)|null|\n"}}