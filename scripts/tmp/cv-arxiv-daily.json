{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.15055": "|**2024-01-26**|**Deep learning-based approach for tomato classification in complex scenes**|\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u590d\u6742\u573a\u666f\u756a\u8304\u5206\u7c7b\u65b9\u6cd5|Mikael A. Mousse, Bethel C. A. R. K. Atohoun, Cina Motamed|Tracking ripening tomatoes is time consuming and labor intensive. Artificial intelligence technologies combined with those of computer vision can help users optimize the process of monitoring the ripening status of plants. To this end, we have proposed a tomato ripening monitoring approach based on deep learning in complex scenes. The objective is to detect mature tomatoes and harvest them in a timely manner. The proposed approach is declined in two parts. Firstly, the images of the scene are transmitted to the pre-processing layer. This process allows the detection of areas of interest (area of the image containing tomatoes). Then, these images are used as input to the maturity detection layer. This layer, based on a deep neural network learning algorithm, classifies the tomato thumbnails provided to it in one of the following five categories: green, brittle, pink, pale red, mature red. The experiments are based on images collected from the internet gathered through searches using tomato state across diverse languages including English, German, French, and Spanish. The experimental results of the maturity detection layer on a dataset composed of images of tomatoes taken under the extreme conditions, gave a good classification rate.|\u8ffd\u8e2a\u6210\u719f\u7684\u897f\u7ea2\u67ff\u65e2\u8017\u65f6\u53c8\u8d39\u529b\u3002\u4eba\u5de5\u667a\u80fd\u6280\u672f\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u4f18\u5316\u76d1\u6d4b\u690d\u7269\u6210\u719f\u72b6\u6001\u7684\u8fc7\u7a0b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u590d\u6742\u573a\u666f\u4e0b\u6df1\u5ea6\u5b66\u4e60\u7684\u756a\u8304\u6210\u719f\u76d1\u6d4b\u65b9\u6cd5\u3002\u76ee\u7684\u662f\u68c0\u6d4b\u6210\u719f\u7684\u897f\u7ea2\u67ff\u5e76\u53ca\u65f6\u6536\u83b7\u3002\u63d0\u8bae\u7684\u65b9\u6cd5\u5206\u4e24\u90e8\u5206\u88ab\u62d2\u7edd\u3002\u9996\u5148\uff0c\u573a\u666f\u56fe\u50cf\u88ab\u4f20\u8f93\u5230\u9884\u5904\u7406\u5c42\u3002\u6b64\u8fc7\u7a0b\u5141\u8bb8\u68c0\u6d4b\u611f\u5174\u8da3\u7684\u533a\u57df\uff08\u5305\u542b\u897f\u7ea2\u67ff\u7684\u56fe\u50cf\u533a\u57df\uff09\u3002\u7136\u540e\uff0c\u8fd9\u4e9b\u56fe\u50cf\u7528\u4f5c\u6210\u719f\u5ea6\u68c0\u6d4b\u5c42\u7684\u8f93\u5165\u3002\u8be5\u5c42\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c06\u63d0\u4f9b\u7ed9\u5b83\u7684\u756a\u8304\u7f29\u7565\u56fe\u5206\u4e3a\u4ee5\u4e0b\u4e94\u7c7b\u4e4b\u4e00\uff1a\u7eff\u8272\u3001\u8106\u6027\u3001\u7c89\u7ea2\u8272\u3001\u6de1\u7ea2\u8272\u3001\u6210\u719f\u7ea2\u8272\u3002\u8fd9\u4e9b\u5b9e\u9a8c\u57fa\u4e8e\u4ece\u4e92\u8054\u7f51\u4e0a\u6536\u96c6\u7684\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u662f\u901a\u8fc7\u4f7f\u7528\u756a\u8304\u72b6\u6001\u5728\u82f1\u8bed\u3001\u5fb7\u8bed\u3001\u6cd5\u8bed\u548c\u897f\u73ed\u7259\u8bed\u7b49\u591a\u79cd\u8bed\u8a00\u4e2d\u8fdb\u884c\u641c\u7d22\u800c\u6536\u96c6\u7684\u3002\u6210\u719f\u5ea6\u68c0\u6d4b\u5c42\u5728\u7531\u6781\u7aef\u6761\u4ef6\u4e0b\u62cd\u6444\u7684\u897f\u7ea2\u67ff\u56fe\u50cf\u7ec4\u6210\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u7ed9\u51fa\u4e86\u826f\u597d\u7684\u5206\u7c7b\u7387\u3002|[2401.15055v1](http://arxiv.org/pdf/2401.15055v1)|null|\n", "2401.15048": "|**2024-01-26**|**Unrecognizable Yet Identifiable: Image Distortion with Preserved Embeddings**|\u65e0\u6cd5\u8bc6\u522b\u4f46\u53ef\u8bc6\u522b\uff1a\u4fdd\u7559\u5d4c\u5165\u7684\u56fe\u50cf\u5931\u771f|Dmytro Zakharov, Oleksandr Kuznetsov, Emanuele Frontoni|In the realm of security applications, biometric authentication systems play a crucial role, yet one often encounters challenges concerning privacy and security while developing one. One of the most fundamental challenges lies in avoiding storing biometrics directly in the storage but still achieving decently high accuracy. Addressing this issue, we contribute to both artificial intelligence and engineering fields. We introduce an innovative image distortion technique that effectively renders facial images unrecognizable to the eye while maintaining their identifiability by neural network models. From the theoretical perspective, we explore how reliable state-of-the-art biometrics recognition neural networks are by checking the maximal degree of image distortion, which leaves the predicted identity unchanged. On the other hand, applying this technique demonstrates a practical solution to the engineering challenge of balancing security, precision, and performance in biometric authentication systems. Through experimenting on the widely used datasets, we assess the effectiveness of our method in preserving AI feature representation and distorting relative to conventional metrics. We also compare our method with previously used approaches.|\u5728\u5b89\u5168\u5e94\u7528\u9886\u57df\uff0c\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4f46\u5728\u5f00\u53d1\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u65f6\u7ecf\u5e38\u9047\u5230\u9690\u79c1\u548c\u5b89\u5168\u65b9\u9762\u7684\u6311\u6218\u3002\u6700\u57fa\u672c\u7684\u6311\u6218\u4e4b\u4e00\u5728\u4e8e\u907f\u514d\u5c06\u751f\u7269\u8bc6\u522b\u6570\u636e\u76f4\u63a5\u5b58\u50a8\u5728\u5b58\u50a8\u5668\u4e2d\uff0c\u4f46\u4ecd\u80fd\u5b9e\u73b0\u76f8\u5f53\u9ad8\u7684\u51c6\u786e\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5728\u4eba\u5de5\u667a\u80fd\u548c\u5de5\u7a0b\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u56fe\u50cf\u5931\u771f\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u4f7f\u4eba\u773c\u65e0\u6cd5\u8bc6\u522b\u9762\u90e8\u56fe\u50cf\uff0c\u540c\u65f6\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4fdd\u6301\u5176\u53ef\u8bc6\u522b\u6027\u3002\u4ece\u7406\u8bba\u89d2\u5ea6\u6765\u770b\uff0c\u6211\u4eec\u901a\u8fc7\u68c0\u67e5\u56fe\u50cf\u5931\u771f\u7684\u6700\u5927\u7a0b\u5ea6\u6765\u63a2\u7d22\u6700\u5148\u8fdb\u7684\u751f\u7269\u8bc6\u522b\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u9760\u6027\uff0c\u8fd9\u4f7f\u5f97\u9884\u6d4b\u7684\u8eab\u4efd\u4fdd\u6301\u4e0d\u53d8\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5e94\u7528\u8be5\u6280\u672f\u5c55\u793a\u4e86\u5e73\u8861\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3001\u7cbe\u5ea6\u548c\u6027\u80fd\u7684\u5de5\u7a0b\u6311\u6218\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u5bf9\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u7559\u4eba\u5de5\u667a\u80fd\u7279\u5f81\u8868\u793a\u548c\u76f8\u5bf9\u4e8e\u4f20\u7edf\u6307\u6807\u7684\u626d\u66f2\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u8fd8\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u4ee5\u524d\u4f7f\u7528\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002|[2401.15048v1](http://arxiv.org/pdf/2401.15048v1)|null|\n", "2401.14919": "|**2024-01-26**|**PARSAC: Accelerating Robust Multi-Model Fitting with Parallel Sample Consensus**|PARSAC\uff1a\u901a\u8fc7\u5e76\u884c\u6837\u672c\u5171\u8bc6\u52a0\u901f\u7a33\u5065\u7684\u591a\u6a21\u578b\u62df\u5408|Florian Kluger, Bodo Rosenhahn|We present a real-time method for robust estimation of multiple instances of geometric models from noisy data. Geometric models such as vanishing points, planar homographies or fundamental matrices are essential for 3D scene analysis. Previous approaches discover distinct model instances in an iterative manner, thus limiting their potential for speedup via parallel computation. In contrast, our method detects all model instances independently and in parallel. A neural network segments the input data into clusters representing potential model instances by predicting multiple sets of sample and inlier weights. Using the predicted weights, we determine the model parameters for each potential instance separately in a RANSAC-like fashion. We train the neural network via task-specific loss functions, i.e. we do not require a ground-truth segmentation of the input data. As suitable training data for homography and fundamental matrix fitting is scarce, we additionally present two new synthetic datasets. We demonstrate state-of-the-art performance on these as well as multiple established datasets, with inference times as small as five milliseconds per image.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u9c81\u68d2\u5730\u4f30\u8ba1\u51e0\u4f55\u6a21\u578b\u7684\u591a\u4e2a\u5b9e\u4f8b\u3002\u6d88\u5931\u70b9\u3001\u5e73\u9762\u5355\u5e94\u6027\u6216\u57fa\u672c\u77e9\u9635\u7b49\u51e0\u4f55\u6a21\u578b\u5bf9\u4e8e 3D \u573a\u666f\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u524d\u7684\u65b9\u6cd5\u4ee5\u8fed\u4ee3\u65b9\u5f0f\u53d1\u73b0\u4e0d\u540c\u7684\u6a21\u578b\u5b9e\u4f8b\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u52a0\u901f\u7684\u6f5c\u529b\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u72ec\u7acb\u4e14\u5e76\u884c\u5730\u68c0\u6d4b\u6240\u6709\u6a21\u578b\u5b9e\u4f8b\u3002\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u9884\u6d4b\u591a\u7ec4\u6837\u672c\u548c\u5185\u70b9\u6743\u91cd\uff0c\u5c06\u8f93\u5165\u6570\u636e\u5206\u5272\u6210\u4ee3\u8868\u6f5c\u5728\u6a21\u578b\u5b9e\u4f8b\u7684\u7c07\u3002\u4f7f\u7528\u9884\u6d4b\u7684\u6743\u91cd\uff0c\u6211\u4eec\u4ee5\u7c7b\u4f3c RANSAC \u7684\u65b9\u5f0f\u5206\u522b\u786e\u5b9a\u6bcf\u4e2a\u6f5c\u5728\u5b9e\u4f8b\u7684\u6a21\u578b\u53c2\u6570\u3002\u6211\u4eec\u901a\u8fc7\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u635f\u5931\u51fd\u6570\u6765\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u5373\u6211\u4eec\u4e0d\u9700\u8981\u8f93\u5165\u6570\u636e\u7684\u771f\u5b9e\u5206\u5272\u3002\u7531\u4e8e\u5355\u5e94\u6027\u548c\u57fa\u672c\u77e9\u9635\u62df\u5408\u7684\u5408\u9002\u8bad\u7ec3\u6570\u636e\u5f88\u5c11\uff0c\u6211\u4eec\u53e6\u5916\u63d0\u4f9b\u4e86\u4e24\u4e2a\u65b0\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u6211\u4eec\u5728\u8fd9\u4e9b\u4ee5\u53ca\u591a\u4e2a\u5df2\u5efa\u7acb\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6bcf\u4e2a\u56fe\u50cf\u7684\u63a8\u7406\u65f6\u95f4\u77ed\u81f3\u4e94\u6beb\u79d2\u3002|[2401.14919v1](http://arxiv.org/pdf/2401.14919v1)|null|\n", "2401.14856": "|**2024-01-26**|**Memory-Inspired Temporal Prompt Interaction for Text-Image Classification**|\u7528\u4e8e\u6587\u672c\u56fe\u50cf\u5206\u7c7b\u7684\u53d7\u8bb0\u5fc6\u542f\u53d1\u7684\u65f6\u95f4\u63d0\u793a\u4ea4\u4e92|Xinyao Yu, Hao Sun, Ziwei Niu, Rui Qin, Zhenjia Bai, Yen-Wei Chen, Lanfen Lin|In recent years, large-scale pre-trained multimodal models (LMM) generally emerge to integrate the vision and language modalities, achieving considerable success in various natural language processing and computer vision tasks. The growing size of LMMs, however, results in a significant computational cost for fine-tuning these models for downstream tasks. Hence, prompt-based interaction strategy is studied to align modalities more efficiently. In this contex, we propose a novel prompt-based multimodal interaction strategy inspired by human memory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Our proposed method involves in two stages as in human memory strategy: the acquiring stage, and the consolidation and activation stage. We utilize temporal prompts on intermediate layers to imitate the acquiring stage, leverage similarity-based prompt interaction to imitate memory consolidation, and employ prompt generation strategy to imitate memory activation. The main strength of our paper is that we interact the prompt vectors on intermediate layers to leverage sufficient information exchange between modalities, with compressed trainable parameters and memory usage. We achieve competitive results on several datasets with relatively small memory usage and 2.0M of trainable parameters (about 1% of the pre-trained foundation model).|\u8fd1\u5e74\u6765\uff0c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u666e\u904d\u51fa\u73b0\uff0c\u4ee5\u6574\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\uff0c\u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u6210\u529f\u3002\u7136\u800c\uff0cLMM \u89c4\u6a21\u7684\u4e0d\u65ad\u589e\u957f\u5bfc\u81f4\u4e86\u4e3a\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u8fd9\u4e9b\u6a21\u578b\u7684\u5de8\u5927\u8ba1\u7b97\u6210\u672c\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u4ea4\u4e92\u7b56\u7565\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u8c03\u6574\u6a21\u5f0f\u3002\u5728\u8fd9\u79cd\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u53d7\u5230\u4eba\u7c7b\u8bb0\u5fc6\u7b56\u7565\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u7b56\u7565\uff0c\u5373\u8bb0\u5fc6\u542f\u53d1\u7684\u65f6\u95f4\u63d0\u793a\u4ea4\u4e92\uff08MITP\uff09\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u6d89\u53ca\u4eba\u7c7b\u8bb0\u5fc6\u7b56\u7565\u7684\u4e24\u4e2a\u9636\u6bb5\uff1a\u83b7\u53d6\u9636\u6bb5\u4ee5\u53ca\u5de9\u56fa\u548c\u6fc0\u6d3b\u9636\u6bb5\u3002\u6211\u4eec\u5229\u7528\u4e2d\u95f4\u5c42\u4e0a\u7684\u65f6\u95f4\u63d0\u793a\u6765\u6a21\u62df\u83b7\u53d6\u9636\u6bb5\uff0c\u5229\u7528\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u63d0\u793a\u4ea4\u4e92\u6765\u6a21\u62df\u8bb0\u5fc6\u5de9\u56fa\uff0c\u5e76\u91c7\u7528\u63d0\u793a\u751f\u6210\u7b56\u7565\u6765\u6a21\u62df\u8bb0\u5fc6\u6fc0\u6d3b\u3002\u6211\u4eec\u8bba\u6587\u7684\u4e3b\u8981\u4f18\u70b9\u662f\u6211\u4eec\u5728\u4e2d\u95f4\u5c42\u4e0a\u4ea4\u4e92\u63d0\u793a\u5411\u91cf\uff0c\u4ee5\u5229\u7528\u6a21\u6001\u4e4b\u95f4\u5145\u5206\u7684\u4fe1\u606f\u4ea4\u6362\uff0c\u5e76\u538b\u7f29\u53ef\u8bad\u7ec3\u53c2\u6570\u548c\u5185\u5b58\u4f7f\u7528\u3002\u6211\u4eec\u5728\u5185\u5b58\u4f7f\u7528\u76f8\u5bf9\u8f83\u5c0f\u548c 2.0M \u53ef\u8bad\u7ec3\u53c2\u6570\uff08\u7ea6\u5360\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684 1%\uff09\u7684\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002|[2401.14856v1](http://arxiv.org/pdf/2401.14856v1)|null|\n", "2401.14845": "|**2024-01-26**|**Adaptive Point Transformer**|\u81ea\u9002\u5e94\u70b9\u53d8\u538b\u5668|Alessandro Baiocchi, Indro Spinelli, Alessandro Nicolosi, Simone Scardapane|The recent surge in 3D data acquisition has spurred the development of geometric deep learning models for point cloud processing, boosted by the remarkable success of transformers in natural language processing. While point cloud transformers (PTs) have achieved impressive results recently, their quadratic scaling with respect to the point cloud size poses a significant scalability challenge for real-world applications. To address this issue, we propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model augmented by an adaptive token selection mechanism. AdaPT dynamically reduces the number of tokens during inference, enabling efficient processing of large point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust the computational cost of the model at inference time without the need for retraining or fine-tuning separate models. Our extensive experimental evaluation on point cloud classification tasks demonstrates that AdaPT significantly reduces computational complexity while maintaining competitive accuracy compared to standard PTs. The code for AdaPT is made publicly available.|\u6700\u8fd1 3D \u6570\u636e\u91c7\u96c6\u7684\u6fc0\u589e\u523a\u6fc0\u4e86\u70b9\u4e91\u5904\u7406\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u800c Transformer \u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\u53d6\u5f97\u7684\u5de8\u5927\u6210\u529f\u4e5f\u63a8\u52a8\u4e86\u8fd9\u4e00\u53d1\u5c55\u3002\u867d\u7136\u70b9\u4e91\u53d8\u6362\u5668 (PT) \u6700\u8fd1\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\uff0c\u4f46\u5b83\u4eec\u76f8\u5bf9\u4e8e\u70b9\u4e91\u5927\u5c0f\u7684\u4e8c\u6b21\u7f29\u653e\u5bf9\u5b9e\u9645\u5e94\u7528\u7a0b\u5e8f\u63d0\u51fa\u4e86\u91cd\u5927\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u70b9\u4e91\u53d8\u6362\u5668\uff08AdaPT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u9002\u5e94\u4ee4\u724c\u9009\u62e9\u673a\u5236\u589e\u5f3a\u7684\u6807\u51c6 PT \u6a21\u578b\u3002 AdaPT \u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u51cf\u5c11\u6807\u8bb0\u6570\u91cf\uff0c\u4ece\u800c\u5b9e\u73b0\u5927\u578b\u70b9\u4e91\u7684\u9ad8\u6548\u5904\u7406\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u9884\u7b97\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u63a8\u7406\u65f6\u7075\u6d3b\u8c03\u6574\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u5355\u72ec\u7684\u6a21\u578b\u3002\u6211\u4eec\u5bf9\u70b9\u4e91\u5206\u7c7b\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u6807\u51c6 PT \u76f8\u6bd4\uff0cAdaPT \u663e\u7740\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u3002 AdaPT \u7684\u4ee3\u7801\u5df2\u516c\u5f00\u3002|[2401.14845v1](http://arxiv.org/pdf/2401.14845v1)|null|\n", "2401.14838": "|**2024-01-26**|**Multi-modality action recognition based on dual feature shift in vehicle cabin monitoring**|\u8f66\u53a2\u76d1\u63a7\u4e2d\u57fa\u4e8e\u53cc\u7279\u5f81\u8f6c\u6362\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u8bc6\u522b|Dan Lin, Philip Hann Yung Lee, Yiming Li, Ruoyu Wang, Kim-Hui Yap, Bingbing Li, You Shing Ngim|Driver Action Recognition (DAR) is crucial in vehicle cabin monitoring systems. In real-world applications, it is common for vehicle cabins to be equipped with cameras featuring different modalities. However, multi-modality fusion strategies for the DAR task within car cabins have rarely been studied. In this paper, we propose a novel yet efficient multi-modality driver action recognition method based on dual feature shift, named DFS. DFS first integrates complementary features across modalities by performing modality feature interaction. Meanwhile, DFS achieves the neighbour feature propagation within single modalities, by feature shifting among temporal frames. To learn common patterns and improve model efficiency, DFS shares feature extracting stages among multiple modalities. Extensive experiments have been carried out to verify the effectiveness of the proposed DFS model on the Drive\\&Act dataset. The results demonstrate that DFS achieves good performance and improves the efficiency of multi-modality driver action recognition.|\u9a7e\u9a76\u5458\u52a8\u4f5c\u8bc6\u522b (DAR) \u5728\u8f66\u53a2\u76d1\u63a7\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8f66\u53a2\u5185\u901a\u5e38\u914d\u5907\u4e0d\u540c\u6a21\u5f0f\u7684\u6444\u50cf\u5934\u3002\u7136\u800c\uff0c\u8f66\u8231\u5185 DAR \u4efb\u52a1\u7684\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u5374\u5f88\u5c11\u88ab\u7814\u7a76\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u7279\u5f81\u8f6c\u6362\u7684\u65b0\u9896\u800c\u6709\u6548\u7684\u591a\u6a21\u6001\u9a7e\u9a76\u5458\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\uff0c\u79f0\u4e3aDFS\u3002 DFS \u9996\u5148\u901a\u8fc7\u6267\u884c\u6a21\u6001\u7279\u5f81\u4ea4\u4e92\u6765\u96c6\u6210\u8de8\u6a21\u6001\u7684\u4e92\u8865\u7279\u5f81\u3002\u540c\u65f6\uff0cDFS \u901a\u8fc7\u65f6\u95f4\u5e27\u4e4b\u95f4\u7684\u7279\u5f81\u79fb\u4f4d\u6765\u5b9e\u73b0\u5355\u6a21\u6001\u5185\u7684\u90bb\u8fd1\u7279\u5f81\u4f20\u64ad\u3002\u4e3a\u4e86\u5b66\u4e60\u5e38\u89c1\u6a21\u5f0f\u5e76\u63d0\u9ad8\u6a21\u578b\u6548\u7387\uff0cDFS \u5728\u591a\u79cd\u6a21\u6001\u4e4b\u95f4\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u9636\u6bb5\u3002\u5df2\u7ecf\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u6240\u63d0\u51fa\u7684 DFS \u6a21\u578b\u5728 Drive\\&Act \u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cDFS\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u9a7e\u9a76\u5458\u52a8\u4f5c\u8bc6\u522b\u7684\u6548\u7387\u3002|[2401.14838v1](http://arxiv.org/pdf/2401.14838v1)|null|\n", "2401.14832": "|**2024-01-26**|**Text Image Inpainting via Global Structure-Guided Diffusion Models**|\u901a\u8fc7\u5168\u5c40\u7ed3\u6784\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6587\u672c\u56fe\u50cf\u4fee\u590d|Shipeng Zhu, Pengfei Fang, Chenjie Zhu, Zuoyan Zhao, Qiang Xu, Hui Xue|Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure. These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification. Notably, current inpainting techniques often fail to adequately address this problem and have difficulties restoring accurate text images along with reasonable and consistent styles. Formulating this as an open problem of text image inpainting, this paper aims to build a benchmark to facilitate its study. In doing so, we establish two specific text inpainting datasets which contain scene text images and handwritten text images, respectively. Each of them includes images revamped by real-life and synthetic datasets, featuring pairs of original images, corrupted images, and other assistant information. On top of the datasets, we further develop a novel neural framework, Global Structure-guided Diffusion Model (GSDM), as a potential solution. Leveraging the global structure of the text as a prior, the proposed GSDM develops an efficient diffusion model to recover clean texts. The efficacy of our approach is demonstrated by thorough empirical study, including a substantial boost in both recognition accuracy and image quality. These findings not only highlight the effectiveness of our method but also underscore its potential to enhance the broader field of text image understanding and processing. Code and datasets are available at: https://github.com/blackprotoss/GSDM.|\u73b0\u5b9e\u4e16\u754c\u7684\u6587\u672c\u53ef\u80fd\u4f1a\u56e0\u73af\u5883\u6216\u4eba\u4e3a\u56e0\u7d20\u5f15\u8d77\u7684\u8150\u8680\u95ee\u9898\u800c\u53d7\u5230\u635f\u574f\uff0c\u8fd9\u963b\u788d\u4e86\u6587\u672c\u5b8c\u6574\u6837\u5f0f\uff08\u4f8b\u5982\u7eb9\u7406\u548c\u7ed3\u6784\uff09\u7684\u4fdd\u5b58\u3002\u8fd9\u4e9b\u8150\u8680\u95ee\u9898\uff0c\u4f8b\u5982\u6d82\u9e26\u6807\u5fd7\u548c\u4e0d\u5b8c\u6574\u7684\u7b7e\u540d\uff0c\u7ed9\u7406\u89e3\u6587\u672c\u5e26\u6765\u4e86\u56f0\u96be\uff0c\u4ece\u800c\u7ed9\u4e0b\u6e38\u5e94\u7528\uff08\u4f8b\u5982\u573a\u666f\u6587\u672c\u8bc6\u522b\u548c\u7b7e\u540d\u8bc6\u522b\uff09\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u524d\u7684\u4fee\u590d\u6280\u672f\u901a\u5e38\u65e0\u6cd5\u5145\u5206\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u4e14\u96be\u4ee5\u6062\u590d\u51c6\u786e\u7684\u6587\u672c\u56fe\u50cf\u4ee5\u53ca\u5408\u7406\u4e14\u4e00\u81f4\u7684\u6837\u5f0f\u3002\u672c\u6587\u5c06\u5176\u8868\u8ff0\u4e3a\u6587\u672c\u56fe\u50cf\u4fee\u590d\u7684\u5f00\u653e\u95ee\u9898\uff0c\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u57fa\u51c6\u6765\u4fc3\u8fdb\u5176\u7814\u7a76\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e24\u4e2a\u7279\u5b9a\u7684\u6587\u672c\u4fee\u590d\u6570\u636e\u96c6\uff0c\u5206\u522b\u5305\u542b\u573a\u666f\u6587\u672c\u56fe\u50cf\u548c\u624b\u5199\u6587\u672c\u56fe\u50cf\u3002\u5b83\u4eec\u4e2d\u7684\u6bcf\u4e00\u4e2a\u90fd\u5305\u542b\u6839\u636e\u73b0\u5b9e\u751f\u6d3b\u548c\u5408\u6210\u6570\u636e\u96c6\u4fee\u6539\u7684\u56fe\u50cf\uff0c\u5177\u6709\u6210\u5bf9\u7684\u539f\u59cb\u56fe\u50cf\u3001\u635f\u574f\u7684\u56fe\u50cf\u548c\u5176\u4ed6\u8f85\u52a9\u4fe1\u606f\u3002\u5728\u6570\u636e\u96c6\u4e4b\u4e0a\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u6846\u67b6\uff0c\u5373\u5168\u5c40\u7ed3\u6784\u5f15\u5bfc\u6269\u6563\u6a21\u578b\uff08GSDM\uff09\uff0c\u4f5c\u4e3a\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5229\u7528\u6587\u672c\u7684\u5168\u5c40\u7ed3\u6784\u4f5c\u4e3a\u5148\u9a8c\uff0c\u6240\u63d0\u51fa\u7684 GSDM \u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6269\u6563\u6a21\u578b\u6765\u6062\u590d\u5e72\u51c0\u7684\u6587\u672c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u901a\u8fc7\u5f7b\u5e95\u7684\u5b9e\u8bc1\u7814\u7a76\u5f97\u5230\u4e86\u8bc1\u660e\uff0c\u5305\u62ec\u8bc6\u522b\u7cbe\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\u7684\u5927\u5e45\u63d0\u9ad8\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e0d\u4ec5\u51f8\u663e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u800c\u4e14\u5f3a\u8c03\u4e86\u5176\u589e\u5f3a\u66f4\u5e7f\u6cdb\u7684\u6587\u672c\u56fe\u50cf\u7406\u89e3\u548c\u5904\u7406\u9886\u57df\u7684\u6f5c\u529b\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u83b7\u53d6\uff1ahttps://github.com/blackprotoss/GSDM\u3002|[2401.14832v1](http://arxiv.org/pdf/2401.14832v1)|null|\n", "2401.14792": "|**2024-01-26**|**Deep Variational Privacy Funnel: General Modeling with Applications in Face Recognition**|\u6df1\u5ea6\u53d8\u5206\u9690\u79c1\u6f0f\u6597\uff1a\u4eba\u8138\u8bc6\u522b\u5e94\u7528\u7684\u901a\u7528\u5efa\u6a21|Behrooz Razeghi, Parsa Rahimi, S\u00e9bastien Marcel|In this study, we harness the information-theoretic Privacy Funnel (PF) model to develop a method for privacy-preserving representation learning using an end-to-end training framework. We rigorously address the trade-off between obfuscation and utility. Both are quantified through the logarithmic loss, a measure also recognized as self-information loss. This exploration deepens the interplay between information-theoretic privacy and representation learning, offering substantive insights into data protection mechanisms for both discriminative and generative models. Importantly, we apply our model to state-of-the-art face recognition systems. The model demonstrates adaptability across diverse inputs, from raw facial images to both derived or refined embeddings, and is competent in tasks such as classification, reconstruction, and generation.|\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u4fe1\u606f\u8bba\u9690\u79c1\u6f0f\u6597\uff08PF\uff09\u6a21\u578b\u6765\u5f00\u53d1\u4e00\u79cd\u4f7f\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\u6765\u4fdd\u62a4\u9690\u79c1\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002\u6211\u4eec\u4e25\u683c\u89e3\u51b3\u6df7\u6dc6\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u4e24\u8005\u90fd\u662f\u901a\u8fc7\u5bf9\u6570\u635f\u5931\u6765\u91cf\u5316\u7684\uff0c\u8fd9\u79cd\u5ea6\u91cf\u4e5f\u88ab\u8ba4\u4e3a\u662f\u81ea\u6211\u4fe1\u606f\u635f\u5931\u3002\u8fd9\u4e00\u63a2\u7d22\u52a0\u6df1\u4e86\u4fe1\u606f\u8bba\u9690\u79c1\u548c\u8868\u5f81\u5b66\u4e60\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u5224\u522b\u6a21\u578b\u548c\u751f\u6210\u6a21\u578b\u7684\u6570\u636e\u4fdd\u62a4\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u8d28\u6027\u89c1\u89e3\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u5c06\u6211\u4eec\u7684\u6a21\u578b\u5e94\u7528\u4e8e\u6700\u5148\u8fdb\u7684\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u3002\u8be5\u6a21\u578b\u5c55\u793a\u4e86\u4ece\u539f\u59cb\u9762\u90e8\u56fe\u50cf\u5230\u6d3e\u751f\u6216\u7cbe\u70bc\u5d4c\u5165\u7684\u5404\u79cd\u8f93\u5165\u7684\u9002\u5e94\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u80dc\u4efb\u5206\u7c7b\u3001\u91cd\u5efa\u548c\u751f\u6210\u7b49\u4efb\u52a1\u3002|[2401.14792v1](http://arxiv.org/pdf/2401.14792v1)|null|\n", "2401.14729": "|**2024-01-26**|**Sketch and Refine: Towards Fast and Accurate Lane Detection**|\u8349\u56fe\u548c\u7ec6\u5316\uff1a\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684\u8f66\u9053\u68c0\u6d4b|Chao Chen, Jie Liu, Chang Zhou, Jie Tang, Gangshan Wu|Lane detection is to determine the precise location and shape of lanes on the road. Despite efforts made by current methods, it remains a challenging task due to the complexity of real-world scenarios. Existing approaches, whether proposal-based or keypoint-based, suffer from depicting lanes effectively and efficiently. Proposal-based methods detect lanes by distinguishing and regressing a collection of proposals in a streamlined top-down way, yet lack sufficient flexibility in lane representation. Keypoint-based methods, on the other hand, construct lanes flexibly from local descriptors, which typically entail complicated post-processing. In this paper, we present a \"Sketch-and-Refine\" paradigm that utilizes the merits of both keypoint-based and proposal-based methods. The motivation is that local directions of lanes are semantically simple and clear. At the \"Sketch\" stage, local directions of keypoints can be easily estimated by fast convolutional layers. Then we can build a set of lane proposals accordingly with moderate accuracy. At the \"Refine\" stage, we further optimize these proposals via a novel Lane Segment Association Module (LSAM), which allows adaptive lane segment adjustment. Last but not least, we propose multi-level feature integration to enrich lane feature representations more efficiently. Based on the proposed \"Sketch and Refine\" paradigm, we propose a fast yet effective lane detector dubbed \"SRLane\". Experiments show that our SRLane can run at a fast speed (i.e., 278 FPS) while yielding an F1 score of 78.9\\%. The source code is available at: https://github.com/passerer/SRLane.|\u8f66\u9053\u68c0\u6d4b\u662f\u786e\u5b9a\u9053\u8def\u4e0a\u8f66\u9053\u7684\u7cbe\u786e\u4f4d\u7f6e\u548c\u5f62\u72b6\u3002\u5c3d\u7ba1\u5f53\u524d\u65b9\u6cd5\u505a\u51fa\u4e86\u52aa\u529b\uff0c\u4f46\u7531\u4e8e\u73b0\u5b9e\u573a\u666f\u7684\u590d\u6742\u6027\uff0c\u8fd9\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u57fa\u4e8e\u63d0\u6848\u7684\u8fd8\u662f\u57fa\u4e8e\u5173\u952e\u70b9\u7684\uff0c\u90fd\u65e0\u6cd5\u6709\u6548\u4e14\u9ad8\u6548\u5730\u63cf\u7ed8\u8f66\u9053\u3002\u57fa\u4e8e\u63d0\u6848\u7684\u65b9\u6cd5\u901a\u8fc7\u4ee5\u7b80\u5316\u7684\u81ea\u4e0a\u800c\u4e0b\u7684\u65b9\u5f0f\u533a\u5206\u548c\u56de\u5f52\u63d0\u6848\u96c6\u5408\u6765\u68c0\u6d4b\u8f66\u9053\uff0c\u4f46\u5728\u8f66\u9053\u8868\u793a\u65b9\u9762\u7f3a\u4e4f\u8db3\u591f\u7684\u7075\u6d3b\u6027\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u65b9\u6cd5\u53ef\u4ee5\u6839\u636e\u5c40\u90e8\u63cf\u8ff0\u7b26\u7075\u6d3b\u5730\u6784\u9020\u8f66\u9053\uff0c\u8fd9\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u540e\u5904\u7406\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u8349\u56fe\u548c\u7ec6\u5316\u201d\u8303\u4f8b\uff0c\u8be5\u8303\u4f8b\u5229\u7528\u4e86\u57fa\u4e8e\u5173\u952e\u70b9\u548c\u57fa\u4e8e\u63d0\u6848\u7684\u65b9\u6cd5\u7684\u4f18\u70b9\u3002\u5176\u52a8\u673a\u662f\u8f66\u9053\u7684\u5c40\u90e8\u65b9\u5411\u5728\u8bed\u4e49\u4e0a\u7b80\u5355\u4e14\u6e05\u6670\u3002\u5728\u201c\u8349\u56fe\u201d\u9636\u6bb5\uff0c\u53ef\u4ee5\u901a\u8fc7\u5feb\u901f\u5377\u79ef\u5c42\u8f7b\u677e\u4f30\u8ba1\u5173\u952e\u70b9\u7684\u5c40\u90e8\u65b9\u5411\u3002\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u76f8\u5e94\u5730\u5efa\u7acb\u4e00\u7ec4\u5177\u6709\u4e2d\u7b49\u7cbe\u5ea6\u7684\u8f66\u9053\u5efa\u8bae\u3002\u5728\u201c\u4f18\u5316\u201d\u9636\u6bb5\uff0c\u6211\u4eec\u901a\u8fc7\u65b0\u9896\u7684\u8f66\u9053\u6bb5\u5173\u8054\u6a21\u5757\uff08LSAM\uff09\u8fdb\u4e00\u6b65\u4f18\u5316\u8fd9\u4e9b\u5efa\u8bae\uff0c\u8be5\u6a21\u5757\u5141\u8bb8\u81ea\u9002\u5e94\u8f66\u9053\u6bb5\u8c03\u6574\u3002\u6700\u540e\u4f46\u5e76\u975e\u6700\u4e0d\u91cd\u8981\u7684\u4e00\u70b9\u662f\uff0c\u6211\u4eec\u63d0\u51fa\u591a\u7ea7\u7279\u5f81\u96c6\u6210\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u4e30\u5bcc\u8f66\u9053\u7279\u5f81\u8868\u793a\u3002\u57fa\u4e8e\u6240\u63d0\u51fa\u7684\u201c\u8349\u56fe\u548c\u7ec6\u5316\u201d\u8303\u4f8b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u800c\u6709\u6548\u7684\u8f66\u9053\u68c0\u6d4b\u5668\uff0c\u79f0\u4e3a\u201cSRLane\u201d\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 SRLane \u53ef\u4ee5\u5feb\u901f\u8fd0\u884c\uff08\u5373 278 FPS\uff09\uff0c\u540c\u65f6\u4ea7\u751f 78.9% \u7684 F1 \u5206\u6570\u3002\u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/passerer/SRLane\u3002|[2401.14729v1](http://arxiv.org/pdf/2401.14729v1)|null|\n", "2401.14719": "|**2024-01-26**|**pLitterStreet: Street Level Plastic Litter Detection and Mapping**|pLitterStreet\uff1a\u8857\u9053\u5851\u6599\u5783\u573e\u68c0\u6d4b\u548c\u7ed8\u56fe|Sriram Reddy Mandhati, N. Lakmal Deshapriya, Chatura Lavanga Mendis, Kavinda Gunasekara, Frank Yrle, Angsana Chaksan, Sujit Sanjeev|Plastic pollution is a critical environmental issue, and detecting and monitoring plastic litter is crucial to mitigate its impact. This paper presents the methodology of mapping street-level litter, focusing primarily on plastic waste and the location of trash bins. Our methodology involves employing a deep learning technique to identify litter and trash bins from street-level imagery taken by a camera mounted on a vehicle. Subsequently, we utilized heat maps to visually represent the distribution of litter and trash bins throughout cities. Additionally, we provide details about the creation of an open-source dataset (\"pLitterStreet\") which was developed and utilized in our approach. The dataset contains more than 13,000 fully annotated images collected from vehicle-mounted cameras and includes bounding box labels. To evaluate the effectiveness of our dataset, we tested four well known state-of-the-art object detection algorithms (Faster R-CNN, RetinaNet, YOLOv3, and YOLOv5), achieving an average precision (AP) above 40%. While the results show average metrics, our experiments demonstrated the reliability of using vehicle-mounted cameras for plastic litter mapping. The \"pLitterStreet\" can also be a valuable resource for researchers and practitioners to develop and further improve existing machine learning models for detecting and mapping plastic litter in an urban environment. The dataset is open-source and more details about the dataset and trained models can be found at https://github.com/gicait/pLitter.|\u5851\u6599\u6c61\u67d3\u662f\u4e00\u4e2a\u4e25\u91cd\u7684\u73af\u5883\u95ee\u9898\uff0c\u68c0\u6d4b\u548c\u76d1\u6d4b\u5851\u6599\u5783\u573e\u5bf9\u4e8e\u51cf\u8f7b\u5176\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u7ed8\u5236\u8857\u9053\u5783\u573e\u56fe\u7684\u65b9\u6cd5\uff0c\u4e3b\u8981\u5173\u6ce8\u5851\u6599\u5783\u573e\u548c\u5783\u573e\u6876\u7684\u4f4d\u7f6e\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d89\u53ca\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u4ece\u5b89\u88c5\u5728\u8f66\u8f86\u4e0a\u7684\u6444\u50cf\u5934\u62cd\u6444\u7684\u8857\u9053\u56fe\u50cf\u4e2d\u8bc6\u522b\u5783\u573e\u548c\u5783\u573e\u6876\u3002\u968f\u540e\uff0c\u6211\u4eec\u5229\u7528\u70ed\u56fe\u76f4\u89c2\u5730\u8868\u793a\u6574\u4e2a\u57ce\u5e02\u5783\u573e\u548c\u5783\u573e\u6876\u7684\u5206\u5e03\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u6709\u5173\u5728\u6211\u4eec\u7684\u65b9\u6cd5\u4e2d\u5f00\u53d1\u548c\u4f7f\u7528\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff08\u201cpLitterStreet\u201d\uff09\u521b\u5efa\u7684\u8be6\u7ec6\u4fe1\u606f\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u4ece\u8f66\u8f7d\u6444\u50cf\u5934\u6536\u96c6\u7684 13,000 \u591a\u5f20\u5b8c\u5168\u6ce8\u91ca\u7684\u56fe\u50cf\uff0c\u5e76\u5305\u62ec\u8fb9\u754c\u6846\u6807\u7b7e\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u6d4b\u8bd5\u4e86\u56db\u79cd\u4f17\u6240\u5468\u77e5\u7684\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\uff08Faster R-CNN\u3001RetinaNet\u3001YOLOv3 \u548c YOLOv5\uff09\uff0c\u5b9e\u73b0\u4e86 40% \u4ee5\u4e0a\u7684\u5e73\u5747\u7cbe\u5ea6 (AP)\u3002\u867d\u7136\u7ed3\u679c\u663e\u793a\u7684\u662f\u5e73\u5747\u6307\u6807\uff0c\u4f46\u6211\u4eec\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u4f7f\u7528\u8f66\u8f7d\u6444\u50cf\u5934\u8fdb\u884c\u5851\u6599\u5783\u573e\u6d4b\u7ed8\u7684\u53ef\u9760\u6027\u3002 \u201cpLitterStreet\u201d\u4e5f\u53ef\u4ee5\u6210\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u5f00\u53d1\u548c\u8fdb\u4e00\u6b65\u6539\u8fdb\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u7ed8\u5236\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5851\u6599\u5783\u573e\u3002\u8be5\u6570\u636e\u96c6\u662f\u5f00\u6e90\u7684\uff0c\u6709\u5173\u8be5\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6a21\u578b\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728 https://github.com/gicait/pLitter \u627e\u5230\u3002|[2401.14719v1](http://arxiv.org/pdf/2401.14719v1)|null|\n", "2401.14705": "|**2024-01-26**|**Additional Look into GAN-based Augmentation for Deep Learning COVID-19 Image Classification**|\u5bf9\u57fa\u4e8e GAN \u7684\u6df1\u5ea6\u5b66\u4e60 COVID-19 \u56fe\u50cf\u5206\u7c7b\u589e\u5f3a\u7684\u8fdb\u4e00\u6b65\u7814\u7a76|Oleksandr Fedoruk, Konrad Klimaszewski, Aleksander Ogonowski, Micha\u0142 Kruk|The availability of training data is one of the main limitations in deep learning applications for medical imaging. Data augmentation is a popular approach to overcome this problem. A new approach is a Machine Learning based augmentation, in particular usage of Generative Adversarial Networks (GAN). In this case, GANs generate images similar to the original dataset so that the overall training data amount is bigger, which leads to better performance of trained networks. A GAN model consists of two networks, a generator and a discriminator interconnected in a feedback loop which creates a competitive environment. This work is a continuation of the previous research where we trained StyleGAN2-ADA by Nvidia on the limited COVID-19 chest X-ray image dataset. In this paper, we study the dependence of the GAN-based augmentation performance on dataset size with a focus on small samples. Two datasets are considered, one with 1000 images per class (4000 images in total) and the second with 500 images per class (2000 images in total). We train StyleGAN2-ADA with both sets and then, after validating the quality of generated images, we use trained GANs as one of the augmentations approaches in multi-class classification problems. We compare the quality of the GAN-based augmentation approach to two different approaches (classical augmentation and no augmentation at all) by employing transfer learning-based classification of COVID-19 chest X-ray images. The results are quantified using different classification quality metrics and compared to the results from the literature. The GAN-based augmentation approach is found to be comparable with classical augmentation in the case of medium and large datasets but underperforms in the case of smaller datasets. The correlation between the size of the original dataset and the quality of classification is visible independently from the augmentation approach.|\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u7528\u6027\u662f\u533b\u5b66\u6210\u50cf\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u7684\u4e3b\u8981\u9650\u5236\u4e4b\u4e00\u3002\u6570\u636e\u589e\u5f3a\u662f\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u6d41\u884c\u65b9\u6cd5\u3002\u4e00\u79cd\u65b0\u65b9\u6cd5\u662f\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u589e\u5f3a\uff0c\u7279\u522b\u662f\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u4f7f\u7528\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cGAN \u4f1a\u751f\u6210\u4e0e\u539f\u59cb\u6570\u636e\u96c6\u76f8\u4f3c\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u4f7f\u6574\u4f53\u8bad\u7ec3\u6570\u636e\u91cf\u66f4\u5927\uff0c\u4ece\u800c\u5bfc\u81f4\u8bad\u7ec3\u7f51\u7edc\u7684\u6027\u80fd\u66f4\u597d\u3002 GAN \u6a21\u578b\u7531\u4e24\u4e2a\u7f51\u7edc\u7ec4\u6210\uff0c\u5373\u751f\u6210\u5668\u548c\u9274\u522b\u5668\uff0c\u5b83\u4eec\u5728\u53cd\u9988\u73af\u8def\u4e2d\u4e92\u8fde\uff0c\u4ece\u800c\u521b\u5efa\u7ade\u4e89\u73af\u5883\u3002\u8fd9\u9879\u5de5\u4f5c\u662f\u4e4b\u524d\u7814\u7a76\u7684\u5ef6\u7eed\uff0c\u6211\u4eec\u5728\u6709\u9650\u7684 COVID-19 \u80f8\u90e8 X \u5c04\u7ebf\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86 Nvidia \u7684 StyleGAN2-ADA\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u57fa\u4e8e GAN \u7684\u589e\u5f3a\u6027\u80fd\u5bf9\u6570\u636e\u96c6\u5927\u5c0f\u7684\u4f9d\u8d56\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u5c0f\u6837\u672c\u3002\u8003\u8651\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u6570\u636e\u96c6\u6bcf\u7c7b 1000 \u4e2a\u56fe\u50cf\uff08\u603b\u5171 4000 \u4e2a\u56fe\u50cf\uff09\uff0c\u7b2c\u4e8c\u4e2a\u6570\u636e\u96c6\u6bcf\u7c7b 500 \u4e2a\u56fe\u50cf\uff08\u603b\u5171 2000 \u4e2a\u56fe\u50cf\uff09\u3002\u6211\u4eec\u4f7f\u7528\u8fd9\u4e24\u4e2a\u96c6\u5408\u8bad\u7ec3 StyleGAN2-ADA\uff0c\u7136\u540e\u5728\u9a8c\u8bc1\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u7ecf\u8fc7\u8bad\u7ec3\u7684 GAN \u4f5c\u4e3a\u591a\u7c7b\u5206\u7c7b\u95ee\u9898\u7684\u589e\u5f3a\u65b9\u6cd5\u4e4b\u4e00\u3002\u6211\u4eec\u901a\u8fc7\u91c7\u7528\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684 COVID-19 \u80f8\u90e8 X \u5c04\u7ebf\u56fe\u50cf\u5206\u7c7b\uff0c\u5c06\u57fa\u4e8e GAN \u7684\u589e\u5f3a\u65b9\u6cd5\u4e0e\u4e24\u79cd\u4e0d\u540c\u65b9\u6cd5\uff08\u7ecf\u5178\u589e\u5f3a\u548c\u6839\u672c\u4e0d\u589e\u5f3a\uff09\u7684\u8d28\u91cf\u8fdb\u884c\u6bd4\u8f83\u3002\u4f7f\u7528\u4e0d\u540c\u7684\u5206\u7c7b\u8d28\u91cf\u6307\u6807\u5bf9\u7ed3\u679c\u8fdb\u884c\u91cf\u5316\uff0c\u5e76\u4e0e\u6587\u732e\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e2d\u578b\u548c\u5927\u578b\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e GAN \u7684\u589e\u5f3a\u65b9\u6cd5\u4e0e\u7ecf\u5178\u589e\u5f3a\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u539f\u59cb\u6570\u636e\u96c6\u7684\u5927\u5c0f\u548c\u5206\u7c7b\u8d28\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u72ec\u7acb\u4e8e\u589e\u5f3a\u65b9\u6cd5\u662f\u53ef\u89c1\u7684\u3002|[2401.14705v1](http://arxiv.org/pdf/2401.14705v1)|null|\n", "2401.14686": "|**2024-01-26**|**SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation**|SSR\uff1aSAM \u662f\u7528\u4e8e\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u7684\u5f3a\u6b63\u5219\u5316\u5668|Yanqi Ge, Ye Huang, Wen Li, Lixin Duan|We introduced SSR, which utilizes SAM (segment-anything) as a strong regularizer during training, to greatly enhance the robustness of the image encoder for handling various domains. Specifically, given the fact that SAM is pre-trained with a large number of images over the internet, which cover a diverse variety of domains, the feature encoding extracted by the SAM is obviously less dependent on specific domains when compared to the traditional ImageNet pre-trained image encoder. Meanwhile, the ImageNet pre-trained image encoder is still a mature choice of backbone for the semantic segmentation task, especially when the SAM is category-irrelevant. As a result, our SSR provides a simple yet highly effective design. It uses the ImageNet pre-trained image encoder as the backbone, and the intermediate feature of each stage (ie there are 4 stages in MiT-B5) is regularized by SAM during training. After extensive experimentation on GTA5$\\rightarrow$Cityscapes, our SSR significantly improved performance over the baseline without introducing any extra inference overhead.|\u6211\u4eec\u5f15\u5165\u4e86 SSR\uff0c\u5b83\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5229\u7528 SAM\uff08segment-anything\uff09\u4f5c\u4e3a\u5f3a\u5927\u7684\u6b63\u5219\u5316\u5668\uff0c\u4ee5\u5927\u5927\u589e\u5f3a\u56fe\u50cf\u7f16\u7801\u5668\u5904\u7406\u5404\u79cd\u57df\u7684\u9c81\u68d2\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8003\u8651\u5230 SAM \u662f\u901a\u8fc7\u4e92\u8054\u7f51\u4e0a\u7684\u5927\u91cf\u56fe\u50cf\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\uff0c\u8fd9\u4e9b\u56fe\u50cf\u6db5\u76d6\u4e86\u4e0d\u540c\u7684\u9886\u57df\uff0c\u4e0e\u4f20\u7edf\u7684 ImageNet \u9884\u8bad\u7ec3\u76f8\u6bd4\uff0cSAM \u63d0\u53d6\u7684\u7279\u5f81\u7f16\u7801\u5bf9\u7279\u5b9a\u9886\u57df\u7684\u4f9d\u8d56\u6027\u660e\u663e\u8f83\u5c0f\u3002 -\u7ecf\u8fc7\u8bad\u7ec3\u7684\u56fe\u50cf\u7f16\u7801\u5668\u3002\u540c\u65f6\uff0cImageNet \u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\u4ecd\u7136\u662f\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u6210\u719f\u9aa8\u5e72\u9009\u62e9\uff0c\u7279\u522b\u662f\u5f53 SAM \u4e0e\u7c7b\u522b\u65e0\u5173\u65f6\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684 SSR \u63d0\u4f9b\u200b\u200b\u4e86\u7b80\u5355\u800c\u9ad8\u6548\u7684\u8bbe\u8ba1\u3002\u5b83\u4f7f\u7528ImageNet\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u7f16\u7801\u5668\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u6bcf\u4e2a\u9636\u6bb5\u7684\u4e2d\u95f4\u7279\u5f81\uff08\u5373MiT-B5\u4e2d\u67094\u4e2a\u9636\u6bb5\uff09\u5728\u8bad\u7ec3\u671f\u95f4\u901a\u8fc7SAM\u8fdb\u884c\u6b63\u5219\u5316\u3002\u7ecf\u8fc7\u5bf9 GTA5$\\rightarrow$Cityscapes \u7684\u5e7f\u6cdb\u5b9e\u9a8c\u540e\uff0c\u6211\u4eec\u7684 SSR \u663e\u7740\u63d0\u9ad8\u4e86\u57fa\u51c6\u6027\u80fd\uff0c\u800c\u6ca1\u6709\u5f15\u5165\u4efb\u4f55\u989d\u5916\u7684\u63a8\u7406\u5f00\u9500\u3002|[2401.14686v1](http://arxiv.org/pdf/2401.14686v1)|null|\n", "2401.14675": "|**2024-01-26**|**Multi-model learning by sequential reading of untrimmed videos for action recognition**|\u901a\u8fc7\u987a\u5e8f\u8bfb\u53d6\u672a\u4fee\u526a\u7684\u89c6\u9891\u8fdb\u884c\u591a\u6a21\u578b\u5b66\u4e60\u4ee5\u8fdb\u884c\u52a8\u4f5c\u8bc6\u522b|Kodai Kamiya, Toru Tamaki|We propose a new method for learning videos by aggregating multiple models by sequentially extracting video clips from untrimmed video. The proposed method reduces the correlation between clips by feeding clips to multiple models in turn and synchronizes these models through federated learning. Experimental results show that the proposed method improves the performance compared to the no synchronization.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4ece\u672a\u4fee\u526a\u7684\u89c6\u9891\u4e2d\u987a\u5e8f\u63d0\u53d6\u89c6\u9891\u526a\u8f91\u6765\u805a\u5408\u591a\u4e2a\u6a21\u578b\u6765\u5b66\u4e60\u89c6\u9891\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u526a\u8f91\u4f9d\u6b21\u9988\u9001\u5230\u591a\u4e2a\u6a21\u578b\u6765\u51cf\u5c11\u526a\u8f91\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u540c\u6b65\u8fd9\u4e9b\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e0d\u540c\u6b65\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6027\u80fd\u3002|[2401.14675v1](http://arxiv.org/pdf/2401.14675v1)|null|\n", "2401.14661": "|**2024-01-26**|**From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection with Super Resolution**|\u4ece\u6a21\u7cca\u5230\u51fa\u8272\u7684\u68c0\u6d4b\uff1a\u57fa\u4e8e YOLOv5 \u7684\u8d85\u5206\u8fa8\u7387\u7a7a\u4e2d\u7269\u4f53\u68c0\u6d4b|Ragib Amin Nihal, Benjamin Yen, Katsutoshi Itoyama, Kazuhiro Nakadai|The demand for accurate object detection in aerial imagery has surged with the widespread use of drones and satellite technology. Traditional object detection models, trained on datasets biased towards large objects, struggle to perform optimally in aerial scenarios where small, densely clustered objects are prevalent. To address this challenge, we present an innovative approach that combines super-resolution and an adapted lightweight YOLOv5 architecture. We employ a range of datasets, including VisDrone-2023, SeaDroneSee, VEDAI, and NWPU VHR-10, to evaluate our model's performance. Our Super Resolved YOLOv5 architecture features Transformer encoder blocks, allowing the model to capture global context and context information, leading to improved detection results, especially in high-density, occluded conditions. This lightweight model not only delivers improved accuracy but also ensures efficient resource utilization, making it well-suited for real-time applications. Our experimental results demonstrate the model's superior performance in detecting small and densely clustered objects, underlining the significance of dataset choice and architectural adaptation for this specific task. In particular, the method achieves 52.5% mAP on VisDrone, exceeding top prior works. This approach promises to significantly advance object detection in aerial imagery, contributing to more accurate and reliable results in a variety of real-world applications.|\u968f\u7740\u65e0\u4eba\u673a\u548c\u536b\u661f\u6280\u672f\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u5bf9\u822a\u7a7a\u56fe\u50cf\u4e2d\u7cbe\u786e\u7269\u4f53\u68c0\u6d4b\u7684\u9700\u6c42\u6fc0\u589e\u3002\u4f20\u7edf\u7684\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u662f\u5728\u504f\u5411\u4e8e\u5927\u578b\u7269\u4f53\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\uff0c\u5f88\u96be\u5728\u5c0f\u578b\u3001\u5bc6\u96c6\u96c6\u7fa4\u7269\u4f53\u666e\u904d\u5b58\u5728\u7684\u7a7a\u4e2d\u573a\u666f\u4e2d\u53d1\u6325\u6700\u4f73\u6027\u80fd\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u5c06\u8d85\u5206\u8fa8\u7387\u548c\u7ecf\u8fc7\u8c03\u6574\u7684\u8f7b\u91cf\u7ea7 YOLOv5 \u67b6\u6784\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u4f7f\u7528\u4e00\u7cfb\u5217\u6570\u636e\u96c6\uff08\u5305\u62ec VisDrone-2023\u3001SeaDroneSee\u3001VEDAI \u548c NWPU VHR-10\uff09\u6765\u8bc4\u4f30\u6211\u4eec\u6a21\u578b\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684 Super Resolved YOLOv5 \u67b6\u6784\u5177\u6709 Transformer \u7f16\u7801\u5668\u5757\uff0c\u5141\u8bb8\u6a21\u578b\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u6539\u5584\u68c0\u6d4b\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5bc6\u5ea6\u3001\u906e\u6321\u6761\u4ef6\u4e0b\u3002\u8fd9\u79cd\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u800c\u4e14\u786e\u4fdd\u4e86\u8d44\u6e90\u7684\u9ad8\u6548\u5229\u7528\uff0c\u4f7f\u5176\u975e\u5e38\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u7a0b\u5e8f\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u5728\u68c0\u6d4b\u5c0f\u578b\u4e14\u5bc6\u96c6\u96c6\u7fa4\u7684\u5bf9\u8c61\u65b9\u9762\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u96c6\u9009\u62e9\u548c\u67b6\u6784\u9002\u5e94\u5bf9\u4e8e\u8fd9\u4e00\u7279\u5b9a\u4efb\u52a1\u7684\u91cd\u8981\u6027\u3002\u7279\u522b\u662f\uff0c\u8be5\u65b9\u6cd5\u5728 VisDrone \u4e0a\u5b9e\u73b0\u4e86 52.5% \u7684 mAP\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u9876\u7ea7\u5de5\u4f5c\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u671b\u663e\u7740\u63a8\u8fdb\u822a\u7a7a\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u6709\u52a9\u4e8e\u5728\u5404\u79cd\u73b0\u5b9e\u5e94\u7528\u4e2d\u83b7\u5f97\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684\u7ed3\u679c\u3002|[2401.14661v1](http://arxiv.org/pdf/2401.14661v1)|null|\n", "2401.14641": "|**2024-01-26**|**Super Efficient Neural Network for Compression Artifacts Reduction and Super Resolution**|\u7528\u4e8e\u51cf\u5c11\u538b\u7f29\u4f2a\u5f71\u548c\u8d85\u5206\u8fa8\u7387\u7684\u8d85\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc|Wen Ma, Qiuwen Lou, Arman Kazemi, Julian Faraone, Tariq Afzal|Video quality can suffer from limited internet speed while being streamed by users. Compression artifacts start to appear when the bitrate decreases to match the available bandwidth. Existing algorithms either focus on removing the compression artifacts at the same video resolution, or on upscaling the video resolution but not removing the artifacts. Super resolution-only approaches will amplify the artifacts along with the details by default. We propose a lightweight convolutional neural network (CNN)-based algorithm which simultaneously performs artifacts reduction and super resolution (ARSR) by enhancing the feature extraction layers and designing a custom training dataset. The output of this neural network is evaluated for test streams compressed at low bitrates using variable bitrate (VBR) encoding. The output video quality shows a 4-6 increase in video multi-method assessment fusion (VMAF) score compared to traditional interpolation upscaling approaches such as Lanczos or Bicubic.|\u7528\u6237\u4f20\u8f93\u89c6\u9891\u65f6\uff0c\u89c6\u9891\u8d28\u91cf\u53ef\u80fd\u4f1a\u56e0\u4e92\u8054\u7f51\u901f\u5ea6\u6709\u9650\u800c\u53d7\u5230\u5f71\u54cd\u3002\u5f53\u6bd4\u7279\u7387\u964d\u4f4e\u4ee5\u5339\u914d\u53ef\u7528\u5e26\u5bbd\u65f6\uff0c\u538b\u7f29\u4f2a\u5f71\u5f00\u59cb\u51fa\u73b0\u3002\u73b0\u6709\u7b97\u6cd5\u8981\u4e48\u4e13\u6ce8\u4e8e\u6d88\u9664\u76f8\u540c\u89c6\u9891\u5206\u8fa8\u7387\u4e0b\u7684\u538b\u7f29\u4f2a\u5f71\uff0c\u8981\u4e48\u4e13\u6ce8\u4e8e\u63d0\u9ad8\u89c6\u9891\u5206\u8fa8\u7387\u4f46\u4e0d\u6d88\u9664\u4f2a\u5f71\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4ec5\u9650\u8d85\u5206\u8fa8\u7387\u7684\u65b9\u6cd5\u4f1a\u653e\u5927\u4f2a\u5f71\u4ee5\u53ca\u7ec6\u8282\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u5c42\u548c\u8bbe\u8ba1\u81ea\u5b9a\u4e49\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u540c\u65f6\u6267\u884c\u4f2a\u5f71\u51cf\u5c11\u548c\u8d85\u5206\u8fa8\u7387\uff08ARSR\uff09\u3002\u8be5\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u9488\u5bf9\u4f7f\u7528\u53ef\u53d8\u6bd4\u7279\u7387 (VBR) \u7f16\u7801\u4ee5\u4f4e\u6bd4\u7279\u7387\u538b\u7f29\u7684\u6d4b\u8bd5\u6d41\u8fdb\u884c\u8bc4\u4f30\u3002\u4e0e Lanczos \u6216 Bicubic \u7b49\u4f20\u7edf\u63d2\u503c\u5347\u7ea7\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8f93\u51fa\u89c6\u9891\u8d28\u91cf\u663e\u793a\u89c6\u9891\u591a\u65b9\u6cd5\u8bc4\u4f30\u878d\u5408 (VMAF) \u5206\u6570\u63d0\u9ad8\u4e86 4-6\u3002|[2401.14641v1](http://arxiv.org/pdf/2401.14641v1)|null|\n", "2401.14579": "|**2024-01-26**|**Recognizing Multiple Ingredients in Food Images Using a Single-Ingredient Classification Model**|\u4f7f\u7528\u5355\u4e00\u6210\u5206\u5206\u7c7b\u6a21\u578b\u8bc6\u522b\u98df\u54c1\u56fe\u50cf\u4e2d\u7684\u591a\u79cd\u6210\u5206|Kun Fu, Ying Dai|Recognizing food images presents unique challenges due to the variable spatial layout and shape changes of ingredients with different cooking and cutting methods. This study introduces an advanced approach for recognizing ingredients segmented from food images. The method localizes the candidate regions of the ingredients using the locating and sliding window techniques. Then, these regions are assigned into ingredient classes using a CNN (Convolutional Neural Network)-based single-ingredient classification model trained on a dataset of single-ingredient images. To address the challenge of processing speed in multi-ingredient recognition, a novel model pruning method is proposed that enhances the efficiency of the classification model. Subsequently, the multi-ingredient identification is achieved through a decision-making scheme, incorporating two novel algorithms. The single-ingredient image dataset, designed in accordance with the book entitled \"New Food Ingredients List FOODS 2021\", encompasses 9982 images across 110 diverse categories, emphasizing variety in ingredient shapes. In addition, a multi-ingredient image dataset is developed to rigorously evaluate the performance of our approach. Experimental results validate the effectiveness of our method, particularly highlighting its improved capability in recognizing multiple ingredients. This marks a significant advancement in the field of food image analysis.|\u7531\u4e8e\u4e0d\u540c\u70f9\u996a\u548c\u5207\u5272\u65b9\u6cd5\u4e0b\u98df\u6750\u7684\u7a7a\u95f4\u5e03\u5c40\u548c\u5f62\u72b6\u53d8\u5316\u5404\u5f02\uff0c\u8bc6\u522b\u98df\u7269\u56fe\u50cf\u9762\u4e34\u7740\u72ec\u7279\u7684\u6311\u6218\u3002\u8fd9\u9879\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u4ece\u98df\u7269\u56fe\u50cf\u4e2d\u5206\u5272\u7684\u6210\u5206\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u5b9a\u4f4d\u548c\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\u6765\u5b9a\u4f4d\u6210\u5206\u7684\u5019\u9009\u533a\u57df\u3002\u7136\u540e\uff0c\u4f7f\u7528\u5728\u5355\u4e00\u6210\u5206\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u57fa\u4e8e CNN\uff08\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u5355\u4e00\u6210\u5206\u5206\u7c7b\u6a21\u578b\u5c06\u8fd9\u4e9b\u533a\u57df\u5206\u914d\u5230\u6210\u5206\u7c7b\u522b\u4e2d\u3002\u4e3a\u4e86\u89e3\u51b3\u591a\u6210\u5206\u8bc6\u522b\u4e2d\u5904\u7406\u901f\u5ea6\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u526a\u679d\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6a21\u578b\u7684\u6548\u7387\u3002\u968f\u540e\uff0c\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u65b0\u9896\u7b97\u6cd5\u7684\u51b3\u7b56\u65b9\u6848\u5b9e\u73b0\u4e86\u591a\u6210\u5206\u8bc6\u522b\u3002\u8be5\u5355\u4e00\u6210\u5206\u56fe\u50cf\u6570\u636e\u96c6\u662f\u6839\u636e\u300aNew Food Ingredients List FOODS 2021\u300b\u4e00\u4e66\u8bbe\u8ba1\u7684\uff0c\u5305\u542b 110 \u4e2a\u4e0d\u540c\u7c7b\u522b\u7684 9982 \u5f20\u56fe\u50cf\uff0c\u5f3a\u8c03\u6210\u5206\u5f62\u72b6\u7684\u591a\u6837\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u591a\u6210\u5206\u56fe\u50cf\u6570\u636e\u96c6\u6765\u4e25\u683c\u8bc4\u4f30\u6211\u4eec\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u5176\u8bc6\u522b\u591a\u79cd\u6210\u5206\u7684\u80fd\u529b\u7684\u63d0\u9ad8\u3002\u8fd9\u6807\u5fd7\u7740\u98df\u54c1\u56fe\u50cf\u5206\u6790\u9886\u57df\u7684\u91cd\u5927\u8fdb\u6b65\u3002|[2401.14579v1](http://arxiv.org/pdf/2401.14579v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2401.14895": "|**2024-01-26**|**MPTQ-ViT:Mixed-PrecisionPost-TrainingQuantizationforVisionTransformer**|MPTQ-ViT\uff1aVisionTransformer \u7684\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u540e\u91cf\u5316|Yu-Shan Tai, An-Yeu, Wu|While vision transformers (ViTs) have shown great potential in computer vision tasks, their intense computation and memory requirements pose challenges for practical applications. Existing post-training quantization methods leverage value redistribution or specialized quantizers to address the non-normal distribution in ViTs. However, without considering the asymmetry in activations and relying on hand-crafted settings, these methods often struggle to maintain performance under low-bit quantization. To overcome these challenges, we introduce SmoothQuant with bias term (SQ-b) to alleviate the asymmetry issue and reduce the clamping loss. We also introduce optimal scaling factor ratio search (OPT-m) to determine quantization parameters by a data-dependent mechanism automatically. To further enhance the compressibility, we incorporate the above-mentioned techniques and propose a mixed-precision post-training quantization framework for vision transformers (MPTQ-ViT). We develop greedy mixed-precision quantization (Greedy MP) to allocate layer-wise bit-width considering both model performance and compressibility. Our experiments on ViT, DeiT, and Swin demonstrate significant accuracy improvements compared with SOTA on the ImageNet dataset. Specifically, our proposed methods achieve accuracy improvements ranging from 0.90% to 23.35% on 4-bit ViTs with single-precision and from 3.82% to 78.14% on 5-bit fully quantized ViTs with mixed-precision.|\u867d\u7136\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u5927\u91cf\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u7ed9\u5b9e\u9645\u5e94\u7528\u5e26\u6765\u4e86\u6311\u6218\u3002\u73b0\u6709\u7684\u8bad\u7ec3\u540e\u91cf\u5316\u65b9\u6cd5\u5229\u7528\u503c\u91cd\u65b0\u5206\u914d\u6216\u4e13\u95e8\u7684\u91cf\u5316\u5668\u6765\u89e3\u51b3 ViT \u4e2d\u7684\u975e\u6b63\u6001\u5206\u5e03\u3002\u7136\u800c\uff0c\u5982\u679c\u4e0d\u8003\u8651\u6fc0\u6d3b\u7684\u4e0d\u5bf9\u79f0\u6027\u5e76\u4f9d\u8d56\u4e8e\u624b\u5de5\u8bbe\u7f6e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u5f88\u96be\u5728\u4f4e\u4f4d\u91cf\u5316\u4e0b\u4fdd\u6301\u6027\u80fd\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5e26\u6709\u504f\u7f6e\u9879 (SQ-b) \u7684 SmoothQuant\uff0c\u4ee5\u7f13\u89e3\u4e0d\u5bf9\u79f0\u95ee\u9898\u5e76\u51cf\u5c11\u94b3\u4f4d\u635f\u8017\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u6700\u4f73\u7f29\u653e\u56e0\u5b50\u6bd4\u7387\u641c\u7d22\uff08OPT-m\uff09\uff0c\u4ee5\u901a\u8fc7\u6570\u636e\u4f9d\u8d56\u673a\u5236\u81ea\u52a8\u786e\u5b9a\u91cf\u5316\u53c2\u6570\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u53ef\u538b\u7f29\u6027\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u4e0a\u8ff0\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u53d8\u6362\u5668\u7684\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u540e\u91cf\u5316\u6846\u67b6\uff08MPTQ-ViT\uff09\u3002\u6211\u4eec\u5f00\u53d1\u8d2a\u5a6a\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\uff08Greedy MP\uff09\u6765\u5206\u914d\u5206\u5c42\u4f4d\u5bbd\uff0c\u540c\u65f6\u8003\u8651\u6a21\u578b\u6027\u80fd\u548c\u53ef\u538b\u7f29\u6027\u3002\u6211\u4eec\u5728 ViT\u3001DeiT \u548c Swin \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e ImageNet \u6570\u636e\u96c6\u4e0a\u7684 SOTA \u76f8\u6bd4\uff0c\u51c6\u786e\u6027\u6709\u4e86\u663e\u7740\u63d0\u9ad8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5355\u7cbe\u5ea6\u7684 4 \u4f4d ViT \u4e0a\u5b9e\u73b0\u4e86 0.90% \u5230 23.35% \u7684\u7cbe\u5ea6\u6539\u8fdb\uff0c\u5728\u6df7\u5408\u7cbe\u5ea6\u7684 5 \u4f4d\u5b8c\u5168\u91cf\u5316 ViT \u4e0a\u5b9e\u73b0\u4e86 3.82% \u5230 78.14% \u7684\u7cbe\u5ea6\u6539\u8fdb\u3002|[2401.14895v1](http://arxiv.org/pdf/2401.14895v1)|null|\n"}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2401.15075": "|**2024-01-26**|**Annotated Hands for Generative Models**|\u751f\u6210\u6a21\u578b\u7684\u5e26\u6ce8\u91ca\u7684\u624b|Yue Yang, Atith N Gandhi, Greg Turk|Generative models such as GANs and diffusion models have demonstrated impressive image generation capabilities. Despite these successes, these systems are surprisingly poor at creating images with hands. We propose a novel training framework for generative models that substantially improves the ability of such systems to create hand images. Our approach is to augment the training images with three additional channels that provide annotations to hands in the image. These annotations provide additional structure that coax the generative model to produce higher quality hand images. We demonstrate this approach on two different generative models: a generative adversarial network and a diffusion model. We demonstrate our method both on a new synthetic dataset of hand images and also on real photographs that contain hands. We measure the improved quality of the generated hands through higher confidence in finger joint identification using an off-the-shelf hand detector.|GAN \u548c\u6269\u6563\u6a21\u578b\u7b49\u751f\u6210\u6a21\u578b\u5df2\u7ecf\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fd9\u4e9b\u6210\u529f\uff0c\u4f46\u8fd9\u4e9b\u7cfb\u7edf\u5728\u7528\u624b\u521b\u5efa\u56fe\u50cf\u65b9\u9762\u5374\u8868\u73b0\u5f97\u4ee4\u4eba\u60ca\u8bb6\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u53ef\u4ee5\u5927\u5927\u63d0\u9ad8\u6b64\u7c7b\u7cfb\u7edf\u521b\u5efa\u624b\u90e8\u56fe\u50cf\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u901a\u8fc7\u4e09\u4e2a\u9644\u52a0\u901a\u9053\u6765\u589e\u5f3a\u8bad\u7ec3\u56fe\u50cf\uff0c\u8fd9\u4e9b\u901a\u9053\u4e3a\u56fe\u50cf\u4e2d\u7684\u624b\u63d0\u4f9b\u6ce8\u91ca\u3002\u8fd9\u4e9b\u6ce8\u91ca\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u7ed3\u6784\uff0c\u53ef\u4ee5\u5f15\u5bfc\u751f\u6210\u6a21\u578b\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u624b\u90e8\u56fe\u50cf\u3002\u6211\u4eec\u5728\u4e24\u79cd\u4e0d\u540c\u7684\u751f\u6210\u6a21\u578b\u4e0a\u6f14\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\uff1a\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u6269\u6563\u6a21\u578b\u3002\u6211\u4eec\u5728\u65b0\u7684\u624b\u90e8\u56fe\u50cf\u5408\u6210\u6570\u636e\u96c6\u548c\u5305\u542b\u624b\u90e8\u7684\u771f\u5b9e\u7167\u7247\u4e0a\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u73b0\u6210\u7684\u624b\u90e8\u68c0\u6d4b\u5668\u5bf9\u6307\u5173\u8282\u8bc6\u522b\u8fdb\u884c\u66f4\u9ad8\u7684\u7f6e\u4fe1\u5ea6\u6765\u6d4b\u91cf\u751f\u6210\u7684\u624b\u90e8\u8d28\u91cf\u7684\u63d0\u9ad8\u3002|[2401.15075v1](http://arxiv.org/pdf/2401.15075v1)|**[link](https://github.com/YY-GX/Annotated-Hands-Dataset)**|\n"}, "\u591a\u6a21\u6001": {"2401.15071": "|**2024-01-26**|**From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities**|\u4ece GPT-4 \u5230 Gemini \u53ca\u5176\u4ed6\uff1a\u901a\u8fc7\u56db\u79cd\u6a21\u5f0f\u8bc4\u4f30 MLLM \u7684\u666e\u904d\u6027\u3001\u53ef\u4fe1\u5ea6\u548c\u56e0\u679c\u5173\u7cfb|Chaochao Lu, Chen Qian, Guodong Zheng, Hongxing Fan, Hongzhi Gao, Jie Zhang, Jing Shao, Jingyi Deng, Jinlan Fu, Kexin Huang, et.al.|Multi-modal Large Language Models (MLLMs) have shown impressive abilities in generating reasonable responses with respect to multi-modal contents. However, there is still a wide gap between the performance of recent MLLM-based applications and the expectation of the broad public, even though the most powerful OpenAI's GPT-4 and Google's Gemini have been deployed. This paper strives to enhance understanding of the gap through the lens of a qualitative study on the generalizability, trustworthiness, and causal reasoning capabilities of recent proprietary and open-source MLLMs across four modalities: ie, text, code, image, and video, ultimately aiming to improve the transparency of MLLMs. We believe these properties are several representative factors that define the reliability of MLLMs, in supporting various downstream applications. To be specific, we evaluate the closed-source GPT-4 and Gemini and 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designed cases, where the qualitative results are then summarized into 12 scores (ie, 4 modalities times 3 properties). In total, we uncover 14 empirical findings that are useful to understand the capabilities and limitations of both proprietary and open-source MLLMs, towards more reliable downstream multi-modal applications.|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u9488\u5bf9\u591a\u6a21\u6001\u5185\u5bb9\u751f\u6210\u5408\u7406\u54cd\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5df2\u7ecf\u90e8\u7f72\u4e86\u6700\u5f3a\u5927\u7684OpenAI\u7684GPT-4\u548cGoogle\u7684Gemini\uff0c\u4f46\u6700\u8fd1\u57fa\u4e8eMLLM\u7684\u5e94\u7528\u7a0b\u5e8f\u7684\u6027\u80fd\u4e0e\u5e7f\u5927\u516c\u4f17\u7684\u671f\u671b\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u5f88\u5927\u5dee\u8ddd\u3002\u672c\u6587\u81f4\u529b\u4e8e\u901a\u8fc7\u5bf9\u6700\u65b0\u4e13\u6709\u548c\u5f00\u6e90 MLLM \u8de8\u56db\u79cd\u6a21\u5f0f\uff08\u5373\u6587\u672c\u3001\u4ee3\u7801\u3001\u56fe\u50cf\u548c\u89c6\u9891\uff09\u7684\u666e\u904d\u6027\u3001\u53ef\u4fe1\u5ea6\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\u7684\u5b9a\u6027\u7814\u7a76\u6765\u589e\u5f3a\u5bf9\u5dee\u8ddd\u7684\u7406\u89e3\u3002\u65e8\u5728\u63d0\u9ad8 MLLM \u7684\u900f\u660e\u5ea6\u3002\u6211\u4eec\u76f8\u4fe1\u8fd9\u4e9b\u7279\u6027\u662f\u5b9a\u4e49 MLLM \u5728\u652f\u6301\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\u65b9\u9762\u7684\u53ef\u9760\u6027\u7684\u51e0\u4e2a\u4ee3\u8868\u6027\u56e0\u7d20\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u95ed\u6e90 GPT-4 \u548c Gemini \u4ee5\u53ca 6 \u4e2a\u5f00\u6e90 LLM \u548c MLLM\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86 230 \u4e2a\u624b\u52a8\u8bbe\u8ba1\u7684\u6848\u4f8b\uff0c\u7136\u540e\u5c06\u5b9a\u6027\u7ed3\u679c\u603b\u7ed3\u4e3a 12 \u4e2a\u5206\u6570\uff08\u5373 4 \u79cd\u6a21\u5f0f\u4e58\u4ee5 3 \u4e2a\u5c5e\u6027\uff09\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u53d1\u73b0\u4e86 14 \u9879\u5b9e\u8bc1\u7814\u7a76\u7ed3\u679c\uff0c\u6709\u52a9\u4e8e\u4e86\u89e3\u4e13\u6709\u548c\u5f00\u6e90 MLLM \u7684\u529f\u80fd\u548c\u5c40\u9650\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u4e0b\u6e38\u591a\u6a21\u5f0f\u5e94\u7528\u3002|[2401.15071v1](http://arxiv.org/pdf/2401.15071v1)|null|\n"}, "LLM": {"2401.14772": "|**2024-01-26**|**Spatial Transcriptomics Analysis of Zero-shot Gene Expression Prediction**|\u96f6\u6837\u672c\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u7684\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u5206\u6790|Yan Yang, Md Zakir Hossain, Xuesong Li, Shafin Rahman, Eric Stone|Spatial transcriptomics (ST) captures gene expression within distinct regions (i.e., windows) of a tissue slide. Traditional supervised learning frameworks applied to model ST are constrained to predicting expression from slide image windows for gene types seen during training, failing to generalize to unseen gene types. To overcome this limitation, we propose a semantic guided network (SGN), a pioneering zero-shot framework for predicting gene expression from slide image windows. Considering a gene type can be described by functionality and phenotype, we dynamically embed a gene type to a vector per its functionality and phenotype, and employ this vector to project slide image windows to gene expression in feature space, unleashing zero-shot expression prediction for unseen gene types. The gene type functionality and phenotype are queried with a carefully designed prompt from a pre-trained large language model (LLM). On standard benchmark datasets, we demonstrate competitive zero-shot performance compared to past state-of-the-art supervised learning approaches.|\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66 (ST) \u6355\u83b7\u7ec4\u7ec7\u73bb\u7247\u4e0d\u540c\u533a\u57df\uff08\u5373\u7a97\u53e3\uff09\u5185\u7684\u57fa\u56e0\u8868\u8fbe\u3002\u5e94\u7528\u4e8e\u6a21\u578b ST \u7684\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4ec5\u9650\u4e8e\u4ece\u5e7b\u706f\u7247\u56fe\u50cf\u7a97\u53e3\u9884\u6d4b\u8bad\u7ec3\u671f\u95f4\u770b\u5230\u7684\u57fa\u56e0\u7c7b\u578b\u7684\u8868\u8fbe\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u770b\u4e0d\u89c1\u7684\u57fa\u56e0\u7c7b\u578b\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u5f15\u5bfc\u7f51\u7edc\uff08SGN\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5f00\u521b\u6027\u7684\u96f6\u6837\u672c\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5e7b\u706f\u7247\u56fe\u50cf\u7a97\u53e3\u9884\u6d4b\u57fa\u56e0\u8868\u8fbe\u3002\u8003\u8651\u5230\u57fa\u56e0\u7c7b\u578b\u53ef\u4ee5\u901a\u8fc7\u529f\u80fd\u548c\u8868\u578b\u6765\u63cf\u8ff0\uff0c\u6211\u4eec\u6839\u636e\u5176\u529f\u80fd\u548c\u8868\u578b\u52a8\u6001\u5730\u5c06\u57fa\u56e0\u7c7b\u578b\u5d4c\u5165\u5230\u5411\u91cf\u4e2d\uff0c\u5e76\u5229\u7528\u8be5\u5411\u91cf\u5c06\u5e7b\u706f\u7247\u56fe\u50cf\u7a97\u53e3\u6295\u5f71\u5230\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u57fa\u56e0\u8868\u8fbe\uff0c\u4ece\u800c\u5b9e\u73b0\u96f6\u6837\u672c\u8868\u8fbe\u9884\u6d4b\u770b\u4e0d\u89c1\u7684\u57fa\u56e0\u7c7b\u578b\u3002\u901a\u8fc7\u9884\u5148\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u6765\u67e5\u8be2\u57fa\u56e0\u7c7b\u578b\u529f\u80fd\u548c\u8868\u578b\u3002\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u8fc7\u53bb\u6700\u5148\u8fdb\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002|[2401.14772v1](http://arxiv.org/pdf/2401.14772v1)|null|\n"}, "Transformer": {"2401.14938": "|**2024-01-26**|**DAM: Diffusion Activation Maximization for 3D Global Explanations**|DAM\uff1a3D \u5168\u5c40\u89e3\u91ca\u7684\u6269\u6563\u6fc0\u6d3b\u6700\u5927\u5316|Hanxiao Tan|In recent years, the performance of point cloud models has been rapidly improved. However, due to the limited amount of relevant explainability studies, the unreliability and opacity of these black-box models may lead to potential risks in applications where human lives are at stake, e.g. autonomous driving or healthcare. This work proposes a DDPM-based point cloud global explainability method (DAM) that leverages Point Diffusion Transformer (PDT), a novel point-wise symmetric model, with dual-classifier guidance to generate high-quality global explanations. In addition, an adapted path gradient integration method for DAM is proposed, which not only provides a global overview of the saliency maps for point cloud categories, but also sheds light on how the attributions of the explanations vary during the generation process. Extensive experiments indicate that our method outperforms existing ones in terms of perceptibility, representativeness, and diversity, with a significant reduction in generation time. Our code is available at: https://github.com/Explain3D/DAM|\u8fd1\u5e74\u6765\uff0c\u70b9\u4e91\u6a21\u578b\u7684\u6027\u80fd\u5f97\u5230\u4e86\u5feb\u901f\u63d0\u5347\u3002\u7136\u800c\uff0c\u7531\u4e8e\u76f8\u5173\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u6570\u91cf\u6709\u9650\uff0c\u8fd9\u4e9b\u9ed1\u76d2\u6a21\u578b\u7684\u4e0d\u53ef\u9760\u6027\u548c\u4e0d\u900f\u660e\u6027\u53ef\u80fd\u4f1a\u5728\u5371\u53ca\u4eba\u7c7b\u751f\u547d\u7684\u5e94\u7528\u4e2d\u5e26\u6765\u6f5c\u5728\u98ce\u9669\uff0c\u4f8b\u5982\uff1a\u81ea\u52a8\u9a7e\u9a76\u6216\u533b\u7597\u4fdd\u5065\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e DDPM \u7684\u70b9\u4e91\u5168\u5c40\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5 (DAM)\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u70b9\u6269\u6563\u53d8\u6362\u5668 (PDT)\uff08\u4e00\u79cd\u65b0\u9896\u7684\u9010\u70b9\u5bf9\u79f0\u6a21\u578b\uff09\u548c\u53cc\u5206\u7c7b\u5668\u6307\u5bfc\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5168\u5c40\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e DAM \u7684\u8def\u5f84\u68af\u5ea6\u79ef\u5206\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u70b9\u4e91\u7c7b\u522b\u663e\u7740\u6027\u56fe\u7684\u5168\u5c40\u6982\u8ff0\uff0c\u800c\u4e14\u8fd8\u63ed\u793a\u4e86\u89e3\u91ca\u7684\u5c5e\u6027\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5982\u4f55\u53d8\u5316\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u53ef\u611f\u77e5\u6027\u3001\u4ee3\u8868\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u751f\u6210\u65f6\u95f4\u663e\u7740\u51cf\u5c11\u3002\u6211\u4eec\u7684\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/Explain3D/DAM|[2401.14938v1](http://arxiv.org/pdf/2401.14938v1)|null|\n", "2401.14828": "|**2024-01-26**|**TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts**|TIP-Editor\uff1a\u9075\u5faa\u6587\u672c\u63d0\u793a\u548c\u56fe\u50cf\u63d0\u793a\u7684\u7cbe\u786e 3D \u7f16\u8f91\u5668|Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, Ying Shan|Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness. However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description. To this end, we propose a 3D scene editing framework, TIPEditor, that accepts both text and image prompts and a 3D bounding box to specify the editing region. With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance. Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box. Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as the 3D representation to facilitate local editing while keeping the background unchanged. Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively.|\u6587\u672c\u9a71\u52a8\u7684 3D \u573a\u666f\u7f16\u8f91\u56e0\u5176\u65b9\u4fbf\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6587\u672c\u63cf\u8ff0\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4ecd\u7136\u7f3a\u4e4f\u5bf9\u7f16\u8f91\u7ed3\u679c\u7684\u6307\u5b9a\u5916\u89c2\u548c\u4f4d\u7f6e\u7684\u7cbe\u786e\u63a7\u5236\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a 3D \u573a\u666f\u7f16\u8f91\u6846\u67b6 TIPEditor\uff0c\u5b83\u63a5\u53d7\u6587\u672c\u548c\u56fe\u50cf\u63d0\u793a\u4ee5\u53ca\u7528\u4e8e\u6307\u5b9a\u7f16\u8f91\u533a\u57df\u7684 3D \u8fb9\u754c\u6846\u3002\u901a\u8fc7\u56fe\u7247\u63d0\u793a\uff0c\u7528\u6237\u53ef\u4ee5\u65b9\u4fbf\u5730\u6307\u5b9a\u76ee\u6807\u5185\u5bb9\u7684\u8be6\u7ec6\u5916\u89c2/\u98ce\u683c\uff0c\u4e0e\u6587\u5b57\u63cf\u8ff0\u76f8\u8f85\u76f8\u6210\uff0c\u5b9e\u73b0\u5916\u89c2\u7684\u7cbe\u786e\u63a7\u5236\u3002\u5177\u4f53\u6765\u8bf4\uff0cTIP-Editor \u91c7\u7528\u9010\u6b65 2D \u4e2a\u6027\u5316\u7b56\u7565\u6765\u66f4\u597d\u5730\u5b66\u4e60\u73b0\u6709\u573a\u666f\u548c\u53c2\u8003\u56fe\u50cf\u7684\u8868\u793a\uff0c\u5176\u4e2d\u63d0\u51fa\u5b9a\u4f4d\u635f\u5931\u4ee5\u9f13\u52b1\u8fb9\u754c\u6846\u6307\u5b9a\u7684\u6b63\u786e\u5bf9\u8c61\u653e\u7f6e\u3002\u6b64\u5916\uff0cTIPEditor \u4f7f\u7528\u660e\u786e\u4e14\u7075\u6d3b\u7684 3D \u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a 3D \u8868\u793a\uff0c\u4ee5\u65b9\u4fbf\u672c\u5730\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u80cc\u666f\u4e0d\u53d8\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTIP-Editor\u80fd\u591f\u5728\u6307\u5b9a\u7684\u8fb9\u754c\u6846\u533a\u57df\u5185\u6309\u7167\u6587\u672c\u548c\u56fe\u50cf\u63d0\u793a\u8fdb\u884c\u51c6\u786e\u7684\u7f16\u8f91\uff0c\u5728\u7f16\u8f91\u8d28\u91cf\u4ee5\u53ca\u4e0e\u63d0\u793a\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5bf9\u9f50\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u3002|[2401.14828v1](http://arxiv.org/pdf/2401.14828v1)|null|\n", "2401.14807": "|**2024-01-26**|**PL-FSCIL: Harnessing the Power of Prompts for Few-Shot Class-Incremental Learning**|PL-FSCIL\uff1a\u5229\u7528\u63d0\u793a\u7684\u529b\u91cf\u8fdb\u884c\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60|Songsong Tian, Lusi Li, Weijun Li, Hang Ran, Li Li, Xin Ning|Few-Shot Class-Incremental Learning (FSCIL) aims to enable deep neural networks to learn new tasks incrementally from a small number of labeled samples without forgetting previously learned tasks, closely mimicking human learning patterns. In this paper, we propose a novel approach called Prompt Learning for FSCIL (PL-FSCIL), which harnesses the power of prompts in conjunction with a pre-trained Vision Transformer (ViT) model to address the challenges of FSCIL effectively. Our work pioneers the use of visual prompts in FSCIL, which is characterized by its notable simplicity. PL-FSCIL consists of two distinct prompts: the Domain Prompt and the FSCIL Prompt. Both are vectors that augment the model by embedding themselves into the attention layer of the ViT model. Specifically, the Domain Prompt assists the ViT model in adapting to new data domains. The task-specific FSCIL Prompt, coupled with a prototype classifier, amplifies the model's ability to effectively handle FSCIL tasks. We validate the efficacy of PL-FSCIL on widely used benchmark datasets such as CIFAR-100 and CUB-200. The results showcase competitive performance, underscoring its promising potential for real-world applications where high-quality data is often scarce. The source code is available at: https://github.com/TianSongS/PL-FSCIL.|Few-Shot Class-Incremental Learning (FSCIL) \u65e8\u5728\u4f7f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u4ece\u5c11\u91cf\u6807\u8bb0\u6837\u672c\u4e2d\u589e\u91cf\u5b66\u4e60\u65b0\u4efb\u52a1\uff0c\u800c\u4e0d\u4f1a\u5fd8\u8bb0\u4ee5\u524d\u5b66\u8fc7\u7684\u4efb\u52a1\uff0c\u4ece\u800c\u5bc6\u5207\u6a21\u4eff\u4eba\u7c7b\u7684\u5b66\u4e60\u6a21\u5f0f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a FSCIL \u63d0\u793a\u5b66\u4e60 (PL-FSCIL) \u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u63d0\u793a\u7684\u529b\u91cf\u4e0e\u9884\u8bad\u7ec3\u7684 Vision Transformer (ViT) \u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u5e94\u5bf9 FSCIL \u7684\u6311\u6218\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5f00\u521b\u4e86\u5728 FSCIL \u4e2d\u4f7f\u7528\u89c6\u89c9\u63d0\u793a\u7684\u5148\u6cb3\uff0c\u5176\u7279\u70b9\u662f\u975e\u5e38\u7b80\u5355\u3002 PL-FSCIL \u5305\u542b\u4e24\u4e2a\u4e0d\u540c\u7684\u63d0\u793a\uff1a\u57df\u63d0\u793a\u548c FSCIL \u63d0\u793a\u3002\u4e24\u8005\u90fd\u662f\u901a\u8fc7\u5c06\u81ea\u8eab\u5d4c\u5165\u5230 ViT \u6a21\u578b\u7684\u6ce8\u610f\u529b\u5c42\u6765\u589e\u5f3a\u6a21\u578b\u7684\u5411\u91cf\u3002\u5177\u4f53\u6765\u8bf4\uff0cDomain Prompt \u5e2e\u52a9 ViT \u6a21\u578b\u9002\u5e94\u65b0\u7684\u6570\u636e\u57df\u3002\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684 FSCIL \u63d0\u793a\u4e0e\u539f\u578b\u5206\u7c7b\u5668\u76f8\u7ed3\u5408\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u6709\u6548\u5904\u7406 FSCIL \u4efb\u52a1\u7684\u80fd\u529b\u3002\u6211\u4eec\u5728 CIFAR-100 \u548c CUB-200 \u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86 PL-FSCIL \u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u5c55\u793a\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u5176\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u5f80\u5f80\u7a00\u7f3a\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002\u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/TianSongS/PL-FSCIL\u3002|[2401.14807v1](http://arxiv.org/pdf/2401.14807v1)|null|\n", "2401.14754": "|**2024-01-26**|**VJT: A Video Transformer on Joint Tasks of Deblurring, Low-light Enhancement and Denoising**|VJT\uff1a\u4e00\u79cd\u7528\u4e8e\u53bb\u6a21\u7cca\u3001\u4f4e\u5149\u589e\u5f3a\u548c\u53bb\u566a\u8054\u5408\u4efb\u52a1\u7684\u89c6\u9891\u8f6c\u6362\u5668|Yuxiang Hui, Yang Liu, Yaofang Liu, Fan Jia, Jinshan Pan, Raymond Chan, Tieyong Zeng|Video restoration task aims to recover high-quality videos from low-quality observations. This contains various important sub-tasks, such as video denoising, deblurring and low-light enhancement, since video often faces different types of degradation, such as blur, low light, and noise. Even worse, these kinds of degradation could happen simultaneously when taking videos in extreme environments. This poses significant challenges if one wants to remove these artifacts at the same time. In this paper, to the best of our knowledge, we are the first to propose an efficient end-to-end video transformer approach for the joint task of video deblurring, low-light enhancement, and denoising. This work builds a novel multi-tier transformer where each tier uses a different level of degraded video as a target to learn the features of video effectively. Moreover, we carefully design a new tier-to-tier feature fusion scheme to learn video features incrementally and accelerate the training process with a suitable adaptive weighting scheme. We also provide a new Multiscene-Lowlight-Blur-Noise (MLBN) dataset, which is generated according to the characteristics of the joint task based on the RealBlur dataset and YouTube videos to simulate realistic scenes as far as possible. We have conducted extensive experiments, compared with many previous state-of-the-art methods, to show the effectiveness of our approach clearly.|\u89c6\u9891\u6062\u590d\u4efb\u52a1\u65e8\u5728\u4ece\u4f4e\u8d28\u91cf\u7684\u89c2\u5bdf\u4e2d\u6062\u590d\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u3002\u8fd9\u5305\u542b\u5404\u79cd\u91cd\u8981\u7684\u5b50\u4efb\u52a1\uff0c\u4f8b\u5982\u89c6\u9891\u53bb\u566a\u3001\u53bb\u6a21\u7cca\u548c\u4f4e\u5149\u589e\u5f3a\uff0c\u56e0\u4e3a\u89c6\u9891\u7ecf\u5e38\u9762\u4e34\u4e0d\u540c\u7c7b\u578b\u7684\u9000\u5316\uff0c\u4f8b\u5982\u6a21\u7cca\u3001\u4f4e\u5149\u548c\u566a\u58f0\u3002\u66f4\u7cdf\u7cd5\u7684\u662f\uff0c\u5728\u6781\u7aef\u73af\u5883\u4e2d\u62cd\u6444\u89c6\u9891\u65f6\uff0c\u8fd9\u4e9b\u9000\u5316\u53ef\u80fd\u4f1a\u540c\u65f6\u53d1\u751f\u3002\u5982\u679c\u60f3\u8981\u540c\u65f6\u5220\u9664\u8fd9\u4e9b\u4f2a\u5f71\uff0c\u8fd9\u4f1a\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u89c6\u9891\u53d8\u6362\u5668\u65b9\u6cd5\uff0c\u7528\u4e8e\u89c6\u9891\u53bb\u6a21\u7cca\u3001\u4f4e\u5149\u589e\u5f3a\u548c\u53bb\u566a\u7684\u8054\u5408\u4efb\u52a1\u3002\u8fd9\u9879\u5de5\u4f5c\u6784\u5efa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c42\u53d8\u538b\u5668\uff0c\u5176\u4e2d\u6bcf\u4e00\u5c42\u4f7f\u7528\u4e0d\u540c\u7ea7\u522b\u7684\u964d\u7ea7\u89c6\u9891\u4f5c\u4e3a\u76ee\u6807\u6765\u6709\u6548\u5730\u5b66\u4e60\u89c6\u9891\u7684\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7cbe\u5fc3\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u7279\u5f81\u878d\u5408\u65b9\u6848\uff0c\u4ee5\u589e\u91cf\u5b66\u4e60\u89c6\u9891\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5408\u9002\u7684\u81ea\u9002\u5e94\u52a0\u6743\u65b9\u6848\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u65b0\u7684Multiscene-Lowlight-Blur-Noise\uff08MLBN\uff09\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u662f\u6839\u636e\u57fa\u4e8eRealBlur\u6570\u636e\u96c6\u548cYouTube\u89c6\u9891\u7684\u8054\u5408\u4efb\u52a1\u7684\u7279\u70b9\u751f\u6210\u7684\uff0c\u4ee5\u5c3d\u53ef\u80fd\u6a21\u62df\u771f\u5b9e\u7684\u573a\u666f\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4e0e\u8bb8\u591a\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u4ee5\u6e05\u695a\u5730\u8868\u660e\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2401.14754v1](http://arxiv.org/pdf/2401.14754v1)|null|\n", "2401.14749": "|**2024-01-26**|**Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes**|\u57fa\u4e8e\u80fd\u91cf\u7684\u6a21\u578b\u5e73\u8861\u7684\u62d3\u6251\u611f\u77e5\u63a2\u7d22\uff1aToric QC-LDPC \u7801\u548c\u53cc\u66f2 MET QC-LDPC \u7801|Vasiliy Usatyuk, Denis Sapozhnikov, Sergey Egorov|This paper presents a method for achieving equilibrium in the ISING Hamiltonian when confronted with unevenly distributed charges on an irregular grid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our approach involves dimensionally expanding the system, substituting charges with circulants, and representing distances through circulant shifts. This results in a systematic mapping of the charge system onto a space, transforming the irregular grid into a uniform configuration, applicable to Torical and Circular Hyperboloid Topologies. The paper covers fundamental definitions and notations related to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine. It explores the marginalization problem in code on the graph probabilistic models for evaluating the partition function, encompassing exact and approximate estimation techniques. Rigorous proof is provided for the attainability of equilibrium states for the Boltzmann machine under Torical and Circular Hyperboloid, paving the way for the application of our methodology. Practical applications of our approach are investigated in Finite Geometry QC-LDPC Codes, specifically in Material Science. The paper further explores its effectiveness in the realm of Natural Language Processing Transformer Deep Neural Networks, examining Generalized Repeat Accumulate Codes, Spatially-Coupled and Cage-Graph QC-LDPC Codes. The versatile and impactful nature of our topology-aware hardware-efficient quasi-cycle codes equilibrium method is showcased across diverse scientific domains without the use of specific section delineations.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728 ISING \u54c8\u5bc6\u987f\u91cf\u9047\u5230\u4e0d\u89c4\u5219\u7f51\u683c\u4e0a\u5206\u5e03\u4e0d\u5747\u5300\u7684\u7535\u8377\u65f6\u5b9e\u73b0\u5e73\u8861\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\uff08\u591a\u8fb9\uff09QC-LDPC \u7801\u548c\u73bb\u5c14\u5179\u66fc\u673a\uff0c\u5305\u62ec\u5bf9\u7cfb\u7edf\u8fdb\u884c\u7ef4\u5ea6\u6269\u5c55\u3001\u7528\u5faa\u73af\u4ee3\u66ff\u7535\u8377\u4ee5\u53ca\u901a\u8fc7\u5faa\u73af\u79fb\u4f4d\u8868\u793a\u8ddd\u79bb\u3002\u8fd9\u5bfc\u81f4\u7535\u8377\u7cfb\u7edf\u5230\u7a7a\u95f4\u7684\u7cfb\u7edf\u6620\u5c04\uff0c\u5c06\u4e0d\u89c4\u5219\u7f51\u683c\u8f6c\u53d8\u4e3a\u5747\u5300\u914d\u7f6e\uff0c\u9002\u7528\u4e8e\u73af\u9762\u548c\u5706\u5f62\u53cc\u66f2\u9762\u62d3\u6251\u3002\u672c\u6587\u6db5\u76d6\u4e86\u4e0e QC-LDPC \u7801\u3001\u591a\u8fb9 QC-LDPC \u7801\u548c\u73bb\u5c14\u5179\u66fc\u673a\u76f8\u5173\u7684\u57fa\u672c\u5b9a\u4e49\u548c\u7b26\u53f7\u3002\u5b83\u63a2\u8ba8\u4e86\u7528\u4e8e\u8bc4\u4f30\u914d\u5206\u51fd\u6570\u7684\u56fe\u6982\u7387\u6a21\u578b\u4ee3\u7801\u4e2d\u7684\u8fb9\u7f18\u5316\u95ee\u9898\uff0c\u5305\u62ec\u7cbe\u786e\u548c\u8fd1\u4f3c\u4f30\u8ba1\u6280\u672f\u3002\u4e3a\u73bb\u5c14\u5179\u66fc\u673a\u5728\u73af\u53cc\u66f2\u9762\u548c\u5706\u53cc\u66f2\u9762\u4e0b\u8fbe\u5230\u5e73\u8861\u6001\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8bc1\u660e\uff0c\u4e3a\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u5728\u6709\u9650\u51e0\u4f55 QC-LDPC \u4ee3\u7801\u4e2d\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u3002\u672c\u6587\u8fdb\u4e00\u6b65\u63a2\u8ba8\u4e86\u5176\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406 Transformer \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u7814\u7a76\u4e86\u5e7f\u4e49\u91cd\u590d\u7d2f\u79ef\u7801\u3001\u7a7a\u95f4\u8026\u5408\u548c\u7b3c\u56fe QC-LDPC \u7801\u3002\u6211\u4eec\u7684\u62d3\u6251\u611f\u77e5\u786c\u4ef6\u9ad8\u6548\u51c6\u5faa\u73af\u4ee3\u7801\u5e73\u8861\u65b9\u6cd5\u7684\u591a\u529f\u80fd\u6027\u548c\u5f71\u54cd\u529b\u5728\u4e0d\u540c\u7684\u79d1\u5b66\u9886\u57df\u5f97\u5230\u4e86\u5c55\u793a\uff0c\u800c\u65e0\u9700\u4f7f\u7528\u7279\u5b9a\u7684\u90e8\u5206\u63cf\u8ff0\u3002|[2401.14749v1](http://arxiv.org/pdf/2401.14749v1)|null|\n"}, "Nerf": {"2401.15029": "|**2024-01-26**|**Learning Neural Radiance Fields of Forest Structure for Scalable and Fine Monitoring**|\u5b66\u4e60\u68ee\u6797\u7ed3\u6784\u7684\u795e\u7ecf\u8f90\u5c04\u573a\u4ee5\u8fdb\u884c\u53ef\u6269\u5c55\u548c\u7cbe\u7ec6\u76d1\u63a7|Juan Castorena|This work leverages neural radiance fields and remote sensing for forestry applications. Here, we show neural radiance fields offer a wide range of possibilities to improve upon existing remote sensing methods in forest monitoring. We present experiments that demonstrate their potential to: (1) express fine features of forest 3D structure, (2) fuse available remote sensing modalities and (3), improve upon 3D structure derived forest metrics. Altogether, these properties make neural fields an attractive computational tool with great potential to further advance the scalability and accuracy of forest monitoring programs.|\u8fd9\u9879\u5de5\u4f5c\u5229\u7528\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u9065\u611f\u8fdb\u884c\u6797\u4e1a\u5e94\u7528\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u795e\u7ecf\u8f90\u5c04\u573a\u4e3a\u6539\u8fdb\u68ee\u6797\u76d1\u6d4b\u4e2d\u73b0\u6709\u9065\u611f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u53ef\u80fd\u6027\u3002\u6211\u4eec\u63d0\u51fa\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5b83\u4eec\u7684\u6f5c\u529b\uff1a(1) \u8868\u8fbe\u68ee\u6797 3D \u7ed3\u6784\u7684\u7cbe\u7ec6\u7279\u5f81\uff0c(2) \u878d\u5408\u53ef\u7528\u7684\u9065\u611f\u6a21\u5f0f\uff0c\u4ee5\u53ca (3) \u6539\u8fdb 3D \u7ed3\u6784\u884d\u751f\u7684\u68ee\u6797\u6307\u6807\u3002\u603b\u800c\u8a00\u4e4b\uff0c\u8fd9\u4e9b\u7279\u6027\u4f7f\u795e\u7ecf\u573a\u6210\u4e3a\u4e00\u79cd\u6709\u5438\u5f15\u529b\u7684\u8ba1\u7b97\u5de5\u5177\uff0c\u5177\u6709\u8fdb\u4e00\u6b65\u63d0\u9ad8\u68ee\u6797\u76d1\u6d4b\u9879\u76ee\u7684\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u7684\u5de8\u5927\u6f5c\u529b\u3002|[2401.15029v1](http://arxiv.org/pdf/2401.15029v1)|null|\n", "2401.14726": "|**2024-01-26**|**3D Reconstruction and New View Synthesis of Indoor Environments based on a Dual Neural Radiance Field**|\u57fa\u4e8e\u53cc\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u5ba4\u5185\u73af\u58833D\u91cd\u5efa\u4e0e\u65b0\u89c6\u56fe\u5408\u6210|Zhenyu Bao, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu, Qing Li, Guoping Qiu|Simultaneously achieving 3D reconstruction and new view synthesis for indoor environments has widespread applications but is technically very challenging. State-of-the-art methods based on implicit neural functions can achieve excellent 3D reconstruction results, but their performances on new view synthesis can be unsatisfactory. The exciting development of neural radiance field (NeRF) has revolutionized new view synthesis, however, NeRF-based models can fail to reconstruct clean geometric surfaces. We have developed a dual neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry reconstruction and view rendering. Du-NeRF contains two geometric fields, one derived from the SDF field to facilitate geometric reconstruction and the other derived from the density field to boost new view synthesis. One of the innovative features of Du-NeRF is that it decouples a view-independent component from the density field and uses it as a label to supervise the learning process of the SDF field. This reduces shape-radiance ambiguity and enables geometry and color to benefit from each other during the learning process. Extensive experiments demonstrate that Du-NeRF can significantly improve the performance of novel view synthesis and 3D reconstruction for indoor environments and it is particularly effective in constructing areas containing fine geometries that do not obey multi-view color consistency.|\u540c\u65f6\u5b9e\u73b0\u5ba4\u5185\u73af\u5883\u7684 3D \u91cd\u5efa\u548c\u65b0\u89c6\u56fe\u5408\u6210\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u4f46\u5728\u6280\u672f\u4e0a\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u3002\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u51fd\u6570\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u51fa\u8272\u7684 3D \u91cd\u5efa\u7ed3\u679c\uff0c\u4f46\u5b83\u4eec\u5728\u65b0\u89c6\u56fe\u5408\u6210\u4e0a\u7684\u6027\u80fd\u53ef\u80fd\u65e0\u6cd5\u4ee4\u4eba\u6ee1\u610f\u3002\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u4ee4\u4eba\u5174\u594b\u7684\u53d1\u5c55\u5f7b\u5e95\u6539\u53d8\u4e86\u65b0\u7684\u89c6\u56fe\u5408\u6210\uff0c\u7136\u800c\uff0c\u57fa\u4e8e NeRF \u7684\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u91cd\u5efa\u5e72\u51c0\u7684\u51e0\u4f55\u8868\u9762\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u53cc\u795e\u7ecf\u8f90\u5c04\u573a\uff08Du-NeRF\uff09\u6765\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u91cd\u5efa\u548c\u89c6\u56fe\u6e32\u67d3\u3002 Du-NeRF \u5305\u542b\u4e24\u4e2a\u51e0\u4f55\u573a\uff0c\u4e00\u4e2a\u6e90\u81ea SDF \u573a\u4ee5\u4fc3\u8fdb\u51e0\u4f55\u91cd\u5efa\uff0c\u53e6\u4e00\u4e2a\u6e90\u81ea\u5bc6\u5ea6\u573a\u4ee5\u4fc3\u8fdb\u65b0\u89c6\u56fe\u5408\u6210\u3002 Du-NeRF\u7684\u521b\u65b0\u7279\u70b9\u4e4b\u4e00\u662f\u5b83\u5c06\u4e0e\u89c6\u56fe\u65e0\u5173\u7684\u5206\u91cf\u4ece\u5bc6\u5ea6\u573a\u4e2d\u89e3\u8026\u51fa\u6765\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6807\u7b7e\u6765\u76d1\u7763SDF\u573a\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002\u8fd9\u51cf\u5c11\u4e86\u5f62\u72b6-\u8f90\u5c04\u5ea6\u7684\u6a21\u7cca\u6027\uff0c\u5e76\u4f7f\u51e0\u4f55\u548c\u989c\u8272\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u76f8\u4e92\u53d7\u76ca\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDu-NeRF \u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u5ba4\u5185\u73af\u5883\u7684\u65b0\u9896\u89c6\u56fe\u5408\u6210\u548c 3D \u91cd\u5efa\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6784\u5efa\u5305\u542b\u4e0d\u9075\u5b88\u591a\u89c6\u56fe\u989c\u8272\u4e00\u81f4\u6027\u7684\u7cbe\u7ec6\u51e0\u4f55\u5f62\u72b6\u7684\u533a\u57df\u65f6\u7279\u522b\u6709\u6548\u3002|[2401.14726v1](http://arxiv.org/pdf/2401.14726v1)|null|\n"}, "3DGS": {}, "3D/CG": {"2401.14861": "|**2024-01-26**|**Implicit Neural Representation for Physics-driven Actuated Soft Bodies**|\u7269\u7406\u9a71\u52a8\u9a71\u52a8\u8f6f\u4f53\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a|Lingchen Yang, Byungsoo Kim, Gaspard Zoss, Baran G\u00f6zc\u00fc, Markus Gross, Barbara Solenthaler|Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal's dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.|\u4e3b\u52a8\u8f6f\u4f53\u53ef\u4ee5\u901a\u8fc7\u5f15\u8d77\u53d8\u5f62\u7684\u5185\u90e8\u9a71\u52a8\u673a\u5236\u5f71\u54cd\u5176\u5f62\u72b6\u3002\u4e0e\u6700\u8fd1\u7684\u5de5\u4f5c\u7c7b\u4f3c\uff0c\u672c\u6587\u5229\u7528\u53ef\u5fae\u5206\u3001\u51c6\u9759\u6001\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\u5c42\u6765\u4f18\u5316\u7531\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u7684\u9a71\u52a8\u4fe1\u53f7\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u662f\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\u6765\u63a7\u5236\u4e3b\u52a8\u8f6f\u4f53\u7684\u901a\u7528\u9690\u5f0f\u516c\u5f0f\uff0c\u8be5\u51fd\u6570\u80fd\u591f\u5b9e\u73b0\u4ece\u6750\u6599\u7a7a\u95f4\u4e2d\u7684\u7a7a\u95f4\u70b9\u5230\u9a71\u52a8\u503c\u7684\u8fde\u7eed\u6620\u5c04\u3002\u8fd9\u4e00\u7279\u6027\u4f7f\u6211\u4eec\u80fd\u591f\u6355\u83b7\u4fe1\u53f7\u7684\u4e3b\u9891\u7387\uff0c\u4ece\u800c\u4f7f\u8be5\u65b9\u6cd5\u4e0e\u79bb\u6563\u5316\u65e0\u5173\u5e76\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002\u6211\u4eec\u5c06\u9690\u5f0f\u6a21\u578b\u6269\u5c55\u5230\u4e0b\u988c\u8fd0\u52a8\u5b66\u7684\u7279\u5b9a\u60c5\u51b5\u4e0b\u7684\u9762\u90e8\u52a8\u753b\uff0c\u5e76\u8868\u660e\u6211\u4eec\u53ef\u4ee5\u53ef\u9760\u5730\u518d\u73b0\u7528\u9ad8\u8d28\u91cf\u6355\u83b7\u7cfb\u7edf\u6355\u83b7\u7684\u9762\u90e8\u8868\u60c5\u3002\u6211\u4eec\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u4f53\u79ef\u8f6f\u4f53\u3001\u4eba\u4f53\u59ff\u52bf\u548c\u9762\u90e8\u8868\u60c5\uff0c\u5c55\u793a\u4e86\u827a\u672f\u5bb6\u53cb\u597d\u7684\u7279\u6027\uff0c\u4f8b\u5982\u5bf9\u6f5c\u5728\u7a7a\u95f4\u7684\u7b80\u5355\u63a7\u5236\u548c\u6d4b\u8bd5\u65f6\u7684\u5206\u8fa8\u7387\u4e0d\u53d8\u6027\u3002|[2401.14861v1](http://arxiv.org/pdf/2401.14861v1)|null|\n", "2401.14785": "|**2024-01-26**|**SimpleEgo: Predicting Probabilistic Body Pose from Egocentric Cameras**|SimpleEgo\uff1a\u4ece\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u76f8\u673a\u9884\u6d4b\u6982\u7387\u8eab\u4f53\u59ff\u52bf|Hanz Cuevas-Velasquez, Charlie Hewitt, Sadegh Aliakbarian, Tadas Baltru\u0161aitis|Our work addresses the problem of egocentric human pose estimation from downwards-facing cameras on head-mounted devices (HMD). This presents a challenging scenario, as parts of the body often fall outside of the image or are occluded. Previous solutions minimize this problem by using fish-eye camera lenses to capture a wider view, but these can present hardware design issues. They also predict 2D heat-maps per joint and lift them to 3D space to deal with self-occlusions, but this requires large network architectures which are impractical to deploy on resource-constrained HMDs. We predict pose from images captured with conventional rectilinear camera lenses. This resolves hardware design issues, but means body parts are often out of frame. As such, we directly regress probabilistic joint rotations represented as matrix Fisher distributions for a parameterized body model. This allows us to quantify pose uncertainties and explain out-of-frame or occluded joints. This also removes the need to compute 2D heat-maps and allows for simplified DNN architectures which require less compute. Given the lack of egocentric datasets using rectilinear camera lenses, we introduce the SynthEgo dataset, a synthetic dataset with 60K stereo images containing high diversity of pose, shape, clothing and skin tone. Our approach achieves state-of-the-art results for this challenging configuration, reducing mean per-joint position error by 23% overall and 58% for the lower body. Our architecture also has eight times fewer parameters and runs twice as fast as the current state-of-the-art. Experiments show that training on our synthetic dataset leads to good generalization to real world images without fine-tuning.|\u6211\u4eec\u7684\u5de5\u4f5c\u89e3\u51b3\u4e86\u5934\u6234\u5f0f\u8bbe\u5907 (HMD) \u4e0a\u7684\u671d\u4e0b\u6444\u50cf\u5934\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u95ee\u9898\u3002\u8fd9\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\uff0c\u56e0\u4e3a\u8eab\u4f53\u7684\u67d0\u4e9b\u90e8\u5206\u7ecf\u5e38\u843d\u5728\u56fe\u50cf\u4e4b\u5916\u6216\u88ab\u906e\u6321\u3002\u4ee5\u524d\u7684\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u4f7f\u7528\u9c7c\u773c\u76f8\u673a\u955c\u5934\u6355\u6349\u66f4\u5e7f\u9614\u7684\u89c6\u91ce\u6765\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u53ef\u80fd\u4f1a\u5e26\u6765\u786c\u4ef6\u8bbe\u8ba1\u95ee\u9898\u3002\u4ed6\u4eec\u8fd8\u9884\u6d4b\u6bcf\u4e2a\u5173\u8282\u7684 2D \u70ed\u56fe\u5e76\u5c06\u5176\u63d0\u5347\u5230 3D \u7a7a\u95f4\u4ee5\u5904\u7406\u81ea\u906e\u6321\uff0c\u4f46\u8fd9\u9700\u8981\u5927\u578b\u7f51\u7edc\u67b6\u6784\uff0c\u800c\u8fd9\u5728\u8d44\u6e90\u6709\u9650\u7684 HMD \u4e0a\u90e8\u7f72\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002\u6211\u4eec\u6839\u636e\u4f20\u7edf\u76f4\u7ebf\u76f8\u673a\u955c\u5934\u62cd\u6444\u7684\u56fe\u50cf\u6765\u9884\u6d4b\u59ff\u52bf\u3002\u8fd9\u89e3\u51b3\u4e86\u786c\u4ef6\u8bbe\u8ba1\u95ee\u9898\uff0c\u4f46\u610f\u5473\u7740\u8eab\u4f53\u90e8\u4f4d\u7ecf\u5e38\u8131\u79bb\u6846\u67b6\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u76f4\u63a5\u5bf9\u53c2\u6570\u5316\u8eab\u4f53\u6a21\u578b\u7684\u8868\u793a\u4e3a\u77e9\u9635\u8d39\u820d\u5c14\u5206\u5e03\u7684\u6982\u7387\u5173\u8282\u65cb\u8f6c\u8fdb\u884c\u56de\u5f52\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u91cf\u5316\u59ff\u52bf\u7684\u4e0d\u786e\u5b9a\u6027\u5e76\u89e3\u91ca\u6846\u67b6\u5916\u6216\u95ed\u585e\u7684\u5173\u8282\u3002\u8fd9\u4e5f\u6d88\u9664\u4e86\u8ba1\u7b97 2D \u70ed\u56fe\u7684\u9700\u8981\uff0c\u5e76\u5141\u8bb8\u9700\u8981\u66f4\u5c11\u8ba1\u7b97\u7684\u7b80\u5316 DNN \u67b6\u6784\u3002\u9274\u4e8e\u7f3a\u4e4f\u4f7f\u7528\u76f4\u7ebf\u76f8\u673a\u955c\u5934\u7684\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SynthEgo \u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 60K \u7acb\u4f53\u56fe\u50cf\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u9ad8\u5ea6\u591a\u6837\u6027\u7684\u59ff\u52bf\u3001\u5f62\u72b6\u3001\u670d\u88c5\u548c\u80a4\u8272\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8fd9\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u914d\u7f6e\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5c06\u6bcf\u4e2a\u5173\u8282\u7684\u5e73\u5747\u4f4d\u7f6e\u8bef\u5dee\u603b\u4f53\u964d\u4f4e\u4e86 23%\uff0c\u4e0b\u534a\u8eab\u964d\u4f4e\u4e86 58%\u3002\u6211\u4eec\u7684\u67b6\u6784\u7684\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u4e86\u516b\u500d\uff0c\u8fd0\u884c\u901f\u5ea6\u662f\u5f53\u524d\u6700\u5148\u8fdb\u6280\u672f\u7684\u4e24\u500d\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u6211\u4eec\u7684\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u5f88\u597d\u5730\u6982\u62ec\u73b0\u5b9e\u4e16\u754c\u7684\u56fe\u50cf\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u5fae\u8c03\u3002|[2401.14785v1](http://arxiv.org/pdf/2401.14785v1)|null|\n", "2401.14733": "|**2024-01-26**|**Personality Perception in Human Videos Altered by Motion Transfer Networks**|\u8fd0\u52a8\u4f20\u8f93\u7f51\u7edc\u6539\u53d8\u4eba\u7c7b\u89c6\u9891\u4e2d\u7684\u4e2a\u6027\u611f\u77e5|Ayda Yurto\u011flu, Sinan Sonlu, Yal\u0131m Do\u011fan, U\u011fur G\u00fcd\u00fckbay|The successful portrayal of personality in digital characters improves communication and immersion. Current research focuses on expressing personality through modifying animations using heuristic rules or data-driven models. While studies suggest motion style highly influences the apparent personality, the role of appearance can be similarly essential. This work analyzes the influence of movement and appearance on the perceived personality of short videos altered by motion transfer networks. We label the personalities in conference video clips with a user study to determine the samples that best represent the Five-Factor model's high, neutral, and low traits. We alter these videos using the Thin-Plate Spline Motion Model, utilizing the selected samples as the source and driving inputs. We follow five different cases to study the influence of motion and appearance on personality perception. Our comparative study reveals that motion and appearance influence different factors: motion strongly affects perceived extraversion, and appearance helps convey agreeableness and neuroticism.|\u6570\u5b57\u89d2\u8272\u7684\u6210\u529f\u4e2a\u6027\u523b\u753b\u53ef\u4ee5\u6539\u5584\u6c9f\u901a\u548c\u6c89\u6d78\u611f\u3002\u76ee\u524d\u7684\u7814\u7a76\u91cd\u70b9\u662f\u901a\u8fc7\u4f7f\u7528\u542f\u53d1\u5f0f\u89c4\u5219\u6216\u6570\u636e\u9a71\u52a8\u6a21\u578b\u4fee\u6539\u52a8\u753b\u6765\u8868\u8fbe\u4e2a\u6027\u3002\u867d\u7136\u7814\u7a76\u8868\u660e\u52a8\u4f5c\u98ce\u683c\u5bf9\u8868\u9762\u4e2a\u6027\u6709\u5f88\u5927\u5f71\u54cd\uff0c\u4f46\u5916\u8868\u7684\u4f5c\u7528\u4e5f\u540c\u6837\u91cd\u8981\u3002\u8fd9\u9879\u5de5\u4f5c\u5206\u6790\u4e86\u8fd0\u52a8\u548c\u5916\u89c2\u5bf9\u8fd0\u52a8\u4f20\u8f93\u7f51\u7edc\u6539\u53d8\u7684\u77ed\u89c6\u9891\u611f\u77e5\u4e2a\u6027\u7684\u5f71\u54cd\u3002\u6211\u4eec\u901a\u8fc7\u7528\u6237\u7814\u7a76\u6765\u6807\u8bb0\u4f1a\u8bae\u89c6\u9891\u526a\u8f91\u4e2d\u7684\u4eba\u7269\uff0c\u4ee5\u786e\u5b9a\u6700\u80fd\u4ee3\u8868\u4e94\u56e0\u7d20\u6a21\u578b\u7684\u9ad8\u3001\u4e2d\u548c\u4f4e\u7279\u5f81\u7684\u6837\u672c\u3002\u6211\u4eec\u4f7f\u7528\u8584\u677f\u6837\u6761\u8fd0\u52a8\u6a21\u578b\u4fee\u6539\u8fd9\u4e9b\u89c6\u9891\uff0c\u5229\u7528\u9009\u5b9a\u7684\u6837\u672c\u4f5c\u4e3a\u6e90\u548c\u9a71\u52a8\u8f93\u5165\u3002\u6211\u4eec\u6309\u7167\u4e94\u4e2a\u4e0d\u540c\u7684\u6848\u4f8b\u6765\u7814\u7a76\u8fd0\u52a8\u548c\u5916\u8868\u5bf9\u4eba\u683c\u611f\u77e5\u7684\u5f71\u54cd\u3002\u6211\u4eec\u7684\u6bd4\u8f83\u7814\u7a76\u8868\u660e\uff0c\u8fd0\u52a8\u548c\u5916\u89c2\u5f71\u54cd\u4e0d\u540c\u7684\u56e0\u7d20\uff1a\u8fd0\u52a8\u5f3a\u70c8\u5f71\u54cd\u611f\u77e5\u7684\u5916\u5411\u6027\uff0c\u800c\u5916\u89c2\u6709\u52a9\u4e8e\u4f20\u8fbe\u5b9c\u4eba\u6027\u548c\u795e\u7ecf\u8d28\u3002|[2401.14733v1](http://arxiv.org/pdf/2401.14733v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2401.14966": "|**2024-01-26**|**Masked Pre-trained Model Enables Universal Zero-shot Denoiser**|\u63a9\u853d\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5b9e\u73b0\u901a\u7528\u96f6\u6837\u672c\u964d\u566a\u5668|Xiaoxiao Ma, Zhixiang Wei, Yi Jin, Pengyang Ling, Tianle Liu, Ben Wang, Junkang Dai, Huaian Chen, Enhong Chen|In this work, we observe that the model, which is trained on vast general images using masking strategy, has been naturally embedded with the distribution knowledge regarding natural images, and thus spontaneously attains the underlying potential for strong image denoising. Based on this observation, we propose a novel zero-shot denoising paradigm, i.e., Masked Pre-train then Iterative fill (MPI). MPI pre-trains a model with masking and fine-tunes it for denoising of a single image with unseen noise degradation. Concretely, the proposed MPI comprises two key procedures: 1) Masked Pre-training involves training a model on multiple natural images with random masks to gather generalizable representations, allowing for practical applications in varying noise degradation and even in distinct image types. 2) Iterative filling is devised to efficiently fuse pre-trained knowledge for denoising. Similar to but distinct from pre-training, random masking is retained to bridge the gap, but only the predicted parts covered by masks are assembled for efficiency, which enables high-quality denoising within a limited number of iterations. Comprehensive experiments across various noisy scenarios underscore the notable advances of proposed MPI over previous approaches with a marked reduction in inference time. Code is available at https://github.com/krennic999/MPI.git.|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u8be5\u6a21\u578b\u4f7f\u7528\u63a9\u853d\u7b56\u7565\u5728\u5927\u91cf\u666e\u901a\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u81ea\u7136\u5730\u5d4c\u5165\u4e86\u6709\u5173\u81ea\u7136\u56fe\u50cf\u7684\u5206\u5e03\u77e5\u8bc6\uff0c\u4ece\u800c\u81ea\u53d1\u5730\u83b7\u5f97\u4e86\u5f3a\u56fe\u50cf\u53bb\u566a\u7684\u6f5c\u5728\u6f5c\u529b\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u6837\u672c\u53bb\u566a\u8303\u4f8b\uff0c\u5373\u63a9\u6a21\u9884\u8bad\u7ec3\u7136\u540e\u8fed\u4ee3\u586b\u5145\uff08MPI\uff09\u3002 MPI \u901a\u8fc7\u63a9\u853d\u6765\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5bf9\u5355\u4e2a\u56fe\u50cf\u8fdb\u884c\u53bb\u566a\uff0c\u4ece\u800c\u5b9e\u73b0\u770b\u4e0d\u89c1\u7684\u566a\u58f0\u9000\u5316\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6240\u63d0\u51fa\u7684 MPI \u5305\u62ec\u4e24\u4e2a\u5173\u952e\u8fc7\u7a0b\uff1a1\uff09\u63a9\u7801\u9884\u8bad\u7ec3\u6d89\u53ca\u4f7f\u7528\u968f\u673a\u63a9\u7801\u5728\u591a\u4e2a\u81ea\u7136\u56fe\u50cf\u4e0a\u8bad\u7ec3\u6a21\u578b\u4ee5\u6536\u96c6\u53ef\u6982\u62ec\u7684\u8868\u793a\uff0c\u4ece\u800c\u5141\u8bb8\u5728\u4e0d\u540c\u7684\u566a\u58f0\u9000\u5316\u751a\u81f3\u4e0d\u540c\u7684\u56fe\u50cf\u7c7b\u578b\u4e2d\u8fdb\u884c\u5b9e\u9645\u5e94\u7528\u3002 2\uff09\u8fed\u4ee3\u586b\u5145\u65e8\u5728\u6709\u6548\u878d\u5408\u9884\u5148\u8bad\u7ec3\u7684\u77e5\u8bc6\u4ee5\u8fdb\u884c\u53bb\u566a\u3002\u4e0e\u9884\u8bad\u7ec3\u7c7b\u4f3c\u4f46\u4e0d\u540c\u7684\u662f\uff0c\u4fdd\u7559\u4e86\u968f\u673a\u63a9\u853d\u4ee5\u5f25\u8865\u5dee\u8ddd\uff0c\u4f46\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\uff0c\u4ec5\u7ec4\u88c5\u63a9\u853d\u8986\u76d6\u7684\u9884\u6d4b\u90e8\u5206\uff0c\u4ece\u800c\u5728\u6709\u9650\u7684\u8fed\u4ee3\u6b21\u6570\u5185\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u53bb\u566a\u3002\u8de8\u5404\u79cd\u566a\u58f0\u573a\u666f\u7684\u7efc\u5408\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u6240\u63d0\u51fa\u7684 MPI \u76f8\u5bf9\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u7684\u663e\u7740\u8fdb\u6b65\uff0c\u5e76\u4e14\u63a8\u7406\u65f6\u95f4\u663e\u7740\u51cf\u5c11\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/krennic999/MPI.git \u83b7\u53d6\u3002|[2401.14966v1](http://arxiv.org/pdf/2401.14966v1)|null|\n"}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "\u5176\u4ed6": {"2401.15022": "|**2024-01-26**|**Machine learning-based analysis of glioma tissue sections: a review**|\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u795e\u7ecf\u80f6\u8d28\u7624\u7ec4\u7ec7\u5207\u7247\u5206\u6790\uff1a\u7efc\u8ff0|Jan-Philipp Redlich, Friedrich Feuerhake, Joachim Weis, Nadine S. Schaadt, Sarah Teuber-Hanselmann, Christoph Buck, Sabine Luttmann, Andrea Eberle, Stefan Nikolin, Arno Appenzeller, et.al.|In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in isolation (10/70) or in addition to the TCGA datasets (11/70). Current approaches mostly rely on convolutional neural networks (53/70) for analyzing tissue at 20x magnification (30/70). A new field of research is the integration of clinical data, omics data, or magnetic resonance imaging (27/70). So far, machine learning-based methods have achieved promising results, but are not yet used in real clinical settings. Future work should focus on the independent validation of methods on larger, multi-site datasets with high-quality and up-to-date clinical and molecular pathology annotations to demonstrate routine applicability.|\u8fd1\u5e74\u6765\uff0c\u795e\u7ecf\u80f6\u8d28\u7624\u7684\u8bca\u65ad\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\u3002\u4f7f\u7528\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6280\u672f\u5bf9\u795e\u7ecf\u80f6\u8d28\u7624\u7ec4\u7ec7\u8fdb\u884c\u7ec4\u7ec7\u5b66\u8bc4\u4f30\u4e3a\u652f\u6301\u8bca\u65ad\u548c\u7ed3\u679c\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002\u4e3a\u4e86\u6982\u8ff0\u5f53\u524d\u7684\u7814\u7a76\u72b6\u51b5\uff0c\u672c\u7efc\u8ff0\u5ba1\u67e5\u4e86 70 \u9879\u516c\u5f00\u7684\u7814\u7a76\uff0c\u8fd9\u4e9b\u7814\u7a76\u6d89\u53ca\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u67d3\u8272\u4eba\u7c7b\u795e\u7ecf\u80f6\u8d28\u7624\u7ec4\u7ec7\u5207\u7247\u7684\u5206\u6790\uff0c\u6db5\u76d6\u4e9a\u578b\u5206\u578b (16/70)\u3001\u5206\u7ea7 (23/70) \u7684\u8bca\u65ad\u4efb\u52a1\u3001\u5206\u5b50\u6807\u8bb0\u9884\u6d4b\uff0813/70\uff09\u548c\u751f\u5b58\u9884\u6d4b\uff0827/70\uff09\u3002\u6240\u6709\u7814\u7a76\u5747\u5728\u65b9\u6cd5\u5b66\u65b9\u9762\u4ee5\u53ca\u4e34\u5e8a\u9002\u7528\u6027\u65b9\u9762\u8fdb\u884c\u4e86\u5ba1\u67e5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u76ee\u524d\u7814\u7a76\u7684\u91cd\u70b9\u662f\u8bc4\u4f30\u6210\u4eba\u578b\u5f25\u6f2b\u6027\u80f6\u8d28\u7624\u7684\u82cf\u6728\u7cbe\u548c\u4f0a\u7ea2\u67d3\u8272\u7ec4\u7ec7\u5207\u7247\u3002\u5927\u591a\u6570\u7814\u7a76 (49/70) \u57fa\u4e8e\u764c\u75c7\u57fa\u56e0\u7ec4\u56fe\u8c31 (TCGA) \u4e2d\u516c\u5f00\u7684\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u548c\u4f4e\u7ea7\u522b\u80f6\u8d28\u7624\u6570\u636e\u96c6\uff0c\u53ea\u6709\u5c11\u6570\u7814\u7a76\u5355\u72ec\u4f7f\u7528\u5176\u4ed6\u6570\u636e\u96c6 (10/70) \u6216\u9664TCGA \u6570\u636e\u96c6 (11/70)\u3002\u5f53\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u9760\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (53/70) \u4ee5 20 \u500d\u653e\u5927\u500d\u7387 (30/70) \u5206\u6790\u7ec4\u7ec7\u3002\u4e00\u4e2a\u65b0\u7684\u7814\u7a76\u9886\u57df\u662f\u4e34\u5e8a\u6570\u636e\u3001\u7ec4\u5b66\u6570\u636e\u6216\u78c1\u5171\u632f\u6210\u50cf\u7684\u6574\u5408 (27/70)\u3002\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u5df2\u7ecf\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u5c1a\u672a\u5e94\u7528\u4e8e\u5b9e\u9645\u7684\u4e34\u5e8a\u73af\u5883\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5e94\u4fa7\u91cd\u4e8e\u5728\u66f4\u5927\u7684\u591a\u7ad9\u70b9\u6570\u636e\u96c6\u4e0a\u5bf9\u65b9\u6cd5\u8fdb\u884c\u72ec\u7acb\u9a8c\u8bc1\uff0c\u5e76\u63d0\u4f9b\u9ad8\u8d28\u91cf\u548c\u6700\u65b0\u7684\u4e34\u5e8a\u548c\u5206\u5b50\u75c5\u7406\u5b66\u6ce8\u91ca\uff0c\u4ee5\u8bc1\u660e\u5e38\u89c4\u9002\u7528\u6027\u3002|[2401.15022v1](http://arxiv.org/pdf/2401.15022v1)|null|\n", "2401.15002": "|**2024-01-26**|**BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning**|BackdoorBench\uff1a\u540e\u95e8\u5b66\u4e60\u7684\u7efc\u5408\u57fa\u51c6\u548c\u5206\u6790|Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen|As an emerging and vital topic for studying deep neural networks' vulnerability (DNNs), backdoor learning has attracted increasing interest in recent years, and many seminal backdoor attack and defense algorithms are being developed successively or concurrently, in the status of a rapid arms race. However, mainly due to the diverse settings, and the difficulties of implementation and reproducibility of existing works, there is a lack of a unified and standardized benchmark of backdoor learning, causing unfair comparisons, and unreliable conclusions (e.g., misleading, biased or even false conclusions). Consequently, it is difficult to evaluate the current progress and design the future development roadmap of this literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. Our benchmark makes three valuable contributions to the research community. 1) We provide an integrated implementation of state-of-the-art (SOTA) backdoor learning algorithms (currently including 16 attack and 27 defense algorithms), based on an extensible modular-based codebase. 2) We conduct comprehensive evaluations of 12 attacks against 16 defenses, with 5 poisoning ratios, based on 4 models and 4 datasets, thus 11,492 pairs of evaluations in total. 3) Based on above evaluations, we present abundant analysis from 8 perspectives via 18 useful analysis tools, and provide several inspiring insights about backdoor learning. We hope that our efforts could build a solid foundation of backdoor learning to facilitate researchers to investigate existing algorithms, develop more innovative algorithms, and explore the intrinsic mechanism of backdoor learning. Finally, we have created a user-friendly website at http://backdoorbench.com, which collects all important information of BackdoorBench, including codebase, docs, leaderboard, and model Zoo.|\u4f5c\u4e3a\u7814\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u6f0f\u6d1e\u7684\u65b0\u5174\u91cd\u8981\u8bfe\u9898\uff0c\u540e\u95e8\u5b66\u4e60\u8fd1\u5e74\u6765\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u8bb8\u591a\u5f00\u521b\u6027\u7684\u540e\u95e8\u653b\u51fb\u548c\u9632\u5fa1\u7b97\u6cd5\u6b63\u5728\u76f8\u7ee7\u6216\u540c\u65f6\u5f00\u53d1\uff0c\u5904\u4e8e\u5feb\u901f\u519b\u5907\u7ade\u8d5b\u7684\u72b6\u6001\u3002\u7136\u800c\uff0c\u4e3b\u8981\u7531\u4e8e\u8bbe\u7f6e\u7684\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u73b0\u6709\u4f5c\u54c1\u7684\u5b9e\u65bd\u548c\u518d\u73b0\u6027\u7684\u56f0\u96be\uff0c\u7f3a\u4e4f\u7edf\u4e00\u548c\u89c4\u8303\u7684\u540e\u95e8\u5b66\u4e60\u57fa\u51c6\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u7684\u6bd4\u8f83\u548c\u4e0d\u53ef\u9760\u7684\u7ed3\u8bba\uff08\u4f8b\u5982\u8bef\u5bfc\u3001\u6709\u504f\u89c1\u751a\u81f3\u9519\u8bef\u7684\u7ed3\u8bba\uff09\u7ed3\u8bba\uff09\u3002\u56e0\u6b64\uff0c\u5f88\u96be\u8bc4\u4f30\u8be5\u6587\u732e\u5f53\u524d\u7684\u8fdb\u5c55\u5e76\u8bbe\u8ba1\u672a\u6765\u7684\u53d1\u5c55\u8def\u7ebf\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u79cd\u56f0\u5883\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u540d\u4e3a BackdoorBench \u7684\u7efc\u5408\u540e\u95e8\u5b66\u4e60\u57fa\u51c6\u3002\u6211\u4eec\u7684\u57fa\u51c6\u4e3a\u7814\u7a76\u754c\u505a\u51fa\u4e86\u4e09\u9879\u5b9d\u8d35\u7684\u8d21\u732e\u3002 1\uff09\u6211\u4eec\u57fa\u4e8e\u53ef\u6269\u5c55\u7684\u6a21\u5757\u5316\u4ee3\u7801\u5e93\uff0c\u63d0\u4f9b\u6700\u5148\u8fdb\uff08SOTA\uff09\u540e\u95e8\u5b66\u4e60\u7b97\u6cd5\uff08\u76ee\u524d\u5305\u62ec 16 \u79cd\u653b\u51fb\u7b97\u6cd5\u548c 27 \u79cd\u9632\u5fa1\u7b97\u6cd5\uff09\u7684\u96c6\u6210\u5b9e\u73b0\u3002 2\uff09\u57fa\u4e8e4\u4e2a\u6a21\u578b\u548c4\u4e2a\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5bf912\u79cd\u653b\u51fb\u5bf916\u79cd\u9632\u5fa1\u30015\u79cd\u4e2d\u6bd2\u7387\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u603b\u517111,492\u5bf9\u8bc4\u4f30\u3002 3\uff09\u57fa\u4e8e\u4e0a\u8ff0\u8bc4\u4f30\uff0c\u6211\u4eec\u901a\u8fc718\u4e2a\u6709\u7528\u7684\u5206\u6790\u5de5\u5177\u4ece8\u4e2a\u89d2\u5ea6\u8fdb\u884c\u4e86\u4e30\u5bcc\u7684\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5173\u4e8e\u540e\u95e8\u5b66\u4e60\u7684\u542f\u53d1\u6027\u89c1\u89e3\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u52aa\u529b\u80fd\u591f\u4e3a\u540e\u95e8\u5b66\u4e60\u5960\u5b9a\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u65b9\u4fbf\u7814\u7a76\u4eba\u5458\u7814\u7a76\u73b0\u6709\u7b97\u6cd5\uff0c\u5f00\u53d1\u66f4\u591a\u521b\u65b0\u7b97\u6cd5\uff0c\u63a2\u7d22\u540e\u95e8\u5b66\u4e60\u7684\u5185\u5728\u673a\u5236\u3002\u6700\u540e\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u7f51\u7ad9http://backdoorbench.com\uff0c\u8be5\u7f51\u7ad9\u6536\u96c6\u4e86BackdoorBench\u7684\u6240\u6709\u91cd\u8981\u4fe1\u606f\uff0c\u5305\u62ec\u4ee3\u7801\u5e93\u3001\u6587\u6863\u3001\u6392\u884c\u699c\u548c\u6a21\u578bZoo\u3002|[2401.15002v1](http://arxiv.org/pdf/2401.15002v1)|null|\n", "2401.14948": "|**2024-01-26**|**Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training**|\u4fdd\u5b58-\u66f4\u65b0-\u4fee\u8ba2\u4ee5\u89e3\u51b3\u5bf9\u6297\u6027\u8bad\u7ec3\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u6743\u8861|Shruthi Gowda, Bahram Zonooz, Elahe Arani|Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also aids in mitigating \"robust overfitting\". Furthermore, our study provides valuable insights into the mechanisms of selective adversarial training and offers a promising avenue for future research.|\u5bf9\u6297\u6027\u8bad\u7ec3\u63d0\u9ad8\u4e86\u795e\u7ecf\u7f51\u7edc\u5bf9\u6297\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5c3d\u7ba1\u662f\u4ee5\u727a\u7272\u6807\u51c6\u6cdb\u5316\u548c\u9c81\u68d2\u6cdb\u5316\u4e4b\u95f4\u7684\u6743\u8861\u4e3a\u4ee3\u4ef7\u7684\u3002\u4e3a\u4e86\u63ed\u793a\u9a71\u52a8\u8fd9\u4e00\u73b0\u8c61\u7684\u6f5c\u5728\u56e0\u7d20\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u4ece\u6807\u51c6\u73af\u5883\u8fc7\u6e21\u5230\u5bf9\u6297\u73af\u5883\u671f\u95f4\u7684\u5206\u5c42\u5b66\u4e60\u80fd\u529b\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6709\u9009\u62e9\u5730\u66f4\u65b0\u7279\u5b9a\u5c42\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u5c42\u53ef\u4ee5\u5927\u5927\u589e\u5f3a\u7f51\u7edc\u7684\u5b66\u4e60\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 CURE\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5b83\u5229\u7528\u68af\u5ea6\u7a81\u51fa\u6807\u51c6\u6765\u6267\u884c\u6743\u91cd\u7684\u9009\u62e9\u6027\u4fdd\u5b58\u3001\u66f4\u65b0\u548c\u4fee\u8ba2\u3002\u91cd\u8981\u7684\u662f\uff0cCURE \u7684\u8bbe\u8ba1\u4e0e\u6570\u636e\u96c6\u548c\u67b6\u6784\u65e0\u5173\uff0c\u786e\u4fdd\u5176\u5728\u5404\u79cd\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\u5b83\u6709\u6548\u5730\u89e3\u51b3\u4e86\u8bb0\u5fc6\u548c\u8fc7\u5ea6\u62df\u5408\u95ee\u9898\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u6b64\u5916\uff0c\u8fd9\u79cd\u8bad\u7ec3\u65b9\u6cd5\u8fd8\u6709\u52a9\u4e8e\u51cf\u8f7b\u201c\u9c81\u68d2\u8fc7\u5ea6\u62df\u5408\u201d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7814\u7a76\u4e3a\u9009\u62e9\u6027\u5bf9\u6297\u8bad\u7ec3\u7684\u673a\u5236\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002|[2401.14948v1](http://arxiv.org/pdf/2401.14948v1)|null|\n", "2401.14846": "|**2024-01-26**|**Understanding Domain Generalization: A Noise Robustness Perspective**|\u7406\u89e3\u57df\u6cdb\u5316\uff1a\u566a\u58f0\u9c81\u68d2\u6027\u89c6\u89d2|Rui Qiao, Bryan Kian Hsiang Low|Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We conjecture that the failure mode of ERM arising from spurious correlations may be less pronounced in practice.|\u5c3d\u7ba1\u9886\u57df\u6cdb\u5316\uff08DG\uff09\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u6ca1\u6709\u660e\u786e\u7684\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\u73b0\u6709\u7684 DG \u7b97\u6cd5\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u4f18\u4e8e\u7ecf\u5178\u7684\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\uff0c\u6211\u4eec\u4ece\u6807\u7b7e\u566a\u58f0\u7684\u89d2\u5ea6\u7814\u7a76\u4e86 DG \u7b97\u6cd5\u76f8\u5bf9\u4e8e ERM \u662f\u5426\u6709\u4f18\u52bf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u6709\u9650\u6837\u672c\u5206\u6790\u8868\u660e\uff0c\u6807\u7b7e\u566a\u58f0\u52a0\u5267\u4e86 ERM \u7684\u865a\u5047\u76f8\u5173\u6027\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u7834\u574f\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u8bf4\u660e\u5373\u4f7f\u5b58\u5728\u865a\u5047\u76f8\u5173\u6027\uff0cDG \u7b97\u6cd5\u5728\u6709\u9650\u6837\u672c\u8bad\u7ec3\u671f\u95f4\u4e5f\u8868\u73b0\u51fa\u9690\u5f0f\u6807\u7b7e\u566a\u58f0\u9c81\u68d2\u6027\u3002\u8fd9\u79cd\u7406\u60f3\u7684\u7279\u6027\u6709\u52a9\u4e8e\u51cf\u8f7b\u865a\u5047\u76f8\u5173\u6027\u5e76\u63d0\u9ad8\u5408\u6210\u5b9e\u9a8c\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u5bf9\u73b0\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5176\u4ed6\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e ERM \u76f8\u6bd4\uff0c\u6807\u7b7e\u566a\u58f0\u9c81\u68d2\u6027\u5e76\u4e0d\u4e00\u5b9a\u4f1a\u8f6c\u5316\u4e3a\u66f4\u597d\u7684\u6027\u80fd\u3002\u6211\u4eec\u63a8\u6d4b\uff0c\u7531\u865a\u5047\u76f8\u5173\u6027\u5f15\u8d77\u7684 ERM \u5931\u6548\u6a21\u5f0f\u5728\u5b9e\u8df5\u4e2d\u53ef\u80fd\u4e0d\u592a\u660e\u663e\u3002|[2401.14846v1](http://arxiv.org/pdf/2401.14846v1)|null|\n", "2401.14831": "|**2024-01-26**|**The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances**|\u673a\u5668\u89c6\u89c9\u51b0\u5c71\u89e3\u91ca\uff1a\u901a\u8fc7\u8003\u8651\u6574\u4f53\u73af\u5883\u60c5\u51b5\u63a8\u8fdb\u52a8\u6001\u6d4b\u8bd5|Hubert Padusinski, Thilo Braun, Christian Steinhauser, Lennart Ries, Eric Sax|Are we heading for an iceberg with the current testing of machine vision? This work delves into the landscape of Machine Vision (MV) testing, which is heavily required in Highly Automated Driving (HAD) systems. Utilizing the metaphorical notion of navigating towards an iceberg, we discuss the potential shortcomings concealed within current testing strategies. We emphasize the urgent need for a deeper understanding of how to deal with the opaque functions of MV in development processes. As overlooked considerations can cost lives. Our main contribution is the hierarchical level model, which we call Granularity Grades. The model encourages a refined exploration of the multi-scaled depths of understanding about the circumstances of environments in which MV is intended to operate. This model aims to provide a holistic overview of all entities that may impact MV functions, ranging from relations of individual entities like object attributes to entire environmental scenes. The application of our model delivers a structured exploration of entities in a specific domain, their relationships and assigning results of a MV-under-test to construct an entity-relationship graph. Through clustering patterns of relations in the graph general MV deficits are arguable. In Summary, our work contributes to a more nuanced and systematized identification of deficits of a MV test object in correlation to holistic circumstances in HAD operating domains.|\u5f53\u524d\u7684\u673a\u5668\u89c6\u89c9\u6d4b\u8bd5\u662f\u5426\u6b63\u5728\u8d70\u5411\u51b0\u5c71\uff1f\u8fd9\u9879\u5de5\u4f5c\u6df1\u5165\u7814\u7a76\u4e86\u673a\u5668\u89c6\u89c9 (MV) \u6d4b\u8bd5\u7684\u9886\u57df\uff0c\u8fd9\u5728\u9ad8\u5ea6\u81ea\u52a8\u9a7e\u9a76 (HAD) \u7cfb\u7edf\u4e2d\u662f\u975e\u5e38\u9700\u8981\u7684\u3002\u5229\u7528\u8d70\u5411\u51b0\u5c71\u7684\u9690\u55bb\u6982\u5ff5\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u5f53\u524d\u6d4b\u8bd5\u7b56\u7565\u4e2d\u9690\u85cf\u7684\u6f5c\u5728\u7f3a\u9677\u3002\u6211\u4eec\u5f3a\u8c03\u8feb\u5207\u9700\u8981\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u5982\u4f55\u5904\u7406\u5f00\u53d1\u8fc7\u7a0b\u4e2d MV \u7684\u4e0d\u900f\u660e\u529f\u80fd\u3002\u5ffd\u89c6\u8fd9\u4e9b\u56e0\u7d20\u53ef\u80fd\u4f1a\u5bfc\u81f4\u751f\u547d\u635f\u5931\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u662f\u5c42\u6b21\u6a21\u578b\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u7c92\u5ea6\u7b49\u7ea7\u3002\u8be5\u6a21\u578b\u9f13\u52b1\u5bf9 MV \u9884\u671f\u8fd0\u884c\u73af\u5883\u7684\u591a\u5c3a\u5ea6\u6df1\u5ea6\u8fdb\u884c\u7cbe\u7ec6\u63a2\u7d22\u3002\u8be5\u6a21\u578b\u65e8\u5728\u63d0\u4f9b\u53ef\u80fd\u5f71\u54cd MV \u529f\u80fd\u7684\u6240\u6709\u5b9e\u4f53\u7684\u6574\u4f53\u6982\u8ff0\uff0c\u8303\u56f4\u4ece\u5bf9\u8c61\u5c5e\u6027\u7b49\u5355\u4e2a\u5b9e\u4f53\u7684\u5173\u7cfb\u5230\u6574\u4e2a\u73af\u5883\u573a\u666f\u3002\u6211\u4eec\u7684\u6a21\u578b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5bf9\u7279\u5b9a\u9886\u57df\u4e2d\u7684\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\u7684\u7ed3\u6784\u5316\u63a2\u7d22\uff0c\u5e76\u5206\u914d\u88ab\u6d4b MV \u7684\u7ed3\u679c\u4ee5\u6784\u5efa\u5b9e\u4f53\u5173\u7cfb\u56fe\u3002\u901a\u8fc7\u56fe\u4e2d\u5173\u7cfb\u7684\u805a\u7c7b\u6a21\u5f0f\uff0c\u4e00\u822c MV \u7f3a\u9677\u662f\u6709\u4e89\u8bae\u7684\u3002\u603b\u4e4b\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u6709\u52a9\u4e8e\u66f4\u7ec6\u81f4\u3001\u66f4\u7cfb\u7edf\u5730\u8bc6\u522b MV \u6d4b\u8bd5\u5bf9\u8c61\u4e0e HAD \u64cd\u4f5c\u57df\u6574\u4f53\u73af\u5883\u76f8\u5173\u7684\u7f3a\u9677\u3002|[2401.14831v1](http://arxiv.org/pdf/2401.14831v1)|null|\n", "2401.14786": "|**2024-01-26**|**Study of the gOMP Algorithm for Recovery of Compressed Sensed Hyperspectral Images**|\u538b\u7f29\u611f\u77e5\u9ad8\u5149\u8c31\u56fe\u50cf\u6062\u590d\u7684gOMP\u7b97\u6cd5\u7814\u7a76|Jon Alvarez Justo, Milica Orlandic|Hyperspectral Imaging (HSI) is used in a wide range of applications such as remote sensing, yet the transmission of the HS images by communication data links becomes challenging due to the large number of spectral bands that the HS images contain together with the limited data bandwidth available in real applications. Compressive Sensing reduces the images by randomly subsampling the spectral bands of each spatial pixel and then it performs the image reconstruction of all the bands using recovery algorithms which impose sparsity in a certain transform domain. Since the image pixels are not strictly sparse, this work studies a data sparsification pre-processing stage prior to compression to ensure the sparsity of the pixels. The sparsified images are compressed $2.5\\times$ and then recovered using the Generalized Orthogonal Matching Pursuit algorithm (gOMP) characterized by high accuracy, low computational requirements and fast convergence. The experiments are performed in five conventional hyperspectral images where the effect of different sparsification levels in the quality of the uncompressed as well as the recovered images is studied. It is concluded that the gOMP algorithm reconstructs the hyperspectral images with higher accuracy as well as faster convergence when the pixels are highly sparsified and hence at the expense of reducing the quality of the recovered images with respect to the original images.|\u9ad8\u5149\u8c31\u6210\u50cf (HSI) \u5e7f\u6cdb\u5e94\u7528\u4e8e\u9065\u611f\u7b49\u9886\u57df\uff0c\u4f46\u7531\u4e8e HS \u56fe\u50cf\u5305\u542b\u5927\u91cf\u5149\u8c31\u5e26\u4ee5\u53ca\u6709\u9650\u7684\u6570\u636e\u5e26\u5bbd\uff0c\u56e0\u6b64\u901a\u8fc7\u901a\u4fe1\u6570\u636e\u94fe\u8def\u4f20\u8f93 HS \u56fe\u50cf\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u7528\u3002\u538b\u7f29\u611f\u77e5\u901a\u8fc7\u5bf9\u6bcf\u4e2a\u7a7a\u95f4\u50cf\u7d20\u7684\u5149\u8c31\u5e26\u8fdb\u884c\u968f\u673a\u4e8c\u6b21\u91c7\u6837\u6765\u51cf\u5c11\u56fe\u50cf\uff0c\u7136\u540e\u4f7f\u7528\u5728\u7279\u5b9a\u53d8\u6362\u57df\u4e2d\u65bd\u52a0\u7a00\u758f\u6027\u7684\u6062\u590d\u7b97\u6cd5\u5bf9\u6240\u6709\u9891\u5e26\u8fdb\u884c\u56fe\u50cf\u91cd\u5efa\u3002\u7531\u4e8e\u56fe\u50cf\u50cf\u7d20\u5e76\u4e0d\u662f\u4e25\u683c\u7a00\u758f\u7684\uff0c\u56e0\u6b64\u8fd9\u9879\u5de5\u4f5c\u7814\u7a76\u4e86\u538b\u7f29\u4e4b\u524d\u7684\u6570\u636e\u7a00\u758f\u9884\u5904\u7406\u9636\u6bb5\uff0c\u4ee5\u786e\u4fdd\u50cf\u7d20\u7684\u7a00\u758f\u6027\u3002\u7a00\u758f\u56fe\u50cf\u88ab\u538b\u7f29 $2.5\\times$\uff0c\u7136\u540e\u4f7f\u7528\u5e7f\u4e49\u6b63\u4ea4\u5339\u914d\u8ffd\u8e2a\u7b97\u6cd5 (gOMP) \u8fdb\u884c\u6062\u590d\uff0c\u8be5\u7b97\u6cd5\u5177\u6709\u7cbe\u5ea6\u9ad8\u3001\u8ba1\u7b97\u8981\u6c42\u4f4e\u548c\u6536\u655b\u901f\u5ea6\u5feb\u7684\u7279\u70b9\u3002\u5b9e\u9a8c\u5728\u4e94\u5e45\u4f20\u7edf\u9ad8\u5149\u8c31\u56fe\u50cf\u4e2d\u8fdb\u884c\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u7a00\u758f\u5316\u7ea7\u522b\u5bf9\u672a\u538b\u7f29\u56fe\u50cf\u548c\u6062\u590d\u56fe\u50cf\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u7ed3\u8bba\u662f\uff0c\u5f53\u50cf\u7d20\u9ad8\u5ea6\u7a00\u758f\u65f6\uff0cgOMP \u7b97\u6cd5\u80fd\u591f\u4ee5\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u91cd\u5efa\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u56e0\u6b64\u4f1a\u964d\u4f4e\u6062\u590d\u56fe\u50cf\u76f8\u5bf9\u4e8e\u539f\u59cb\u56fe\u50cf\u7684\u8d28\u91cf\u3002|[2401.14786v1](http://arxiv.org/pdf/2401.14786v1)|null|\n", "2401.14762": "|**2024-01-26**|**A Comparative Study of Compressive Sensing Algorithms for Hyperspectral Imaging Reconstruction**|\u9ad8\u5149\u8c31\u6210\u50cf\u91cd\u5efa\u538b\u7f29\u611f\u77e5\u7b97\u6cd5\u7684\u6bd4\u8f83\u7814\u7a76|Jon Alvarez Justo, Daniela Lupu, Milica Orlandic, Ion Necoara, Tor Arne Johansen|Hyperspectral Imaging comprises excessive data consequently leading to significant challenges for data processing, storage and transmission. Compressive Sensing has been used in the field of Hyperspectral Imaging as a technique to compress the large amount of data. This work addresses the recovery of hyperspectral images 2.5x compressed. A comparative study in terms of the accuracy and the performance of the convex FISTA/ADMM in addition to the greedy gOMP/BIHT/CoSaMP recovery algorithms is presented. The results indicate that the algorithms recover successfully the compressed data, yet the gOMP algorithm achieves superior accuracy and faster recovery in comparison to the other algorithms at the expense of high dependence on unknown sparsity level of the data to recover.|\u9ad8\u5149\u8c31\u6210\u50cf\u5305\u542b\u8fc7\u591a\u7684\u6570\u636e\uff0c\u56e0\u6b64\u7ed9\u6570\u636e\u5904\u7406\u3001\u5b58\u50a8\u548c\u4f20\u8f93\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u538b\u7f29\u611f\u77e5\u4f5c\u4e3a\u4e00\u79cd\u538b\u7f29\u5927\u91cf\u6570\u636e\u7684\u6280\u672f\u5df2\u88ab\u7528\u4e8e\u9ad8\u5149\u8c31\u6210\u50cf\u9886\u57df\u3002\u8fd9\u9879\u5de5\u4f5c\u81f4\u529b\u4e8e\u6062\u590d 2.5 \u500d\u538b\u7f29\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u3002\u9664\u4e86\u8d2a\u5a6a\u7684 gOMP/BIHT/CoSaMP \u6062\u590d\u7b97\u6cd5\u5916\uff0c\u8fd8\u5bf9\u51f8 FISTA/ADMM \u7684\u51c6\u786e\u6027\u548c\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\u7814\u7a76\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u6210\u529f\u5730\u6062\u590d\u4e86\u538b\u7f29\u6570\u636e\uff0c\u4f46\u4e0e\u5176\u4ed6\u7b97\u6cd5\u76f8\u6bd4\uff0cgOMP \u7b97\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u5feb\u7684\u6062\u590d\u901f\u5ea6\uff0c\u4f46\u4ee3\u4ef7\u662f\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u8981\u6062\u590d\u7684\u6570\u636e\u7684\u672a\u77e5\u7a00\u758f\u7a0b\u5ea6\u3002|[2401.14762v1](http://arxiv.org/pdf/2401.14762v1)|null|\n", "2401.14718": "|**2024-01-26**|**A Survey on Video Prediction: From Deterministic to Generative Approaches**|\u89c6\u9891\u9884\u6d4b\u8c03\u67e5\uff1a\u4ece\u786e\u5b9a\u6027\u65b9\u6cd5\u5230\u751f\u6210\u65b9\u6cd5|Ruibo Ming, Zhewei Huang, Zhuoxuan Ju, Jianming Hu, Lihui Peng, Shuchang Zhou|Video prediction, a fundamental task in computer vision, aims to enable models to generate sequences of future frames based on existing video content. This task has garnered widespread application across various domains. In this paper, we comprehensively survey both historical and contemporary works in this field, encompassing the most widely used datasets and algorithms. Our survey scrutinizes the challenges and evolving landscape of video prediction within the realm of computer vision. We propose a novel taxonomy centered on the stochastic nature of video prediction algorithms. This taxonomy accentuates the gradual transition from deterministic to generative prediction methodologies, underlining significant advancements and shifts in approach.|\u89c6\u9891\u9884\u6d4b\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u9879\u57fa\u672c\u4efb\u52a1\uff0c\u65e8\u5728\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u73b0\u6709\u89c6\u9891\u5185\u5bb9\u751f\u6210\u672a\u6765\u5e27\u7684\u5e8f\u5217\u3002\u8fd9\u9879\u4efb\u52a1\u5df2\u5728\u5404\u4e2a\u9886\u57df\u83b7\u5f97\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5168\u9762\u8c03\u67e5\u4e86\u8be5\u9886\u57df\u7684\u5386\u53f2\u548c\u5f53\u4ee3\u4f5c\u54c1\uff0c\u6db5\u76d6\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u548c\u7b97\u6cd5\u3002\u6211\u4eec\u7684\u8c03\u67e5\u4ed4\u7ec6\u5ba1\u89c6\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5185\u89c6\u9891\u9884\u6d4b\u7684\u6311\u6218\u548c\u4e0d\u65ad\u53d1\u5c55\u7684\u524d\u666f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u89c6\u9891\u9884\u6d4b\u7b97\u6cd5\u7684\u968f\u673a\u6027\u8d28\u4e3a\u4e2d\u5fc3\u7684\u65b0\u9896\u5206\u7c7b\u6cd5\u3002\u8fd9\u79cd\u5206\u7c7b\u5f3a\u8c03\u4e86\u4ece\u786e\u5b9a\u6027\u9884\u6d4b\u65b9\u6cd5\u5230\u751f\u6210\u6027\u9884\u6d4b\u65b9\u6cd5\u7684\u9010\u6e10\u8fc7\u6e21\uff0c\u5f3a\u8c03\u4e86\u65b9\u6cd5\u7684\u91cd\u5927\u8fdb\u6b65\u548c\u8f6c\u53d8\u3002|[2401.14718v1](http://arxiv.org/pdf/2401.14718v1)|null|\n", "2401.14707": "|**2024-01-26**|**Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement**|\u901a\u8fc7\u7279\u5f81\u89e3\u7f20\u6765\u7f29\u5c0f\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u7279\u5f81\u5dee\u8ddd|Nuoyan Zhou, Dawei Zhou, Decheng Liu, Xinbo Gao, Nannan Wang|Deep neural networks are vulnerable to adversarial samples. Adversarial fine-tuning methods aim to enhance adversarial robustness through fine-tuning the naturally pre-trained model in an adversarial training manner. However, we identify that some latent features of adversarial samples are confused by adversarial perturbation and lead to an unexpectedly increasing gap between features in the last hidden layer of natural and adversarial samples. To address this issue, we propose a disentanglement-based approach to explicitly model and further remove the latent features that cause the feature gap. Specifically, we introduce a feature disentangler to separate out the latent features from the features of the adversarial samples, thereby boosting robustness by eliminating the latent features. Besides, we align features in the pre-trained model with features of adversarial samples in the fine-tuned model, to further benefit from the features from natural samples without confusion. Empirical evaluations on three benchmark datasets demonstrate that our approach surpasses existing adversarial fine-tuning methods and adversarial training baselines.|\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5f88\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u6837\u672c\u7684\u5f71\u54cd\u3002\u5bf9\u6297\u6027\u5fae\u8c03\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u4ee5\u5bf9\u6297\u6027\u8bad\u7ec3\u7684\u65b9\u5f0f\u5fae\u8c03\u81ea\u7136\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u589e\u5f3a\u5bf9\u6297\u6027\u7684\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u5bf9\u6297\u6837\u672c\u7684\u4e00\u4e9b\u6f5c\u5728\u7279\u5f81\u88ab\u5bf9\u6297\u6270\u52a8\u6240\u6df7\u6dc6\uff0c\u5e76\u5bfc\u81f4\u81ea\u7136\u6837\u672c\u548c\u5bf9\u6297\u6837\u672c\u7684\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u5c42\u7684\u7279\u5f81\u4e4b\u95f4\u7684\u5dee\u8ddd\u610f\u5916\u589e\u5927\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u7f20\u7ed3\u7684\u65b9\u6cd5\u6765\u663e\u5f0f\u5efa\u6a21\u5e76\u8fdb\u4e00\u6b65\u6d88\u9664\u5bfc\u81f4\u7279\u5f81\u5dee\u8ddd\u7684\u6f5c\u5728\u7279\u5f81\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7279\u5f81\u89e3\u7f20\u5668\uff0c\u5c06\u6f5c\u5728\u7279\u5f81\u4ece\u5bf9\u6297\u6837\u672c\u7684\u7279\u5f81\u4e2d\u5206\u79bb\u51fa\u6765\uff0c\u4ece\u800c\u901a\u8fc7\u6d88\u9664\u6f5c\u5728\u7279\u5f81\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u4e0e\u5fae\u8c03\u6a21\u578b\u4e2d\u5bf9\u6297\u6837\u672c\u7684\u7279\u5f81\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ee5\u8fdb\u4e00\u6b65\u53d7\u76ca\u4e8e\u81ea\u7136\u6837\u672c\u7684\u7279\u5f81\u800c\u4e0d\u4f1a\u6df7\u6dc6\u3002\u5bf9\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5bf9\u6297\u6027\u5fae\u8c03\u65b9\u6cd5\u548c\u5bf9\u6297\u6027\u8bad\u7ec3\u57fa\u7ebf\u3002|[2401.14707v1](http://arxiv.org/pdf/2401.14707v1)|null|\n", "2401.14626": "|**2024-01-26**|**Towards Lifelong Scene Graph Generation with Knowledge-ware In-context Prompt Learning**|\u901a\u8fc7\u77e5\u8bc6\u4ef6\u4e0a\u4e0b\u6587\u5373\u65f6\u5b66\u4e60\u5b9e\u73b0\u7ec8\u8eab\u573a\u666f\u56fe\u751f\u6210|Tao He, Tongtong Wu, Dongyang Zhang, Guiduo Duan, Ke Qin, Yuan-Fang Li|Scene graph generation (SGG) endeavors to predict visual relationships between pairs of objects within an image. Prevailing SGG methods traditionally assume a one-off learning process for SGG. This conventional paradigm may necessitate repetitive training on all previously observed samples whenever new relationships emerge, mitigating the risk of forgetting previously acquired knowledge. This work seeks to address this pitfall inherent in a suite of prior relationship predictions. Motivated by the achievements of in-context learning in pretrained language models, our approach imbues the model with the capability to predict relationships and continuously acquire novel knowledge without succumbing to catastrophic forgetting. To achieve this goal, we introduce a novel and pragmatic framework for scene graph generation, namely Lifelong Scene Graph Generation (LSGG), where tasks, such as predicates, unfold in a streaming fashion. In this framework, the model is constrained to exclusive training on the present task, devoid of access to previously encountered training data, except for a limited number of exemplars, but the model is tasked with inferring all predicates it has encountered thus far. Rigorous experiments demonstrate the superiority of our proposed method over state-of-the-art SGG models in the context of LSGG across a diverse array of metrics. Besides, extensive experiments on the two mainstream benchmark datasets, VG and Open-Image(v6), show the superiority of our proposed model to a number of competitive SGG models in terms of continuous learning and conventional settings. Moreover, comprehensive ablation experiments demonstrate the effectiveness of each component in our model.|\u573a\u666f\u56fe\u751f\u6210\uff08SGG\uff09\u81f4\u529b\u4e8e\u9884\u6d4b\u56fe\u50cf\u4e2d\u5bf9\u8c61\u5bf9\u4e4b\u95f4\u7684\u89c6\u89c9\u5173\u7cfb\u3002\u4f20\u7edf\u4e0a\uff0c\u6d41\u884c\u7684 SGG \u65b9\u6cd5\u5047\u8bbe SGG \u662f\u4e00\u6b21\u6027\u5b66\u4e60\u8fc7\u7a0b\u3002\u6bcf\u5f53\u51fa\u73b0\u65b0\u5173\u7cfb\u65f6\uff0c\u8fd9\u79cd\u4f20\u7edf\u8303\u5f0f\u53ef\u80fd\u9700\u8981\u5bf9\u6240\u6709\u5148\u524d\u89c2\u5bdf\u5230\u7684\u6837\u672c\u8fdb\u884c\u91cd\u590d\u8bad\u7ec3\uff0c\u4ece\u800c\u51cf\u8f7b\u5fd8\u8bb0\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u7684\u98ce\u9669\u3002\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u89e3\u51b3\u4e00\u7cfb\u5217\u5148\u9a8c\u5173\u7cfb\u9884\u6d4b\u4e2d\u56fa\u6709\u7684\u9677\u9631\u3002\u53d7\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0a\u4e0b\u6587\u5b66\u4e60\u53d6\u5f97\u7684\u6210\u5c31\u7684\u6fc0\u52b1\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u6a21\u578b\u5177\u6709\u9884\u6d4b\u5173\u7cfb\u5e76\u4e0d\u65ad\u83b7\u53d6\u65b0\u77e5\u8bc6\u800c\u4e0d\u4f1a\u5c48\u670d\u4e8e\u707e\u96be\u6027\u9057\u5fd8\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u5b9e\u7528\u7684\u573a\u666f\u56fe\u751f\u6210\u6846\u67b6\uff0c\u5373\u7ec8\u8eab\u573a\u666f\u56fe\u751f\u6210\uff08LSGG\uff09\uff0c\u5176\u4e2d\u8c13\u8bcd\u7b49\u4efb\u52a1\u4ee5\u6d41\u5f0f\u65b9\u5f0f\u5c55\u5f00\u3002\u5728\u6b64\u6846\u67b6\u4e2d\uff0c\u6a21\u578b\u4ec5\u9650\u4e8e\u5bf9\u5f53\u524d\u4efb\u52a1\u8fdb\u884c\u6392\u4ed6\u6027\u8bad\u7ec3\uff0c\u9664\u4e86\u6709\u9650\u6570\u91cf\u7684\u6837\u672c\u5916\uff0c\u65e0\u6cd5\u8bbf\u95ee\u4ee5\u524d\u9047\u5230\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u6a21\u578b\u7684\u4efb\u52a1\u662f\u63a8\u65ad\u8fc4\u4eca\u4e3a\u6b62\u9047\u5230\u7684\u6240\u6709\u8c13\u8bcd\u3002\u4e25\u683c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 LSGG \u7684\u80cc\u666f\u4e0b\u5728\u5404\u79cd\u6307\u6807\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 SGG \u6a21\u578b\u3002\u6b64\u5916\uff0c\u5bf9\u4e24\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6570\u636e\u96c6 VG \u548c Open-Image(v6) \u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u548c\u5e38\u89c4\u8bbe\u7f6e\u65b9\u9762\u4f18\u4e8e\u8bb8\u591a\u7ade\u4e89\u6027 SGG \u6a21\u578b\u3002\u6b64\u5916\uff0c\u5168\u9762\u7684\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u6a21\u578b\u4e2d\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002|[2401.14626v1](http://arxiv.org/pdf/2401.14626v1)|null|\n", "2401.14587": "|**2024-01-26**|**CNA-TTA: Clean and Noisy Region Aware Feature Learning within Clusters for Online-Offline Test-Time Adaptation**|CNA-TTA\uff1a\u96c6\u7fa4\u5185\u7684\u5e72\u51c0\u548c\u5608\u6742\u533a\u57df\u611f\u77e5\u7279\u5f81\u5b66\u4e60\uff0c\u7528\u4e8e\u5728\u7ebf\u79bb\u7ebf\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94|Hyeonwoo Cho, Chanmin Park, Jinyoung Kim, Won Hwa Kim|A domain shift occurs when training (source) and test (target) data diverge in their distribution. Test-time adaptation (TTA) addresses the domain shift problem, aiming to adopt a trained model on the source domain to the target domain in a scenario where only a well-trained source model and unlabeled target data are available. In this scenario, handling false labels in the target domain is crucial because they negatively impact the model performance. To deal with this problem, we propose to utilize cluster structure (i.e., {`Clean'} and {`Noisy'} regions within each cluster) in the target domain formulated by the source model. Given an initial clustering of target samples, we first partition clusters into {`Clean'} and {`Noisy'} regions defined based on cluster prototype (i.e., centroid of each cluster). As these regions have totally different distributions of the true pseudo-labels, we adopt distinct training strategies for the clean and noisy regions: we selectively train the target with clean pseudo-labels in the clean region, whereas we introduce mixup inputs representing intermediate features between clean and noisy regions to increase the compactness of the cluster. We conducted extensive experiments on multiple datasets in online/offline TTA settings, whose results demonstrate that our method, {CNA-TTA}, achieves state-of-the-art for most cases.|\u5f53\u8bad\u7ec3\uff08\u6e90\uff09\u548c\u6d4b\u8bd5\uff08\u76ee\u6807\uff09\u6570\u636e\u7684\u5206\u5e03\u51fa\u73b0\u5206\u6b67\u65f6\uff0c\u5c31\u4f1a\u53d1\u751f\u57df\u8f6c\u79fb\u3002\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u89e3\u51b3\u4e86\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u65e8\u5728\u5728\u53ea\u6709\u7ecf\u8fc7\u826f\u597d\u8bad\u7ec3\u7684\u6e90\u6a21\u578b\u548c\u672a\u6807\u8bb0\u7684\u76ee\u6807\u6570\u636e\u53ef\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u6e90\u57df\u4e0a\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u91c7\u7528\u5230\u76ee\u6807\u57df\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5904\u7406\u76ee\u6807\u57df\u4e2d\u7684\u9519\u8bef\u6807\u7b7e\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f1a\u5bf9\u6a21\u578b\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u5728\u6e90\u6a21\u578b\u5236\u5b9a\u7684\u76ee\u6807\u57df\u4e2d\u5229\u7528\u96c6\u7fa4\u7ed3\u6784\uff08\u5373\u6bcf\u4e2a\u96c6\u7fa4\u5185\u7684{`Clean'}\u548c{`Noisy'}\u533a\u57df\uff09\u3002\u7ed9\u5b9a\u76ee\u6807\u6837\u672c\u7684\u521d\u59cb\u805a\u7c7b\uff0c\u6211\u4eec\u9996\u5148\u5c06\u805a\u7c7b\u5212\u5206\u4e3a\u57fa\u4e8e\u805a\u7c7b\u539f\u578b\uff08\u5373\u6bcf\u4e2a\u805a\u7c7b\u7684\u8d28\u5fc3\uff09\u5b9a\u4e49\u7684\u201c\u5e72\u51c0\u201d\u548c\u201c\u5608\u6742\u201d\u533a\u57df\u3002\u7531\u4e8e\u8fd9\u4e9b\u533a\u57df\u5177\u6709\u5b8c\u5168\u4e0d\u540c\u7684\u771f\u5b9e\u4f2a\u6807\u7b7e\u5206\u5e03\uff0c\u56e0\u6b64\u6211\u4eec\u5bf9\u5e72\u51c0\u533a\u57df\u548c\u566a\u58f0\u533a\u57df\u91c7\u7528\u4e0d\u540c\u7684\u8bad\u7ec3\u7b56\u7565\uff1a\u6211\u4eec\u5728\u5e72\u51c0\u533a\u57df\u4e2d\u9009\u62e9\u6027\u5730\u4f7f\u7528\u5e72\u51c0\u4f2a\u6807\u7b7e\u8bad\u7ec3\u76ee\u6807\uff0c\u800c\u6211\u4eec\u5f15\u5165\u4ee3\u8868\u4e2d\u95f4\u7279\u5f81\u7684\u6df7\u5408\u8f93\u5165\u5e72\u51c0\u548c\u5608\u6742\u7684\u533a\u57df\uff0c\u4ee5\u589e\u52a0\u96c6\u7fa4\u7684\u7d27\u51d1\u6027\u3002\u6211\u4eec\u5728\u5728\u7ebf/\u79bb\u7ebf TTA \u8bbe\u7f6e\u4e2d\u5bf9\u591a\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5176\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5 {CNA-TTA} \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002|[2401.14587v1](http://arxiv.org/pdf/2401.14587v1)|null|\n"}}