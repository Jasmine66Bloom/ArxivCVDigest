{"\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2401.03638": "|**2024-01-08**|**Unifying Graph Contrastive Learning via Graph Message Augmentation**|\u901a\u8fc7\u56fe\u6d88\u606f\u589e\u5f3a\u7edf\u4e00\u56fe\u5bf9\u6bd4\u5b66\u4e60|Ziyan Zhang, Bo Jiang, Jin Tang, Bin Luo|Graph contrastive learning is usually performed by first conducting Graph Data Augmentation (GDA) and then employing a contrastive learning pipeline to train GNNs. As we know that GDA is an important issue for graph contrastive learning. Various GDAs have been developed recently which mainly involve dropping or perturbing edges, nodes, node attributes and edge attributes. However, to our knowledge, it still lacks a universal and effective augmentor that is suitable for different types of graph data. To address this issue, in this paper, we first introduce the graph message representation of graph data. Based on it, we then propose a novel Graph Message Augmentation (GMA), a universal scheme for reformulating many existing GDAs. The proposed unified GMA not only gives a new perspective to understand many existing GDAs but also provides a universal and more effective graph data augmentation for graph self-supervised learning tasks. Moreover, GMA introduces an easy way to implement the mixup augmentor which is natural for images but usually challengeable for graphs. Based on the proposed GMA, we then propose a unified graph contrastive learning, termed Graph Message Contrastive Learning (GMCL), that employs attribution-guided universal GMA for graph contrastive learning. Experiments on many graph learning tasks demonstrate the effectiveness and benefits of the proposed GMA and GMCL approaches.|\u56fe\u5bf9\u6bd4\u5b66\u4e60\u901a\u5e38\u9996\u5148\u8fdb\u884c\u56fe\u6570\u636e\u589e\u5f3a\uff08GDA\uff09\uff0c\u7136\u540e\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u7ba1\u9053\u6765\u8bad\u7ec3 GNN\u3002\u4f17\u6240\u5468\u77e5\uff0cGDA \u662f\u56fe\u5bf9\u6bd4\u5b66\u4e60\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002\u6700\u8fd1\u5f00\u53d1\u4e86\u5404\u79cdGDA\uff0c\u4e3b\u8981\u6d89\u53ca\u4e22\u5f03\u6216\u6270\u52a8\u8fb9\u3001\u8282\u70b9\u3001\u8282\u70b9\u5c5e\u6027\u548c\u8fb9\u5c5e\u6027\u3002\u7136\u800c\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u5b83\u4ecd\u7136\u7f3a\u4e4f\u9002\u5408\u4e0d\u540c\u7c7b\u578b\u56fe\u6570\u636e\u7684\u901a\u7528\u4e14\u6709\u6548\u7684\u589e\u5f3a\u5668\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u4ecb\u7ecd\u56fe\u6570\u636e\u7684\u56fe\u6d88\u606f\u8868\u793a\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u6d88\u606f\u589e\u5f3a\uff08GMA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u91cd\u65b0\u5236\u5b9a\u8bb8\u591a\u73b0\u6709 GDA \u7684\u901a\u7528\u65b9\u6848\u3002\u6240\u63d0\u51fa\u7684\u7edf\u4e00GMA\u4e0d\u4ec5\u4e3a\u7406\u89e3\u8bb8\u591a\u73b0\u6709\u7684GDA\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u800c\u4e14\u4e3a\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u66f4\u6709\u6548\u7684\u56fe\u6570\u636e\u589e\u5f3a\u3002\u6b64\u5916\uff0cGMA \u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u6df7\u5408\u589e\u5f3a\u5668\uff0c\u8fd9\u5bf9\u4e8e\u56fe\u50cf\u6765\u8bf4\u5f88\u81ea\u7136\uff0c\u4f46\u5bf9\u4e8e\u56fe\u5f62\u6765\u8bf4\u901a\u5e38\u5177\u6709\u6311\u6218\u6027\u3002\u57fa\u4e8e\u6240\u63d0\u51fa\u7684 GMA\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\uff0c\u79f0\u4e3a\u56fe\u6d88\u606f\u5bf9\u6bd4\u5b66\u4e60\uff08GMCL\uff09\uff0c\u5b83\u91c7\u7528\u5f52\u56e0\u5f15\u5bfc\u7684\u901a\u7528 GMA \u8fdb\u884c\u56fe\u5bf9\u6bd4\u5b66\u4e60\u3002\u8bb8\u591a\u56fe\u5b66\u4e60\u4efb\u52a1\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 GMA \u548c GMCL \u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u70b9\u3002|[2401.03638v1](http://arxiv.org/pdf/2401.03638v1)|null|\n", "2401.03615": "|**2024-01-08**|**Automated Detection of Myopic Maculopathy in MMAC 2023: Achievements in Classification, Segmentation, and Spherical Equivalent Prediction**|MMAC 2023 \u4e2d\u8fd1\u89c6\u9ec4\u6591\u75c5\u53d8\u7684\u81ea\u52a8\u68c0\u6d4b\uff1a\u5206\u7c7b\u3001\u5206\u5272\u548c\u7403\u9762\u7b49\u6548\u9884\u6d4b\u65b9\u9762\u7684\u6210\u5c31|Yihao Li, Philippe Zhang, Yubo Tan, Jing Zhang, Zhihan Wang, Weili Jiang, Pierre-Henri Conze, Mathieu Lamard, Gwenol\u00e9 Quellec, Mostafa El Habib Daho|Myopic macular degeneration is the most common complication of myopia and the primary cause of vision loss in individuals with pathological myopia. Early detection and prompt treatment are crucial in preventing vision impairment due to myopic maculopathy. This was the focus of the Myopic Maculopathy Analysis Challenge (MMAC), in which we participated. In task 1, classification of myopic maculopathy, we employed the contrastive learning framework, specifically SimCLR, to enhance classification accuracy by effectively capturing enriched features from unlabeled data. This approach not only improved the intrinsic understanding of the data but also elevated the performance of our classification model. For Task 2 (segmentation of myopic maculopathy plus lesions), we have developed independent segmentation models tailored for different lesion segmentation tasks and implemented a test-time augmentation strategy to further enhance the model's performance. As for Task 3 (prediction of spherical equivalent), we have designed a deep regression model based on the data distribution of the dataset and employed an integration strategy to enhance the model's prediction accuracy. The results we obtained are promising and have allowed us to position ourselves in the Top 6 of the classification task, the Top 2 of the segmentation task, and the Top 1 of the prediction task. The code is available at \\url{https://github.com/liyihao76/MMAC_LaTIM_Solution}.|\u8fd1\u89c6\u6027\u9ec4\u6591\u53d8\u6027\u662f\u8fd1\u89c6\u6700\u5e38\u89c1\u7684\u5e76\u53d1\u75c7\uff0c\u4e5f\u662f\u75c5\u7406\u6027\u8fd1\u89c6\u60a3\u8005\u89c6\u529b\u4e27\u5931\u7684\u4e3b\u8981\u539f\u56e0\u3002\u65e9\u671f\u53d1\u73b0\u548c\u53ca\u65f6\u6cbb\u7597\u5bf9\u4e8e\u9884\u9632\u8fd1\u89c6\u9ec4\u6591\u75c5\u5f15\u8d77\u7684\u89c6\u529b\u635f\u5bb3\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u662f\u6211\u4eec\u53c2\u52a0\u7684\u8fd1\u89c6\u9ec4\u6591\u75c5\u53d8\u5206\u6790\u6311\u6218\u8d5b (MMAC) \u7684\u7126\u70b9\u3002\u5728\u4efb\u52a1 1\uff08\u8fd1\u89c6\u6027\u9ec4\u6591\u75c5\u53d8\u7684\u5206\u7c7b\uff09\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08\u7279\u522b\u662f SimCLR\uff09\uff0c\u901a\u8fc7\u6709\u6548\u5730\u4ece\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u6355\u83b7\u4e30\u5bcc\u7684\u7279\u5f81\u6765\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5bf9\u6570\u636e\u7684\u5185\u5728\u7406\u89e3\uff0c\u800c\u4e14\u63d0\u9ad8\u4e86\u5206\u7c7b\u6a21\u578b\u7684\u6027\u80fd\u3002\u5bf9\u4e8e\u4efb\u52a12\uff08\u8fd1\u89c6\u9ec4\u6591\u75c5\u53d8\u52a0\u75c5\u53d8\u7684\u5206\u5272\uff09\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u9488\u5bf9\u4e0d\u540c\u75c5\u53d8\u5206\u5272\u4efb\u52a1\u7684\u72ec\u7acb\u5206\u5272\u6a21\u578b\uff0c\u5e76\u5b9e\u65bd\u4e86\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\u7b56\u7565\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u6a21\u578b\u7684\u6027\u80fd\u3002\u5bf9\u4e8e\u4efb\u52a13\uff08\u7403\u5f53\u91cf\u7684\u9884\u6d4b\uff09\uff0c\u6211\u4eec\u6839\u636e\u6570\u636e\u96c6\u7684\u6570\u636e\u5206\u5e03\u8bbe\u8ba1\u4e86\u6df1\u5ea6\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u96c6\u6210\u7b56\u7565\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002\u6211\u4eec\u83b7\u5f97\u7684\u7ed3\u679c\u4ee4\u4eba\u9f13\u821e\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u8dfb\u8eab\u5206\u7c7b\u4efb\u52a1\u524d 6 \u540d\u3001\u5206\u5272\u4efb\u52a1\u524d 2 \u540d\u548c\u9884\u6d4b\u4efb\u52a1\u524d 1 \u540d\u3002\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/liyihao76/MMAC_LaTIM_Solution} \u83b7\u53d6\u3002|[2401.03615v1](http://arxiv.org/pdf/2401.03615v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.04105": "|**2024-01-08**|**Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning**|Dr$^2$Net\uff1a\u7528\u4e8e\u5185\u5b58\u9ad8\u6548\u5fae\u8c03\u7684\u52a8\u6001\u53ef\u9006\u53cc\u6b8b\u5dee\u7f51\u7edc|Chen Zhao, Shuming Liu, Karttikeya Mangalam, Guocheng Qian, Fatimah Zohra, Abdulmohsen Alghannam, Jitendra Malik, Bernard Ghanem|Large pretrained models are increasingly crucial in modern computer vision tasks. These models are typically used in downstream tasks by end-to-end finetuning, which is highly memory-intensive for tasks with high-resolution data, e.g., video understanding, small object detection, and point cloud analysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks, or Dr$^2$Net, a novel family of network architectures that acts as a surrogate network to finetune a pretrained model with substantially reduced memory consumption. Dr$^2$Net contains two types of residual connections, one maintaining the residual structure in the pretrained models, and the other making the network reversible. Due to its reversibility, intermediate activations, which can be reconstructed from output, are cleared from memory during training. We use two coefficients on either type of residual connections respectively, and introduce a dynamic training strategy that seamlessly transitions the pretrained model to a reversible network with much higher numerical precision. We evaluate Dr$^2$Net on various pretrained models and various tasks, and show that it can reach comparable performance to conventional finetuning but with significantly less memory usage.|\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u73b0\u4ee3\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u901a\u8fc7\u7aef\u5230\u7aef\u5fae\u8c03\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff0c\u8fd9\u5bf9\u4e8e\u5177\u6709\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u7684\u4efb\u52a1\u6765\u8bf4\u662f\u9ad8\u5ea6\u5185\u5b58\u5bc6\u96c6\u578b\u7684\uff0c\u4f8b\u5982\u89c6\u9891\u7406\u89e3\u3001\u5c0f\u7269\u4f53\u68c0\u6d4b\u548c\u70b9\u4e91\u5206\u6790\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u52a8\u6001\u53ef\u9006\u53cc\u6b8b\u5dee\u7f51\u7edc\uff08Dynamic Reversible Dual-Residual Networks\uff09\uff0c\u6216 Dr$^2$Net\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u7edc\u67b6\u6784\u5bb6\u65cf\uff0c\u5b83\u5145\u5f53\u4ee3\u7406\u7f51\u7edc\u6765\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u540c\u65f6\u663e\u7740\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002 Dr$^2$Net \u5305\u542b\u4e24\u79cd\u7c7b\u578b\u7684\u6b8b\u5dee\u8fde\u63a5\uff0c\u4e00\u79cd\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u6b8b\u5dee\u7ed3\u6784\uff0c\u53e6\u4e00\u79cd\u4f7f\u7f51\u7edc\u53ef\u9006\u3002\u7531\u4e8e\u5176\u53ef\u9006\u6027\uff0c\u53ef\u4ee5\u4ece\u8f93\u51fa\u91cd\u5efa\u7684\u4e2d\u95f4\u6fc0\u6d3b\u5728\u8bad\u7ec3\u671f\u95f4\u4ece\u5185\u5b58\u4e2d\u6e05\u9664\u3002\u6211\u4eec\u5206\u522b\u5728\u4efb\u4e00\u7c7b\u578b\u7684\u6b8b\u5dee\u8fde\u63a5\u4e0a\u4f7f\u7528\u4e24\u4e2a\u7cfb\u6570\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u65e0\u7f1d\u8fc7\u6e21\u5230\u5177\u6709\u66f4\u9ad8\u6570\u503c\u7cbe\u5ea6\u7684\u53ef\u9006\u7f51\u7edc\u3002\u6211\u4eec\u5728\u5404\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5404\u79cd\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86 Dr$^2$Net\uff0c\u5e76\u8868\u660e\u5b83\u53ef\u4ee5\u8fbe\u5230\u4e0e\u4f20\u7edf\u5fae\u8c03\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u91cf\u663e\u7740\u51cf\u5c11\u3002|[2401.04105v1](http://arxiv.org/pdf/2401.04105v1)|null|\n", "2401.04023": "|**2024-01-08**|**Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video Classification**|\u7528\u4e8e\u97f3\u89c6\u9891\u5206\u7c7b\u7684\u9ad8\u6548\u591a\u5c3a\u5ea6\u591a\u6a21\u6001\u74f6\u9888\u53d8\u538b\u5668|Wentao Zhu|In recent years, researchers combine both audio and video signals to deal with challenges where actions are not well represented or captured by visual cues. However, how to effectively leverage the two modalities is still under development. In this work, we develop a multiscale multimodal Transformer (MMT) that leverages hierarchical representation learning. Particularly, MMT is composed of a novel multiscale audio Transformer (MAT) and a multiscale video Transformer [43]. To learn a discriminative cross-modality fusion, we further design multimodal supervised contrastive objectives called audio-video contrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly align the two modalities. MMT surpasses previous state-of-the-art approaches by 7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy without external training data. Moreover, the proposed MAT significantly outperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark datasets, and is about 3% more efficient based on the number of FLOPs and 9.8% more efficient based on GPU memory usage.|\u8fd1\u5e74\u6765\uff0c\u7814\u7a76\u4eba\u5458\u5c06\u97f3\u9891\u548c\u89c6\u9891\u4fe1\u53f7\u7ed3\u5408\u8d77\u6765\uff0c\u4ee5\u5e94\u5bf9\u89c6\u89c9\u7ebf\u7d22\u65e0\u6cd5\u5f88\u597d\u5730\u8868\u793a\u6216\u6355\u83b7\u52a8\u4f5c\u7684\u6311\u6218\u3002\u7136\u800c\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u8fd9\u4e24\u79cd\u6a21\u5f0f\u4ecd\u5728\u7814\u7a76\u4e2d\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528\u5206\u5c42\u8868\u793a\u5b66\u4e60\u7684\u591a\u5c3a\u5ea6\u591a\u6a21\u6001 Transformer (MMT)\u3002\u7279\u522b\u5730\uff0cMMT\u7531\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u97f3\u9891\u53d8\u538b\u5668\uff08MAT\uff09\u548c\u591a\u5c3a\u5ea6\u89c6\u9891\u53d8\u538b\u5668\u7ec4\u6210[43]\u3002\u4e3a\u4e86\u5b66\u4e60\u6709\u533a\u522b\u7684\u8de8\u6a21\u6001\u878d\u5408\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\u591a\u6a21\u6001\u76d1\u7763\u5bf9\u6bd4\u76ee\u6807\uff0c\u79f0\u4e3a\u97f3\u9891\u89c6\u9891\u5bf9\u6bd4\u635f\u5931\uff08AVC\uff09\u548c\u6a21\u5185\u5bf9\u6bd4\u635f\u5931\uff08IMC\uff09\uff0c\u5b83\u4eec\u53ef\u4ee5\u7a33\u5065\u5730\u5bf9\u9f50\u4e24\u79cd\u6a21\u6001\u3002\u5728\u6ca1\u6709\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0cMMT \u5728 Kinetics-Sounds \u548c VGGSound \u4e0a\u7684 top-1 \u51c6\u786e\u5ea6\u65b9\u9762\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5206\u522b\u63d0\u9ad8\u4e86 7.3% \u548c 2.1%\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684 MAT \u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u7740\u4f18\u4e8e AST [28] 22.2%\u30014.4% \u548c 4.7%\uff0c\u5e76\u4e14\u57fa\u4e8e FLOP \u6570\u91cf\u7684\u6548\u7387\u63d0\u9ad8\u4e86\u7ea6 3%\uff0c\u57fa\u4e8e GPU \u5185\u5b58\u4f7f\u7528\u7684\u6548\u7387\u63d0\u9ad8\u4e86 9.8%\u3002|[2401.04023v1](http://arxiv.org/pdf/2401.04023v1)|null|\n", "2401.03989": "|**2024-01-08**|**MS-DETR: Efficient DETR Training with Mixed Supervision**|MS-DETR\uff1a\u6df7\u5408\u76d1\u7763\u4e0b\u7684\u9ad8\u6548 DETR \u8bad\u7ec3|Chuyang Zhao, Yifan Sun, Wenhao Wang, Qiang Chen, Errui Ding, Yi Yang, Jingdong Wang|DETR accomplishes end-to-end object detection through iteratively generating multiple object candidates based on image features and promoting one candidate for each ground-truth object. The traditional training procedure using one-to-one supervision in the original DETR lacks direct supervision for the object detection candidates.   We aim at improving the DETR training efficiency by explicitly supervising the candidate generation procedure through mixing one-to-one supervision and one-to-many supervision. Our approach, namely MS-DETR, is simple, and places one-to-many supervision to the object queries of the primary decoder that is used for inference. In comparison to existing DETR variants with one-to-many supervision, such as Group DETR and Hybrid DETR, our approach does not need additional decoder branches or object queries. The object queries of the primary decoder in our approach directly benefit from one-to-many supervision and thus are superior in object candidate prediction. Experimental results show that our approach outperforms related DETR variants, such as DN-DETR, Hybrid DETR, and Group DETR, and the combination with related DETR variants further improves the performance.|DETR\u901a\u8fc7\u57fa\u4e8e\u56fe\u50cf\u7279\u5f81\u8fed\u4ee3\u751f\u6210\u591a\u4e2a\u76ee\u6807\u5019\u9009\u8005\u5e76\u4e3a\u6bcf\u4e2a\u771f\u5b9e\u76ee\u6807\u63d0\u5347\u4e00\u4e2a\u5019\u9009\u8005\u6765\u5b8c\u6210\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b\u3002\u539f\u59cb DETR \u4e2d\u4f7f\u7528\u4e00\u5bf9\u4e00\u76d1\u7763\u7684\u4f20\u7edf\u8bad\u7ec3\u8fc7\u7a0b\u7f3a\u4e4f\u5bf9\u76ee\u6807\u68c0\u6d4b\u5019\u9009\u8005\u7684\u76f4\u63a5\u76d1\u7763\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u6df7\u5408\u4e00\u5bf9\u4e00\u76d1\u7763\u548c\u4e00\u5bf9\u591a\u76d1\u7763\u6765\u660e\u786e\u76d1\u7763\u5019\u9009\u751f\u6210\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u9ad8 DETR \u8bad\u7ec3\u6548\u7387\u3002\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5373 MS-DETR\uff0c\u5f88\u7b80\u5355\uff0c\u5e76\u5bf9\u7528\u4e8e\u63a8\u7406\u7684\u4e3b\u89e3\u7801\u5668\u7684\u5bf9\u8c61\u67e5\u8be2\u8fdb\u884c\u4e00\u5bf9\u591a\u7684\u76d1\u7763\u3002\u4e0e\u5177\u6709\u4e00\u5bf9\u591a\u76d1\u7763\u7684\u73b0\u6709 DETR \u53d8\u4f53\uff08\u4f8b\u5982 Group DETR \u548c Hybrid DETR\uff09\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u9700\u8981\u989d\u5916\u7684\u89e3\u7801\u5668\u5206\u652f\u6216\u5bf9\u8c61\u67e5\u8be2\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e2d\u4e3b\u89e3\u7801\u5668\u7684\u5bf9\u8c61\u67e5\u8be2\u76f4\u63a5\u53d7\u76ca\u4e8e\u4e00\u5bf9\u591a\u76d1\u7763\uff0c\u56e0\u6b64\u5728\u5bf9\u8c61\u5019\u9009\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u76f8\u5173\u7684 DETR \u53d8\u4f53\uff0c\u4f8b\u5982 DN-DETR\u3001Hybrid DETR \u548c Group DETR\uff0c\u5e76\u4e14\u4e0e\u76f8\u5173 DETR \u53d8\u4f53\u7684\u7ec4\u5408\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6027\u80fd\u3002|[2401.03989v1](http://arxiv.org/pdf/2401.03989v1)|null|\n", "2401.03939": "|**2024-01-08**|**Multi-scale attention-based instance segmentation for measuring crystals with large size variation**|\u57fa\u4e8e\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u7684\u5b9e\u4f8b\u5206\u5272\uff0c\u7528\u4e8e\u6d4b\u91cf\u5c3a\u5bf8\u53d8\u5316\u8f83\u5927\u7684\u6676\u4f53|Theresa Neubauer, Astrid Berg, Maria Wimmer, Dimitrios Lenis, David Major, Philip Matthias Winter, Gaia Romana De Paolis, Johannes Novotny, Daniel L\u00fcftner, Katja Reinharter, et.al.|Quantitative measurement of crystals in high-resolution images allows for important insights into underlying material characteristics. Deep learning has shown great progress in vision-based automatic crystal size measurement, but current instance segmentation methods reach their limits with images that have large variation in crystal size or hard to detect crystal boundaries. Even small image segmentation errors, such as incorrectly fused or separated segments, can significantly lower the accuracy of the measured results. Instead of improving the existing pixel-wise boundary segmentation methods, we propose to use an instance-based segmentation method, which gives more robust segmentation results to improve measurement accuracy. Our novel method enhances flow maps with a size-aware multi-scale attention module. The attention module adaptively fuses information from multiple scales and focuses on the most relevant scale for each segmented image area. We demonstrate that our proposed attention fusion strategy outperforms state-of-the-art instance and boundary segmentation methods, as well as simple average fusion of multi-scale predictions. We evaluate our method on a refractory raw material dataset of high-resolution images with large variation in crystal size and show that our model can be used to calculate the crystal size more accurately than existing methods.|\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5bf9\u6676\u4f53\u8fdb\u884c\u5b9a\u91cf\u6d4b\u91cf\u53ef\u4ee5\u6df1\u5165\u4e86\u89e3\u6f5c\u5728\u7684\u6750\u6599\u7279\u6027\u3002\u6df1\u5ea6\u5b66\u4e60\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u52a8\u6676\u4f53\u5c3a\u5bf8\u6d4b\u91cf\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\uff0c\u4f46\u5f53\u524d\u7684\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\u5728\u5904\u7406\u6676\u4f53\u5c3a\u5bf8\u53d8\u5316\u8f83\u5927\u6216\u96be\u4ee5\u68c0\u6d4b\u6676\u4f53\u8fb9\u754c\u7684\u56fe\u50cf\u65f6\u8fbe\u5230\u4e86\u6781\u9650\u3002\u5373\u4f7f\u5f88\u5c0f\u7684\u56fe\u50cf\u5206\u5272\u9519\u8bef\uff0c\u4f8b\u5982\u4e0d\u6b63\u786e\u5730\u878d\u5408\u6216\u5206\u79bb\u7684\u7247\u6bb5\uff0c\u4e5f\u4f1a\u663e\u7740\u964d\u4f4e\u6d4b\u91cf\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u57fa\u4e8e\u5b9e\u4f8b\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u6539\u8fdb\u73b0\u6709\u7684\u9010\u50cf\u7d20\u8fb9\u754c\u5206\u5272\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u5206\u5272\u7ed3\u679c\u4ee5\u63d0\u9ad8\u6d4b\u91cf\u7cbe\u5ea6\u3002\u6211\u4eec\u7684\u65b0\u9896\u65b9\u6cd5\u901a\u8fc7\u5c3a\u5bf8\u611f\u77e5\u7684\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u4e86\u6d41\u7a0b\u56fe\u3002\u6ce8\u610f\u529b\u6a21\u5757\u81ea\u9002\u5e94\u5730\u878d\u5408\u6765\u81ea\u591a\u4e2a\u5c3a\u5ea6\u7684\u4fe1\u606f\uff0c\u5e76\u5173\u6ce8\u6bcf\u4e2a\u5206\u5272\u56fe\u50cf\u533a\u57df\u6700\u76f8\u5173\u7684\u5c3a\u5ea6\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u878d\u5408\u7b56\u7565\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5b9e\u4f8b\u548c\u8fb9\u754c\u5206\u5272\u65b9\u6cd5\u4ee5\u53ca\u591a\u5c3a\u5ea6\u9884\u6d4b\u7684\u7b80\u5355\u5e73\u5747\u878d\u5408\u3002\u6211\u4eec\u5728\u6676\u4f53\u5c3a\u5bf8\u53d8\u5316\u8f83\u5927\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u8010\u706b\u539f\u6750\u6599\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u8868\u660e\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u5730\u8ba1\u7b97\u6676\u4f53\u5c3a\u5bf8\u3002|[2401.03939v1](http://arxiv.org/pdf/2401.03939v1)|null|\n", "2401.03907": "|**2024-01-08**|**RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM**|RoboFusion\uff1a\u901a\u8fc7 SAM \u5b9e\u73b0\u7a33\u5065\u7684\u591a\u6a21\u6001 3D \u7269\u4f53\u68c0\u6d4b|Ziying Song, Guoxing Zhang, Lin Liu, Lei Yang, Shaoqing Xu, Caiyan Jia, Feiyang Jia, Li Wang|Multi-modal 3D object detectors are dedicated to exploring secure and reliable perception systems for autonomous driving (AD). However, while achieving state-of-the-art (SOTA) performance on clean benchmark datasets, they tend to overlook the complexity and harsh conditions of real-world environments. Meanwhile, with the emergence of visual foundation models (VFMs), opportunities and challenges are presented for improving the robustness and generalization of multi-modal 3D object detection in autonomous driving. Therefore, we propose RoboFusion, a robust framework that leverages VFMs like SAM to tackle out-of-distribution (OOD) noise scenarios. We first adapt the original SAM for autonomous driving scenarios named SAM-AD. To align SAM or SAM-AD with multi-modal methods, we then introduce AD-FPN for upsampling the image features extracted by SAM. We employ wavelet decomposition to denoise the depth-guided images for further noise reduction and weather interference. Lastly, we employ self-attention mechanisms to adaptively reweight the fused features, enhancing informative features while suppressing excess noise. In summary, our RoboFusion gradually reduces noise by leveraging the generalization and robustness of VFMs, thereby enhancing the resilience of multi-modal 3D object detection. Consequently, our RoboFusion achieves state-of-the-art performance in noisy scenarios, as demonstrated by the KITTI-C and nuScenes-C benchmarks.|\u591a\u6a21\u6001 3D \u7269\u4f53\u63a2\u6d4b\u5668\u81f4\u529b\u4e8e\u63a2\u7d22\u5b89\u5168\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76 (AD) \u611f\u77e5\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u867d\u7136\u5728\u5e72\u51c0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684 (SOTA) \u6027\u80fd\uff0c\u4f46\u4ed6\u4eec\u5f80\u5f80\u5ffd\u89c6\u4e86\u73b0\u5b9e\u73af\u5883\u7684\u590d\u6742\u6027\u548c\u6076\u52a3\u6761\u4ef6\u3002\u540c\u65f6\uff0c\u968f\u7740\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u7684\u51fa\u73b0\uff0c\u4e3a\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u6a21\u6001 3D \u7269\u4f53\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u5e26\u6765\u4e86\u673a\u9047\u548c\u6311\u6218\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa RoboFusion\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u5229\u7528 SAM \u7b49 VFM \u6765\u89e3\u51b3\u5206\u5e03\u5916 (OOD) \u566a\u58f0\u573a\u666f\u3002\u6211\u4eec\u9996\u5148\u5c06\u539f\u59cb SAM \u6539\u7f16\u4e3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\uff0c\u547d\u540d\u4e3a SAM-AD\u3002\u4e3a\u4e86\u5c06 SAM \u6216 SAM-AD \u4e0e\u591a\u6a21\u6001\u65b9\u6cd5\u7ed3\u5408\u8d77\u6765\uff0c\u6211\u4eec\u5f15\u5165\u4e86 AD-FPN \u5bf9 SAM \u63d0\u53d6\u7684\u56fe\u50cf\u7279\u5f81\u8fdb\u884c\u4e0a\u91c7\u6837\u3002\u6211\u4eec\u91c7\u7528\u5c0f\u6ce2\u5206\u89e3\u5bf9\u6df1\u5ea6\u5f15\u5bfc\u56fe\u50cf\u8fdb\u884c\u53bb\u566a\uff0c\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u566a\u58f0\u548c\u5929\u6c14\u5e72\u6270\u3002\u6700\u540e\uff0c\u6211\u4eec\u91c7\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u52a0\u6743\u878d\u5408\u7279\u5f81\uff0c\u589e\u5f3a\u4fe1\u606f\u7279\u5f81\uff0c\u540c\u65f6\u6291\u5236\u591a\u4f59\u7684\u566a\u58f0\u3002\u603b\u4e4b\uff0c\u6211\u4eec\u7684 RoboFusion \u901a\u8fc7\u5229\u7528 VFM \u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u9010\u6e10\u964d\u4f4e\u566a\u58f0\uff0c\u4ece\u800c\u589e\u5f3a\u591a\u6a21\u6001 3D \u5bf9\u8c61\u68c0\u6d4b\u7684\u5f39\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684 RoboFusion \u5728\u5608\u6742\u7684\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6b63\u5982 KITTI-C \u548c nuScenes-C \u57fa\u51c6\u6d4b\u8bd5\u6240\u8bc1\u660e\u7684\u90a3\u6837\u3002|[2401.03907v1](http://arxiv.org/pdf/2401.03907v1)|null|\n", "2401.03872": "|**2024-01-08**|**A New Dataset and a Distractor-Aware Architecture for Transparent Object Tracking**|\u7528\u4e8e\u900f\u660e\u5bf9\u8c61\u8ddf\u8e2a\u7684\u65b0\u6570\u636e\u96c6\u548c\u5e72\u6270\u611f\u77e5\u67b6\u6784|Alan Lukezic, Ziga Trojer, Jiri Matas, Matej Kristan|Performance of modern trackers degrades substantially on transparent objects compared to opaque objects. This is largely due to two distinct reasons. Transparent objects are unique in that their appearance is directly affected by the background. Furthermore, transparent object scenes often contain many visually similar objects (distractors), which often lead to tracking failure. However, development of modern tracking architectures requires large training sets, which do not exist in transparent object tracking. We present two contributions addressing the aforementioned issues. We propose the first transparent object tracking training dataset Trans2k that consists of over 2k sequences with 104,343 images overall, annotated by bounding boxes and segmentation masks. Standard trackers trained on this dataset consistently improve by up to 16%. Our second contribution is a new distractor-aware transparent object tracker (DiTra) that treats localization accuracy and target identification as separate tasks and implements them by a novel architecture. DiTra sets a new state-of-the-art in transparent object tracking and generalizes well to opaque objects.|\u4e0e\u4e0d\u900f\u660e\u7269\u4f53\u76f8\u6bd4\uff0c\u73b0\u4ee3\u8ddf\u8e2a\u5668\u7684\u6027\u80fd\u5728\u900f\u660e\u7269\u4f53\u4e0a\u663e\u7740\u4e0b\u964d\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u4e24\u4e2a\u4e0d\u540c\u7684\u539f\u56e0\u3002\u900f\u660e\u5bf9\u8c61\u7684\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u5b83\u4eec\u7684\u5916\u89c2\u76f4\u63a5\u53d7\u80cc\u666f\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u900f\u660e\u7269\u4f53\u573a\u666f\u901a\u5e38\u5305\u542b\u8bb8\u591a\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7684\u7269\u4f53\uff08\u5e72\u6270\u7269\uff09\uff0c\u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u8ddf\u8e2a\u5931\u8d25\u3002\u7136\u800c\uff0c\u73b0\u4ee3\u8ddf\u8e2a\u67b6\u6784\u7684\u5f00\u53d1\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u96c6\uff0c\u800c\u900f\u660e\u5bf9\u8c61\u8ddf\u8e2a\u4e2d\u4e0d\u5b58\u5728\u8fd9\u79cd\u60c5\u51b5\u3002\u6211\u4eec\u9488\u5bf9\u4e0a\u8ff0\u95ee\u9898\u63d0\u51fa\u4e86\u4e24\u9879\u200b\u200b\u8d21\u732e\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u900f\u660e\u5bf9\u8c61\u8ddf\u8e2a\u8bad\u7ec3\u6570\u636e\u96c6 Trans2k\uff0c\u5b83\u7531\u8d85\u8fc7 2k \u4e2a\u5e8f\u5217\u7ec4\u6210\uff0c\u603b\u5171\u6709 104,343 \u4e2a\u56fe\u50cf\uff0c\u7531\u8fb9\u754c\u6846\u548c\u5206\u5272\u63a9\u6a21\u6ce8\u91ca\u3002\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6807\u51c6\u8ddf\u8e2a\u5668\u6301\u7eed\u6539\u8fdb\u9ad8\u8fbe 16%\u3002\u6211\u4eec\u7684\u7b2c\u4e8c\u4e2a\u8d21\u732e\u662f\u4e00\u79cd\u65b0\u7684\u5e72\u6270\u611f\u77e5\u900f\u660e\u5bf9\u8c61\u8ddf\u8e2a\u5668\uff08DiTra\uff09\uff0c\u5b83\u5c06\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u76ee\u6807\u8bc6\u522b\u89c6\u4e3a\u5355\u72ec\u7684\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u67b6\u6784\u6765\u5b9e\u73b0\u5b83\u4eec\u3002 DiTra \u5728\u900f\u660e\u5bf9\u8c61\u8ddf\u8e2a\u65b9\u9762\u6811\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5e76\u4e14\u53ef\u4ee5\u5f88\u597d\u5730\u63a8\u5e7f\u5230\u4e0d\u900f\u660e\u5bf9\u8c61\u3002|[2401.03872v1](http://arxiv.org/pdf/2401.03872v1)|null|\n", "2401.03846": "|**2024-01-08**|**UFO: Unidentified Foreground Object Detection in 3D Point Cloud**|UFO\uff1a3D \u70b9\u4e91\u4e2d\u7684\u4e0d\u660e\u524d\u666f\u7269\u4f53\u68c0\u6d4b|Hyunjun Choi, Hawook Jeong, Jin Young Choi|In this paper, we raise a new issue on Unidentified Foreground Object (UFO) detection in 3D point clouds, which is a crucial technology in autonomous driving in the wild. UFO detection is challenging in that existing 3D object detectors encounter extremely hard challenges in both 3D localization and Out-of-Distribution (OOD) detection. To tackle these challenges, we suggest a new UFO detection framework including three tasks: evaluation protocol, methodology, and benchmark. The evaluation includes a new approach to measure the performance on our goal, i.e. both localization and OOD detection of UFOs. The methodology includes practical techniques to enhance the performance of our goal. The benchmark is composed of the KITTI Misc benchmark and our additional synthetic benchmark for modeling a more diverse range of UFOs. The proposed framework consistently enhances performance by a large margin across all four baseline detectors: SECOND, PointPillars, PV-RCNN, and PartA2, giving insight for future work on UFO detection in the wild.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 3D \u70b9\u4e91\u4e2d\u7684\u4e0d\u660e\u524d\u666f\u7269\u4f53 (UFO) \u68c0\u6d4b\u7684\u65b0\u95ee\u9898\uff0c\u8fd9\u662f\u91ce\u5916\u81ea\u52a8\u9a7e\u9a76\u7684\u4e00\u9879\u5173\u952e\u6280\u672f\u3002 UFO \u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u73b0\u6709\u7684 3D \u7269\u4f53\u68c0\u6d4b\u5668\u5728 3D \u5b9a\u4f4d\u548c\u5206\u5e03\u5916 (OOD) \u68c0\u6d4b\u65b9\u9762\u90fd\u9047\u5230\u4e86\u6781\u5176\u4e25\u5cfb\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684 UFO \u68c0\u6d4b\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u4efb\u52a1\uff1a\u8bc4\u4f30\u534f\u8bae\u3001\u65b9\u6cd5\u8bba\u548c\u57fa\u51c6\u3002\u8be5\u8bc4\u4f30\u5305\u62ec\u4e00\u79cd\u8861\u91cf\u6211\u4eec\u76ee\u6807\u7ee9\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u5373 UFO \u7684\u5b9a\u4f4d\u548c OOD \u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u63d0\u9ad8\u6211\u4eec\u76ee\u6807\u7ee9\u6548\u7684\u5b9e\u7528\u6280\u672f\u3002\u8be5\u57fa\u51c6\u7531 KITTI Misc \u57fa\u51c6\u548c\u6211\u4eec\u7528\u4e8e\u5efa\u6a21\u66f4\u591a\u6837\u5316\u7684 UFO \u7684\u9644\u52a0\u7efc\u5408\u57fa\u51c6\u7ec4\u6210\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u6301\u7eed\u5927\u5e45\u63d0\u9ad8\u4e86\u6240\u6709\u56db\u4e2a\u57fa\u7ebf\u63a2\u6d4b\u5668\u7684\u6027\u80fd\uff1aSECOND\u3001PointPillars\u3001PV-RCNN \u548c PartA2\uff0c\u4e3a\u672a\u6765\u7684\u91ce\u5916 UFO \u63a2\u6d4b\u5de5\u4f5c\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002|[2401.03846v1](http://arxiv.org/pdf/2401.03846v1)|null|\n", "2401.03844": "|**2024-01-08**|**Fully Attentional Networks with Self-emerging Token Labeling**|\u5177\u6709\u81ea\u6211\u51fa\u73b0\u7684\u4ee4\u724c\u6807\u7b7e\u7684\u5b8c\u5168\u6ce8\u610f\u529b\u7f51\u7edc|Bingyin Zhao, Zhiding Yu, Shiyi Lan, Yutao Cheng, Anima Anandkumar, Yingjie Lao, Jose M. Alvarez|Recent studies indicate that Vision Transformers (ViTs) are robust against out-of-distribution scenarios. In particular, the Fully Attentional Network (FAN) - a family of ViT backbones, has achieved state-of-the-art robustness. In this paper, we revisit the FAN models and improve their pre-training with a self-emerging token labeling (STL) framework. Our method contains a two-stage training framework. Specifically, we first train a FAN token labeler (FAN-TL) to generate semantically meaningful patch token labels, followed by a FAN student model training stage that uses both the token labels and the original class label. With the proposed STL framework, our best model based on FAN-L-Hybrid (77.3M parameters) achieves 84.8% Top-1 accuracy and 42.1% mCE on ImageNet-1K and ImageNet-C, and sets a new state-of-the-art for ImageNet-A (46.1%) and ImageNet-R (56.6%) without using extra data, outperforming the original FAN counterpart by significant margins. The proposed framework also demonstrates significantly enhanced performance on downstream tasks such as semantic segmentation, with up to 1.7% improvement in robustness over the counterpart model. Code is available at https://github.com/NVlabs/STL.|\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u89c6\u89c9\u53d8\u538b\u5668 (ViT) \u5bf9\u4e8e\u5206\u5e03\u5916\u573a\u666f\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u7279\u522b\u662f\uff0c\u5168\u6ce8\u610f\u529b\u7f51\u7edc\uff08FAN\uff09\u2014\u2014\u4e00\u7cfb\u5217 ViT \u4e3b\u5e72\u7f51\u7edc\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6 FAN \u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u81ea\u6211\u751f\u6210\u7684\u4ee4\u724c\u6807\u8bb0\uff08STL\uff09\u6846\u67b6\u6539\u8fdb\u5176\u9884\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u542b\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u8bad\u7ec3\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u8bad\u7ec3 FAN \u4ee4\u724c\u6807\u8bb0\u5668 (FAN-TL) \u4ee5\u751f\u6210\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u8865\u4e01\u4ee4\u724c\u6807\u7b7e\uff0c\u7136\u540e\u662f\u4f7f\u7528\u4ee4\u724c\u6807\u7b7e\u548c\u539f\u59cb\u7c7b\u6807\u7b7e\u7684 FAN \u5b66\u751f\u6a21\u578b\u8bad\u7ec3\u9636\u6bb5\u3002\u5229\u7528\u6240\u63d0\u51fa\u7684 STL \u6846\u67b6\uff0c\u6211\u4eec\u57fa\u4e8e FAN-L-Hybrid\uff0877.3M \u53c2\u6570\uff09\u7684\u6700\u4f73\u6a21\u578b\u5728 ImageNet-1K \u548c ImageNet-C \u4e0a\u5b9e\u73b0\u4e86 84.8% Top-1 \u51c6\u786e\u7387\u548c 42.1% mCE\uff0c\u5e76\u521b\u4e0b\u4e86\u65b0\u7684\u6700\u9ad8\u6c34\u5e73-art \u7528\u4e8e ImageNet-A (46.1%) \u548c ImageNet-R (56.6%)\uff0c\u65e0\u9700\u4f7f\u7528\u989d\u5916\u6570\u636e\uff0c\u660e\u663e\u4f18\u4e8e\u539f\u59cb FAN \u5bf9\u5e94\u9879\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u8fd8\u5c55\u793a\u4e86\u8bed\u4e49\u5206\u5272\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u663e\u7740\u589e\u5f3a\uff0c\u4e0e\u5bf9\u5e94\u6a21\u578b\u76f8\u6bd4\uff0c\u9c81\u68d2\u6027\u63d0\u9ad8\u4e86 1.7%\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/NVlabs/STL \u83b7\u53d6\u3002|[2401.03844v1](http://arxiv.org/pdf/2401.03844v1)|null|\n", "2401.03836": "|**2024-01-08**|**WidthFormer: Toward Efficient Transformer-based BEV View Transformation**|WidthFormer\uff1a\u5b9e\u73b0\u57fa\u4e8e Transformer \u7684\u9ad8\u6548 BEV \u89c6\u56fe\u8f6c\u6362|Chenhongyi Yang, Tianwei Lin, Lichao Huang, Elliot J. Crowley|In this work, we present WidthFormer, a novel transformer-based Bird's-Eye-View (BEV) 3D detection method tailored for real-time autonomous-driving applications. WidthFormer is computationally efficient, robust and does not require any special engineering effort to deploy. In this work, we propose a novel 3D positional encoding mechanism capable of accurately encapsulating 3D geometric information, which enables our model to generate high-quality BEV representations with only a single transformer decoder layer. This mechanism is also beneficial for existing sparse 3D object detectors. Inspired by the recently-proposed works, we further improve our model's efficiency by vertically compressing the image features when serving as attention keys and values. We also introduce two modules to compensate for potential information loss due to feature compression. Experimental evaluation on the widely-used nuScenes 3D object detection benchmark demonstrates that our method outperforms previous approaches across different 3D detection architectures. More importantly, our model is highly efficient. For example, when using $256\\times 704$ input images, it achieves 1.5 ms latency on NVIDIA 3090 GPU. Furthermore, WidthFormer also exhibits strong robustness to different degrees of camera perturbations. Our study offers valuable insights into the deployment of BEV transformation methods in real-world, complex road environments. Code is available at https://github.com/ChenhongyiYang/WidthFormer .|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 WidthFormer\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u4e3a\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u91cf\u8eab\u5b9a\u5236\u7684\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u9e1f\u77b0 (BEV) 3D \u68c0\u6d4b\u65b9\u6cd5\u3002 WidthFormer \u8ba1\u7b97\u9ad8\u6548\u3001\u7a33\u5065\uff0c\u4e0d\u9700\u8981\u4efb\u4f55\u7279\u6b8a\u7684\u5de5\u7a0b\u5de5\u4f5c\u5373\u53ef\u90e8\u7f72\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 3D \u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u80fd\u591f\u51c6\u786e\u5c01\u88c5 3D \u51e0\u4f55\u4fe1\u606f\uff0c\u8fd9\u4f7f\u5f97\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u4ec5\u4f7f\u7528\u5355\u4e2a Transformer \u89e3\u7801\u5668\u5c42\u751f\u6210\u9ad8\u8d28\u91cf\u7684 BEV \u8868\u793a\u3002\u8fd9\u79cd\u673a\u5236\u5bf9\u4e8e\u73b0\u6709\u7684\u7a00\u758f 3D \u7269\u4f53\u68c0\u6d4b\u5668\u4e5f\u6709\u597d\u5904\u3002\u53d7\u6700\u8fd1\u63d0\u51fa\u7684\u5de5\u4f5c\u7684\u542f\u53d1\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u7528\u4f5c\u6ce8\u610f\u952e\u548c\u503c\u65f6\u5782\u76f4\u538b\u7f29\u56fe\u50cf\u7279\u5f81\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u7684\u6548\u7387\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e24\u4e2a\u6a21\u5757\u6765\u8865\u507f\u7531\u4e8e\u7279\u5f81\u538b\u7f29\u800c\u5bfc\u81f4\u7684\u6f5c\u5728\u4fe1\u606f\u4e22\u5931\u3002\u5bf9\u5e7f\u6cdb\u4f7f\u7528\u7684 nuScenes 3D \u5bf9\u8c61\u68c0\u6d4b\u57fa\u51c6\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684 3D \u68c0\u6d4b\u67b6\u6784\u4e2d\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u6a21\u578b\u975e\u5e38\u9ad8\u6548\u3002\u4f8b\u5982\uff0c\u5f53\u4f7f\u7528 256 \u7f8e\u5143\u00d7 704 \u7f8e\u5143\u7684\u8f93\u5165\u56fe\u50cf\u65f6\uff0c\u5b83\u5728 NVIDIA 3090 GPU \u4e0a\u5b9e\u73b0\u4e86 1.5 \u6beb\u79d2\u7684\u5ef6\u8fdf\u3002\u6b64\u5916\uff0cWidthFormer \u5bf9\u4e0d\u540c\u7a0b\u5ea6\u7684\u76f8\u673a\u6270\u52a8\u4e5f\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u4e3a\u5728\u73b0\u5b9e\u590d\u6742\u7684\u9053\u8def\u73af\u5883\u4e2d\u90e8\u7f72\u7eaf\u7535\u52a8\u6c7d\u8f66\u6539\u9020\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/ChenhongyiYang/WidthFormer \u83b7\u53d6\u3002|[2401.03836v1](http://arxiv.org/pdf/2401.03836v1)|null|\n", "2401.03828": "|**2024-01-08**|**A multimodal gesture recognition dataset for desktop human-computer interaction**|\u7528\u4e8e\u684c\u9762\u4eba\u673a\u4ea4\u4e92\u7684\u591a\u6a21\u6001\u624b\u52bf\u8bc6\u522b\u6570\u636e\u96c6|Qi Wang, Fengchao Zhu, Guangming Zhu, Liang Zhang, Ning Li, Eryang Gao|Gesture recognition is an indispensable component of natural and efficient human-computer interaction technology, particularly in desktop-level applications, where it can significantly enhance people's productivity. However, the current gesture recognition community lacks a suitable desktop-level (top-view perspective) dataset for lightweight gesture capture devices. In this study, we have established a dataset named GR4DHCI. What distinguishes this dataset is its inherent naturalness, intuitive characteristics, and diversity. Its primary purpose is to serve as a valuable resource for the development of desktop-level portable applications. GR4DHCI comprises over 7,000 gesture samples and a total of 382,447 frames for both Stereo IR and skeletal modalities. We also address the variances in hand positioning during desktop interactions by incorporating 27 different hand positions into the dataset. Building upon the GR4DHCI dataset, we conducted a series of experimental studies, the results of which demonstrate that the fine-grained classification blocks proposed in this paper can enhance the model's recognition accuracy. Our dataset and experimental findings presented in this paper are anticipated to propel advancements in desktop-level gesture recognition research.|\u624b\u52bf\u8bc6\u522b\u662f\u81ea\u7136\u9ad8\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u6280\u672f\u4e0d\u53ef\u6216\u7f3a\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u7279\u522b\u662f\u5728\u684c\u9762\u7ea7\u5e94\u7528\u4e2d\uff0c\u5b83\u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u4eba\u4eec\u7684\u751f\u4ea7\u529b\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u624b\u52bf\u8bc6\u522b\u793e\u533a\u7f3a\u4e4f\u9002\u5408\u8f7b\u91cf\u7ea7\u624b\u52bf\u6355\u83b7\u8bbe\u5907\u7684\u684c\u9762\u7ea7\uff08\u9876\u89c6\u56fe\uff09\u6570\u636e\u96c6\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u540d\u4e3a GR4DHCI \u7684\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u7684\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u5176\u56fa\u6709\u7684\u81ea\u7136\u6027\u3001\u76f4\u89c2\u7279\u5f81\u548c\u591a\u6837\u6027\u3002\u5176\u4e3b\u8981\u76ee\u7684\u662f\u4f5c\u4e3a\u5f00\u53d1\u684c\u9762\u7ea7\u4fbf\u643a\u5f0f\u5e94\u7528\u7a0b\u5e8f\u7684\u5b9d\u8d35\u8d44\u6e90\u3002 GR4DHCI \u5305\u542b 7,000 \u591a\u4e2a\u624b\u52bf\u6837\u672c\u4ee5\u53ca\u7acb\u4f53\u7ea2\u5916\u548c\u9aa8\u9abc\u6a21\u5f0f\u7684\u603b\u5171 382,\u200b\u200b447 \u5e27\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5c06 27 \u4e2a\u4e0d\u540c\u7684\u624b\u90e8\u4f4d\u7f6e\u5408\u5e76\u5230\u6570\u636e\u96c6\u4e2d\u6765\u89e3\u51b3\u684c\u9762\u4ea4\u4e92\u671f\u95f4\u624b\u90e8\u4f4d\u7f6e\u7684\u5dee\u5f02\u3002\u57fa\u4e8eGR4DHCI\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u5b9e\u9a8c\u7814\u7a76\uff0c\u7ed3\u679c\u8868\u660e\u672c\u6587\u63d0\u51fa\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\u5757\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u8bc6\u522b\u7cbe\u5ea6\u3002\u6211\u4eec\u5728\u672c\u6587\u4e2d\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u7ed3\u679c\u9884\u8ba1\u5c06\u63a8\u52a8\u684c\u9762\u7ea7\u624b\u52bf\u8bc6\u522b\u7814\u7a76\u7684\u8fdb\u6b65\u3002|[2401.03828v1](http://arxiv.org/pdf/2401.03828v1)|null|\n", "2401.03753": "|**2024-01-08**|**Color-$S^{4}L$: Self-supervised Semi-supervised Learning with Image Colorization**|Color-$S^{4}L$\uff1a\u5177\u6709\u56fe\u50cf\u7740\u8272\u7684\u81ea\u76d1\u7763\u534a\u76d1\u7763\u5b66\u4e60|Hanxiao Chen|This work addresses the problem of semi-supervised image classification tasks with the integration of several effective self-supervised pretext tasks. Different from widely-used consistency regularization within semi-supervised learning, we explored a novel self-supervised semi-supervised learning framework (Color-$S^{4}L$) especially with image colorization proxy task and deeply evaluate performances of various network architectures in such special pipeline. Also, we demonstrated its effectiveness and optimal performance on CIFAR-10, SVHN and CIFAR-100 datasets in comparison to previous supervised and semi-supervised optimal methods.|\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u96c6\u6210\u51e0\u4e2a\u6709\u6548\u7684\u81ea\u76d1\u7763\u501f\u53e3\u4efb\u52a1\u6765\u89e3\u51b3\u534a\u76d1\u7763\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u95ee\u9898\u3002\u4e0e\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u4e00\u81f4\u6027\u6b63\u5219\u5316\u4e0d\u540c\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff08Color-$S^{4}L$\uff09\uff0c\u7279\u522b\u662f\u56fe\u50cf\u7740\u8272\u4ee3\u7406\u4efb\u52a1\uff0c\u5e76\u6df1\u5165\u8bc4\u4f30\u5404\u79cd\u7f51\u7edc\u7684\u6027\u80fd\u8fd9\u79cd\u7279\u6b8a\u7ba1\u9053\u4e2d\u7684\u67b6\u6784\u3002\u6b64\u5916\uff0c\u4e0e\u4e4b\u524d\u7684\u76d1\u7763\u548c\u534a\u76d1\u7763\u4f18\u5316\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u5728 CIFAR-10\u3001SVHN \u548c CIFAR-100 \u6570\u636e\u96c6\u4e0a\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u6700\u4f73\u6027\u80fd\u3002|[2401.03753v1](http://arxiv.org/pdf/2401.03753v1)|null|\n", "2401.03749": "|**2024-01-08**|**Flying Bird Object Detection Algorithm in Surveillance Video**|\u76d1\u63a7\u89c6\u9891\u4e2d\u7684\u98de\u9e1f\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5|Ziwei Sun, Zexi Hua, Hengchao Li, Yan Li|Aiming at the characteristics of the flying bird object in surveillance video, such as the single frame image feature is not obvious, the size is small in most cases, and asymmetric, this paper proposes a Flying Bird Object Detection method for Surveillance Video (FBOD-SV). Firstly, a new feature aggregation module, the Correlation Attention Feature Aggregation (Co-Attention-FA) module, is designed to aggregate the features of the flying bird object according to the bird object's correlation on multiple consecutive frames of images. Secondly, a Flying Bird Object Detection Network (FBOD-Net) with down-sampling and then up-sampling is designed, which uses a large feature layer that fuses fine spatial information and large receptive field information to detect special multi-scale (mostly small-scale) bird objects. Finally, the SimOTA dynamic label allocation method is applied to One-Category object detection, and the SimOTA-OC dynamic label strategy is proposed to solve the difficult problem of label allocation caused by irregular flying bird objects. In this paper, the algorithm's performance is verified by the experimental data set of the surveillance video of the flying bird object of the traction substation. The experimental results show that the surveillance video flying bird object detection method proposed in this paper effectively improves the detection performance of flying bird objects.|\u9488\u5bf9\u76d1\u63a7\u89c6\u9891\u4e2d\u98de\u9e1f\u76ee\u6807\u5355\u5e27\u56fe\u50cf\u7279\u5f81\u4e0d\u660e\u663e\u3001\u591a\u6570\u60c5\u51b5\u4e0b\u5c3a\u5bf8\u8f83\u5c0f\u3001\u4e0d\u5bf9\u79f0\u7b49\u7279\u70b9\uff0c\u63d0\u51fa\u4e00\u79cd\u76d1\u63a7\u89c6\u9891\u98de\u9e1f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08FBOD- SV\uff09\u3002\u9996\u5148\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u7279\u5f81\u805a\u5408\u6a21\u5757\uff0c\u5373\u76f8\u5173\u6ce8\u610f\u7279\u5f81\u805a\u5408\uff08Co-Attention-FA\uff09\u6a21\u5757\uff0c\u6839\u636e\u9e1f\u5bf9\u8c61\u5728\u591a\u4e2a\u8fde\u7eed\u5e27\u56fe\u50cf\u4e0a\u7684\u76f8\u5173\u6027\u6765\u805a\u5408\u98de\u9e1f\u5bf9\u8c61\u7684\u7279\u5f81\u3002\u5176\u6b21\uff0c\u8bbe\u8ba1\u4e86\u5148\u4e0b\u91c7\u6837\u518d\u4e0a\u91c7\u6837\u7684\u98de\u9e1f\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff08FBOD-Net\uff09\uff0c\u8be5\u7f51\u7edc\u4f7f\u7528\u878d\u5408\u7cbe\u7ec6\u7a7a\u95f4\u4fe1\u606f\u548c\u5927\u611f\u53d7\u91ce\u4fe1\u606f\u7684\u5927\u7279\u5f81\u5c42\u6765\u68c0\u6d4b\u7279\u6b8a\u7684\u591a\u5c3a\u5ea6\uff08\u5927\u591a\u662f\u5c0f\u5c3a\u5ea6\uff09 -\u89c4\u6a21\uff09\u9e1f\u7c7b\u7269\u4f53\u3002\u6700\u540e\uff0c\u5c06SimOTA\u52a8\u6001\u6807\u7b7e\u5206\u914d\u65b9\u6cd5\u5e94\u7528\u4e8eOne-Category\u76ee\u6807\u68c0\u6d4b\uff0c\u63d0\u51faSimOTA-OC\u52a8\u6001\u6807\u7b7e\u7b56\u7565\uff0c\u89e3\u51b3\u4e0d\u89c4\u5219\u98de\u9e1f\u76ee\u6807\u5e26\u6765\u7684\u6807\u7b7e\u5206\u914d\u96be\u9898\u3002\u672c\u6587\u901a\u8fc7\u7275\u5f15\u53d8\u7535\u7ad9\u98de\u9e1f\u7269\u4f53\u76d1\u63a7\u89c6\u9891\u5b9e\u9a8c\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u76d1\u63a7\u89c6\u9891\u98de\u9e1f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u98de\u9e1f\u76ee\u6807\u7684\u68c0\u6d4b\u6027\u80fd\u3002|[2401.03749v1](http://arxiv.org/pdf/2401.03749v1)|null|\n", "2401.03742": "|**2024-01-08**|**Flowmind2Digital: The First Comprehensive Flowmind Recognition and Conversion Approach**|Flowmind2Digital\uff1a\u7b2c\u4e00\u4e2a\u5168\u9762\u7684 Flowmind \u8bc6\u522b\u548c\u8f6c\u6362\u65b9\u6cd5|Huanyu Liu, Jianfeng Cai, Tingjia Zhang, Hongsheng Li, Siyuan Wang, Guangming Zhu, Syed Afaq Ali Shah, Mohammed Bennamoun, Liang Zhang|Flowcharts and mind maps, collectively known as flowmind, are vital in daily activities, with hand-drawn versions facilitating real-time collaboration. However, there's a growing need to digitize them for efficient processing. Automated conversion methods are essential to overcome manual conversion challenges. Existing sketch recognition methods face limitations in practical situations, being field-specific and lacking digital conversion steps. Our paper introduces the Flowmind2digital method and hdFlowmind dataset to address these challenges. Flowmind2digital, utilizing neural networks and keypoint detection, achieves a record 87.3% accuracy on our dataset, surpassing previous methods by 11.9%. The hdFlowmind dataset, comprising 1,776 annotated flowminds across 22 scenarios, outperforms existing datasets. Additionally, our experiments emphasize the importance of simple graphics, enhancing accuracy by 9.3%.|\u6d41\u7a0b\u56fe\u548c\u601d\u7ef4\u5bfc\u56fe\uff08\u7edf\u79f0\u4e3a\u201cflowmind\u201d\uff09\u5728\u65e5\u5e38\u6d3b\u52a8\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u624b\u7ed8\u7248\u672c\u6709\u52a9\u4e8e\u5b9e\u65f6\u534f\u4f5c\u3002\u7136\u800c\uff0c\u8d8a\u6765\u8d8a\u9700\u8981\u5c06\u5b83\u4eec\u6570\u5b57\u5316\u4ee5\u8fdb\u884c\u9ad8\u6548\u5904\u7406\u3002\u81ea\u52a8\u8f6c\u6362\u65b9\u6cd5\u5bf9\u4e8e\u514b\u670d\u624b\u52a8\u8f6c\u6362\u6311\u6218\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u8349\u56fe\u8bc6\u522b\u65b9\u6cd5\u5728\u5b9e\u9645\u60c5\u51b5\u4e2d\u9762\u4e34\u7740\u5c40\u9650\u6027\uff0c\u5373\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u4e14\u7f3a\u4e4f\u6570\u5b57\u8f6c\u6362\u6b65\u9aa4\u3002\u6211\u4eec\u7684\u8bba\u6587\u4ecb\u7ecd\u4e86 Flowmind2digital \u65b9\u6cd5\u548c hdFlowmind \u6570\u636e\u96c6\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002 Flowmind2digital \u5229\u7528\u795e\u7ecf\u7f51\u7edc\u548c\u5173\u952e\u70b9\u68c0\u6d4b\uff0c\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u521b\u7eaa\u5f55\u7684 87.3% \u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u9ad8\u51fa 11.9%\u3002 hdFlowmind \u6570\u636e\u96c6\u5305\u542b 22 \u4e2a\u573a\u666f\u4e2d\u7684 1,776 \u4e2a\u5e26\u6ce8\u91ca\u7684 flowmind\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u7b80\u5355\u56fe\u5f62\u7684\u91cd\u8981\u6027\uff0c\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 9.3%\u3002|[2401.03742v1](http://arxiv.org/pdf/2401.03742v1)|null|\n", "2401.03695": "|**2024-01-08**|**A Large-scale Empirical Study on Improving the Fairness of Deep Learning Models**|\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u516c\u5e73\u6027\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76|Junjie Yang, Jiajun Jiang, Zeyu Sun, Junjie Chen|Fairness has been a critical issue that affects the adoption of deep learning models in real practice. To improve model fairness, many existing methods have been proposed and evaluated to be effective in their own contexts. However, there is still no systematic evaluation among them for a comprehensive comparison under the same context, which makes it hard to understand the performance distinction among them, hindering the research progress and practical adoption of them. To fill this gap, this paper endeavours to conduct the first large-scale empirical study to comprehensively compare the performance of existing state-of-the-art fairness improving techniques. Specifically, we target the widely-used application scenario of image classification, and utilized three different datasets and five commonly-used performance metrics to assess in total 13 methods from diverse categories. Our findings reveal substantial variations in the performance of each method across different datasets and sensitive attributes, indicating over-fitting on specific datasets by many existing methods. Furthermore, different fairness evaluation metrics, due to their distinct focuses, yield significantly different assessment results. Overall, we observe that pre-processing methods and in-processing methods outperform post-processing methods, with pre-processing methods exhibiting the best performance. Our empirical study offers comprehensive recommendations for enhancing fairness in deep learning models. We approach the problem from multiple dimensions, aiming to provide a uniform evaluation platform and inspire researchers to explore more effective fairness solutions via a set of implications.|\u516c\u5e73\u6027\u4e00\u76f4\u662f\u5f71\u54cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b9e\u9645\u5b9e\u8df5\u4e2d\u91c7\u7528\u7684\u5173\u952e\u95ee\u9898\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u7684\u516c\u5e73\u6027\uff0c\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u88ab\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e3a\u5728\u5404\u81ea\u7684\u73af\u5883\u4e2d\u6709\u6548\u3002\u7136\u800c\uff0c\u76ee\u524d\u8fd8\u6ca1\u6709\u5bf9\u5b83\u4eec\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4ef7\uff0c\u5728\u76f8\u540c\u80cc\u666f\u4e0b\u8fdb\u884c\u7efc\u5408\u6bd4\u8f83\uff0c\u8fd9\u4f7f\u5f97\u4eba\u4eec\u5f88\u96be\u7406\u89e3\u5b83\u4eec\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u963b\u788d\u4e86\u5b83\u4eec\u7684\u7814\u7a76\u8fdb\u5c55\u548c\u5b9e\u9645\u5e94\u7528\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u81f4\u529b\u4e8e\u8fdb\u884c\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u4ee5\u5168\u9762\u6bd4\u8f83\u73b0\u6709\u6700\u5148\u8fdb\u7684\u516c\u5e73\u6027\u6539\u8fdb\u6280\u672f\u7684\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9488\u5bf9\u5e7f\u6cdb\u4f7f\u7528\u7684\u56fe\u50cf\u5206\u7c7b\u5e94\u7528\u573a\u666f\uff0c\u5229\u7528\u4e09\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u5e38\u7528\u7684\u6027\u80fd\u6307\u6807\u6765\u8bc4\u4f30\u4e0d\u540c\u7c7b\u522b\u7684\u603b\u5171 13 \u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u6bcf\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u654f\u611f\u5c5e\u6027\u4e0a\u7684\u6027\u80fd\u5b58\u5728\u5de8\u5927\u5dee\u5f02\uff0c\u8868\u660e\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u7684\u8fc7\u5ea6\u62df\u5408\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u7684\u516c\u5e73\u6027\u8bc4\u4ef7\u6307\u6807\u7531\u4e8e\u4fa7\u91cd\u70b9\u4e0d\u540c\uff0c\u5176\u8bc4\u4ef7\u7ed3\u679c\u4e5f\u5b58\u5728\u663e\u7740\u5dee\u5f02\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u9884\u5904\u7406\u65b9\u6cd5\u548c\u5904\u7406\u4e2d\u65b9\u6cd5\u4f18\u4e8e\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u5176\u4e2d\u9884\u5904\u7406\u65b9\u6cd5\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u4e3a\u589e\u5f3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5efa\u8bae\u3002\u6211\u4eec\u4ece\u591a\u4e2a\u7ef4\u5ea6\u6765\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u7684\u542b\u4e49\u6fc0\u52b1\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u66f4\u6709\u6548\u7684\u516c\u5e73\u89e3\u51b3\u65b9\u6848\u3002|[2401.03695v1](http://arxiv.org/pdf/2401.03695v1)|**[link](https://github.com/junjie1003/DL-Fairness-Study)**|\n", "2401.03665": "|**2024-01-08**|**Primitive Geometry Segment Pre-training for 3D Medical Image Segmentation**|3D \u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u539f\u59cb\u51e0\u4f55\u5206\u5272\u9884\u8bad\u7ec3|Ryu Tadokoro, Ryosuke Yamada, Kodai Nakashima, Ryo Nakamura, Hirokatsu Kataoka|The construction of 3D medical image datasets presents several issues, including requiring significant financial costs in data collection and specialized expertise for annotation, as well as strict privacy concerns for patient confidentiality compared to natural image datasets. Therefore, it has become a pressing issue in 3D medical image segmentation to enable data-efficient learning with limited 3D medical data and supervision. A promising approach is pre-training, but improving its performance in 3D medical image segmentation is difficult due to the small size of existing 3D medical image datasets. We thus present the Primitive Geometry Segment Pre-training (PrimGeoSeg) method to enable the learning of 3D semantic features by pre-training segmentation tasks using only primitive geometric objects for 3D medical image segmentation. PrimGeoSeg performs more accurate and efficient 3D medical image segmentation without manual data collection and annotation. Further, experimental results show that PrimGeoSeg on SwinUNETR improves performance over learning from scratch on BTCV, MSD (Task06), and BraTS datasets by 3.7%, 4.4%, and 0.3%, respectively. Remarkably, the performance was equal to or better than state-of-the-art self-supervised learning despite the equal number of pre-training data. From experimental results, we conclude that effective pre-training can be achieved by looking at primitive geometric objects only. Code and dataset are available at https://github.com/SUPER-TADORY/PrimGeoSeg.|3D \u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u7684\u6784\u5efa\u63d0\u51fa\u4e86\u51e0\u4e2a\u95ee\u9898\uff0c\u5305\u62ec\u5728\u6570\u636e\u6536\u96c6\u548c\u6ce8\u91ca\u65b9\u9762\u9700\u8981\u5927\u91cf\u7684\u8d22\u52a1\u6210\u672c\uff0c\u4ee5\u53ca\u4e0e\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u76f8\u6bd4\u5bf9\u60a3\u8005\u4fdd\u5bc6\u6027\u7684\u4e25\u683c\u9690\u79c1\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u5728\u6709\u9650\u76843D\u533b\u5b66\u6570\u636e\u548c\u76d1\u7763\u4e0b\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u7684\u5b66\u4e60\u5df2\u6210\u4e3a3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u4e00\u4e2a\u7d27\u8feb\u95ee\u9898\u3002\u9884\u8bad\u7ec3\u662f\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u73b0\u6709 3D \u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u89c4\u6a21\u8f83\u5c0f\uff0c\u56e0\u6b64\u5f88\u96be\u63d0\u9ad8\u5176\u5728 3D \u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u539f\u59cb\u51e0\u4f55\u5206\u5272\u9884\u8bad\u7ec3\uff08PrimGeoSeg\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u539f\u59cb\u51e0\u4f55\u5bf9\u8c61\u8fdb\u884c 3D \u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u9884\u8bad\u7ec3\u5206\u5272\u4efb\u52a1\u6765\u5b9e\u73b0 3D \u8bed\u4e49\u7279\u5f81\u7684\u5b66\u4e60\u3002 PrimGeoSeg \u53ef\u4ee5\u6267\u884c\u66f4\u51c6\u786e\u3001\u66f4\u9ad8\u6548\u7684 3D \u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u65e0\u9700\u624b\u52a8\u6570\u636e\u6536\u96c6\u548c\u6ce8\u91ca\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSwinUNETR \u4e0a\u7684 PrimGeoSeg \u6bd4\u5728 BTCV\u3001MSD (Task06) \u548c BraTS \u6570\u636e\u96c6\u4e0a\u4ece\u5934\u5f00\u59cb\u5b66\u4e60\u7684\u6027\u80fd\u5206\u522b\u63d0\u9ad8\u4e86 3.7%\u30014.4% \u548c 0.3%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5c3d\u7ba1\u9884\u8bad\u7ec3\u6570\u636e\u6570\u91cf\u76f8\u540c\uff0c\u4f46\u5176\u6027\u80fd\u7b49\u4e8e\u6216\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u3002\u6839\u636e\u5b9e\u9a8c\u7ed3\u679c\uff0c\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff0c\u4ec5\u901a\u8fc7\u67e5\u770b\u539f\u59cb\u51e0\u4f55\u5bf9\u8c61\u5c31\u53ef\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u9884\u8bad\u7ec3\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u5728 https://github.com/SUPER-TADORY/PrimGeoSeg \u83b7\u53d6\u3002|[2401.03665v1](http://arxiv.org/pdf/2401.03665v1)|null|\n", "2401.03664": "|**2024-01-08**|**Dual-Channel Reliable Breast Ultrasound Image Classification Based on Explainable Attribution and Uncertainty Quantification**|\u57fa\u4e8e\u53ef\u89e3\u91ca\u5f52\u56e0\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u53cc\u901a\u9053\u53ef\u9760\u4e73\u817a\u8d85\u58f0\u56fe\u50cf\u5206\u7c7b|Shuge Lei, Haonan Hu, Dasheng Sun, Huabin Zhang, Kehong Yuan, Jian Dai, Jijun Tang, Yan Tong|This paper focuses on the classification task of breast ultrasound images and researches on the reliability measurement of classification results. We proposed a dual-channel evaluation framework based on the proposed inference reliability and predictive reliability scores. For the inference reliability evaluation, human-aligned and doctor-agreed inference rationales based on the improved feature attribution algorithm SP-RISA are gracefully applied. Uncertainty quantification is used to evaluate the predictive reliability via the Test Time Enhancement. The effectiveness of this reliability evaluation framework has been verified on our breast ultrasound clinical dataset YBUS, and its robustness is verified on the public dataset BUSI. The expected calibration errors on both datasets are significantly lower than traditional evaluation methods, which proves the effectiveness of our proposed reliability measurement.|\u672c\u6587\u4e3b\u8981\u9488\u5bf9\u4e73\u817a\u8d85\u58f0\u56fe\u50cf\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u5bf9\u5206\u7c7b\u7ed3\u679c\u7684\u53ef\u9760\u6027\u5ea6\u91cf\u8fdb\u884c\u7814\u7a76\u3002\u6211\u4eec\u6839\u636e\u6240\u63d0\u51fa\u7684\u63a8\u7406\u53ef\u9760\u6027\u548c\u9884\u6d4b\u53ef\u9760\u6027\u5206\u6570\u63d0\u51fa\u4e86\u53cc\u901a\u9053\u8bc4\u4f30\u6846\u67b6\u3002\u5bf9\u4e8e\u63a8\u7406\u53ef\u9760\u6027\u8bc4\u4f30\uff0c\u4f18\u96c5\u5730\u5e94\u7528\u4e86\u57fa\u4e8e\u6539\u8fdb\u7684\u7279\u5f81\u5f52\u56e0\u7b97\u6cd5 SP-RISA \u7684\u4eba\u6027\u5316\u548c\u533b\u751f\u540c\u610f\u7684\u63a8\u7406\u539f\u7406\u3002\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7528\u4e8e\u901a\u8fc7\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\u6765\u8bc4\u4f30\u9884\u6d4b\u53ef\u9760\u6027\u3002\u8be5\u53ef\u9760\u6027\u8bc4\u4f30\u6846\u67b6\u7684\u6709\u6548\u6027\u5df2\u5728\u6211\u4eec\u7684\u4e73\u817a\u8d85\u58f0\u4e34\u5e8a\u6570\u636e\u96c6 YBUS \u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u5176\u7a33\u5065\u6027\u5728\u516c\u5171\u6570\u636e\u96c6 BUSI \u4e0a\u5f97\u5230\u9a8c\u8bc1\u3002\u4e24\u4e2a\u6570\u636e\u96c6\u7684\u9884\u671f\u6821\u51c6\u8bef\u5dee\u5747\u663e\u7740\u4f4e\u4e8e\u4f20\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8fd9\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u53ef\u9760\u6027\u6d4b\u91cf\u7684\u6709\u6548\u6027\u3002|[2401.03664v1](http://arxiv.org/pdf/2401.03664v1)|null|\n", "2401.03637": "|**2024-01-08**|**Inverse-like Antagonistic Scene Text Spotting via Reading-Order Estimation and Dynamic Sampling**|\u901a\u8fc7\u9605\u8bfb\u987a\u5e8f\u4f30\u8ba1\u548c\u52a8\u6001\u91c7\u6837\u8fdb\u884c\u7c7b\u9006\u5bf9\u6297\u573a\u666f\u6587\u672c\u8bc6\u522b|Shi-Xue Zhang, Chun Yang, Xiaobin Zhu, Hongyang Zhou, Hongfa Wang, Xu-Cheng Yin|Scene text spotting is a challenging task, especially for inverse-like scene text, which has complex layouts, e.g., mirrored, symmetrical, or retro-flexed. In this paper, we propose a unified end-to-end trainable inverse-like antagonistic text spotting framework dubbed IATS, which can effectively spot inverse-like scene texts without sacrificing general ones. Specifically, we propose an innovative reading-order estimation module (REM) that extracts reading-order information from the initial text boundary generated by an initial boundary module (IBM). To optimize and train REM, we propose a joint reading-order estimation loss consisting of a classification loss, an orthogonality loss, and a distribution loss. With the help of IBM, we can divide the initial text boundary into two symmetric control points and iteratively refine the new text boundary using a lightweight boundary refinement module (BRM) for adapting to various shapes and scales. To alleviate the incompatibility between text detection and recognition, we propose a dynamic sampling module (DSM) with a thin-plate spline that can dynamically sample appropriate features for recognition in the detected text region. Without extra supervision, the DSM can proactively learn to sample appropriate features for text recognition through the gradient returned by the recognition module. Extensive experiments on both challenging scene text and inverse-like scene text datasets demonstrate that our method achieves superior performance both on irregular and inverse-like text spotting.|\u573a\u666f\u6587\u672c\u8bc6\u522b\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u590d\u6742\u5e03\u5c40\uff08\u4f8b\u5982\u955c\u50cf\u3001\u5bf9\u79f0\u6216\u53cd\u6298\uff09\u7684\u7c7b\u4f3c\u53cd\u5411\u7684\u573a\u666f\u6587\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u7c7b\u9006\u5bf9\u6297\u6027\u6587\u672c\u8bc6\u522b\u6846\u67b6\uff0c\u79f0\u4e3a IATS\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u8bc6\u522b\u7c7b\u9006\u573a\u666f\u6587\u672c\uff0c\u800c\u65e0\u9700\u727a\u7272\u901a\u7528\u6587\u672c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u9605\u8bfb\u987a\u5e8f\u4f30\u8ba1\u6a21\u5757\uff08REM\uff09\uff0c\u5b83\u4ece\u521d\u59cb\u8fb9\u754c\u6a21\u5757\uff08IBM\uff09\u751f\u6210\u7684\u521d\u59cb\u6587\u672c\u8fb9\u754c\u4e2d\u63d0\u53d6\u9605\u8bfb\u987a\u5e8f\u4fe1\u606f\u3002\u4e3a\u4e86\u4f18\u5316\u548c\u8bad\u7ec3 REM\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u9605\u8bfb\u987a\u5e8f\u4f30\u8ba1\u635f\u5931\uff0c\u5176\u4e2d\u5305\u62ec\u5206\u7c7b\u635f\u5931\u3001\u6b63\u4ea4\u6027\u635f\u5931\u548c\u5206\u5e03\u635f\u5931\u3002\u5728 IBM \u7684\u5e2e\u52a9\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u521d\u59cb\u6587\u672c\u8fb9\u754c\u5212\u5206\u4e3a\u4e24\u4e2a\u5bf9\u79f0\u63a7\u5236\u70b9\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8fb9\u754c\u7ec6\u5316\u6a21\u5757 (BRM) \u8fed\u4ee3\u7ec6\u5316\u65b0\u7684\u6587\u672c\u8fb9\u754c\uff0c\u4ee5\u9002\u5e94\u5404\u79cd\u5f62\u72b6\u548c\u6bd4\u4f8b\u3002\u4e3a\u4e86\u7f13\u89e3\u6587\u672c\u68c0\u6d4b\u548c\u8bc6\u522b\u4e4b\u95f4\u7684\u4e0d\u517c\u5bb9\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u8584\u677f\u6837\u6761\u7684\u52a8\u6001\u91c7\u6837\u6a21\u5757\uff08DSM\uff09\uff0c\u53ef\u4ee5\u52a8\u6001\u91c7\u6837\u9002\u5f53\u7684\u7279\u5f81\u4ee5\u5728\u68c0\u6d4b\u5230\u7684\u6587\u672c\u533a\u57df\u4e2d\u8fdb\u884c\u8bc6\u522b\u3002\u5728\u6ca1\u6709\u989d\u5916\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0cDSM \u53ef\u4ee5\u901a\u8fc7\u8bc6\u522b\u6a21\u5757\u8fd4\u56de\u7684\u68af\u5ea6\u4e3b\u52a8\u5b66\u4e60\u91c7\u6837\u9002\u5f53\u7684\u7279\u5f81\u4ee5\u8fdb\u884c\u6587\u672c\u8bc6\u522b\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u6587\u672c\u548c\u7c7b\u4f3c\u9006\u7684\u573a\u666f\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u89c4\u5219\u548c\u7c7b\u4f3c\u9006\u7684\u6587\u672c\u8bc6\u522b\u4e0a\u90fd\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002|[2401.03637v1](http://arxiv.org/pdf/2401.03637v1)|null|\n"}, "OCR": {"2401.03914": "|**2024-01-08**|**D3PRefiner: A Diffusion-based Denoise Method for 3D Human Pose Refinement**|D3PRefiner\uff1a\u7528\u4e8e 3D \u4eba\u4f53\u59ff\u52bf\u7ec6\u5316\u7684\u57fa\u4e8e\u6269\u6563\u7684\u964d\u566a\u65b9\u6cd5|Danqi Yan, Qing Gao, Yuepeng Qian, Xinxing Chen, Chenglong Fu, Yuquan Leng|Three-dimensional (3D) human pose estimation using a monocular camera has gained increasing attention due to its ease of implementation and the abundance of data available from daily life. However, owing to the inherent depth ambiguity in images, the accuracy of existing monocular camera-based 3D pose estimation methods remains unsatisfactory, and the estimated 3D poses usually include much noise. By observing the histogram of this noise, we find each dimension of the noise follows a certain distribution, which indicates the possibility for a neural network to learn the mapping between noisy poses and ground truth poses. In this work, in order to obtain more accurate 3D poses, a Diffusion-based 3D Pose Refiner (D3PRefiner) is proposed to refine the output of any existing 3D pose estimator. We first introduce a conditional multivariate Gaussian distribution to model the distribution of noisy 3D poses, using paired 2D poses and noisy 3D poses as conditions to achieve greater accuracy. Additionally, we leverage the architecture of current diffusion models to convert the distribution of noisy 3D poses into ground truth 3D poses. To evaluate the effectiveness of the proposed method, two state-of-the-art sequence-to-sequence 3D pose estimators are used as basic 3D pose estimation models, and the proposed method is evaluated on different types of 2D poses and different lengths of the input sequence. Experimental results demonstrate the proposed architecture can significantly improve the performance of current sequence-to-sequence 3D pose estimators, with a reduction of at least 10.3% in the mean per joint position error (MPJPE) and at least 11.0% in the Procrustes MPJPE (P-MPJPE).|\u4f7f\u7528\u5355\u76ee\u76f8\u673a\u8fdb\u884c\u4e09\u7ef4 (3D) \u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u7531\u4e8e\u5176\u6613\u4e8e\u5b9e\u65bd\u4e14\u65e5\u5e38\u751f\u6d3b\u4e2d\u53ef\u83b7\u5f97\u7684\u6570\u636e\u4e30\u5bcc\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u7531\u4e8e\u56fe\u50cf\u56fa\u6709\u7684\u6df1\u5ea6\u6a21\u7cca\u6027\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u5355\u76ee\u76f8\u673a\u76843D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u7684\u7cbe\u5ea6\u4ecd\u7136\u4e0d\u80fd\u4ee4\u4eba\u6ee1\u610f\uff0c\u5e76\u4e14\u4f30\u8ba1\u76843D\u59ff\u6001\u901a\u5e38\u5305\u542b\u5927\u91cf\u566a\u58f0\u3002\u901a\u8fc7\u89c2\u5bdf\u8be5\u566a\u58f0\u7684\u76f4\u65b9\u56fe\uff0c\u6211\u4eec\u53d1\u73b0\u566a\u58f0\u7684\u6bcf\u4e2a\u7ef4\u5ea6\u90fd\u9075\u5faa\u4e00\u5b9a\u7684\u5206\u5e03\uff0c\u8fd9\u8868\u660e\u795e\u7ecf\u7f51\u7edc\u6709\u53ef\u80fd\u5b66\u4e60\u566a\u58f0\u59ff\u52bf\u548c\u5730\u9762\u771f\u5b9e\u59ff\u52bf\u4e4b\u95f4\u7684\u6620\u5c04\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u4e3a\u4e86\u83b7\u5f97\u66f4\u51c6\u786e\u7684 3D \u59ff\u6001\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684 3D \u59ff\u6001\u7ec6\u5316\u5668\uff08D3PRefiner\uff09\u6765\u7ec6\u5316\u4efb\u4f55\u73b0\u6709 3D \u59ff\u6001\u4f30\u8ba1\u5668\u7684\u8f93\u51fa\u3002\u6211\u4eec\u9996\u5148\u5f15\u5165\u6761\u4ef6\u591a\u5143\u9ad8\u65af\u5206\u5e03\u6765\u5bf9\u566a\u58f0 3D \u59ff\u52bf\u7684\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u4f7f\u7528\u914d\u5bf9 2D \u59ff\u52bf\u548c\u566a\u58f0 3D \u59ff\u52bf\u4f5c\u4e3a\u6761\u4ef6\u6765\u5b9e\u73b0\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u5f53\u524d\u6269\u6563\u6a21\u578b\u7684\u67b6\u6784\u5c06\u566a\u58f0 3D \u59ff\u52bf\u7684\u5206\u5e03\u8f6c\u6362\u4e3a\u5730\u9762\u771f\u5b9e 3D \u59ff\u52bf\u3002\u4e3a\u4e86\u8bc4\u4f30\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528\u4e24\u4e2a\u6700\u5148\u8fdb\u7684\u5e8f\u5217\u5230\u5e8f\u52173D\u59ff\u6001\u4f30\u8ba1\u5668\u4f5c\u4e3a\u57fa\u672c3D\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u540c\u7c7b\u578b\u76842D\u59ff\u6001\u548c\u4e0d\u540c\u957f\u5ea6\u76842D\u59ff\u6001\u4e0a\u8bc4\u4f30\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u8f93\u5165\u5e8f\u5217\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u6784\u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u5f53\u524d\u5e8f\u5217\u5230\u5e8f\u5217 3D \u4f4d\u59ff\u4f30\u8ba1\u5668\u7684\u6027\u80fd\uff0c\u5e73\u5747\u6bcf\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee (MPJPE) \u51cf\u5c11\u81f3\u5c11 10.3%\uff0cProcrustes MPJPE \u81f3\u5c11\u51cf\u5c11 11.0%\uff08 P-MPJPE\uff09\u3002|[2401.03914v1](http://arxiv.org/pdf/2401.03914v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2401.04099": "|**2024-01-08**|**AGG: Amortized Generative 3D Gaussians for Single Image to 3D**|AGG\uff1a\u7528\u4e8e\u5355\u56fe\u50cf\u5230 3D \u7684\u644a\u9500\u751f\u6210 3D \u9ad8\u65af|Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat|Given the growing need for automatic 3D content creation pipelines, various 3D representations have been studied to generate 3D objects from a single image. Due to its superior rendering efficiency, 3D Gaussian splatting-based models have recently excelled in both 3D reconstruction and generation. 3D Gaussian splatting approaches for image to 3D generation are often optimization-based, requiring many computationally expensive score-distillation steps. To overcome these challenges, we introduce an Amortized Generative 3D Gaussian framework (AGG) that instantly produces 3D Gaussians from a single image, eliminating the need for per-instance optimization. Utilizing an intermediate hybrid representation, AGG decomposes the generation of 3D Gaussian locations and other appearance attributes for joint optimization. Moreover, we propose a cascaded pipeline that first generates a coarse representation of the 3D data and later upsamples it with a 3D Gaussian super-resolution module. Our method is evaluated against existing optimization-based 3D Gaussian frameworks and sampling-based pipelines utilizing other 3D representations, where AGG showcases competitive generation abilities both qualitatively and quantitatively while being several orders of magnitude faster. Project page: https://ir1d.github.io/AGG/|\u9274\u4e8e\u5bf9\u81ea\u52a8 3D \u5185\u5bb9\u521b\u5efa\u7ba1\u9053\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u4eba\u4eec\u5df2\u7ecf\u7814\u7a76\u4e86\u5404\u79cd 3D \u8868\u793a\u5f62\u5f0f\uff0c\u4ee5\u4ece\u5355\u4e2a\u56fe\u50cf\u751f\u6210 3D \u5bf9\u8c61\u3002\u7531\u4e8e\u5176\u5353\u8d8a\u7684\u6e32\u67d3\u6548\u7387\uff0c\u57fa\u4e8e 3D \u9ad8\u65af\u55b7\u5c04\u7684\u6a21\u578b\u6700\u8fd1\u5728 3D \u91cd\u5efa\u548c\u751f\u6210\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002\u7528\u4e8e\u56fe\u50cf\u5230 3D \u751f\u6210\u7684 3D \u9ad8\u65af\u5206\u5e03\u65b9\u6cd5\u901a\u5e38\u662f\u57fa\u4e8e\u4f18\u5316\u7684\uff0c\u9700\u8981\u8bb8\u591a\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u5206\u6570\u84b8\u998f\u6b65\u9aa4\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u644a\u9500\u751f\u6210 3D \u9ad8\u65af\u6846\u67b6 (AGG)\uff0c\u5b83\u53ef\u4ee5\u7acb\u5373\u4ece\u5355\u4e2a\u56fe\u50cf\u751f\u6210 3D \u9ad8\u65af\uff0c\u4ece\u800c\u65e0\u9700\u5bf9\u6bcf\u4e2a\u5b9e\u4f8b\u8fdb\u884c\u4f18\u5316\u3002 AGG \u5229\u7528\u4e2d\u95f4\u6df7\u5408\u8868\u793a\uff0c\u5206\u89e3 3D \u9ad8\u65af\u4f4d\u7f6e\u548c\u5176\u4ed6\u5916\u89c2\u5c5e\u6027\u7684\u751f\u6210\uff0c\u4ee5\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ea7\u8054\u7ba1\u9053\uff0c\u9996\u5148\u751f\u6210 3D \u6570\u636e\u7684\u7c97\u7565\u8868\u793a\uff0c\u7136\u540e\u4f7f\u7528 3D \u9ad8\u65af\u8d85\u5206\u8fa8\u7387\u6a21\u5757\u5bf9\u5176\u8fdb\u884c\u4e0a\u91c7\u6837\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6839\u636e\u73b0\u6709\u7684\u57fa\u4e8e\u4f18\u5316\u7684 3D \u9ad8\u65af\u6846\u67b6\u548c\u5229\u7528\u5176\u4ed6 3D \u8868\u793a\u7684\u57fa\u4e8e\u91c7\u6837\u7684\u7ba1\u9053\u8fdb\u884c\u8bc4\u4f30\uff0c\u5176\u4e2d AGG \u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u90fd\u5c55\u793a\u4e86\u6709\u7ade\u4e89\u529b\u7684\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u901f\u5ea6\u5feb\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://ir1d.github.io/AGG/|[2401.04099v1](http://arxiv.org/pdf/2401.04099v1)|null|\n"}, "Nerf": {"2401.03890": "|**2024-01-08**|**A Survey on 3D Gaussian Splatting**|3D \u9ad8\u65af\u6cfc\u6e85\u7efc\u8ff0|Guikun Chen, Wenguan Wang|3D Gaussian splatting (3D GS) has recently emerged as a transformative technique in the explicit radiance field and computer graphics landscape. This innovative approach, characterized by the utilization of millions of 3D Gaussians, represents a significant departure from the neural radiance field (NeRF) methodologies, which predominantly use implicit, coordinate-based models to map spatial coordinates to pixel values. 3D GS, with its explicit scene representations and differentiable rendering algorithms, not only promises real-time rendering capabilities but also introduces unprecedented levels of control and editability. This positions 3D GS as a potential game-changer for the next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and the driving forces behind the advent of 3D GS, setting the stage for understanding its significance. A focal point of our discussion is the practical applicability of 3D GS. By facilitating real-time performance, 3D GS opens up a plethora of applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading 3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by identifying current challenges and suggesting potential avenues for future research in this domain. Through this survey, we aim to provide a valuable resource for both newcomers and seasoned researchers, fostering further exploration and advancement in applicable and explicit radiance field representation.|3D \u9ad8\u65af\u5206\u5e03 (3D GS) \u6700\u8fd1\u4f5c\u4e3a\u663e\u5f0f\u8f90\u5c04\u573a\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u9886\u57df\u7684\u4e00\u9879\u53d8\u9769\u6027\u6280\u672f\u800c\u51fa\u73b0\u3002\u8fd9\u79cd\u521b\u65b0\u65b9\u6cd5\u7684\u7279\u70b9\u662f\u5229\u7528\u4e86\u6570\u767e\u4e07\u4e2a 3D \u9ad8\u65af\u51fd\u6570\uff0c\u5b83\u4e0e\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u65b9\u6cd5\u6709\u5f88\u5927\u4e0d\u540c\uff0c\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u9690\u5f0f\u7684\u57fa\u4e8e\u5750\u6807\u7684\u6a21\u578b\u5c06\u7a7a\u95f4\u5750\u6807\u6620\u5c04\u5230\u50cf\u7d20\u503c\u3002 3D GS \u51ed\u501f\u5176\u660e\u786e\u7684\u573a\u666f\u8868\u793a\u548c\u53ef\u5fae\u7684\u6e32\u67d3\u7b97\u6cd5\uff0c\u4e0d\u4ec5\u4fdd\u8bc1\u4e86\u5b9e\u65f6\u6e32\u67d3\u529f\u80fd\uff0c\u800c\u4e14\u8fd8\u5f15\u5165\u4e86\u524d\u6240\u672a\u6709\u7684\u63a7\u5236\u548c\u53ef\u7f16\u8f91\u6027\u6c34\u5e73\u3002\u8fd9\u4f7f\u5f97 3D GS \u6210\u4e3a\u4e0b\u4e00\u4ee3 3D \u91cd\u5efa\u548c\u8868\u793a\u7684\u6f5c\u5728\u6e38\u620f\u89c4\u5219\u6539\u53d8\u8005\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u6b21\u7cfb\u7edf\u5730\u6982\u8ff0\u4e86 3D GS \u9886\u57df\u7684\u6700\u65b0\u53d1\u5c55\u548c\u5173\u952e\u8d21\u732e\u3002\u6211\u4eec\u9996\u5148\u8be6\u7ec6\u63a2\u8ba8 3D GS \u51fa\u73b0\u80cc\u540e\u7684\u57fa\u672c\u539f\u7406\u548c\u9a71\u52a8\u529b\uff0c\u4e3a\u7406\u89e3\u5176\u91cd\u8981\u6027\u5960\u5b9a\u57fa\u7840\u3002\u6211\u4eec\u8ba8\u8bba\u7684\u4e00\u4e2a\u7126\u70b9\u662f 3D GS \u7684\u5b9e\u9645\u9002\u7528\u6027\u3002\u901a\u8fc7\u4fc3\u8fdb\u5b9e\u65f6\u6027\u80fd\uff0c3D GS \u5f00\u8f9f\u4e86\u4ece\u865a\u62df\u73b0\u5b9e\u5230\u4ea4\u4e92\u5f0f\u5a92\u4f53\u7b49\u4f17\u591a\u5e94\u7528\u7a0b\u5e8f\u3002\u5bf9\u6b64\u8fdb\u884c\u4e86\u8865\u5145\uff0c\u5bf9\u9886\u5148\u7684 3D GS \u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4ee5\u7a81\u51fa\u5176\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002\u8be5\u8c03\u67e5\u6700\u540e\u786e\u5b9a\u4e86\u5f53\u524d\u7684\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u9014\u5f84\u3002\u901a\u8fc7\u8fd9\u9879\u8c03\u67e5\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u4e3a\u65b0\u624b\u548c\u7ecf\u9a8c\u4e30\u5bcc\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u4fc3\u8fdb\u5728\u9002\u7528\u548c\u660e\u786e\u7684\u8f90\u5c04\u573a\u8868\u793a\u65b9\u9762\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u548c\u8fdb\u6b65\u3002|[2401.03890v1](http://arxiv.org/pdf/2401.03890v1)|null|\n", "2401.03771": "|**2024-01-08**|**NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation**|NeRFmentation\uff1a\u57fa\u4e8e NeRF \u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u589e\u5f3a|Casimir Feldmann, Niall Siegenheim, Nikolas Hars, Lovro Rabuzin, Mert Ertugrul, Luca Wolfart, Marc Pollefeys, Zuria Bauer, Martin R. Oswald|The capabilities of monocular depth estimation (MDE) models are limited by the availability of sufficient and diverse datasets. In the case of MDE models for autonomous driving, this issue is exacerbated by the linearity of the captured data trajectories. We propose a NeRF-based data augmentation pipeline to introduce synthetic data with more diverse viewing directions into training datasets and demonstrate the benefits of our approach to model performance and robustness. Our data augmentation pipeline, which we call \"NeRFmentation\", trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on relevant metrics, and uses them to generate synthetic RGB-D images captured from new viewing directions. In this work, we apply our technique in conjunction with three state-of-the-art MDE architectures on the popular autonomous driving dataset KITTI, augmenting its training set of the Eigen split. We evaluate the resulting performance gain on the original test set, a separate popular driving set, and our own synthetic test set.|\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1 (MDE) \u6a21\u578b\u7684\u529f\u80fd\u53d7\u5230\u8db3\u591f\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u53ef\u7528\u6027\u7684\u9650\u5236\u3002\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684 MDE \u6a21\u578b\uff0c\u6355\u83b7\u7684\u6570\u636e\u8f68\u8ff9\u7684\u7ebf\u6027\u4f1a\u52a0\u5267\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e NeRF \u7684\u6570\u636e\u589e\u5f3a\u7ba1\u9053\uff0c\u5c06\u5177\u6709\u66f4\u591a\u6837\u5316\u89c2\u5bdf\u65b9\u5411\u7684\u5408\u6210\u6570\u636e\u5f15\u5165\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\uff0c\u5e76\u5c55\u793a\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u7684\u597d\u5904\u3002\u6211\u4eec\u7684\u6570\u636e\u589e\u5f3a\u7ba1\u9053\uff08\u6211\u4eec\u79f0\u4e4b\u4e3a\u201cNeRFmentation\u201d\uff09\u5728\u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u573a\u666f\u4e0a\u8bad\u7ec3 NeRF\uff0c\u6839\u636e\u76f8\u5173\u6307\u6807\u8fc7\u6ee4\u6389\u4f4e\u4e8e\u6807\u51c6\u7684 NeRF\uff0c\u5e76\u4f7f\u7528\u5b83\u4eec\u751f\u6210\u4ece\u65b0\u89c2\u770b\u65b9\u5411\u6355\u83b7\u7684\u5408\u6210 RGB-D \u56fe\u50cf\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06\u6211\u4eec\u7684\u6280\u672f\u4e0e\u4e09\u79cd\u6700\u5148\u8fdb\u7684 MDE \u67b6\u6784\u7ed3\u5408\u5e94\u7528\u5728\u6d41\u884c\u7684\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6 KITTI \u4e0a\uff0c\u589e\u5f3a\u4e86\u5176 Eigen split \u7684\u8bad\u7ec3\u96c6\u3002\u6211\u4eec\u5728\u539f\u59cb\u6d4b\u8bd5\u96c6\u3001\u5355\u72ec\u7684\u6d41\u884c\u9a7e\u9a76\u96c6\u548c\u6211\u4eec\u81ea\u5df1\u7684\u7efc\u5408\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6700\u7ec8\u7684\u6027\u80fd\u589e\u76ca\u3002|[2401.03771v1](http://arxiv.org/pdf/2401.03771v1)|null|\n"}, "\u751f\u6210\u6a21\u578b": {"2401.04092": "|**2024-01-08**|**GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation**|GPT-4V(ision) \u662f\u4e00\u6b3e\u7528\u4e8e\u6587\u672c\u8f6c 3D \u751f\u6210\u7684\u4eba\u6027\u5316\u8bc4\u4f30\u5668|Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, Gordon Wetzstein|Despite recent advances in text-to-3D generative methods, there is a notable absence of reliable evaluation metrics. Existing metrics usually focus on a single criterion each, such as how well the asset aligned with the input text. These metrics lack the flexibility to generalize to different evaluation criteria and might not align well with human preferences. Conducting user preference studies is an alternative that offers both adaptability and human-aligned results. User studies, however, can be very expensive to scale. This paper presents an automatic, versatile, and human-aligned evaluation metric for text-to-3D generative models. To this end, we first develop a prompt generator using GPT-4V to generate evaluating prompts, which serve as input to compare text-to-3D models. We further design a method instructing GPT-4V to compare two 3D assets according to user-defined criteria. Finally, we use these pairwise comparison results to assign these models Elo ratings. Experimental results suggest our metric strongly align with human preference across different evaluation criteria.|\u5c3d\u7ba1\u6587\u672c\u5230 3D \u751f\u6210\u65b9\u6cd5\u6700\u8fd1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7136\u660e\u663e\u7f3a\u4e4f\u53ef\u9760\u7684\u8bc4\u4f30\u6307\u6807\u3002\u73b0\u6709\u7684\u6307\u6807\u901a\u5e38\u53ea\u5173\u6ce8\u4e00\u4e2a\u6807\u51c6\uff0c\u4f8b\u5982\u8d44\u4ea7\u4e0e\u8f93\u5165\u6587\u672c\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002\u8fd9\u4e9b\u6307\u6807\u7f3a\u4e4f\u63a8\u5e7f\u5230\u4e0d\u540c\u8bc4\u4f30\u6807\u51c6\u7684\u7075\u6d3b\u6027\uff0c\u5e76\u4e14\u53ef\u80fd\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u592a\u76f8\u7b26\u3002\u8fdb\u884c\u7528\u6237\u504f\u597d\u7814\u7a76\u662f\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u4ee5\u63d0\u4f9b\u9002\u5e94\u6027\u548c\u4eba\u6027\u5316\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u6269\u5c55\u7528\u6237\u7814\u7a76\u7684\u6210\u672c\u53ef\u80fd\u975e\u5e38\u6602\u8d35\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6587\u672c\u8f6c 3D \u751f\u6210\u6a21\u578b\u7684\u81ea\u52a8\u3001\u591a\u529f\u80fd\u4e14\u4eba\u6027\u5316\u7684\u8bc4\u4f30\u6307\u6807\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u4f7f\u7528 GPT-4V \u5f00\u53d1\u4e00\u4e2a\u63d0\u793a\u751f\u6210\u5668\u6765\u751f\u6210\u8bc4\u4f30\u63d0\u793a\uff0c\u4f5c\u4e3a\u6bd4\u8f83\u6587\u672c\u5230 3D \u6a21\u578b\u7684\u8f93\u5165\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u6307\u793a GPT-4V \u6839\u636e\u7528\u6237\u5b9a\u4e49\u7684\u6807\u51c6\u6bd4\u8f83\u4e24\u4e2a 3D \u8d44\u4ea7\u3002\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u6210\u5bf9\u6bd4\u8f83\u7ed3\u679c\u6765\u4e3a\u8fd9\u4e9b\u6a21\u578b\u5206\u914d Elo \u8bc4\u7ea7\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6307\u6807\u5728\u4e0d\u540c\u7684\u8bc4\u4f30\u6807\u51c6\u4e0a\u4e0e\u4eba\u7c7b\u7684\u504f\u597d\u9ad8\u5ea6\u4e00\u81f4\u3002|[2401.04092v1](http://arxiv.org/pdf/2401.04092v1)|null|\n", "2401.03854": "|**2024-01-08**|**TIER: Text and Image Encoder-based Regression for AIGC Image Quality Assessment**|TIER\uff1a\u7528\u4e8e AIGC \u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u57fa\u4e8e\u6587\u672c\u548c\u56fe\u50cf\u7f16\u7801\u5668\u7684\u56de\u5f52|Jiquan Yuan, Xinyan Cao, Jinming Che, Qinyuan Wang, Sen Liang, Wei Ren, Jinlong Lin, Xixin Cao|Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the quality of AI-generated images from a human perception perspective, has emerged as a new topic in computer vision. Unlike common image quality assessment tasks where images are derived from original ones distorted by noise, blur, and compression, in AIGCIQA tasks, images are typically generated by generative models using text prompts. Considerable efforts have been made in the past years to advance AIGCIQA. However, most existing AIGCIQA methods regress predicted scores directly from individual generated images, overlooking the information contained in the text prompts of these images. This oversight partially limits the performance of these AIGCIQA methods. To address this issue, we propose a text and image encoder-based regression (TIER) framework. Specifically, we process the generated images and their corresponding text prompts as inputs, utilizing a text encoder and an image encoder to extract features from these text prompts and generated images, respectively. To demonstrate the effectiveness of our proposed TIER method, we conduct extensive experiments on several mainstream AIGCIQA databases, including AGIQA-1K, AGIQA-3K, and AIGCIQA2023. The experimental results indicate that our proposed TIER method generally demonstrates superior performance compared to baseline in most cases.|\u6700\u8fd1\uff0c\u65e8\u5728\u4ece\u4eba\u7c7b\u611f\u77e5\u89d2\u5ea6\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u7684AIGC\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08AIGCIQA\uff09\u5df2\u6210\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u4e00\u4e2a\u65b0\u8bfe\u9898\u3002\u4e0e\u5e38\u89c1\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e0d\u540c\uff0c\u5728 AIGCIQA \u4efb\u52a1\u4e2d\uff0c\u56fe\u50cf\u901a\u5e38\u7531\u751f\u6210\u6a21\u578b\u4f7f\u7528\u6587\u672c\u63d0\u793a\u751f\u6210\u3002\u8fc7\u53bb\u51e0\u5e74\uff0c\u6211\u4eec\u4e3a\u63a8\u8fdb AGCIQA \u505a\u51fa\u4e86\u5de8\u5927\u52aa\u529b\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684 AIGCIQA \u65b9\u6cd5\u76f4\u63a5\u4ece\u5404\u4e2a\u751f\u6210\u7684\u56fe\u50cf\u56de\u5f52\u9884\u6d4b\u5206\u6570\uff0c\u5ffd\u7565\u4e86\u8fd9\u4e9b\u56fe\u50cf\u7684\u6587\u672c\u63d0\u793a\u4e2d\u5305\u542b\u7684\u4fe1\u606f\u3002\u8fd9\u79cd\u758f\u5ffd\u90e8\u5206\u9650\u5236\u4e86\u8fd9\u4e9b AIGCIQA \u65b9\u6cd5\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u548c\u56fe\u50cf\u7f16\u7801\u5668\u7684\u56de\u5f52\uff08TIER\uff09\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c06\u751f\u6210\u7684\u56fe\u50cf\u53ca\u5176\u76f8\u5e94\u7684\u6587\u672c\u63d0\u793a\u4f5c\u4e3a\u8f93\u5165\u8fdb\u884c\u5904\u7406\uff0c\u5229\u7528\u6587\u672c\u7f16\u7801\u5668\u548c\u56fe\u50cf\u7f16\u7801\u5668\u5206\u522b\u4ece\u8fd9\u4e9b\u6587\u672c\u63d0\u793a\u548c\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u63d0\u53d6\u7279\u5f81\u3002\u4e3a\u4e86\u8bc1\u660e\u6211\u4eec\u63d0\u51fa\u7684 TIER \u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728\u51e0\u4e2a\u4e3b\u6d41 AIGCIQA \u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5305\u62ec AGIQA-1K\u3001AGIQA-3K \u548c AIGCIQA2023\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u7684 TIER \u65b9\u6cd5\u901a\u5e38\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u7684\u6027\u80fd\u3002|[2401.03854v1](http://arxiv.org/pdf/2401.03854v1)|null|\n", "2401.03764": "|**2024-01-08**|**3D-SSGAN: Lifting 2D Semantics for 3D-Aware Compositional Portrait Synthesis**|3D-SSGAN\uff1a\u63d0\u5347 2D \u8bed\u4e49\u4ee5\u5b9e\u73b0 3D \u611f\u77e5\u6784\u56fe\u5408\u6210|Ruiqi Liu, Peng Zheng, Ye Wang, Rui Ma|Existing 3D-aware portrait synthesis methods can generate impressive high-quality images while preserving strong 3D consistency. However, most of them cannot support the fine-grained part-level control over synthesized images. Conversely, some GAN-based 2D portrait synthesis methods can achieve clear disentanglement of facial regions, but they cannot preserve view consistency due to a lack of 3D modeling abilities. To address these issues, we propose 3D-SSGAN, a novel framework for 3D-aware compositional portrait image synthesis. First, a simple yet effective depth-guided 2D-to-3D lifting module maps the generated 2D part features and semantics to 3D. Then, a volume renderer with a novel 3D-aware semantic mask renderer is utilized to produce the composed face features and corresponding masks. The whole framework is trained end-to-end by discriminating between real and synthesized 2D images and their semantic masks. Quantitative and qualitative evaluations demonstrate the superiority of 3D-SSGAN in controllable part-level synthesis while preserving 3D view consistency.|\u73b0\u6709\u7684 3D \u611f\u77e5\u8096\u50cf\u5408\u6210\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684 3D \u4e00\u81f4\u6027\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e2d\u7684\u5927\u591a\u6570\u4e0d\u80fd\u652f\u6301\u5bf9\u5408\u6210\u56fe\u50cf\u7684\u7ec6\u7c92\u5ea6\u90e8\u5206\u7ea7\u63a7\u5236\u3002\u76f8\u53cd\uff0c\u4e00\u4e9b\u57fa\u4e8e GAN \u7684 2D \u4eba\u50cf\u5408\u6210\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u9762\u90e8\u533a\u57df\u7684\u6e05\u6670\u89e3\u5f00\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f 3D \u5efa\u6a21\u80fd\u529b\uff0c\u65e0\u6cd5\u4fdd\u6301\u89c6\u56fe\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 3D-SSGAN\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e 3D \u611f\u77e5\u6784\u56fe\u56fe\u50cf\u5408\u6210\u7684\u65b0\u9896\u6846\u67b6\u3002\u9996\u5148\uff0c\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6df1\u5ea6\u5f15\u5bfc 2D \u5230 3D \u63d0\u5347\u6a21\u5757\u5c06\u751f\u6210\u7684 2D \u96f6\u4ef6\u7279\u5f81\u548c\u8bed\u4e49\u6620\u5c04\u5230 3D\u3002\u7136\u540e\uff0c\u5229\u7528\u5177\u6709\u65b0\u9896\u7684 3D \u611f\u77e5\u8bed\u4e49\u63a9\u6a21\u6e32\u67d3\u5668\u7684\u4f53\u79ef\u6e32\u67d3\u5668\u6765\u751f\u6210\u5408\u6210\u7684\u9762\u90e8\u7279\u5f81\u548c\u76f8\u5e94\u7684\u63a9\u6a21\u3002\u6574\u4e2a\u6846\u67b6\u901a\u8fc7\u533a\u5206\u771f\u5b9e\u548c\u5408\u6210\u7684 2D \u56fe\u50cf\u53ca\u5176\u8bed\u4e49\u63a9\u6a21\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8bc1\u660e\u4e86 3D-SSGAN \u5728\u53ef\u63a7\u96f6\u4ef6\u7ea7\u5408\u6210\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86 3D \u89c6\u56fe\u7684\u4e00\u81f4\u6027\u3002|[2401.03764v1](http://arxiv.org/pdf/2401.03764v1)|null|\n", "2401.03639": "|**2024-01-08**|**Deep Learning for Visual Neuroprosthesis**|\u89c6\u89c9\u795e\u7ecf\u5047\u4f53\u7684\u6df1\u5ea6\u5b66\u4e60|Peter Beech, Shanshan Jia, Zhaofei Yu, Jian K. Liu|The visual pathway involves complex networks of cells and regions which contribute to the encoding and processing of visual information. While some aspects of visual perception are understood, there are still many unanswered questions regarding the exact mechanisms of visual encoding and the organization of visual information along the pathway. This chapter discusses the importance of visual perception and the challenges associated with understanding how visual information is encoded and represented in the brain. Furthermore, this chapter introduces the concept of neuroprostheses: devices designed to enhance or replace bodily functions, and highlights the importance of constructing computational models of the visual pathway in the implementation of such devices. A number of such models, employing the use of deep learning models, are outlined, and their value to understanding visual coding and natural vision is discussed.|\u89c6\u89c9\u901a\u8def\u6d89\u53ca\u590d\u6742\u7684\u7ec6\u80de\u548c\u533a\u57df\u7f51\u7edc\uff0c\u6709\u52a9\u4e8e\u89c6\u89c9\u4fe1\u606f\u7684\u7f16\u7801\u548c\u5904\u7406\u3002\u867d\u7136\u89c6\u89c9\u611f\u77e5\u7684\u67d0\u4e9b\u65b9\u9762\u5df2\u88ab\u4e86\u89e3\uff0c\u4f46\u5173\u4e8e\u89c6\u89c9\u7f16\u7801\u7684\u786e\u5207\u673a\u5236\u548c\u89c6\u89c9\u4fe1\u606f\u6cbf\u901a\u8def\u7684\u7ec4\u7ec7\uff0c\u4ecd\u7136\u5b58\u5728\u8bb8\u591a\u672a\u89e3\u7b54\u7684\u95ee\u9898\u3002\u672c\u7ae0\u8ba8\u8bba\u89c6\u89c9\u611f\u77e5\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u4e0e\u7406\u89e3\u89c6\u89c9\u4fe1\u606f\u5982\u4f55\u5728\u5927\u8111\u4e2d\u7f16\u7801\u548c\u8868\u793a\u76f8\u5173\u7684\u6311\u6218\u3002\u6b64\u5916\uff0c\u672c\u7ae0\u4ecb\u7ecd\u4e86\u795e\u7ecf\u5047\u4f53\u7684\u6982\u5ff5\uff1a\u65e8\u5728\u589e\u5f3a\u6216\u66ff\u4ee3\u8eab\u4f53\u529f\u80fd\u7684\u8bbe\u5907\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5b9e\u65bd\u6b64\u7c7b\u8bbe\u5907\u65f6\u6784\u5efa\u89c6\u89c9\u901a\u8def\u8ba1\u7b97\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002\u6982\u8ff0\u4e86\u8bb8\u591a\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6b64\u7c7b\u6a21\u578b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u5bf9\u7406\u89e3\u89c6\u89c9\u7f16\u7801\u548c\u81ea\u7136\u89c6\u89c9\u7684\u4ef7\u503c\u3002|[2401.03639v1](http://arxiv.org/pdf/2401.03639v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.03851": "|**2024-01-08**|**Aligned with LLM: a new multi-modal training paradigm for encoding fMRI activity in visual cortex**|\u4e0e\u6cd5\u5b66\u7855\u58eb\u4e00\u81f4\uff1a\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u5f0f\u8bad\u7ec3\u8303\u4f8b\uff0c\u7528\u4e8e\u7f16\u7801\u89c6\u89c9\u76ae\u5c42\u7684\u529f\u80fd\u78c1\u5171\u632f\u6210\u50cf\u6d3b\u52a8|Shuxiao Ma, Linyuan Wang, Senbao Hou, Bin Yan|Recently, there has been a surge in the popularity of pre trained large language models (LLMs) (such as GPT-4), sweeping across the entire Natural Language Processing (NLP) and Computer Vision (CV) communities. These LLMs have demonstrated advanced multi-modal understanding capabilities and showcased strong performance across various benchmarks. The LLM has started to embody traits of artificial general intelligence, which holds vital guidance for enhancing brain-like characteristics within visual encoding models. Hence, This paper proposes a new multi-modal training paradigm, aligning with LLM, for encoding fMRI activity in visual cortex. Based on this paradigm, we trained an encoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM). Specifically, we utilize LLM (miniGPT4) to generate descriptive text for all stimulus images, forming a high-quality textual description set. Moreover, we use the pre-trained text encoder (CLIP) to process these detailed descriptions, obtaining the text embedding features. Next, we use the contrast loss function to minimize the distance between the image embedding features and the text embedding features to complete the alignment operation of the stimulus image and text information. With the assistance of the pre-trained LLM, this alignment process facilitates better learning of the visual encoding model, resulting in higher precision. The final experimental results indicate that our training paradigm has significantly aided in enhancing the performance of the visual encoding model.|\u6700\u8fd1\uff0c\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\uff08\u4f8b\u5982 GPT-4\uff09\u7684\u6d41\u884c\u5ea6\u6fc0\u589e\uff0c\u5e2d\u5377\u4e86\u6574\u4e2a\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u548c\u8ba1\u7b97\u673a\u89c6\u89c9 (CV) \u793e\u533a\u3002\u8fd9\u4e9b\u6cd5\u5b66\u7855\u58eb\u5c55\u793a\u4e86\u5148\u8fdb\u7684\u591a\u6a21\u5f0f\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u5f3a\u52b2\u7684\u8868\u73b0\u3002\u6cd5\u5b66\u7855\u58eb\u5df2\u7ecf\u5f00\u59cb\u4f53\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u7279\u5f81\uff0c\u5b83\u4e3a\u589e\u5f3a\u89c6\u89c9\u7f16\u7801\u6a21\u578b\u4e2d\u7684\u7c7b\u8111\u7279\u5f81\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u6cd5\u5b66\u7855\u58eb\u76f8\u7ed3\u5408\u7684\u65b0\u7684\u591a\u6a21\u5f0f\u8bad\u7ec3\u8303\u5f0f\uff0c\u7528\u4e8e\u7f16\u7801\u89c6\u89c9\u76ae\u5c42\u7684\u529f\u80fd\u78c1\u5171\u632f\u6210\u50cf\u6d3b\u52a8\u3002\u57fa\u4e8e\u8fd9\u4e2a\u8303\u5f0f\uff0c\u6211\u4eec\u5728 fMRI \u6570\u636e\u4e2d\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7f16\u7801\u6a21\u578b\uff0c\u540d\u4e3a LLM-\u89c6\u89c9\u7f16\u7801\u6a21\u578b (LLM-VEM)\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528LLM\uff08miniGPT4\uff09\u4e3a\u6240\u6709\u523a\u6fc0\u56fe\u50cf\u751f\u6210\u63cf\u8ff0\u6027\u6587\u672c\uff0c\u5f62\u6210\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u63cf\u8ff0\u96c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u7f16\u7801\u5668\uff08CLIP\uff09\u6765\u5904\u7406\u8fd9\u4e9b\u8be6\u7ec6\u63cf\u8ff0\uff0c\u83b7\u5f97\u6587\u672c\u5d4c\u5165\u7279\u5f81\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u4f7f\u7528\u5bf9\u6bd4\u5ea6\u635f\u5931\u51fd\u6570\u6765\u6700\u5c0f\u5316\u56fe\u50cf\u5d4c\u5165\u7279\u5f81\u548c\u6587\u672c\u5d4c\u5165\u7279\u5f81\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u4ee5\u5b8c\u6210\u523a\u6fc0\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u606f\u7684\u5bf9\u9f50\u64cd\u4f5c\u3002\u5728\u9884\u8bad\u7ec3\u7684LLM\u7684\u5e2e\u52a9\u4e0b\uff0c\u8fd9\u4e2a\u5bf9\u9f50\u8fc7\u7a0b\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u5b66\u4e60\u89c6\u89c9\u7f16\u7801\u6a21\u578b\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002\u6700\u7ec8\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u8bad\u7ec3\u8303\u5f0f\u5bf9\u589e\u5f3a\u89c6\u89c9\u7f16\u7801\u6a21\u578b\u7684\u6027\u80fd\u6709\u663e\u7740\u5e2e\u52a9\u3002|[2401.03851v1](http://arxiv.org/pdf/2401.03851v1)|null|\n", "2401.03806": "|**2024-01-08**|**FM-AE: Frequency-masked Multimodal Autoencoder for Zinc Electrolysis Plate Contact Abnormality Detection**|FM-AE\uff1a\u7528\u4e8e\u950c\u7535\u89e3\u677f\u63a5\u89e6\u5f02\u5e38\u68c0\u6d4b\u7684\u9891\u7387\u5c4f\u853d\u591a\u6a21\u6001\u81ea\u52a8\u7f16\u7801\u5668|Canzong Zhou, Can Zhou, Hongqiu Zhu, Tianhao Liu|Zinc electrolysis is one of the key processes in zinc smelting, and maintaining stable operation of zinc electrolysis is an important factor in ensuring production efficiency and product quality. However, poor contact between the zinc electrolysis cathode and the anode is a common problem that leads to reduced production efficiency and damage to the electrolysis cell. Therefore, online monitoring of the contact status of the plates is crucial for ensuring production quality and efficiency. To address this issue, we propose an end-to-end network, the Frequency-masked Multimodal Autoencoder (FM-AE). This method takes the cell voltage signal and infrared image information as input, and through automatic encoding, fuses the two features together and predicts the poor contact status of the plates through a cascaded detector. Experimental results show that the proposed method maintains high accuracy (86.2%) while having good robustness and generalization ability, effectively detecting poor contact status of the zinc electrolysis cell, providing strong support for production practice.|\u950c\u7535\u89e3\u662f\u950c\u51b6\u70bc\u7684\u5173\u952e\u5de5\u5e8f\u4e4b\u4e00\uff0c\u4fdd\u6301\u950c\u7535\u89e3\u7a33\u5b9a\u8fd0\u884c\u662f\u4fdd\u8bc1\u751f\u4ea7\u6548\u7387\u548c\u4ea7\u54c1\u8d28\u91cf\u7684\u91cd\u8981\u56e0\u7d20\u3002\u7136\u800c\uff0c\u950c\u7535\u89e3\u9634\u6781\u548c\u9633\u6781\u4e4b\u95f4\u7684\u63a5\u89e6\u4e0d\u826f\u662f\u5bfc\u81f4\u751f\u4ea7\u6548\u7387\u964d\u4f4e\u548c\u7535\u89e3\u69fd\u635f\u574f\u7684\u5e38\u89c1\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u5728\u7ebf\u76d1\u6d4b\u677f\u6750\u7684\u63a5\u89e6\u72b6\u6001\u5bf9\u4e8e\u4fdd\u8bc1\u751f\u4ea7\u8d28\u91cf\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u5373\u9891\u7387\u63a9\u853d\u591a\u6a21\u6001\u81ea\u52a8\u7f16\u7801\u5668\uff08FM-AE\uff09\u3002\u8be5\u65b9\u6cd5\u4ee5\u7535\u6c60\u7535\u538b\u4fe1\u53f7\u548c\u7ea2\u5916\u56fe\u50cf\u4fe1\u606f\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u81ea\u52a8\u7f16\u7801\uff0c\u5c06\u4e24\u79cd\u7279\u5f81\u878d\u5408\u5728\u4e00\u8d77\uff0c\u5e76\u901a\u8fc7\u7ea7\u8054\u68c0\u6d4b\u5668\u9884\u6d4b\u6781\u677f\u7684\u4e0d\u826f\u63a5\u89e6\u72b6\u6001\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u7387\uff0886.2%\uff09\uff0c\u540c\u65f6\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u6548\u68c0\u6d4b\u51fa\u950c\u7535\u89e3\u69fd\u7684\u63a5\u89e6\u4e0d\u826f\u72b6\u6001\uff0c\u4e3a\u751f\u4ea7\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u652f\u6301\u3002|[2401.03806v1](http://arxiv.org/pdf/2401.03806v1)|null|\n"}, "LLM": {}, "Transformer": {"2401.03912": "|**2024-01-08**|**Attention-Guided Erasing: A Novel Augmentation Method for Enhancing Downstream Breast Density Classification**|\u6ce8\u610f\u529b\u5f15\u5bfc\u64e6\u9664\uff1a\u4e00\u79cd\u589e\u5f3a\u4e0b\u6e38\u4e73\u817a\u5bc6\u5ea6\u5206\u7c7b\u7684\u65b0\u578b\u589e\u5f3a\u65b9\u6cd5|Adarsh Bhandary Panambur, Hui Yu, Sheethal Bhat, Prathmesh Madhu, Siming Bayer, Andreas Maier|The assessment of breast density is crucial in the context of breast cancer screening, especially in populations with a higher percentage of dense breast tissues. This study introduces a novel data augmentation technique termed Attention-Guided Erasing (AGE), devised to enhance the downstream classification of four distinct breast density categories in mammography following the BI-RADS recommendation in the Vietnamese cohort. The proposed method integrates supplementary information during transfer learning, utilizing visual attention maps derived from a vision transformer backbone trained using the self-supervised DINO method. These maps are utilized to erase background regions in the mammogram images, unveiling only the potential areas of dense breast tissues to the network. Through the incorporation of AGE during transfer learning with varying random probabilities, we consistently surpass classification performance compared to scenarios without AGE and the traditional random erasing transformation. We validate our methodology using the publicly available VinDr-Mammo dataset. Specifically, we attain a mean F1-score of 0.5910, outperforming values of 0.5594 and 0.5691 corresponding to scenarios without AGE and with random erasing (RE), respectively. This superiority is further substantiated by t-tests, revealing a p-value of p<0.0001, underscoring the statistical significance of our approach.|\u4e73\u817a\u5bc6\u5ea6\u7684\u8bc4\u4f30\u5728\u4e73\u817a\u764c\u7b5b\u67e5\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u4e73\u817a\u7ec4\u7ec7\u81f4\u5bc6\u6bd4\u4f8b\u8f83\u9ad8\u7684\u4eba\u7fa4\u4e2d\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u79f0\u4e3a\u6ce8\u610f\u529b\u5f15\u5bfc\u64e6\u9664\uff08AGE\uff09\u7684\u65b0\u578b\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u65e8\u5728\u9075\u5faa\u8d8a\u5357\u961f\u5217\u4e2d\u7684 BI-RADS \u5efa\u8bae\uff0c\u589e\u5f3a\u4e73\u623f X \u5149\u68c0\u67e5\u4e2d\u56db\u79cd\u4e0d\u540c\u4e73\u817a\u5bc6\u5ea6\u7c7b\u522b\u7684\u4e0b\u6e38\u5206\u7c7b\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8fc1\u79fb\u5b66\u4e60\u671f\u95f4\u96c6\u6210\u4e86\u8865\u5145\u4fe1\u606f\uff0c\u5229\u7528\u4ece\u4f7f\u7528\u81ea\u76d1\u7763 DINO \u65b9\u6cd5\u8bad\u7ec3\u7684\u89c6\u89c9\u53d8\u6362\u5668\u9aa8\u5e72\u5bfc\u51fa\u7684\u89c6\u89c9\u6ce8\u610f\u56fe\u3002\u8fd9\u4e9b\u56fe\u7528\u4e8e\u64e6\u9664\u4e73\u623fX\u5149\u68c0\u67e5\u56fe\u50cf\u4e2d\u7684\u80cc\u666f\u533a\u57df\uff0c\u4ec5\u5411\u7f51\u7edc\u63ed\u793a\u81f4\u5bc6\u4e73\u817a\u7ec4\u7ec7\u7684\u6f5c\u5728\u533a\u57df\u3002\u901a\u8fc7\u5728\u5177\u6709\u4e0d\u540c\u968f\u673a\u6982\u7387\u7684\u8fc1\u79fb\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7ed3\u5408 AGE\uff0c\u4e0e\u6ca1\u6709 AGE \u548c\u4f20\u7edf\u968f\u673a\u64e6\u9664\u53d8\u6362\u7684\u573a\u666f\u76f8\u6bd4\uff0c\u6211\u4eec\u59cb\u7ec8\u8d85\u8d8a\u5206\u7c7b\u6027\u80fd\u3002\u6211\u4eec\u4f7f\u7528\u516c\u5f00\u7684 VinDr-Mammo \u6570\u636e\u96c6\u9a8c\u8bc1\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u83b7\u5f97\u4e86 0.5910 \u7684\u5e73\u5747 F1 \u5206\u6570\uff0c\u5206\u522b\u4f18\u4e8e\u6ca1\u6709 AGE \u548c\u968f\u673a\u64e6\u9664 (RE) \u60c5\u51b5\u4e0b\u5bf9\u5e94\u7684\u503c 0.5594 \u548c 0.5691\u3002 t \u68c0\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8fd9\u79cd\u4f18\u8d8a\u6027\uff0c\u663e\u793a p \u503c\u4e3a p<0.0001\uff0c\u5f3a\u8c03\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u7edf\u8ba1\u663e\u7740\u6027\u3002|[2401.03912v1](http://arxiv.org/pdf/2401.03912v1)|null|\n", "2401.03901": "|**2024-01-08**|**STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for Video Question Answering**|STAIR\uff1a\u7528\u4e8e\u89c6\u9891\u95ee\u7b54\u7684\u5177\u6709\u53ef\u5ba1\u6838\u4e2d\u95f4\u7ed3\u679c\u7684\u65f6\u7a7a\u63a8\u7406|Yueqian Wang, Yuxuan Wang, Kai Chen, Dongyan Zhao|Recently we have witnessed the rapid development of video question answering models. However, most models can only handle simple videos in terms of temporal reasoning, and their performance tends to drop when answering temporal-reasoning questions on long and informative videos. To tackle this problem we propose STAIR, a Spatial-Temporal Reasoning model with Auditable Intermediate Results for video question answering. STAIR is a neural module network, which contains a program generator to decompose a given question into a hierarchical combination of several sub-tasks, and a set of lightweight neural modules to complete each of these sub-tasks. Though neural module networks are already widely studied on image-text tasks, applying them to videos is a non-trivial task, as reasoning on videos requires different abilities. In this paper, we define a set of basic video-text sub-tasks for video question answering and design a set of lightweight modules to complete them. Different from most prior works, modules of STAIR return intermediate outputs specific to their intentions instead of always returning attention maps, which makes it easier to interpret and collaborate with pre-trained models. We also introduce intermediate supervision to make these intermediate outputs more accurate. We conduct extensive experiments on several video question answering datasets under various settings to show STAIR's performance, explainability, compatibility with pre-trained models, and applicability when program annotations are not available. Code: https://github.com/yellow-binary-tree/STAIR|\u6700\u8fd1\u6211\u4eec\u89c1\u8bc1\u4e86\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u6a21\u578b\u53ea\u80fd\u5728\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u5904\u7406\u7b80\u5355\u7684\u89c6\u9891\uff0c\u5e76\u4e14\u5728\u56de\u7b54\u957f\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u9891\u4e0a\u7684\u65f6\u95f4\u63a8\u7406\u95ee\u9898\u65f6\uff0c\u5176\u6027\u80fd\u5f80\u5f80\u4f1a\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 STAIR\uff0c\u4e00\u79cd\u65f6\u7a7a\u63a8\u7406\u6a21\u578b\uff0c\u5177\u6709\u7528\u4e8e\u89c6\u9891\u95ee\u7b54\u7684\u53ef\u5ba1\u6838\u4e2d\u95f4\u7ed3\u679c\u3002 STAIR \u662f\u4e00\u4e2a\u795e\u7ecf\u6a21\u5757\u7f51\u7edc\uff0c\u5b83\u5305\u542b\u4e00\u4e2a\u7a0b\u5e8f\u751f\u6210\u5668\uff0c\u7528\u4e8e\u5c06\u7ed9\u5b9a\u95ee\u9898\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\u7684\u5206\u5c42\u7ec4\u5408\uff0c\u4ee5\u53ca\u4e00\u7ec4\u8f7b\u91cf\u7ea7\u795e\u7ecf\u6a21\u5757\u6765\u5b8c\u6210\u6bcf\u4e2a\u5b50\u4efb\u52a1\u3002\u5c3d\u7ba1\u795e\u7ecf\u6a21\u5757\u7f51\u7edc\u5df2\u7ecf\u5728\u56fe\u50cf\u6587\u672c\u4efb\u52a1\u4e0a\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\uff0c\u4f46\u5c06\u5b83\u4eec\u5e94\u7528\u4e8e\u89c6\u9891\u5e76\u4e0d\u662f\u4e00\u4ef6\u7b80\u5355\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u89c6\u9891\u63a8\u7406\u9700\u8981\u4e0d\u540c\u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u7ec4\u7528\u4e8e\u89c6\u9891\u95ee\u7b54\u7684\u57fa\u672c\u89c6\u9891\u6587\u672c\u5b50\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u7ec4\u8f7b\u91cf\u7ea7\u6a21\u5757\u6765\u5b8c\u6210\u5b83\u4eec\u3002\u4e0e\u5927\u591a\u6570\u5148\u524d\u7684\u5de5\u4f5c\u4e0d\u540c\uff0cSTAIR \u7684\u6a21\u5757\u8fd4\u56de\u7279\u5b9a\u4e8e\u5176\u610f\u56fe\u7684\u4e2d\u95f4\u8f93\u51fa\uff0c\u800c\u4e0d\u662f\u603b\u662f\u8fd4\u56de\u6ce8\u610f\u529b\u56fe\uff0c\u8fd9\u4f7f\u5f97\u66f4\u5bb9\u6613\u89e3\u91ca\u548c\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u534f\u4f5c\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e2d\u95f4\u76d1\u7763\uff0c\u4f7f\u8fd9\u4e9b\u4e2d\u95f4\u8f93\u51fa\u66f4\u52a0\u51c6\u786e\u3002\u6211\u4eec\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u5bf9\u591a\u4e2a\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u5c55\u793a STAIR \u7684\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u3001\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u517c\u5bb9\u6027\u4ee5\u53ca\u7a0b\u5e8f\u6ce8\u91ca\u4e0d\u53ef\u7528\u65f6\u7684\u9002\u7528\u6027\u3002\u4ee3\u7801\uff1ahttps://github.com/yellow-binary-tree/STAIR|[2401.03901v1](http://arxiv.org/pdf/2401.03901v1)|null|\n", "2401.03870": "|**2024-01-08**|**Gramformer: Learning Crowd Counting via Graph-Modulated Transformer**|Gramformer\uff1a\u901a\u8fc7\u56fe\u5f62\u8c03\u5236\u53d8\u538b\u5668\u5b66\u4e60\u4eba\u7fa4\u8ba1\u6570|Hui Lin, Zhiheng Ma, Xiaopeng Hong, Qinnan Shangguan, Deyu Meng|Transformer has been popular in recent crowd counting work since it breaks the limited receptive field of traditional CNNs. However, since crowd images always contain a large number of similar patches, the self-attention mechanism in Transformer tends to find a homogenized solution where the attention maps of almost all patches are identical. In this paper, we address this problem by proposing Gramformer: a graph-modulated transformer to enhance the network by adjusting the attention and input node features respectively on the basis of two different types of graphs. Firstly, an attention graph is proposed to diverse attention maps to attend to complementary information. The graph is building upon the dissimilarities between patches, modulating the attention in an anti-similarity fashion. Secondly, a feature-based centrality encoding is proposed to discover the centrality positions or importance of nodes. We encode them with a proposed centrality indices scheme to modulate the node features and similarity relationships. Extensive experiments on four challenging crowd counting datasets have validated the competitiveness of the proposed method. Code is available at {https://github.com/LoraLinH/Gramformer}.|Transformer \u5728\u6700\u8fd1\u7684\u4eba\u7fa4\u7edf\u8ba1\u5de5\u4f5c\u4e2d\u5f88\u53d7\u6b22\u8fce\uff0c\u56e0\u4e3a\u5b83\u6253\u7834\u4e86\u4f20\u7edf CNN \u6709\u9650\u7684\u611f\u53d7\u91ce\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4eba\u7fa4\u56fe\u50cf\u603b\u662f\u5305\u542b\u5927\u91cf\u76f8\u4f3c\u7684\u8865\u4e01\uff0cTransformer \u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u503e\u5411\u4e8e\u627e\u5230\u4e00\u4e2a\u540c\u8d28\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u4e2d\u51e0\u4e4e\u6240\u6709\u8865\u4e01\u7684\u6ce8\u610f\u529b\u56fe\u90fd\u662f\u76f8\u540c\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa Gramformer \u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1a\u4e00\u79cd\u56fe\u8c03\u5236\u53d8\u538b\u5668\uff0c\u901a\u8fc7\u6839\u636e\u4e24\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u56fe\u5206\u522b\u8c03\u6574\u6ce8\u610f\u529b\u548c\u8f93\u5165\u8282\u70b9\u7279\u5f81\u6765\u589e\u5f3a\u7f51\u7edc\u3002\u9996\u5148\uff0c\u9488\u5bf9\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u56fe\u63d0\u51fa\u4e86\u6ce8\u610f\u529b\u56fe\u6765\u5173\u6ce8\u8865\u5145\u4fe1\u606f\u3002\u8be5\u56fe\u5efa\u7acb\u5728\u8865\u4e01\u4e4b\u95f4\u7684\u5dee\u5f02\u4e4b\u4e0a\uff0c\u4ee5\u53cd\u76f8\u4f3c\u7684\u65b9\u5f0f\u8c03\u8282\u6ce8\u610f\u529b\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u7279\u5f81\u7684\u4e2d\u5fc3\u6027\u7f16\u7801\u6765\u53d1\u73b0\u8282\u70b9\u7684\u4e2d\u5fc3\u6027\u4f4d\u7f6e\u6216\u91cd\u8981\u6027\u3002\u6211\u4eec\u4f7f\u7528\u63d0\u51fa\u7684\u4e2d\u5fc3\u6027\u6307\u6570\u65b9\u6848\u5bf9\u5b83\u4eec\u8fdb\u884c\u7f16\u7801\uff0c\u4ee5\u8c03\u6574\u8282\u70b9\u7279\u5f81\u548c\u76f8\u4f3c\u6027\u5173\u7cfb\u3002\u5bf9\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4eba\u7fa4\u8ba1\u6570\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u7ade\u4e89\u529b\u3002\u4ee3\u7801\u53ef\u5728 {https://github.com/LoraLinH/Gramformer} \u83b7\u53d6\u3002|[2401.03870v1](http://arxiv.org/pdf/2401.03870v1)|null|\n", "2401.03792": "|**2024-01-08**|**Monitoring water contaminants in coastal areas through ML algorithms leveraging atmospherically corrected Sentinel-2 data**|\u5229\u7528\u7ecf\u5927\u6c14\u6821\u6b63\u7684 Sentinel-2 \u6570\u636e\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u76d1\u6d4b\u6cbf\u6d77\u5730\u533a\u7684\u6c34\u6c61\u67d3\u7269|Francesca Razzano, Francesco Mauro, Pietro Di Stasio, Gabriele Meoni, Marco Esposito, Gilda Schirinzi, Silvia Liberata Ullo|Monitoring water contaminants is of paramount importance, ensuring public health and environmental well-being. Turbidity, a key parameter, poses a significant problem, affecting water quality. Its accurate assessment is crucial for safeguarding ecosystems and human consumption, demanding meticulous attention and action. For this, our study pioneers a novel approach to monitor the Turbidity contaminant, integrating CatBoost Machine Learning (ML) with high-resolution data from Sentinel-2 Level-2A. Traditional methods are labor-intensive while CatBoost offers an efficient solution, excelling in predictive accuracy. Leveraging atmospherically corrected Sentinel-2 data through the Google Earth Engine (GEE), our study contributes to scalable and precise Turbidity monitoring. A specific tabular dataset derived from Hong Kong contaminants monitoring stations enriches our study, providing region-specific insights. Results showcase the viability of this integrated approach, laying the foundation for adopting advanced techniques in global water quality management.|\u76d1\u6d4b\u6c34\u6c61\u67d3\u7269\u5bf9\u4e8e\u786e\u4fdd\u516c\u4f17\u5065\u5eb7\u548c\u73af\u5883\u798f\u7949\u81f3\u5173\u91cd\u8981\u3002\u6d4a\u5ea6\u662f\u4e00\u4e2a\u5173\u952e\u53c2\u6570\uff0c\u9020\u6210\u4e86\u5f71\u54cd\u6c34\u8d28\u7684\u91cd\u5927\u95ee\u9898\u3002\u5176\u51c6\u786e\u8bc4\u4f30\u5bf9\u4e8e\u4fdd\u62a4\u751f\u6001\u7cfb\u7edf\u548c\u4eba\u7c7b\u6d88\u8d39\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8ba4\u771f\u5173\u6ce8\u5e76\u91c7\u53d6\u884c\u52a8\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u7684\u7814\u7a76\u5f00\u521b\u4e86\u4e00\u79cd\u76d1\u6d4b\u6d4a\u5ea6\u6c61\u67d3\u7269\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06 CatBoost \u673a\u5668\u5b66\u4e60 (ML) \u4e0e Sentinel-2 Level-2A \u7684\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u76f8\u96c6\u6210\u3002\u4f20\u7edf\u65b9\u6cd5\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\uff0c\u800c CatBoost \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u6211\u4eec\u7684\u7814\u7a76\u901a\u8fc7 Google Earth Engine (GEE) \u5229\u7528\u7ecf\u8fc7\u5927\u6c14\u6821\u6b63\u7684 Sentinel-2 \u6570\u636e\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u7cbe\u786e\u7684\u6d4a\u5ea6\u76d1\u6d4b\u3002\u6765\u81ea\u9999\u6e2f\u6c61\u67d3\u7269\u76d1\u6d4b\u7ad9\u7684\u7279\u5b9a\u8868\u683c\u6570\u636e\u96c6\u4e30\u5bcc\u4e86\u6211\u4eec\u7684\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u9488\u5bf9\u7279\u5b9a\u533a\u57df\u7684\u89c1\u89e3\u3002\u7ed3\u679c\u5c55\u793a\u4e86\u8fd9\u79cd\u7efc\u5408\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5728\u5168\u7403\u6c34\u8d28\u7ba1\u7406\u4e2d\u91c7\u7528\u5148\u8fdb\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\u3002|[2401.03792v1](http://arxiv.org/pdf/2401.03792v1)|null|\n", "2401.03785": "|**2024-01-08**|**Identifying Important Group of Pixels using Interactions**|\u4f7f\u7528\u4ea4\u4e92\u8bc6\u522b\u91cd\u8981\u7684\u50cf\u7d20\u7ec4|Kosuke Sumiyasu, Kazuhiko Kawamoto, Hiroshi Kera|To better understand the behavior of image classifiers, it is useful to visualize the contribution of individual pixels to the model prediction. In this study, we propose a method, MoXI~($\\textbf{Mo}$del e$\\textbf{X}$planation by $\\textbf{I}$nteractions), that efficiently and accurately identifies a group of pixels with high prediction confidence. The proposed method employs game-theoretic concepts, Shapley values and interactions, taking into account the effects of individual pixels and the cooperative influence of pixels on model confidence. Theoretical analysis and experiments demonstrate that our method better identifies the pixels that are highly contributing to the model outputs than widely-used visualization methods using Grad-CAM, Attention rollout, and Shapley value. While prior studies have suffered from the exponential computational cost in the computation of Shapley value and interactions, we show that this can be reduced to linear cost for our task.|\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u884c\u4e3a\uff0c\u53ef\u89c6\u5316\u5355\u4e2a\u50cf\u7d20\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u8d21\u732e\u975e\u5e38\u6709\u7528\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5 MoXI~($\\textbf{Mo}$del e$\\textbf{X}$planation by $\\textbf{I}$nteractions)\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u9ad8\u6548\u3001\u51c6\u786e\u5730\u8bc6\u522b\u4e00\u7ec4\u50cf\u7d20\u9ad8\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u535a\u5f08\u8bba\u6982\u5ff5\u3001Shapley \u503c\u548c\u4ea4\u4e92\u4f5c\u7528\uff0c\u8003\u8651\u5230\u5355\u4e2a\u50cf\u7d20\u7684\u5f71\u54cd\u4ee5\u53ca\u50cf\u7d20\u5bf9\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u534f\u540c\u5f71\u54cd\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5e7f\u6cdb\u4f7f\u7528\u7684\u4f7f\u7528 Grad-CAM\u3001Attention rollout \u548c Shapley \u503c\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u66f4\u597d\u5730\u8bc6\u522b\u5bf9\u6a21\u578b\u8f93\u51fa\u8d21\u732e\u8f83\u5927\u7684\u50cf\u7d20\u3002\u867d\u7136\u4e4b\u524d\u7684\u7814\u7a76\u5728\u8ba1\u7b97 Shapley \u503c\u548c\u76f8\u4e92\u4f5c\u7528\u65f6\u9047\u5230\u4e86\u6307\u6570\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\uff0c\u4f46\u6211\u4eec\u8868\u660e\uff0c\u5bf9\u4e8e\u6211\u4eec\u7684\u4efb\u52a1\u6765\u8bf4\uff0c\u8fd9\u53ef\u4ee5\u51cf\u5c11\u4e3a\u7ebf\u6027\u6210\u672c\u3002|[2401.03785v1](http://arxiv.org/pdf/2401.03785v1)|null|\n", "2401.03707": "|**2024-01-08**|**FMA-Net: Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring**|FMA-Net\uff1a\u6d41\u5f15\u5bfc\u52a8\u6001\u8fc7\u6ee4\u548c\u8fed\u4ee3\u7279\u5f81\u7ec6\u5316\uff0c\u7528\u4e8e\u8054\u5408\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u6a21\u7cca|Geunhyuk Youk, Jihyong Oh, Munchurl Kim|We present a joint learning scheme of video super-resolution and deblurring, called VSRDB, to restore clean high-resolution (HR) videos from blurry low-resolution (LR) ones. This joint restoration problem has drawn much less attention compared to single restoration problems. In this paper, we propose a novel flow-guided dynamic filtering (FGDF) and iterative feature refinement with multi-attention (FRMA), which constitutes our VSRDB framework, denoted as FMA-Net. Specifically, our proposed FGDF enables precise estimation of both spatio-temporally-variant degradation and restoration kernels that are aware of motion trajectories through sophisticated motion representation learning. Compared to conventional dynamic filtering, the FGDF enables the FMA-Net to effectively handle large motions into the VSRDB. Additionally, the stacked FRMA blocks trained with our novel temporal anchor (TA) loss, which temporally anchors and sharpens features, refine features in a course-to-fine manner through iterative updates. Extensive experiments demonstrate the superiority of the proposed FMA-Net over state-of-the-art methods in terms of both quantitative and qualitative quality. Codes and pre-trained models are available at: https://kaist-viclab.github.io/fmanet-site|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u6a21\u7cca\u7684\u8054\u5408\u5b66\u4e60\u65b9\u6848\uff0c\u79f0\u4e3a VSRDB\uff0c\u7528\u4e8e\u4ece\u6a21\u7cca\u7684\u4f4e\u5206\u8fa8\u7387 (LR) \u89c6\u9891\u4e2d\u6062\u590d\u5e72\u51c0\u7684\u9ad8\u5206\u8fa8\u7387 (HR) \u89c6\u9891\u3002\u4e0e\u5355\u4e00\u6062\u590d\u95ee\u9898\u76f8\u6bd4\uff0c\u8fd9\u79cd\u8054\u5408\u6062\u590d\u95ee\u9898\u5f15\u8d77\u7684\u5173\u6ce8\u8981\u5c11\u5f97\u591a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d41\u5f15\u5bfc\u52a8\u6001\u8fc7\u6ee4\uff08FGDF\uff09\u548c\u591a\u6ce8\u610f\u529b\u8fed\u4ee3\u7279\u5f81\u7ec6\u5316\uff08FRMA\uff09\uff0c\u5b83\u6784\u6210\u4e86\u6211\u4eec\u7684 VSRDB \u6846\u67b6\uff0c\u8868\u793a\u4e3a FMA-Net\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u7684 FGDF \u80fd\u591f\u7cbe\u786e\u4f30\u8ba1\u65f6\u7a7a\u53d8\u5316\u7684\u9000\u5316\u548c\u6062\u590d\u5185\u6838\uff0c\u8fd9\u4e9b\u5185\u6838\u901a\u8fc7\u590d\u6742\u7684\u8fd0\u52a8\u8868\u793a\u5b66\u4e60\u6765\u4e86\u89e3\u8fd0\u52a8\u8f68\u8ff9\u3002\u4e0e\u4f20\u7edf\u7684\u52a8\u6001\u8fc7\u6ee4\u76f8\u6bd4\uff0cFGDF \u4f7f FMA-Net \u80fd\u591f\u6709\u6548\u5904\u7406 VSRDB \u4e2d\u7684\u5927\u8fd0\u52a8\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u6211\u4eec\u65b0\u9896\u7684\u65f6\u95f4\u951a\u70b9\uff08TA\uff09\u635f\u5931\u8bad\u7ec3\u7684\u5806\u53e0 FRMA \u5757\u53ef\u4ee5\u6682\u65f6\u951a\u5b9a\u548c\u9510\u5316\u7279\u5f81\uff0c\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u4ee5\u4ece\u7c97\u5230\u7ec6\u7684\u65b9\u5f0f\u7ec6\u5316\u7279\u5f81\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 FMA-Net \u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u83b7\u53d6\uff1ahttps://kaist-viclab.github.io/fmanet-site|[2401.03707v1](http://arxiv.org/pdf/2401.03707v1)|null|\n", "2401.03694": "|**2024-01-08**|**GloTSFormer: Global Video Text Spotting Transformer**|GloTSFormer\uff1a\u5168\u7403\u89c6\u9891\u6587\u672c\u8bc6\u522b\u53d8\u538b\u5668|Han Wang, Yanjie Wang, Yang Li, Can Huang|Video Text Spotting (VTS) is a fundamental visual task that aims to predict the trajectories and content of texts in a video. Previous works usually conduct local associations and apply IoU-based distance and complex post-processing procedures to boost performance, ignoring the abundant temporal information and the morphological characteristics in VTS. In this paper, we propose a novel Global Video Text Spotting Transformer GloTSFormer to model the tracking problem as global associations and utilize the Gaussian Wasserstein distance to guide the morphological correlation between frames. Our main contributions can be summarized as three folds. 1). We propose a Transformer-based global tracking method GloTSFormer for VTS and associate multiple frames simultaneously. 2). We introduce a Wasserstein distance-based method to conduct positional associations between frames. 3). We conduct extensive experiments on public datasets. On the ICDAR2015 video dataset, GloTSFormer achieves 56.0 MOTA with 4.6 absolute improvement compared with the previous SOTA method and outperforms the previous Transformer-based method by a significant 8.3 MOTA.|\u89c6\u9891\u6587\u672c\u8bc6\u522b\uff08VTS\uff09\u662f\u4e00\u9879\u57fa\u672c\u7684\u89c6\u89c9\u4efb\u52a1\uff0c\u65e8\u5728\u9884\u6d4b\u89c6\u9891\u4e2d\u6587\u672c\u7684\u8f68\u8ff9\u548c\u5185\u5bb9\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u901a\u5e38\u8fdb\u884c\u5c40\u90e8\u5173\u8054\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e IoU \u7684\u8ddd\u79bb\u548c\u590d\u6742\u7684\u540e\u5904\u7406\u7a0b\u5e8f\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u5ffd\u7565\u4e86 VTS \u4e2d\u4e30\u5bcc\u7684\u65f6\u95f4\u4fe1\u606f\u548c\u5f62\u6001\u7279\u5f81\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5168\u5c40\u89c6\u9891\u6587\u672c\u8bc6\u522b\u8f6c\u6362\u5668 GloTSFormer\uff0c\u5c06\u8ddf\u8e2a\u95ee\u9898\u5efa\u6a21\u4e3a\u5168\u5c40\u5173\u8054\uff0c\u5e76\u5229\u7528\u9ad8\u65af Wasserstein \u8ddd\u79bb\u6765\u6307\u5bfc\u5e27\u4e4b\u95f4\u7684\u5f62\u6001\u76f8\u5173\u6027\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u53ef\u4ee5\u6982\u62ec\u4e3a\u4e09\u4e2a\u65b9\u9762\u3002 1\uff09\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Transformer \u7684 VTS \u5168\u5c40\u8ddf\u8e2a\u65b9\u6cd5 GloTSFormer\uff0c\u5e76\u540c\u65f6\u5173\u8054\u591a\u4e2a\u5e27\u3002 2\uff09\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e Wasserstein \u8ddd\u79bb\u7684\u65b9\u6cd5\u6765\u8fdb\u884c\u5e27\u4e4b\u95f4\u7684\u4f4d\u7f6e\u5173\u8054\u3002 3\uff09\u3002\u6211\u4eec\u5bf9\u516c\u5171\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5728 ICDAR2015 \u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0cGloTSFormer \u5b9e\u73b0\u4e86 56.0 MOTA\uff0c\u4e0e\u4e4b\u524d\u7684 SOTA \u65b9\u6cd5\u76f8\u6bd4\uff0c\u7edd\u5bf9\u63d0\u5347\u4e86 4.6\uff0c\u5e76\u4e14\u6bd4\u4e4b\u524d\u57fa\u4e8e Transformer \u7684\u65b9\u6cd5\u663e\u7740\u63d0\u9ad8\u4e86 8.3 MOTA\u3002|[2401.03694v1](http://arxiv.org/pdf/2401.03694v1)|null|\n"}, "3D/CG": {"2401.03922": "|**2024-01-08**|**Structure-focused Neurodegeneration Convolutional Neural Network for Modeling and Classification of Alzheimer's Disease**|\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u6c0f\u75c5\u5efa\u6a21\u548c\u5206\u7c7b\u7684\u7ed3\u6784\u805a\u7126\u795e\u7ecf\u53d8\u6027\u5377\u79ef\u795e\u7ecf\u7f51\u7edc|Simisola Odimayo, Chollette C. Olisah, Khadija Mohammed|Alzheimer's disease (AD), the predominant form of dementia, poses a growing global challenge and underscores the urgency of accurate and early diagnosis. The clinical technique radiologists adopt for distinguishing between mild cognitive impairment (MCI) and AD using Machine Resonance Imaging (MRI) encounter hurdles because they are not consistent and reliable. Machine learning has been shown to offer promise for early AD diagnosis. However, existing models focused on focal fine-grain features without considerations to focal structural features that give off information on neurodegeneration of the brain cerebral cortex. Therefore, this paper proposes a machine learning (ML) framework that integrates Gamma correction, an image enhancement technique, and includes a structure-focused neurodegeneration convolutional neural network (CNN) architecture called SNeurodCNN for discriminating between AD and MCI. The ML framework leverages the mid-sagittal and para-sagittal brain image viewpoints of the structure-focused Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Through experiments, our proposed machine learning framework shows exceptional performance. The parasagittal viewpoint set achieves 97.8% accuracy, with 97.0% specificity and 98.5% sensitivity. The midsagittal viewpoint is shown to present deeper insights into the structural brain changes given the increase in accuracy, specificity, and sensitivity, which are 98.1% 97.2%, and 99.0%, respectively. Using GradCAM technique, we show that our proposed model is capable of capturing the structural dynamics of MCI and AD which exist about the frontal lobe, occipital lobe, cerebellum, and parietal lobe. Therefore, our model itself as a potential brain structural change Digi-Biomarker for early diagnosis of AD.|\u963f\u5c14\u8328\u6d77\u9ed8\u6c0f\u75c5 (AD) \u662f\u75f4\u5446\u75c7\u7684\u4e3b\u8981\u5f62\u5f0f\uff0c\u7ed9\u5168\u7403\u5e26\u6765\u4e86\u65e5\u76ca\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u5e76\u5f3a\u8c03\u4e86\u51c6\u786e\u548c\u65e9\u671f\u8bca\u65ad\u7684\u7d27\u8feb\u6027\u3002\u653e\u5c04\u79d1\u533b\u751f\u91c7\u7528\u673a\u5668\u5171\u632f\u6210\u50cf (MRI) \u6765\u533a\u5206\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d (MCI) \u548c AD \u7684\u4e34\u5e8a\u6280\u672f\u9047\u5230\u4e86\u969c\u788d\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u4e00\u81f4\u4e14\u4e0d\u53ef\u9760\u3002\u673a\u5668\u5b66\u4e60\u5df2\u88ab\u8bc1\u660e\u4e3a\u65e9\u671f AD \u8bca\u65ad\u63d0\u4f9b\u4e86\u5e0c\u671b\u3002\u7136\u800c\uff0c\u73b0\u6709\u6a21\u578b\u4e13\u6ce8\u4e8e\u5c40\u7076\u6027\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u800c\u6ca1\u6709\u8003\u8651\u63d0\u4f9b\u5927\u8111\u76ae\u5c42\u795e\u7ecf\u53d8\u6027\u4fe1\u606f\u7684\u5c40\u7076\u6027\u7ed3\u6784\u7279\u5f81\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u4f3d\u739b\u6821\u6b63\uff08\u4e00\u79cd\u56fe\u50cf\u589e\u5f3a\u6280\u672f\uff09\uff0c\u5e76\u5305\u62ec\u4e00\u79cd\u540d\u4e3aSNeurodCNN\u7684\u4e13\u6ce8\u4e8e\u7ed3\u6784\u7684\u795e\u7ecf\u53d8\u6027\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u533a\u5206AD\u548cMCI\u3002\u673a\u5668\u5b66\u4e60\u6846\u67b6\u5229\u7528\u4ee5\u7ed3\u6784\u4e3a\u91cd\u70b9\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u6c0f\u75c5\u795e\u7ecf\u5f71\u50cf\u8ba1\u5212 (ADNI) \u6570\u636e\u96c6\u7684\u4e2d\u77e2\u72b6\u548c\u65c1\u77e2\u72b6\u8111\u56fe\u50cf\u89c2\u70b9\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u663e\u793a\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002\u65c1\u77e2\u72b6\u89c6\u70b9\u96c6\u7684\u51c6\u786e\u5ea6\u8fbe\u5230 97.8%\uff0c\u7279\u5f02\u6027\u4e3a 97.0%\uff0c\u7075\u654f\u5ea6\u4e3a 98.5%\u3002\u7531\u4e8e\u51c6\u786e\u6027\u3001\u7279\u5f02\u6027\u548c\u654f\u611f\u6027\u7684\u63d0\u9ad8\uff0c\u6b63\u4e2d\u77e2\u72b6\u89c6\u70b9\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u5927\u8111\u7ed3\u6784\u7684\u53d8\u5316\uff0c\u5206\u522b\u4e3a 98.1%\u300197.2% \u548c 99.0%\u3002\u4f7f\u7528 GradCAM \u6280\u672f\uff0c\u6211\u4eec\u8868\u660e\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u6355\u83b7\u5b58\u5728\u4e8e\u989d\u53f6\u3001\u6795\u53f6\u3001\u5c0f\u8111\u548c\u9876\u53f6\u7684 MCI \u548c AD \u7684\u7ed3\u6784\u52a8\u6001\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u6a21\u578b\u672c\u8eab\u53ef\u4ee5\u4f5c\u4e3a\u6f5c\u5728\u7684\u5927\u8111\u7ed3\u6784\u53d8\u5316\u7684\u6570\u5b57\u751f\u7269\u6807\u8bb0\uff0c\u7528\u4e8e AD \u7684\u65e9\u671f\u8bca\u65ad\u3002|[2401.03922v1](http://arxiv.org/pdf/2401.03922v1)|null|\n", "2401.03765": "|**2024-01-08**|**InvariantOODG: Learning Invariant Features of Point Clouds for Out-of-Distribution Generalization**|InvariantOODG\uff1a\u5b66\u4e60\u70b9\u4e91\u7684\u4e0d\u53d8\u7279\u5f81\u4ee5\u5b9e\u73b0\u5206\u5e03\u5916\u6cdb\u5316|Zhimin Zhang, Xiang Gao, Wei Hu|The convenience of 3D sensors has led to an increase in the use of 3D point clouds in various applications. However, the differences in acquisition devices or scenarios lead to divergence in the data distribution of point clouds, which requires good generalization of point cloud representation learning methods. While most previous methods rely on domain adaptation, which involves fine-tuning pre-trained models on target domain data, this may not always be feasible in real-world scenarios where target domain data may be unavailable. To address this issue, we propose InvariantOODG, which learns invariability between point clouds with different distributions using a two-branch network to extract local-to-global features from original and augmented point clouds. Specifically, to enhance local feature learning of point clouds, we define a set of learnable anchor points that locate the most useful local regions and two types of transformations to augment the input point clouds. The experimental results demonstrate the effectiveness of the proposed model on 3D domain generalization benchmarks.|3D \u4f20\u611f\u5668\u7684\u4fbf\u5229\u6027\u5bfc\u81f4 3D \u70b9\u4e91\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u589e\u52a0\u3002\u7136\u800c\uff0c\u91c7\u96c6\u8bbe\u5907\u6216\u573a\u666f\u7684\u5dee\u5f02\u5bfc\u81f4\u70b9\u4e91\u6570\u636e\u5206\u5e03\u7684\u53d1\u6563\uff0c\u8fd9\u5c31\u9700\u8981\u70b9\u4e91\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002\u867d\u7136\u5927\u591a\u6570\u4ee5\u524d\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u57df\u9002\u5e94\uff0c\u8fd9\u6d89\u53ca\u5bf9\u76ee\u6807\u57df\u6570\u636e\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4f46\u8fd9\u5728\u76ee\u6807\u57df\u6570\u636e\u53ef\u80fd\u4e0d\u53ef\u7528\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u53ef\u80fd\u5e76\u4e0d\u603b\u662f\u53ef\u884c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 InvariantOODG\uff0c\u5b83\u4f7f\u7528\u4e24\u5206\u652f\u7f51\u7edc\u4ece\u539f\u59cb\u70b9\u4e91\u548c\u589e\u5f3a\u70b9\u4e91\u4e2d\u63d0\u53d6\u5c40\u90e8\u5230\u5168\u5c40\u7279\u5f81\u6765\u5b66\u4e60\u4e0d\u540c\u5206\u5e03\u7684\u70b9\u4e91\u4e4b\u95f4\u7684\u4e0d\u53d8\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e3a\u4e86\u589e\u5f3a\u70b9\u4e91\u7684\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u951a\u70b9\u6765\u5b9a\u4f4d\u6700\u6709\u7528\u7684\u5c40\u90e8\u533a\u57df\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e24\u79cd\u7c7b\u578b\u7684\u53d8\u6362\u6765\u589e\u5f3a\u8f93\u5165\u70b9\u4e91\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728 3D \u57df\u6cdb\u5316\u57fa\u51c6\u4e0a\u7684\u6709\u6548\u6027\u3002|[2401.03765v1](http://arxiv.org/pdf/2401.03765v1)|null|\n", "2401.03704": "|**2024-01-08**|**Sur2f: A Hybrid Representation for High-Quality and Efficient Surface Reconstruction from Multi-view Images**|Sur2f\uff1a\u4ece\u591a\u89c6\u56fe\u56fe\u50cf\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u548c\u9ad8\u6548\u8868\u9762\u91cd\u5efa\u7684\u6df7\u5408\u8868\u793a|Zhangjin Huang, Zhihao Liang, Haojie Zhang, Yangkai Lin, Kui Jia|Multi-view surface reconstruction is an ill-posed, inverse problem in 3D vision research. It involves modeling the geometry and appearance with appropriate surface representations. Most of the existing methods rely either on explicit meshes, using surface rendering of meshes for reconstruction, or on implicit field functions, using volume rendering of the fields for reconstruction. The two types of representations in fact have their respective merits. In this work, we propose a new hybrid representation, termed Sur2f, aiming to better benefit from both representations in a complementary manner. Technically, we learn two parallel streams of an implicit signed distance field and an explicit surrogate surface Sur2f mesh, and unify volume rendering of the implicit signed distance function (SDF) and surface rendering of the surrogate mesh with a shared, neural shader; the unified shading promotes their convergence to the same, underlying surface. We synchronize learning of the surrogate mesh by driving its deformation with functions induced from the implicit SDF. In addition, the synchronized surrogate mesh enables surface-guided volume sampling, which greatly improves the sampling efficiency per ray in volume rendering. We conduct thorough experiments showing that Sur$^2$f outperforms existing reconstruction methods and surface representations, including hybrid ones, in terms of both recovery quality and recovery efficiency.|\u591a\u89c6\u56fe\u8868\u9762\u200b\u200b\u91cd\u5efa\u662f 3D \u89c6\u89c9\u7814\u7a76\u4e2d\u7684\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\u3002\u5b83\u6d89\u53ca\u4f7f\u7528\u9002\u5f53\u7684\u8868\u9762\u8868\u793a\u5bf9\u51e0\u4f55\u5f62\u72b6\u548c\u5916\u89c2\u8fdb\u884c\u5efa\u6a21\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u663e\u5f0f\u7f51\u683c\uff0c\u4f7f\u7528\u7f51\u683c\u7684\u8868\u9762\u6e32\u67d3\u8fdb\u884c\u91cd\u5efa\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u9690\u5f0f\u573a\u51fd\u6570\uff0c\u4f7f\u7528\u573a\u7684\u4f53\u6e32\u67d3\u8fdb\u884c\u91cd\u5efa\u3002\u8fd9\u4e24\u79cd\u8868\u8ff0\u5176\u5b9e\u5404\u6709\u5404\u7684\u4f18\u70b9\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u8868\u793a\uff0c\u79f0\u4e3a Sur2f\uff0c\u65e8\u5728\u4ee5\u4e92\u8865\u7684\u65b9\u5f0f\u66f4\u597d\u5730\u4ece\u4e24\u79cd\u8868\u793a\u4e2d\u53d7\u76ca\u3002\u4ece\u6280\u672f\u4e0a\u8bb2\uff0c\u6211\u4eec\u5b66\u4e60\u9690\u5f0f\u7b26\u53f7\u8ddd\u79bb\u573a\u548c\u663e\u5f0f\u4ee3\u7406\u8868\u9762 Sur2f \u7f51\u683c\u7684\u4e24\u4e2a\u5e76\u884c\u6d41\uff0c\u5e76\u4f7f\u7528\u5171\u4eab\u7684\u795e\u7ecf\u7740\u8272\u5668\u7edf\u4e00\u9690\u5f0f\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570 (SDF) \u7684\u4f53\u79ef\u6e32\u67d3\u548c\u4ee3\u7406\u7f51\u683c\u7684\u8868\u9762\u6e32\u67d3\uff1b\u7edf\u4e00\u7684\u9634\u5f71\u4fc3\u8fdb\u5b83\u4eec\u6536\u655b\u5230\u76f8\u540c\u7684\u4e0b\u8868\u9762\u3002\u6211\u4eec\u901a\u8fc7\u7528\u9690\u5f0f SDF \u5bfc\u51fa\u7684\u51fd\u6570\u9a71\u52a8\u4ee3\u7406\u7f51\u683c\u7684\u53d8\u5f62\u6765\u540c\u6b65\u4ee3\u7406\u7f51\u683c\u7684\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u540c\u6b65\u7684\u4ee3\u7406\u7f51\u683c\u53ef\u4ee5\u5b9e\u73b0\u8868\u9762\u5f15\u5bfc\u7684\u4f53\u79ef\u91c7\u6837\uff0c\u8fd9\u5927\u5927\u63d0\u9ad8\u4e86\u4f53\u6e32\u67d3\u4e2d\u6bcf\u6761\u5149\u7ebf\u7684\u91c7\u6837\u6548\u7387\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5f7b\u5e95\u7684\u5b9e\u9a8c\uff0c\u8868\u660e Sur$^2$f \u5728\u6062\u590d\u8d28\u91cf\u548c\u6062\u590d\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u91cd\u5efa\u65b9\u6cd5\u548c\u8868\u9762\u8868\u793a\uff08\u5305\u62ec\u6df7\u5408\u65b9\u6cd5\uff09\u3002|[2401.03704v1](http://arxiv.org/pdf/2401.03704v1)|null|\n", "2401.03641": "|**2024-01-08**|**DME-Driver: Integrating Human Decision Logic and 3D Scene Perception in Autonomous Driving**|DME-Driver\uff1a\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u96c6\u6210\u4eba\u7c7b\u51b3\u7b56\u903b\u8f91\u548c 3D \u573a\u666f\u611f\u77e5|Wencheng Han, Dongqian Guo, Cheng-Zhong Xu, Jianbing Shen|In the field of autonomous driving, two important features of autonomous driving car systems are the explainability of decision logic and the accuracy of environmental perception. This paper introduces DME-Driver, a new autonomous driving system that enhances the performance and reliability of autonomous driving system. DME-Driver utilizes a powerful vision language model as the decision-maker and a planning-oriented perception model as the control signal generator. To ensure explainable and reliable driving decisions, the logical decision-maker is constructed based on a large vision language model. This model follows the logic employed by experienced human drivers and makes decisions in a similar manner. On the other hand, the generation of accurate control signals relies on precise and detailed environmental perception, which is where 3D scene perception models excel. Therefore, a planning oriented perception model is employed as the signal generator. It translates the logical decisions made by the decision-maker into accurate control signals for the self-driving cars. To effectively train the proposed model, a new dataset for autonomous driving was created. This dataset encompasses a diverse range of human driver behaviors and their underlying motivations. By leveraging this dataset, our model achieves high-precision planning accuracy through a logical thinking process.|\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7cfb\u7edf\u7684\u4e24\u4e2a\u91cd\u8981\u7279\u5f81\u662f\u51b3\u7b56\u903b\u8f91\u7684\u53ef\u89e3\u91ca\u6027\u548c\u73af\u5883\u611f\u77e5\u7684\u51c6\u786e\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edfDME-Driver\uff0c\u53ef\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002 DME-Driver\u5229\u7528\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u51b3\u7b56\u8005\uff0c\u5229\u7528\u9762\u5411\u89c4\u5212\u7684\u611f\u77e5\u6a21\u578b\u4f5c\u4e3a\u63a7\u5236\u4fe1\u53f7\u751f\u6210\u5668\u3002\u4e3a\u4e86\u786e\u4fdd\u9a7e\u9a76\u51b3\u7b56\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\uff0c\u903b\u8f91\u51b3\u7b56\u5668\u662f\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7684\u3002\u8be5\u6a21\u578b\u9075\u5faa\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4eba\u7c7b\u9a7e\u9a76\u5458\u6240\u91c7\u7528\u7684\u903b\u8f91\uff0c\u5e76\u4ee5\u7c7b\u4f3c\u7684\u65b9\u5f0f\u505a\u51fa\u51b3\u7b56\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u7cbe\u786e\u63a7\u5236\u4fe1\u53f7\u7684\u751f\u6210\u4f9d\u8d56\u4e8e\u7cbe\u786e\u3001\u8be6\u7ec6\u7684\u73af\u5883\u611f\u77e5\uff0c\u800c\u8fd9\u6b63\u662f 3D \u573a\u666f\u611f\u77e5\u6a21\u578b\u7684\u4f18\u52bf\u6240\u5728\u3002\u56e0\u6b64\uff0c\u91c7\u7528\u9762\u5411\u89c4\u5212\u7684\u611f\u77e5\u6a21\u578b\u4f5c\u4e3a\u4fe1\u53f7\u53d1\u751f\u5668\u3002\u5b83\u5c06\u51b3\u7b56\u8005\u505a\u51fa\u7684\u903b\u8f91\u51b3\u7b56\u8f6c\u5316\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u51c6\u786e\u63a7\u5236\u4fe1\u53f7\u3002\u4e3a\u4e86\u6709\u6548\u5730\u8bad\u7ec3\u6240\u63d0\u51fa\u7684\u6a21\u578b\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u5404\u79cd\u4eba\u7c7b\u9a7e\u9a76\u5458\u884c\u4e3a\u53ca\u5176\u6f5c\u5728\u52a8\u673a\u3002\u901a\u8fc7\u5229\u7528\u8be5\u6570\u636e\u96c6\uff0c\u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7\u903b\u8f91\u601d\u7ef4\u8fc7\u7a0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u89c4\u5212\u51c6\u786e\u6027\u3002|[2401.03641v1](http://arxiv.org/pdf/2401.03641v1)|null|\n"}, "\u56fe\u50cf\u7406\u89e3": {}, "GNN": {}, "\u5176\u4ed6": {"2401.04079": "|**2024-01-08**|**RudolfV: A Foundation Model by Pathologists for Pathologists**|RudolfV\uff1a\u75c5\u7406\u5b66\u5bb6\u4e3a\u75c5\u7406\u5b66\u5bb6\u63d0\u4f9b\u7684\u57fa\u7840\u6a21\u578b|Jonas Dippel, Barbara Feulner, Tobias Winterhoff, Simon Schallenberg, Gabriel Dernbach, Andreas Kunft, Stephan Tietz, Philipp Jurmeister, David Horst, Lukas Ruff, et.al.|Histopathology plays a central role in clinical medicine and biomedical research. While artificial intelligence shows promising results on many pathological tasks, generalization and dealing with rare diseases, where training data is scarce, remains a challenge. Distilling knowledge from unlabeled data into a foundation model before learning from, potentially limited, labeled data provides a viable path to address these challenges. In this work, we extend the state of the art of foundation models for digital pathology whole slide images by semi-automated data curation and incorporating pathologist domain knowledge. Specifically, we combine computational and pathologist domain knowledge (1) to curate a diverse dataset of 103k slides corresponding to 750 million image patches covering data from different fixation, staining, and scanning protocols as well as data from different indications and labs across the EU and US, (2) for grouping semantically similar slides and tissue patches, and (3) to augment the input images during training. We evaluate the resulting model on a set of public and internal benchmarks and show that although our foundation model is trained with an order of magnitude less slides, it performs on par or better than competing models. We expect that scaling our approach to more data and larger models will further increase its performance and capacity to deal with increasingly complex real world tasks in diagnostics and biomedical research.|\u7ec4\u7ec7\u75c5\u7406\u5b66\u5728\u4e34\u5e8a\u533b\u5b66\u548c\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u53d1\u6325\u7740\u6838\u5fc3\u4f5c\u7528\u3002\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u5728\u8bb8\u591a\u75c5\u7406\u4efb\u52a1\u4e0a\u663e\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u6cdb\u5316\u548c\u5904\u7406\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u7f55\u89c1\u75be\u75c5\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u5728\u4ece\u53ef\u80fd\u6709\u9650\u7684\u6807\u8bb0\u6570\u636e\u4e2d\u5b66\u4e60\u4e4b\u524d\uff0c\u5c06\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u7684\u77e5\u8bc6\u63d0\u53d6\u5230\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u9014\u5f84\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u534a\u81ea\u52a8\u6570\u636e\u7ba1\u7406\u548c\u7ed3\u5408\u75c5\u7406\u5b66\u5bb6\u9886\u57df\u77e5\u8bc6\uff0c\u6269\u5c55\u4e86\u6570\u5b57\u75c5\u7406\u5b66\u5168\u5e7b\u706f\u7247\u56fe\u50cf\u57fa\u7840\u6a21\u578b\u7684\u6700\u65b0\u6280\u672f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7ed3\u5408\u8ba1\u7b97\u548c\u75c5\u7406\u5b66\u5bb6\u9886\u57df\u77e5\u8bc6 (1) \u6765\u6574\u7406\u5305\u542b 103,000 \u5f20\u5e7b\u706f\u7247\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5bf9\u5e94\u4e8e 7.5 \u4ebf\u4e2a\u56fe\u50cf\u5757\uff0c\u6db5\u76d6\u6765\u81ea\u4e0d\u540c\u56fa\u5b9a\u3001\u67d3\u8272\u548c\u626b\u63cf\u534f\u8bae\u7684\u6570\u636e\u4ee5\u53ca\u6765\u81ea\u6b27\u76df\u548c\u6b27\u6d32\u4e0d\u540c\u9002\u5e94\u75c7\u548c\u5b9e\u9a8c\u5ba4\u7684\u6570\u636e\u3002 US\uff0c(2) \u7528\u4e8e\u5bf9\u8bed\u4e49\u76f8\u4f3c\u7684\u5e7b\u706f\u7247\u548c\u7ec4\u7ec7\u5757\u8fdb\u884c\u5206\u7ec4\uff0c\u4ee5\u53ca (3) \u5728\u8bad\u7ec3\u671f\u95f4\u589e\u5f3a\u8f93\u5165\u56fe\u50cf\u3002\u6211\u4eec\u5728\u4e00\u7ec4\u516c\u5171\u548c\u5185\u90e8\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u7ed3\u679c\u6a21\u578b\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u6211\u4eec\u7684\u57fa\u7840\u6a21\u578b\u662f\u7528\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u5e7b\u706f\u7247\u8fdb\u884c\u8bad\u7ec3\u7684\uff0c\u4f46\u5b83\u7684\u6027\u80fd\u4e0e\u7ade\u4e89\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u3002\u6211\u4eec\u9884\u8ba1\uff0c\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u591a\u6570\u636e\u548c\u66f4\u5927\u7684\u6a21\u578b\u5c06\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5176\u6027\u80fd\u548c\u80fd\u529b\uff0c\u4ee5\u5904\u7406\u8bca\u65ad\u548c\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u65e5\u76ca\u590d\u6742\u7684\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u3002|[2401.04079v1](http://arxiv.org/pdf/2401.04079v1)|null|\n", "2401.04071": "|**2024-01-08**|**Fun with Flags: Robust Principal Directions via Flag Manifolds**|\u65d7\u5e1c\u7684\u4e50\u8da3\uff1a\u901a\u8fc7\u65d7\u5e1c\u6d41\u5f62\u5b9e\u73b0\u7a33\u5065\u7684\u4e3b\u8981\u65b9\u5411|Nathan Mankovich, Gustau Camps-Valls, Tolga Birdal|Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \\ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, creating novel robust and dual geodesic PCA variations. The remarkable flexibility offered by the 'flagification' introduced here enables even more algorithmic variants identified by specific flag types. Last but not least, we propose an effective convergent solver for these flag-formulations employing the Stiefel manifold. Our empirical results on both real-world and synthetic scenarios, demonstrate the superiority of our novel algorithms, especially in terms of robustness to outliers on manifolds.|\u4e3b\u6210\u5206\u5206\u6790 (PCA) \u53ca\u5176\u5bf9\u6d41\u5f62\u548c\u5f02\u5e38\u6c61\u67d3\u6570\u636e\u7684\u6269\u5c55\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u662f\u4e0d\u53ef\u6216\u7f3a\u7684\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 PCA \u53ca\u5176\u53d8\u4f53\u7684\u7edf\u4e00\u5f62\u5f0f\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ebf\u6027\u5b50\u7a7a\u95f4\u6807\u5fd7\u7684\u6846\u67b6\uff0c\u5373\u7ef4\u5ea6\u9012\u589e\u7684\u5d4c\u5957\u7ebf\u6027\u5b50\u7a7a\u95f4\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5b83\u4e0d\u4ec5\u5141\u8bb8\u901a\u7528\u5b9e\u73b0\uff0c\u800c\u4e14\u8fd8\u4ea7\u751f\u4ee5\u524d\u6ca1\u6709\u63a2\u7d22\u8fc7\u7684\u65b0\u9896\u53d8\u4f53\u3002\u6211\u4eec\u9996\u5148\u6982\u62ec\u4f20\u7edf\u7684 PCA \u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u6700\u5927\u5316\u65b9\u5dee\u6216\u6700\u5c0f\u5316\u91cd\u5efa\u8bef\u5dee\u3002\u6211\u4eec\u901a\u8fc7\u8003\u8651\u5f02\u5e38\u503c\u548c\u6570\u636e\u6d41\u5f62\u6765\u6269\u5c55\u8fd9\u4e9b\u89e3\u91ca\uff0c\u4ee5\u5f00\u53d1\u4e00\u7cfb\u5217\u65b0\u7684\u964d\u7ef4\u7b97\u6cd5\u3002\u4e3a\u4e86\u8bbe\u8ba1\u4e00\u79cd\u901a\u7528\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u6211\u4eec\u5c06\u7a33\u5065\u7684\u5bf9\u5076\u5f62\u5f0f\u7684 PCA \u91cd\u65b0\u8bbe\u8ba1\u4e3a\u6807\u5fd7\u6d41\u5f62\u4e0a\u7684\u4f18\u5316\u95ee\u9898\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u4e3b\u6d4b\u5730\u7ebf\u5206\u6790\uff08\u5207\u7ebf PCA\uff09\u7684\u5207\u7ebf\u7a7a\u95f4\u8fd1\u4f3c\u96c6\u6210\u5230\u8fd9\u4e2a\u57fa\u4e8e\u6807\u5fd7\u7684\u6846\u67b6\u4e2d\uff0c\u521b\u5efa\u65b0\u9896\u7684\u9c81\u68d2\u548c\u53cc\u6d4b\u5730\u7ebf PCA \u53d8\u4f53\u3002\u8fd9\u91cc\u5f15\u5165\u7684\u201c\u6807\u8bb0\u201d\u63d0\u4f9b\u4e86\u663e\u7740\u7684\u7075\u6d3b\u6027\uff0c\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u6807\u8bb0\u7c7b\u578b\u8bc6\u522b\u66f4\u591a\u7684\u7b97\u6cd5\u53d8\u4f53\u3002\u6700\u540e\u4f46\u5e76\u975e\u6700\u4e0d\u91cd\u8981\u7684\u4e00\u70b9\u662f\uff0c\u6211\u4eec\u4e3a\u8fd9\u4e9b\u91c7\u7528 Stiefel \u6d41\u5f62\u7684\u6807\u5fd7\u516c\u5f0f\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6536\u655b\u6c42\u89e3\u5668\u3002\u6211\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u548c\u5408\u6210\u573a\u666f\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b0\u9896\u7b97\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6d41\u5f62\u5f02\u5e38\u503c\u7684\u9c81\u68d2\u6027\u65b9\u9762\u3002|[2401.04071v1](http://arxiv.org/pdf/2401.04071v1)|null|\n", "2401.03993": "|**2024-01-08**|**Behavioural Cloning in VizDoom**|VizDoom \u4e2d\u7684\u884c\u4e3a\u514b\u9686|Ryan Spick, Timothy Bradley, Ayush Raina, Pierluigi Vito Amadori, Guy Moss|This paper describes methods for training autonomous agents to play the game \"Doom 2\" through Imitation Learning (IL) using only pixel data as input. We also explore how Reinforcement Learning (RL) compares to IL for humanness by comparing camera movement and trajectory data. Through behavioural cloning, we examine the ability of individual models to learn varying behavioural traits. We attempt to mimic the behaviour of real players with different play styles, and find we can train agents that behave aggressively, passively, or simply more human-like than traditional AIs. We propose these methods of introducing more depth and human-like behaviour to agents in video games. The trained IL agents perform on par with the average players in our dataset, whilst outperforming the worst players. While performance was not as strong as common RL approaches, it provides much stronger human-like behavioural traits to the agent.|\u672c\u6587\u63cf\u8ff0\u4e86\u4ec5\u4f7f\u7528\u50cf\u7d20\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u8bad\u7ec3\u81ea\u4e3b\u4ee3\u7406\u73a9\u6e38\u620f\u201cDoom 2\u201d\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u6bd4\u8f83\u76f8\u673a\u8fd0\u52a8\u548c\u8f68\u8ff9\u6570\u636e\uff0c\u63a2\u8ba8\u5f3a\u5316\u5b66\u4e60 (RL) \u4e0e IL \u7684\u4eba\u6027\u6bd4\u8f83\u3002\u901a\u8fc7\u884c\u4e3a\u514b\u9686\uff0c\u6211\u4eec\u68c0\u67e5\u4e2a\u4f53\u6a21\u578b\u5b66\u4e60\u4e0d\u540c\u884c\u4e3a\u7279\u5f81\u7684\u80fd\u529b\u3002\u6211\u4eec\u5c1d\u8bd5\u6a21\u4eff\u5177\u6709\u4e0d\u540c\u6e38\u620f\u98ce\u683c\u7684\u771f\u5b9e\u73a9\u5bb6\u7684\u884c\u4e3a\uff0c\u5e76\u53d1\u73b0\u6211\u4eec\u53ef\u4ee5\u8bad\u7ec3\u51fa\u6bd4\u4f20\u7edf\u4eba\u5de5\u667a\u80fd\u8868\u73b0\u5f97\u66f4\u5177\u653b\u51fb\u6027\u3001\u88ab\u52a8\u6027\u6216\u66f4\u50cf\u4eba\u7c7b\u7684\u667a\u80fd\u4f53\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u4e3a\u89c6\u9891\u6e38\u620f\u4e2d\u7684\u4ee3\u7406\u5f15\u5165\u66f4\u591a\u6df1\u5ea6\u548c\u7c7b\u4eba\u884c\u4e3a\u3002\u7ecf\u8fc7\u8bad\u7ec3\u7684 IL \u667a\u80fd\u4f53\u7684\u8868\u73b0\u4e0e\u6211\u4eec\u6570\u636e\u96c6\u4e2d\u7684\u5e73\u5747\u73a9\u5bb6\u76f8\u5f53\uff0c\u540c\u65f6\u8d85\u8fc7\u4e86\u6700\u5dee\u7684\u73a9\u5bb6\u3002\u867d\u7136\u6027\u80fd\u4e0d\u5982\u5e38\u89c1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u90a3\u4e48\u5f3a\u5927\uff0c\u4f46\u5b83\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u7c7b\u4eba\u884c\u4e3a\u7279\u5f81\u3002|[2401.03993v1](http://arxiv.org/pdf/2401.03993v1)|null|\n", "2401.03835": "|**2024-01-08**|**Limitations of Data-Driven Spectral Reconstruction -- An Optics-Aware Analysis**|\u6570\u636e\u9a71\u52a8\u7684\u5149\u8c31\u91cd\u5efa\u7684\u5c40\u9650\u6027\u2014\u2014\u5149\u5b66\u611f\u77e5\u5206\u6790|Qiang Fu, Matheus Souza, Eunsue Choi, Suhyun Shin, Seung-Hwan Baek, Wolfgang Heidrich|Hyperspectral imaging empowers computer vision systems with the distinct capability of identifying materials through recording their spectral signatures. Recent efforts in data-driven spectral reconstruction aim at extracting spectral information from RGB images captured by cost-effective RGB cameras, instead of dedicated hardware.   In this paper we systematically analyze the performance of such methods, evaluating both the practical limitations with respect to current datasets and overfitting, as well as fundamental limits with respect to the nature of the information encoded in the RGB images, and the dependency of this information on the optical system of the camera.   We find that the current models are not robust under slight variations, e.g., in noise level or compression of the RGB file. Both the methods and the datasets are also limited in their ability to cope with metameric colors. This issue can in part be overcome with metameric data augmentation. Moreover, optical lens aberrations can help to improve the encoding of the metameric information into the RGB image, which paves the road towards higher performing spectral imaging and reconstruction approaches.|\u9ad8\u5149\u8c31\u6210\u50cf\u4f7f\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u5177\u6709\u901a\u8fc7\u8bb0\u5f55\u6750\u6599\u7684\u5149\u8c31\u7279\u5f81\u6765\u8bc6\u522b\u6750\u6599\u7684\u72ec\u7279\u80fd\u529b\u3002\u6700\u8fd1\u5728\u6570\u636e\u9a71\u52a8\u7684\u5149\u8c31\u91cd\u5efa\u65b9\u9762\u7684\u52aa\u529b\u65e8\u5728\u4ece\u7ecf\u6d4e\u9ad8\u6548\u7684 RGB \u76f8\u673a\u6355\u83b7\u7684 RGB \u56fe\u50cf\u4e2d\u63d0\u53d6\u5149\u8c31\u4fe1\u606f\uff0c\u800c\u4e0d\u662f\u4e13\u7528\u786c\u4ef6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u6b64\u7c7b\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc4\u4f30\u4e86\u5f53\u524d\u6570\u636e\u96c6\u548c\u8fc7\u5ea6\u62df\u5408\u7684\u5b9e\u9645\u9650\u5236\uff0c\u4ee5\u53ca RGB \u56fe\u50cf\u4e2d\u7f16\u7801\u4fe1\u606f\u7684\u6027\u8d28\u7684\u57fa\u672c\u9650\u5236\uff0c\u4ee5\u53ca\u8be5\u4fe1\u606f\u7684\u4f9d\u8d56\u6027\u5173\u4e8e\u76f8\u673a\u7684\u5149\u5b66\u7cfb\u7edf\u3002\u6211\u4eec\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u8f7b\u5fae\u53d8\u5316\uff08\u4f8b\u5982\u566a\u58f0\u6c34\u5e73\u6216 RGB \u6587\u4ef6\u538b\u7f29\uff09\u4e0b\u5e76\u4e0d\u7a33\u5065\u3002\u8fd9\u4e9b\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u5904\u7406\u540c\u8272\u5f02\u8c31\u989c\u8272\u7684\u80fd\u529b\u4e5f\u53d7\u5230\u9650\u5236\u3002\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u540c\u8272\u5f02\u8c31\u6570\u636e\u589e\u5f3a\u6765\u90e8\u5206\u89e3\u51b3\u3002\u6b64\u5916\uff0c\u5149\u5b66\u955c\u5934\u50cf\u5dee\u6709\u52a9\u4e8e\u6539\u5584\u540c\u8272\u5f02\u8c31\u4fe1\u606f\u5728 RGB \u56fe\u50cf\u4e2d\u7684\u7f16\u7801\uff0c\u8fd9\u4e3a\u5b9e\u73b0\u66f4\u9ad8\u6027\u80fd\u7684\u5149\u8c31\u6210\u50cf\u548c\u91cd\u5efa\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2401.03835v1](http://arxiv.org/pdf/2401.03835v1)|null|\n", "2401.03830": "|**2024-01-08**|**A foundation for exact binarized morphological neural networks**|\u7cbe\u786e\u4e8c\u503c\u5316\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u7840|Theodore Aouad, Hugues Talbot|Training and running deep neural networks (NNs) often demands a lot of computation and energy-intensive specialized hardware (e.g. GPU, TPU...). One way to reduce the computation and power cost is to use binary weight NNs, but these are hard to train because the sign function has a non-smooth gradient. We present a model based on Mathematical Morphology (MM), which can binarize ConvNets without losing performance under certain conditions, but these conditions may not be easy to satisfy in real-world scenarios. To solve this, we propose two new approximation methods and develop a robust theoretical framework for ConvNets binarization using MM. We propose as well regularization losses to improve the optimization. We empirically show that our model can learn a complex morphological network, and explore its performance on a classification task.|\u8bad\u7ec3\u548c\u8fd0\u884c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (NN) \u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u80fd\u6e90\u5bc6\u96c6\u578b\u4e13\u7528\u786c\u4ef6\uff08\u4f8b\u5982 GPU\u3001TPU...\uff09\u3002\u51cf\u5c11\u8ba1\u7b97\u548c\u529f\u8017\u6210\u672c\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528\u4e8c\u5143\u6743\u91cd\u795e\u7ecf\u7f51\u7edc\uff0c\u4f46\u8fd9\u4e9b\u795e\u7ecf\u7f51\u7edc\u5f88\u96be\u8bad\u7ec3\uff0c\u56e0\u4e3a\u7b26\u53f7\u51fd\u6570\u5177\u6709\u975e\u5e73\u6ed1\u68af\u5ea6\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b66\u5f62\u6001\u5b66\uff08MM\uff09\u7684\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u5bf9ConvNet\u8fdb\u884c\u4e8c\u503c\u5316\u800c\u4e0d\u635f\u5931\u6027\u80fd\uff0c\u4f46\u8fd9\u4e9b\u6761\u4ef6\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u53ef\u80fd\u4e0d\u5bb9\u6613\u6ee1\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5e76\u4e3a\u4f7f\u7528 MM \u7684 ConvNets \u4e8c\u503c\u5316\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u7406\u8bba\u6846\u67b6\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u6b63\u5219\u5316\u635f\u5931\u6765\u6539\u8fdb\u4f18\u5316\u3002\u6211\u4eec\u51ed\u7ecf\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u590d\u6742\u7684\u5f62\u6001\u7f51\u7edc\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002|[2401.03830v1](http://arxiv.org/pdf/2401.03830v1)|null|\n", "2401.03814": "|**2024-01-08**|**Gnuastro: visualizing the full dynamic range in color images**|Gnuastro\uff1a\u53ef\u89c6\u5316\u5f69\u8272\u56fe\u50cf\u7684\u5b8c\u6574\u52a8\u6001\u8303\u56f4|Ra\u00fal Infante-Sainz, Mohammad Akhlaghi|Color plays a crucial role in the visualization, interpretation, and analysis of multi-wavelength astronomical images. However, generating color images that accurately represent the full dynamic range of astronomical sources is challenging. In response, Gnuastro v0.22 introduces the program 'astscript-color-faint-gray', which is extensively documented in the Gnuastro manual. It employs a non-linear transformation to assign an 8-bit RGB (Red-Green-Blue) value to brighter pixels, while the fainter ones are shown in an inverse grayscale. This approach enables the simultaneous visualization of low surface brightness features within the same image. This research note is reproducible with Maneage, on the Git commit 48f5408.|\u989c\u8272\u5728\u591a\u6ce2\u957f\u5929\u6587\u56fe\u50cf\u7684\u53ef\u89c6\u5316\u3001\u89e3\u91ca\u548c\u5206\u6790\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u751f\u6210\u51c6\u786e\u4ee3\u8868\u5929\u6587\u6e90\u7684\u5b8c\u6574\u52a8\u6001\u8303\u56f4\u7684\u5f69\u8272\u56fe\u50cf\u5177\u6709\u6311\u6218\u6027\u3002\u4f5c\u4e3a\u56de\u5e94\uff0cGnuastro v0.22 \u5f15\u5165\u4e86\u7a0b\u5e8f\u201castscript-color-faint-gray\u201d\uff0c\u8be5\u7a0b\u5e8f\u5728 Gnuastro \u624b\u518c\u4e2d\u6709\u8be6\u7ec6\u8bb0\u5f55\u3002\u5b83\u91c7\u7528\u975e\u7ebf\u6027\u53d8\u6362\u5c06 8 \u4f4d RGB\uff08\u7ea2-\u7eff-\u84dd\uff09\u503c\u5206\u914d\u7ed9\u8f83\u4eae\u7684\u50cf\u7d20\uff0c\u800c\u8f83\u6697\u7684\u50cf\u7d20\u5219\u4ee5\u53cd\u7070\u5ea6\u663e\u793a\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u540c\u65f6\u53ef\u89c6\u5316\u540c\u4e00\u56fe\u50cf\u4e2d\u7684\u4f4e\u8868\u9762\u4eae\u5ea6\u7279\u5f81\u3002\u8fd9\u4efd\u7814\u7a76\u62a5\u544a\u53ef\u4ee5\u901a\u8fc7 Maneage \u5728 Git \u63d0\u4ea4 48f5408 \u4e0a\u91cd\u73b0\u3002|[2401.03814v1](http://arxiv.org/pdf/2401.03814v1)|null|\n", "2401.03800": "|**2024-01-08**|**MvKSR: Multi-view Knowledge-guided Scene Recovery for Hazy and Rainy Degradation**|MvKSR\uff1a\u591a\u89c6\u56fe\u77e5\u8bc6\u5f15\u5bfc\u7684\u96fe\u973e\u548c\u96e8\u5929\u9000\u5316\u573a\u666f\u6062\u590d|Dong Yang, Wenyu Xu, Yuxu Lu, Yuan Gao, Jingming Zhang, Yu Guo|High-quality imaging is crucial for ensuring safety supervision and intelligent deployment in fields like transportation and industry. It enables precise and detailed monitoring of operations, facilitating timely detection of potential hazards and efficient management. However, adverse weather conditions, such as atmospheric haziness and precipitation, can have a significant impact on image quality. When the atmosphere contains dense haze or water droplets, the incident light scatters, leading to degraded captured images. This degradation is evident in the form of image blur and reduced contrast, increasing the likelihood of incorrect assessments and interpretations by intelligent imaging systems (IIS). To address the challenge of restoring degraded images in hazy and rainy conditions, this paper proposes a novel multi-view knowledge-guided scene recovery network (termed MvKSR). Specifically, guided filtering is performed on the degraded image to separate high/low-frequency components. Subsequently, an en-decoder-based multi-view feature coarse extraction module (MCE) is used to coarsely extract features from different views of the degraded image. The multi-view feature fine fusion module (MFF) will learn and infer the restoration of degraded images through mixed supervision under different views. Additionally, we suggest an atrous residual block to handle global restoration and local repair in hazy/rainy/mixed scenes. Extensive experimental results demonstrate that MvKSR outperforms other state-of-the-art methods in terms of efficiency and stability for restoring degraded scenarios in IIS.|\u9ad8\u8d28\u91cf\u6210\u50cf\u5bf9\u4e8e\u4ea4\u901a\u3001\u5de5\u4e1a\u7b49\u9886\u57df\u7684\u5b89\u5168\u76d1\u7ba1\u548c\u667a\u80fd\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u5b83\u53ef\u4ee5\u5bf9\u64cd\u4f5c\u8fdb\u884c\u7cbe\u786e\u3001\u8be6\u7ec6\u7684\u76d1\u63a7\uff0c\u6709\u5229\u4e8e\u53ca\u65f6\u53d1\u73b0\u6f5c\u5728\u5371\u9669\u5e76\u8fdb\u884c\u9ad8\u6548\u7ba1\u7406\u3002\u7136\u800c\uff0c\u6076\u52a3\u7684\u5929\u6c14\u6761\u4ef6\uff0c\u4f8b\u5982\u5927\u6c14\u96fe\u973e\u548c\u964d\u6c34\uff0c\u4f1a\u5bf9\u56fe\u50cf\u8d28\u91cf\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\u3002\u5f53\u5927\u6c14\u4e2d\u542b\u6709\u6d53\u96fe\u6216\u6c34\u6ef4\u65f6\uff0c\u5165\u5c04\u5149\u4f1a\u53d1\u751f\u6563\u5c04\uff0c\u5bfc\u81f4\u6355\u83b7\u7684\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3002\u8fd9\u79cd\u9000\u5316\u4ee5\u56fe\u50cf\u6a21\u7cca\u548c\u5bf9\u6bd4\u5ea6\u964d\u4f4e\u7684\u5f62\u5f0f\u8868\u73b0\u51fa\u6765\uff0c\u589e\u52a0\u4e86\u667a\u80fd\u6210\u50cf\u7cfb\u7edf (IIS) \u9519\u8bef\u8bc4\u4f30\u548c\u89e3\u91ca\u7684\u53ef\u80fd\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u5728\u96fe\u973e\u548c\u96e8\u5929\u6761\u4ef6\u4e0b\u6062\u590d\u9000\u5316\u56fe\u50cf\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u89c6\u56fe\u77e5\u8bc6\u5f15\u5bfc\u573a\u666f\u6062\u590d\u7f51\u7edc\uff08\u79f0\u4e3aMvKSR\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5bf9\u9000\u5316\u56fe\u50cf\u8fdb\u884c\u5f15\u5bfc\u6ee4\u6ce2\u4ee5\u5206\u79bb\u9ad8\u9891/\u4f4e\u9891\u5206\u91cf\u3002\u968f\u540e\uff0c\u4f7f\u7528\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u591a\u89c6\u56fe\u7279\u5f81\u7c97\u7565\u63d0\u53d6\u6a21\u5757\uff08MCE\uff09\u4ece\u9000\u5316\u56fe\u50cf\u7684\u4e0d\u540c\u89c6\u56fe\u4e2d\u7c97\u7565\u63d0\u53d6\u7279\u5f81\u3002\u591a\u89c6\u56fe\u7279\u5f81\u7cbe\u7ec6\u878d\u5408\u6a21\u5757\uff08MFF\uff09\u5c06\u901a\u8fc7\u4e0d\u540c\u89c6\u56fe\u4e0b\u7684\u6df7\u5408\u76d1\u7763\u6765\u5b66\u4e60\u548c\u63a8\u65ad\u9000\u5316\u56fe\u50cf\u7684\u6062\u590d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u4e00\u4e2a\u7a7a\u6d1e\u7684\u6b8b\u5dee\u5757\u6765\u5904\u7406\u96fe\u973e/\u96e8\u5929/\u6df7\u5408\u573a\u666f\u4e2d\u7684\u5168\u5c40\u6062\u590d\u548c\u5c40\u90e8\u4fee\u590d\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMvKSR \u5728\u6062\u590d IIS \u964d\u7ea7\u573a\u666f\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2401.03800v1](http://arxiv.org/pdf/2401.03800v1)|null|\n", "2401.03788": "|**2024-01-08**|**Low-light Image Enhancement via CLIP-Fourier Guided Wavelet Diffusion**|\u901a\u8fc7 CLIP-\u5085\u7acb\u53f6\u5f15\u5bfc\u5c0f\u6ce2\u6269\u6563\u5b9e\u73b0\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a|Minglong Xue, Jinhong He, Yanyi He, Zhipu Liu, Wenhai Wang, Mingliang Zhou|Low-light image enhancement techniques have significantly progressed, but unstable image quality recovery and unsatisfactory visual perception are still significant challenges. To solve these problems, we propose a novel and robust low-light image enhancement method via CLIP-Fourier Guided Wavelet Diffusion, abbreviated as CFWD. Specifically, we design a guided network with a multiscale visual language in the frequency domain based on the wavelet transform to achieve effective image enhancement iteratively. In addition, we combine the advantages of Fourier transform in detail perception to construct a hybrid frequency domain space with significant perceptual capabilities(HFDPM). This operation guides wavelet diffusion to recover the fine-grained structure of the image and avoid diversity confusion. Extensive quantitative and qualitative experiments on publicly available real-world benchmarks show that our method outperforms existing state-of-the-art methods and better reproduces images similar to normal images. Code is available at https://github.com/He-Jinhong/CFWD.|\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u7740\u8fdb\u6b65\uff0c\u4f46\u4e0d\u7a33\u5b9a\u7684\u56fe\u50cf\u8d28\u91cf\u6062\u590d\u548c\u4e0d\u4ee4\u4eba\u6ee1\u610f\u7684\u89c6\u89c9\u611f\u77e5\u4ecd\u7136\u662f\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u901a\u8fc7 CLIP-\u5085\u7acb\u53f6\u5f15\u5bfc\u5c0f\u6ce2\u6269\u6563\uff08\u7f29\u5199\u4e3a CFWD\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u9c81\u68d2\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u9891\u57df\u591a\u5c3a\u5ea6\u89c6\u89c9\u8bed\u8a00\u7684\u5f15\u5bfc\u7f51\u7edc\uff0c\u4ee5\u8fed\u4ee3\u5730\u5b9e\u73b0\u6709\u6548\u7684\u56fe\u50cf\u589e\u5f3a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7ed3\u5408\u5085\u91cc\u53f6\u53d8\u6362\u5728\u7ec6\u8282\u611f\u77e5\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u6784\u5efa\u4e86\u5177\u6709\u663e\u7740\u611f\u77e5\u80fd\u529b\u7684\u6df7\u5408\u9891\u57df\u7a7a\u95f4\uff08HFDPM\uff09\u3002\u8be5\u64cd\u4f5c\u5f15\u5bfc\u5c0f\u6ce2\u6269\u6563\u6765\u6062\u590d\u56fe\u50cf\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\u5e76\u907f\u514d\u591a\u6837\u6027\u6df7\u4e71\u3002\u5bf9\u516c\u5f00\u7684\u73b0\u5b9e\u4e16\u754c\u57fa\u51c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u66f4\u597d\u5730\u518d\u73b0\u4e86\u4e0e\u6b63\u5e38\u56fe\u50cf\u76f8\u4f3c\u7684\u56fe\u50cf\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/He-Jinhong/CFWD \u83b7\u53d6\u3002|[2401.03788v1](http://arxiv.org/pdf/2401.03788v1)|null|\n", "2401.03621": "|**2024-01-08**|**Machine Learning Applications in Traumatic Brain Injury Diagnosis and Prognosis: A Spotlight on Mild TBI and CT Imaging**|\u673a\u5668\u5b66\u4e60\u5728\u521b\u4f24\u6027\u8111\u635f\u4f24\u8bca\u65ad\u548c\u9884\u540e\u4e2d\u7684\u5e94\u7528\uff1a\u805a\u7126\u8f7b\u5ea6 TBI \u548c CT \u6210\u50cf|Hanem Ellethy, Shekhar S. Chandra, Viktor Vegh|Traumatic Brain Injury (TBI) poses a significant global public health challenge, contributing to high morbidity and mortality rates and placing a substantial economic burden on healthcare systems worldwide. The diagnosis and prognosis of TBI relies on a combination of clinical and imaging data often acquired using a Computed Tomography (CT) scanner. Addressing the multifaceted challenges posed by TBI requires innovative, data-driven approaches, for this complex condition. As such, we provide a summary of the state-of-the-art Machine Learning (ML) and Deep Learning (DL) techniques applied to clinical and images in TBI, with a particular focus on mild TBI (mTBI). We explore the rich spectrum of ML and DL techniques used and highlight their impact in TBI . We categorize ML and DL methods by TBI severity and showcase their application in mTBI and moderate-to-severe TBI scenarios. Finally, we emphasize the role of ML and DL in mTBI diagnosis, where conventional methods often fall short, and comment on the potential of CT-based ML applications in TBI. This review may serve as a source of inspiration for future research endeavours aimed at improving the diagnosis and prognosis of TBI.|\u521b\u4f24\u6027\u8111\u635f\u4f24\uff08TBI\uff09\u5bf9\u5168\u7403\u516c\u5171\u536b\u751f\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u5bfc\u81f4\u9ad8\u53d1\u75c5\u7387\u548c\u6b7b\u4ea1\u7387\uff0c\u5e76\u7ed9\u5168\u7403\u533b\u7597\u4fdd\u5065\u7cfb\u7edf\u5e26\u6765\u6c89\u91cd\u7684\u7ecf\u6d4e\u8d1f\u62c5\u3002 TBI \u7684\u8bca\u65ad\u548c\u9884\u540e\u4f9d\u8d56\u4e8e\u901a\u5e38\u4f7f\u7528\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf (CT) \u626b\u63cf\u4eea\u83b7\u53d6\u7684\u4e34\u5e8a\u548c\u5f71\u50cf\u6570\u636e\u7684\u7ed3\u5408\u3002\u9488\u5bf9\u8fd9\u79cd\u590d\u6742\u7684\u60c5\u51b5\uff0c\u5e94\u5bf9 TBI \u5e26\u6765\u7684\u591a\u65b9\u9762\u6311\u6218\u9700\u8981\u521b\u65b0\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u603b\u7ed3\u4e86\u5e94\u7528\u4e8e TBI \u4e34\u5e8a\u548c\u56fe\u50cf\u7684\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60 (ML) \u548c\u6df1\u5ea6\u5b66\u4e60 (DL) \u6280\u672f\uff0c\u7279\u522b\u5173\u6ce8\u8f7b\u5ea6 TBI (mTBI)\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u6240\u4f7f\u7528\u7684\u4e30\u5bcc\u7684 ML \u548c DL \u6280\u672f\uff0c\u5e76\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728 TBI \u4e2d\u7684\u5f71\u54cd\u3002\u6211\u4eec\u6839\u636e TBI \u4e25\u91cd\u7a0b\u5ea6\u5bf9 ML \u548c DL \u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5c55\u793a\u5b83\u4eec\u5728 mTBI \u548c\u4e2d\u91cd\u5ea6 TBI \u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86 ML \u548c DL \u5728 mTBI \u8bca\u65ad\u4e2d\u7684\u4f5c\u7528\uff08\u4f20\u7edf\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e00\u8981\u6c42\uff09\uff0c\u5e76\u8bc4\u8bba\u4e86\u57fa\u4e8e CT \u7684 ML \u5728 TBI \u4e2d\u5e94\u7528\u7684\u6f5c\u529b\u3002\u8fd9\u7bc7\u7efc\u8ff0\u53ef\u80fd\u4e3a\u672a\u6765\u65e8\u5728\u6539\u5584 TBI \u8bca\u65ad\u548c\u9884\u540e\u7684\u7814\u7a76\u5de5\u4f5c\u63d0\u4f9b\u7075\u611f\u3002|[2401.03621v1](http://arxiv.org/pdf/2401.03621v1)|null|\n"}}