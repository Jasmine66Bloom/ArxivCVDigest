{"\u751f\u6210\u6a21\u578b": {"2402.03305": "|**2024-02-05**|**Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?**|\u6269\u6563\u6a21\u578b\u662f\u5426\u80fd\u591f\u5b66\u4e60\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u4e14\u6709\u6548\u7684\u8868\u793a\uff1f|Qiyao Liang, Ziming Liu, Ila Fiete|Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is generated at the correct $x$ and y location. Furthermore, we show that even under imbalanced datasets where features ($x$- versus $y$-positions) are represented with skewed frequencies, the learning process for $x$ and $y$ is coupled rather than factorized, demonstrating that simple vanilla-flavored diffusion models cannot learn efficient representations in which localization in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest the need for future work to find inductive biases that will push generative models to discover and exploit factorizable independent structures in their inputs, which will be required to vault these models into more data-efficient regimes.|\u6269\u6563\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u4e0d\u5e38\u89c1\u7684\u5e76\u7f6e\u6765\u751f\u6210\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u56fe\u50cf\uff0c\u4f8b\u5982\u5b87\u822a\u5458\u5728\u6708\u7403\u4e0a\u9a91\u9a6c\u5e76\u5177\u6709\u6b63\u786e\u653e\u7f6e\u7684\u9634\u5f71\u3002\u8fd9\u4e9b\u8f93\u51fa\u8868\u660e\u6267\u884c\u7ec4\u5408\u6cdb\u5316\u7684\u80fd\u529b\uff0c\u4f46\u662f\u6a21\u578b\u662f\u5982\u4f55\u505a\u5230\u8fd9\u4e00\u70b9\u7684\u5462\uff1f\u6211\u4eec\u5bf9\u6761\u4ef6 DDPM \u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\uff0c\u5b66\u4e60\u751f\u6210\u4ee5\u6307\u5b9a $x$- \u548c $y$- \u4f4d\u7f6e\u4e3a\u4e2d\u5fc3\u7684 2D \u7403\u5f62\u9ad8\u65af\u51f9\u51f8\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u6f5c\u5728\u8868\u793a\u7684\u51fa\u73b0\u662f\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u5173\u952e\u3002\u5728\u901a\u8fc7\u5b66\u4e60\u83b7\u5f97\u6210\u529f\u8868\u73b0\u7684\u8fc7\u7a0b\u4e2d\uff0c\u8be5\u6a21\u578b\u7ecf\u5386\u4e86\u6f5c\u5728\u8868\u793a\u7684\u4e09\u4e2a\u4e0d\u540c\u9636\u6bb5\uff1a\uff08A \u9636\u6bb5\uff09\u65e0\u6f5c\u5728\u7ed3\u6784\uff0c\uff08B \u9636\u6bb5\uff09\u65e0\u5e8f\u72b6\u6001\u7684 2D \u6d41\u5f62\uff0c\u4ee5\u53ca\uff08C \u9636\u6bb5\uff092D \u6709\u5e8f\u6d41\u5f62\u3002\u5bf9\u5e94\u4e8e\u6bcf\u4e2a\u9636\u6bb5\uff0c\u6211\u4eec\u8bc6\u522b\u51fa\u4e0d\u540c\u6027\u8d28\u7684\u751f\u6210\u884c\u4e3a\uff1a1) \u751f\u6210\u591a\u4e2a\u51f9\u51f8\uff0c2) \u751f\u6210\u4e00\u4e2a\u51f9\u51f8\uff0c\u4f46\u5728\u4e0d\u51c6\u786e\u7684 $x$ \u548c $y$ \u4f4d\u7f6e\uff0c3) \u5728\u6b63\u786e\u7684 $x \u5904\u751f\u6210\u51f9\u51f8$ \u548c y \u4f4d\u7f6e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8868\u660e\uff0c\u5373\u4f7f\u5728\u7279\u5f81\uff08$x$- \u4e0e $y$-\u4f4d\u7f6e\uff09\u4ee5\u503e\u659c\u9891\u7387\u8868\u793a\u7684\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0b\uff0c$x$ \u548c $y$ \u7684\u5b66\u4e60\u8fc7\u7a0b\u4e5f\u662f\u8026\u5408\u7684\u800c\u4e0d\u662f\u5206\u89e3\u7684\uff0c\u8fd9\u8bc1\u660e\u4e86\u7b80\u5355\u7684\u9999\u8349-\u98ce\u683c\u7684\u6269\u6563\u6a21\u578b\u65e0\u6cd5\u5b66\u4e60\u6709\u6548\u7684\u8868\u793a\uff0c\u5176\u4e2d $x$ \u548c $y$ \u4e2d\u7684\u5b9a\u4f4d\u88ab\u5206\u89e3\u4e3a\u5355\u72ec\u7684\u4e00\u7ef4\u4efb\u52a1\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u672a\u6765\u7684\u5de5\u4f5c\u9700\u8981\u627e\u5230\u5f52\u7eb3\u504f\u5dee\uff0c\u4ece\u800c\u63a8\u52a8\u751f\u6210\u6a21\u578b\u53d1\u73b0\u548c\u5229\u7528\u5176\u8f93\u5165\u4e2d\u7684\u53ef\u56e0\u5f0f\u5206\u89e3\u7684\u72ec\u7acb\u7ed3\u6784\uff0c\u8fd9\u5c06\u9700\u8981\u5c06\u8fd9\u4e9b\u6a21\u578b\u7eb3\u5165\u6570\u636e\u6548\u7387\u66f4\u9ad8\u7684\u4f53\u7cfb\u4e2d\u3002|[2402.03305v1](http://arxiv.org/pdf/2402.03305v1)|null|\n", "2402.03299": "|**2024-02-05**|**GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models**|GUARD\uff1a\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u751f\u6210\u81ea\u7136\u8bed\u8a00\u8d8a\u72f1\uff0c\u4ee5\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u5219\u9075\u5b88\u60c5\u51b5|Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang|The discovery of \"jailbreaks\" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.|\u7ed5\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5b89\u5168\u8fc7\u6ee4\u5668\u7684\u201c\u8d8a\u72f1\u201d\u548c\u6709\u5bb3\u54cd\u5e94\u7684\u53d1\u73b0\u9f13\u52b1\u793e\u533a\u5b9e\u65bd\u5b89\u5168\u63aa\u65bd\u3002\u4e00\u9879\u4e3b\u8981\u7684\u5b89\u5168\u63aa\u65bd\u662f\u5728\u53d1\u5e03\u4e4b\u524d\u901a\u8fc7\u8d8a\u72f1\u4e3b\u52a8\u6d4b\u8bd5\u6cd5\u5b66\u7855\u58eb\u3002\u56e0\u6b64\uff0c\u6b64\u7c7b\u6d4b\u8bd5\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5927\u89c4\u6a21\u4e14\u9ad8\u6548\u5730\u751f\u6210\u8d8a\u72f1\u7684\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9075\u5faa\u4e00\u79cd\u65b0\u9896\u800c\u76f4\u89c2\u7684\u7b56\u7565\u6765\u751f\u6210\u4eba\u7c7b\u4e00\u4ee3\u98ce\u683c\u7684\u8d8a\u72f1\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u89d2\u8272\u626e\u6f14\u7cfb\u7edf\uff0c\u4e3a\u7528\u6237\u6cd5\u5b66\u7855\u58eb\u5206\u914d\u56db\u4e2a\u4e0d\u540c\u7684\u89d2\u8272\uff0c\u4ee5\u534f\u4f5c\u5b8c\u6210\u65b0\u7684\u8d8a\u72f1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6536\u96c6\u73b0\u6709\u7684\u8d8a\u72f1\uff0c\u5e76\u4f7f\u7528\u805a\u7c7b\u9891\u7387\u548c\u8bed\u4e49\u6a21\u5f0f\u9010\u53e5\u5c06\u5b83\u4eec\u5206\u6210\u4e0d\u540c\u7684\u72ec\u7acb\u7279\u5f81\u3002\u6211\u4eec\u5c06\u8fd9\u4e9b\u7279\u5f81\u7ec4\u7ec7\u6210\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f7f\u5b83\u4eec\u66f4\u6613\u4e8e\u8bbf\u95ee\u548c\u68c0\u7d22\u3002\u6211\u4eec\u7684\u4e0d\u540c\u89d2\u8272\u7cfb\u7edf\u5c06\u5229\u7528\u8fd9\u4e2a\u77e5\u8bc6\u56fe\u8c31\u6765\u751f\u6210\u65b0\u7684\u8d8a\u72f1\uff0c\u4e8b\u5b9e\u8bc1\u660e\uff0c\u8fd9\u53ef\u4ee5\u6709\u6548\u5730\u8bf1\u5bfc\u6cd5\u5b66\u7855\u58eb\u4ea7\u751f\u4e0d\u9053\u5fb7\u6216\u8fdd\u53cd\u6307\u5357\u7684\u53cd\u5e94\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5728\u6211\u4eec\u7684\u7cfb\u7edf\u4e2d\u5f00\u521b\u4e86\u4e00\u4e2a\u8bbe\u7f6e\uff0c\u8be5\u8bbe\u7f6e\u5c06\u81ea\u52a8\u9075\u5faa\u653f\u5e9c\u53d1\u5e03\u7684\u6307\u5bfc\u65b9\u9488\u6765\u751f\u6210\u8d8a\u72f1\uff0c\u4ee5\u6d4b\u8bd5\u6cd5\u5b66\u7855\u58eb\u662f\u5426\u76f8\u5e94\u5730\u9075\u5faa\u6307\u5bfc\u65b9\u9488\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u7cfb\u7edf\u79f0\u4e3a GUARD\uff08\u901a\u8fc7\u81ea\u9002\u5e94\u89d2\u8272\u626e\u6f14\u8bca\u65ad\u7ef4\u62a4\u6307\u5357\uff09\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u4e86 GUARD \u5728\u4e09\u4e2a\u5c16\u7aef\u5f00\u6e90 LLM\uff08Vicuna-13B\u3001LongChat-7B \u548c Llama-2-7B\uff09\u4ee5\u53ca\u5e7f\u6cdb\u4f7f\u7528\u7684\u5546\u4e1a LLM (ChatGPT) \u4e0a\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u6269\u5c55\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08MiniGPT-v2 \u548c Gemini Vision Pro\uff09\u9886\u57df\uff0c\u5c55\u793a\u4e86 GUARD \u7684\u591a\u529f\u80fd\u6027\uff0c\u5e76\u4e3a\u8de8\u4e0d\u540c\u6a21\u5f0f\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684\u57fa\u4e8e LLM \u7684\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002|[2402.03299v1](http://arxiv.org/pdf/2402.03299v1)|null|\n", "2402.03292": "|**2024-02-05**|**Zero-shot Object-Level OOD Detection with Context-Aware Inpainting**|\u5177\u6709\u4e0a\u4e0b\u6587\u611f\u77e5\u4fee\u590d\u529f\u80fd\u7684\u96f6\u6837\u672c\u5bf9\u8c61\u7ea7 OOD \u68c0\u6d4b|Quang-Huy Nguyen, Jin Peng Zhou, Zhenzhen Liu, Khanh-Huyen Bui, Kilian Q. Weinberger, Dung D. Le|Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.|\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8d8a\u6765\u8d8a\u591a\u5730\u4ee5\u9ed1\u76d2\u4e91\u670d\u52a1\u6216\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5f62\u5f0f\u63d0\u4f9b\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u5176\u8bad\u7ec3\u6570\u636e\u3002\u8fd9\u5f15\u53d1\u4e86\u96f6\u6837\u672c\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u68c0\u6d4b\u4e0d\u5c5e\u4e8e\u5206\u7c7b\u5668\u6807\u7b7e\u96c6\u4f46\u88ab\u9519\u8bef\u5206\u7c7b\u4e3a\u5206\u5e03\u5185\uff08ID\uff09\u5bf9\u8c61\u7684 OOD \u5bf9\u8c61\u3002\u6211\u4eec\u7684\u65b9\u6cd5 RONIN \u4f7f\u7528\u73b0\u6210\u7684\u6269\u6563\u6a21\u578b\u6765\u901a\u8fc7\u4fee\u590d\u6765\u66ff\u6362\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u3002 RONIN \u4f7f\u7528\u9884\u6d4b\u7684 ID \u6807\u7b7e\u6765\u8c03\u8282\u4fee\u590d\u8fc7\u7a0b\uff0c\u4f7f\u8f93\u5165\u5bf9\u8c61\u66f4\u63a5\u8fd1\u5206\u5e03\u57df\u3002\u56e0\u6b64\uff0c\u91cd\u5efa\u7684\u5bf9\u8c61\u5728 ID \u60c5\u51b5\u4e0b\u975e\u5e38\u63a5\u8fd1\u539f\u59cb\u5bf9\u8c61\uff0c\u800c\u5728 OOD \u60c5\u51b5\u4e0b\u5219\u8fdc\u79bb\u539f\u59cb\u5bf9\u8c61\uff0c\u4ece\u800c\u4f7f RONIN \u80fd\u591f\u6709\u6548\u5730\u533a\u5206 ID \u548c OOD \u6837\u672c\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 RONIN \u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff08\u65e0\u8bba\u662f\u96f6\u6837\u672c\u8fd8\u662f\u975e\u96f6\u6837\u672c\u8bbe\u7f6e\uff09\u4e0e\u4e4b\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\u90fd\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002|[2402.03292v1](http://arxiv.org/pdf/2402.03292v1)|null|\n", "2402.03290": "|**2024-02-05**|**InstanceDiffusion: Instance-level Control for Image Generation**|InstanceDiffusion\uff1a\u56fe\u50cf\u751f\u6210\u7684\u5b9e\u4f8b\u7ea7\u63a7\u5236|Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra|Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\\text{box}$ for box inputs, and 25.4% IoU for mask inputs.|\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u65e0\u6cd5\u63a7\u5236\u56fe\u50cf\u4e2d\u7684\u5404\u4e2a\u5b9e\u4f8b\u3002\u6211\u4eec\u5f15\u5165\u4e86 InstanceDiffusion\uff0c\u5b83\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u6dfb\u52a0\u4e86\u7cbe\u786e\u7684\u5b9e\u4f8b\u7ea7\u63a7\u5236\u3002 InstanceDiffusion \u652f\u6301\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u6761\u4ef6\uff0c\u5e76\u5141\u8bb8\u4ee5\u7075\u6d3b\u7684\u65b9\u5f0f\u6307\u5b9a\u5b9e\u4f8b\u4f4d\u7f6e\uff0c\u4f8b\u5982\u7b80\u5355\u7684\u5355\u70b9\u3001\u6d82\u9e26\u3001\u8fb9\u754c\u6846\u6216\u590d\u6742\u7684\u5b9e\u4f8b\u5206\u5272\u63a9\u7801\u53ca\u5176\u7ec4\u5408\u3002\u6211\u4eec\u5efa\u8bae\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u8fdb\u884c\u4e09\u9879\u91cd\u5927\u66f4\u6539\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u5b9e\u4f8b\u7ea7\u63a7\u5236\u3002\u6211\u4eec\u7684 UniFusion \u6a21\u5757\u652f\u6301\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b9e\u4f8b\u7ea7\u6761\u4ef6\uff0cScaleU \u6a21\u5757\u63d0\u9ad8\u4e86\u56fe\u50cf\u4fdd\u771f\u5ea6\uff0c\u6211\u4eec\u7684\u591a\u5b9e\u4f8b\u91c7\u6837\u5668\u6539\u8fdb\u4e86\u591a\u4e2a\u5b9e\u4f8b\u7684\u751f\u6210\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u4f4d\u7f6e\u6761\u4ef6\uff0cInstanceDiffusion \u90fd\u663e\u7740\u8d85\u8d8a\u4e86\u4e13\u95e8\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728 COCO \u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u5bf9\u4e8e\u6846\u8f93\u5165\u7684 AP$_{50}^\\text{box}$ \u4f18\u4e8e\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u6280\u672f 20.4%\uff0c\u5bf9\u4e8e\u63a9\u7801\u8f93\u5165\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6280\u672f 25.4% IoU\u3002|[2402.03290v1](http://arxiv.org/pdf/2402.03290v1)|null|\n", "2402.03227": "|**2024-02-05**|**IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images**|IGUANe\uff1a\u7528\u4e8e\u5927\u8111 MR \u56fe\u50cf\u591a\u4e2d\u5fc3\u534f\u8c03\u7684 3D \u901a\u7528 CycleGAN|Vincent Roca, Gr\u00e9gory Kuchcinski, Jean-Pierre Pruvo, Dorian Manouvriez, Renaud Lopes|In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the transformation of MR images with traveling subjects, the preservation of pairwise distances between MR images within domains, the evolution of volumetric patterns related to age and Alzheimer$^\\prime$s disease (AD), and the performance in age regression and patient classification tasks. Comparisons with other harmonization and normalization methods suggest that IGUANe better preserves individual information in MR images and is more suitable for maintaining and reinforcing variabilities related to age and AD. Future studies may further assess IGUANe in other multicenter contexts, either using the same model or retraining it for applications to different image modalities.|\u5728 MRI \u7814\u7a76\u4e2d\uff0c\u6765\u81ea\u591a\u4e2a\u91c7\u96c6\u90e8\u4f4d\u7684\u6210\u50cf\u6570\u636e\u7684\u805a\u5408\u589e\u52a0\u4e86\u6837\u672c\u91cf\uff0c\u4f46\u53ef\u80fd\u4f1a\u5f15\u5165\u4e0e\u90e8\u4f4d\u76f8\u5173\u7684\u53d8\u5f02\u6027\uff0c\u4ece\u800c\u963b\u788d\u540e\u7eed\u5206\u6790\u7684\u4e00\u81f4\u6027\u3002\u7528\u4e8e\u56fe\u50cf\u7ffb\u8bd1\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5df2\u6210\u4e3a\u8de8\u7ad9\u70b9\u534f\u8c03 MR \u56fe\u50cf\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 IGUANe\uff08\u7edf\u4e00\u5bf9\u6297\u7f51\u7edc\u56fe\u50cf\u751f\u6210\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u539f\u59cb\u7684 3D \u6a21\u578b\uff0c\u5b83\u5229\u7528\u57df\u7ffb\u8bd1\u7684\u4f18\u52bf\u548c\u98ce\u683c\u8f6c\u79fb\u65b9\u6cd5\u7684\u76f4\u63a5\u5e94\u7528\u6765\u5b9e\u73b0\u591a\u4e2d\u5fc3\u8111 MR \u56fe\u50cf\u534f\u8c03\u3002 IGUANe \u901a\u8fc7\u591a\u5bf9\u4e00\u7b56\u7565\u96c6\u6210\u4efb\u610f\u6570\u91cf\u7684\u57df\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ece\u800c\u6269\u5c55\u4e86 CycleGAN \u67b6\u6784\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u56fe\u50cf\uff0c\u751a\u81f3\u6765\u81ea\u672a\u77e5\u7684\u91c7\u96c6\u7ad9\u70b9\uff0c\u4f7f\u5176\u6210\u4e3a\u901a\u7528\u7684\u534f\u8c03\u751f\u6210\u5668\u3002 IGUANe \u5728\u5305\u542b\u6765\u81ea 11 \u4e2a\u4e0d\u540c\u626b\u63cf\u4eea\u7684 T1 \u52a0\u6743\u56fe\u50cf\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u6839\u636e\u6765\u81ea\u770b\u4e0d\u89c1\u7684\u7ad9\u70b9\u7684\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u3002\u8bc4\u4f30\u5305\u62ec MR \u56fe\u50cf\u968f\u79fb\u52a8\u5bf9\u8c61\u7684\u53d8\u6362\u3001\u57df\u5185 MR \u56fe\u50cf\u4e4b\u95f4\u6210\u5bf9\u8ddd\u79bb\u7684\u4fdd\u5b58\u3001\u4e0e\u5e74\u9f84\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5 (AD) \u76f8\u5173\u7684\u4f53\u79ef\u6a21\u5f0f\u7684\u6f14\u53d8\uff0c\u4ee5\u53ca\u5e74\u9f84\u56de\u5f52\u7684\u8868\u73b0\u548c\u60a3\u8005\u5206\u7c7b\u4efb\u52a1\u3002\u4e0e\u5176\u4ed6\u534f\u8c03\u548c\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u6bd4\u8f83\u8868\u660e\uff0cIGUANe \u66f4\u597d\u5730\u4fdd\u7559\u4e86 MR \u56fe\u50cf\u4e2d\u7684\u4e2a\u4f53\u4fe1\u606f\uff0c\u5e76\u4e14\u66f4\u9002\u5408\u7ef4\u62a4\u548c\u589e\u5f3a\u4e0e\u5e74\u9f84\u548c AD \u76f8\u5173\u7684\u53d8\u5f02\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u80fd\u4f1a\u5728\u5176\u4ed6\u591a\u4e2d\u5fc3\u73af\u5883\u4e2d\u8fdb\u4e00\u6b65\u8bc4\u4f30 IGUANe\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u6a21\u578b\u6216\u91cd\u65b0\u8bad\u7ec3\u5b83\u4ee5\u5e94\u7528\u4e8e\u4e0d\u540c\u7684\u56fe\u50cf\u6a21\u5f0f\u3002|[2402.03227v1](http://arxiv.org/pdf/2402.03227v1)|null|\n", "2402.03214": "|**2024-02-05**|**Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?**|\u6709\u673a\u8fd8\u662f\u6269\u6563\uff1a\u6211\u4eec\u53ef\u4ee5\u533a\u5206\u4eba\u7c7b\u827a\u672f\u548c\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u56fe\u50cf\u5417\uff1f|Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, Ben Y. Zhao|The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness.|\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u56fe\u50cf\u7684\u51fa\u73b0\u5f7b\u5e95\u98a0\u8986\u4e86\u827a\u672f\u4e16\u754c\u3002\u4ece\u4eba\u7c7b\u827a\u672f\u4e2d\u8bc6\u522b\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u56fe\u50cf\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5176\u5f71\u54cd\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u800c\u4e0d\u65ad\u589e\u957f\u3002\u5982\u679c\u672a\u80fd\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4e0d\u826f\u884c\u4e3a\u8005\u5c31\u53ef\u4ee5\u6b3a\u9a97\u4e3a\u4eba\u7c7b\u827a\u672f\u652f\u4ed8\u9ad8\u4ef7\u7684\u4e2a\u4eba\uff0c\u4ee5\u53ca\u5236\u5b9a\u653f\u7b56\u7981\u6b62\u4eba\u5de5\u667a\u80fd\u56fe\u50cf\u7684\u516c\u53f8\u3002\u8fd9\u5bf9\u4e8e\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u8bad\u7ec3\u8005\u6765\u8bf4\u4e5f\u81f3\u5173\u91cd\u8981\uff0c\u4ed6\u4eec\u9700\u8981\u8fc7\u6ee4\u8bad\u7ec3\u6570\u636e\u4ee5\u907f\u514d\u6f5c\u5728\u7684\u6a21\u578b\u5d29\u6e83\u3002\u6709\u51e0\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u53ef\u4ee5\u533a\u5206\u4eba\u7c7b\u827a\u672f\u548c\u4eba\u5de5\u667a\u80fd\u56fe\u50cf\uff0c\u5305\u62ec\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u3001\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u7814\u7a76\u5de5\u5177\u4ee5\u53ca\u4e13\u4e1a\u827a\u672f\u5bb6\u5229\u7528\u5176\u827a\u672f\u6280\u5de7\u77e5\u8bc6\u8fdb\u884c\u8bc6\u522b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8bd5\u56fe\u4e86\u89e3\u8fd9\u4e9b\u65b9\u6cd5\u5728\u826f\u6027\u548c\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u5bf9\u6297\u5f53\u4eca\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u7684\u8868\u73b0\u5982\u4f55\u3002\u6211\u4eec\u7b56\u5212 7 \u79cd\u98ce\u683c\u7684\u771f\u5b9e\u4eba\u7c7b\u827a\u672f\uff0c\u4ece 5 \u4e2a\u751f\u6210\u6a21\u578b\u751f\u6210\u5339\u914d\u56fe\u50cf\uff0c\u5e76\u5e94\u7528 8 \u4e2a\u68c0\u6d4b\u5668\uff085 \u4e2a\u81ea\u52a8\u68c0\u6d4b\u5668\u548c 3 \u4e2a\u4e0d\u540c\u7684\u4eba\u7c7b\u7fa4\u4f53\uff0c\u5305\u62ec 180 \u540d\u4f17\u5305\u5de5\u4f5c\u8005\u30014000 \u591a\u540d\u4e13\u4e1a\u827a\u672f\u5bb6\u548c 13 \u540d\u5728\u68c0\u6d4b AI \u65b9\u9762\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4e13\u5bb6\u827a\u672f\u5bb6\uff09\u3002 Hive \u548c\u4e13\u5bb6\u827a\u672f\u5bb6\u90fd\u505a\u5f97\u5f88\u597d\uff0c\u4f46\u4f1a\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u72af\u9519\u8bef\uff08Hive \u5728\u5bf9\u6297\u6027\u6270\u52a8\u65b9\u9762\u8f83\u5f31\uff0c\u800c\u4e13\u5bb6\u827a\u672f\u5bb6\u4f1a\u4ea7\u751f\u66f4\u9ad8\u7684\u8bef\u62a5\uff09\u3002\u6211\u4eec\u76f8\u4fe1\uff0c\u968f\u7740\u6a21\u578b\u7684\u4e0d\u65ad\u53d1\u5c55\uff0c\u8fd9\u4e9b\u5f31\u70b9\u4ecd\u5c06\u5b58\u5728\uff0c\u5e76\u4f7f\u7528\u6211\u4eec\u7684\u6570\u636e\u6765\u8bc1\u660e\u4e3a\u4ec0\u4e48\u4eba\u7c7b\u548c\u81ea\u52a8\u68c0\u6d4b\u5668\u7684\u7ec4\u5408\u56e2\u961f\u53ef\u4ee5\u63d0\u4f9b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u7684\u6700\u4f73\u7ec4\u5408\u3002|[2402.03214v1](http://arxiv.org/pdf/2402.03214v1)|null|\n", "2402.03162": "|**2024-02-05**|**Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion**|Direct-a-Video\uff1a\u901a\u8fc7\u7528\u6237\u63a7\u5236\u7684\u6444\u50cf\u673a\u79fb\u52a8\u548c\u5bf9\u8c61\u8fd0\u52a8\u751f\u6210\u5b9a\u5236\u89c6\u9891|Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, Jing Liao|Recent text-to-video diffusion models have achieved impressive progress. In practice, users often desire the ability to control object motion and camera movement independently for customized video creation. However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models. In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for one or multiple objects and/or camera movements, as if directing a video. We propose a simple yet effective strategy for the decoupled control of object motion and camera movement. Object motion is controlled through spatial cross-attention modulation using the model's inherent priors, requiring no additional optimization. For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters. We further employ an augmentation-based approach to train these layers in a self-supervised manner on a small-scale dataset, eliminating the need for explicit motion annotation. Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios. Extensive experiments demonstrate the superiority and effectiveness of our method. Project page: https://direct-a-video.github.io/.|\u6700\u8fd1\u7684\u6587\u672c\u5230\u89c6\u9891\u4f20\u64ad\u6a21\u578b\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u8fdb\u5c55\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u7528\u6237\u901a\u5e38\u5e0c\u671b\u80fd\u591f\u72ec\u7acb\u63a7\u5236\u5bf9\u8c61\u8fd0\u52a8\u548c\u6444\u50cf\u673a\u79fb\u52a8\u4ee5\u8fdb\u884c\u5b9a\u5236\u89c6\u9891\u521b\u5efa\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u7f3a\u4e4f\u4ee5\u89e3\u8026\u7684\u65b9\u5f0f\u5355\u72ec\u63a7\u5236\u5bf9\u8c61\u8fd0\u52a8\u548c\u76f8\u673a\u8fd0\u52a8\u7684\u91cd\u70b9\uff0c\u8fd9\u9650\u5236\u4e86\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u53ef\u63a7\u6027\u548c\u7075\u6d3b\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Direct-a-Video\uff0c\u8fd9\u662f\u4e00\u79cd\u5141\u8bb8\u7528\u6237\u72ec\u7acb\u6307\u5b9a\u4e00\u4e2a\u6216\u591a\u4e2a\u5bf9\u8c61\u7684\u8fd0\u52a8\u548c/\u6216\u6444\u50cf\u673a\u8fd0\u52a8\u7684\u7cfb\u7edf\uff0c\u5c31\u50cf\u5bfc\u6f14\u89c6\u9891\u4e00\u6837\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u7b56\u7565\u6765\u89e3\u8026\u63a7\u5236\u7269\u4f53\u8fd0\u52a8\u548c\u76f8\u673a\u8fd0\u52a8\u3002\u5bf9\u8c61\u8fd0\u52a8\u662f\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u56fa\u6709\u5148\u9a8c\u7684\u7a7a\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u8c03\u5236\u6765\u63a7\u5236\u7684\uff0c\u4e0d\u9700\u8981\u989d\u5916\u7684\u4f18\u5316\u3002\u5bf9\u4e8e\u76f8\u673a\u8fd0\u52a8\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65b0\u7684\u65f6\u95f4\u4ea4\u53c9\u6ce8\u610f\u5c42\u6765\u89e3\u91ca\u5b9a\u91cf\u7684\u76f8\u673a\u8fd0\u52a8\u53c2\u6570\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u91c7\u7528\u57fa\u4e8e\u589e\u5f3a\u7684\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4ee5\u81ea\u6211\u76d1\u7763\u7684\u65b9\u5f0f\u8bad\u7ec3\u8fd9\u4e9b\u5c42\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u663e\u5f0f\u8fd0\u52a8\u6ce8\u91ca\u7684\u9700\u8981\u3002\u4e24\u4e2a\u7ec4\u4ef6\u72ec\u7acb\u8fd0\u884c\uff0c\u5141\u8bb8\u5355\u72ec\u6216\u7ec4\u5408\u63a7\u5236\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a8\u5e7f\u5230\u5f00\u653e\u57df\u573a\u666f\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u6709\u6548\u6027\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://direct-a-video.github.io/\u3002|[2402.03162v1](http://arxiv.org/pdf/2402.03162v1)|null|\n", "2402.03095": "|**2024-02-05**|**Transcending Adversarial Perturbations: Manifold-Aided Adversarial Examples with Legitimate Semantics**|\u8d85\u8d8a\u5bf9\u6297\u6027\u6270\u52a8\uff1a\u5177\u6709\u5408\u6cd5\u8bed\u4e49\u7684\u591a\u65b9\u9762\u8f85\u52a9\u5bf9\u6297\u6027\u793a\u4f8b|Shuai Li, Xiaoyu Jiang, Xiaoguang Ma|Deep neural networks were significantly vulnerable to adversarial examples manipulated by malicious tiny perturbations. Although most conventional adversarial attacks ensured the visual imperceptibility between adversarial examples and corresponding raw images by minimizing their geometric distance, these constraints on geometric distance led to limited attack transferability, inferior visual quality, and human-imperceptible interpretability. In this paper, we proposed a supervised semantic-transformation generative model to generate adversarial examples with real and legitimate semantics, wherein an unrestricted adversarial manifold containing continuous semantic variations was constructed for the first time to realize a legitimate transition from non-adversarial examples to adversarial ones. Comprehensive experiments on MNIST and industrial defect datasets showed that our adversarial examples not only exhibited better visual quality but also achieved superior attack transferability and more effective explanations for model vulnerabilities, indicating their great potential as generic adversarial examples. The code and pre-trained models were available at https://github.com/shuaili1027/MAELS.git.|\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u975e\u5e38\u5bb9\u6613\u53d7\u5230\u6076\u610f\u5fae\u5c0f\u6270\u52a8\u64cd\u7eb5\u7684\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u5f71\u54cd\u3002\u5c3d\u7ba1\u5927\u591a\u6570\u4f20\u7edf\u7684\u5bf9\u6297\u6027\u653b\u51fb\u901a\u8fc7\u6700\u5c0f\u5316\u5b83\u4eec\u7684\u51e0\u4f55\u8ddd\u79bb\u6765\u786e\u4fdd\u5bf9\u6297\u6027\u793a\u4f8b\u548c\u76f8\u5e94\u7684\u539f\u59cb\u56fe\u50cf\u4e4b\u95f4\u7684\u89c6\u89c9\u4e0d\u53ef\u611f\u77e5\u6027\uff0c\u4f46\u8fd9\u4e9b\u5bf9\u51e0\u4f55\u8ddd\u79bb\u7684\u9650\u5236\u5bfc\u81f4\u6709\u9650\u7684\u653b\u51fb\u53ef\u8f6c\u79fb\u6027\u3001\u8f83\u5dee\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u4eba\u7c7b\u4e0d\u53ef\u611f\u77e5\u7684\u53ef\u89e3\u91ca\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u76d1\u7763\u7684\u8bed\u4e49\u8f6c\u6362\u751f\u6210\u6a21\u578b\u6765\u751f\u6210\u5177\u6709\u771f\u5b9e\u5408\u6cd5\u8bed\u4e49\u7684\u5bf9\u6297\u6027\u793a\u4f8b\uff0c\u5176\u4e2d\u9996\u6b21\u6784\u5efa\u4e86\u5305\u542b\u8fde\u7eed\u8bed\u4e49\u53d8\u5316\u7684\u65e0\u9650\u5236\u5bf9\u6297\u6027\u6d41\u5f62\uff0c\u4ee5\u5b9e\u73b0\u4ece\u975e\u5bf9\u6297\u6027\u793a\u4f8b\u5230\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u5408\u6cd5\u8fc7\u6e21\u90a3\u4e9b\u3002\u5bf9 MNIST \u548c\u5de5\u4e1a\u7f3a\u9677\u6570\u636e\u96c6\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u5bf9\u6297\u6027\u793a\u4f8b\u4e0d\u4ec5\u8868\u73b0\u51fa\u66f4\u597d\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u800c\u4e14\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u653b\u51fb\u53ef\u8f6c\u79fb\u6027\u548c\u5bf9\u6a21\u578b\u6f0f\u6d1e\u7684\u66f4\u6709\u6548\u89e3\u91ca\uff0c\u8868\u660e\u5b83\u4eec\u4f5c\u4e3a\u901a\u7528\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u5de8\u5927\u6f5c\u529b\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728 https://github.com/shuaili1027/MAELS.git \u83b7\u53d6\u3002|[2402.03095v1](http://arxiv.org/pdf/2402.03095v1)|null|\n", "2402.03082": "|**2024-02-05**|**Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing**|\u89c6\u89c9\u6587\u672c\u6ee1\u8db3\u4f4e\u7ea7\u89c6\u89c9\uff1a\u89c6\u89c9\u6587\u672c\u5904\u7406\u7684\u7efc\u5408\u8c03\u67e5|Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, Yu Zhou|Visual text, a pivotal element in both document and scene images, speaks volumes and attracts significant attention in the computer vision domain. Beyond visual text detection and recognition, the field of visual text processing has experienced a surge in research, driven by the advent of fundamental generative models. However, challenges persist due to the unique properties and features that distinguish text from general objects. Effectively leveraging these unique textual characteristics is crucial in visual text processing, as observed in our study. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in this field. Initially, we introduce a hierarchical taxonomy encompassing areas ranging from text image enhancement and restoration to text image manipulation, followed by different learning paradigms. Subsequently, we conduct an in-depth discussion of how specific textual features such as structure, stroke, semantics, style, and spatial context are seamlessly integrated into various tasks. Furthermore, we explore available public datasets and benchmark the reviewed methods on several widely-used datasets. Finally, we identify principal challenges and potential avenues for future research. Our aim is to establish this survey as a fundamental resource, fostering continued exploration and innovation in the dynamic area of visual text processing.|\u89c6\u89c9\u6587\u672c\u662f\u6587\u6863\u548c\u573a\u666f\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u5143\u7d20\uff0c\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u5e76\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u9664\u4e86\u89c6\u89c9\u6587\u672c\u68c0\u6d4b\u548c\u8bc6\u522b\u4e4b\u5916\uff0c\u5728\u57fa\u672c\u751f\u6210\u6a21\u578b\u7684\u51fa\u73b0\u7684\u63a8\u52a8\u4e0b\uff0c\u89c6\u89c9\u6587\u672c\u5904\u7406\u9886\u57df\u7684\u7814\u7a76\u4e5f\u7ecf\u5386\u4e86\u6fc0\u589e\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6587\u672c\u4e0e\u4e00\u822c\u5bf9\u8c61\u7684\u72ec\u7279\u5c5e\u6027\u548c\u7279\u5f81\uff0c\u6311\u6218\u4ecd\u7136\u5b58\u5728\u3002\u6b63\u5982\u6211\u4eec\u7684\u7814\u7a76\u4e2d\u6240\u89c2\u5bdf\u5230\u7684\uff0c\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u72ec\u7279\u7684\u6587\u672c\u7279\u5f81\u5bf9\u4e8e\u89c6\u89c9\u6587\u672c\u5904\u7406\u81f3\u5173\u91cd\u8981\u3002\u5728\u8fd9\u9879\u8c03\u67e5\u4e2d\uff0c\u6211\u4eec\u5bf9\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u5168\u9762\u3001\u591a\u89d2\u5ea6\u7684\u5206\u6790\u3002\u6700\u521d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u5c42\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u4ece\u6587\u672c\u56fe\u50cf\u589e\u5f3a\u548c\u6062\u590d\u5230\u6587\u672c\u56fe\u50cf\u64cd\u4f5c\u7684\u5404\u4e2a\u9886\u57df\uff0c\u7136\u540e\u662f\u4e0d\u540c\u7684\u5b66\u4e60\u8303\u4f8b\u3002\u968f\u540e\uff0c\u6211\u4eec\u6df1\u5165\u8ba8\u8bba\u4e86\u7ed3\u6784\u3001\u7b14\u753b\u3001\u8bed\u4e49\u3001\u98ce\u683c\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7b49\u7279\u5b9a\u6587\u672c\u7279\u5f81\u5982\u4f55\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u4efb\u52a1\u4e2d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u7d22\u53ef\u7528\u7684\u516c\u5171\u6570\u636e\u96c6\uff0c\u5e76\u5728\u51e0\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u5bf9\u5ba1\u67e5\u7684\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u6700\u540e\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u672a\u6765\u7814\u7a76\u7684\u4e3b\u8981\u6311\u6218\u548c\u6f5c\u5728\u9014\u5f84\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u5c06\u8fd9\u9879\u8c03\u67e5\u4f5c\u4e3a\u57fa\u672c\u8d44\u6e90\uff0c\u4fc3\u8fdb\u89c6\u89c9\u6587\u672c\u5904\u7406\u52a8\u6001\u9886\u57df\u7684\u6301\u7eed\u63a2\u7d22\u548c\u521b\u65b0\u3002|[2402.03082v1](http://arxiv.org/pdf/2402.03082v1)|null|\n", "2402.03047": "|**2024-02-05**|**PFDM: Parser-Free Virtual Try-on via Diffusion Model**|PFDM\uff1a\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fdb\u884c\u65e0\u89e3\u6790\u5668\u865a\u62df\u8bd5\u6234|Yunfang Niu, Dong Yi, Lingxiang Wu, Zhiwei Liu, Pengxiang Cai, Jinqiao Wang|Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can \"wear\" garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and outperform both state-of-the-art parser-free and parser-based models.|\u865a\u62df\u8bd5\u7a7f\u53ef\u4ee5\u663e\u7740\u6539\u5584\u5728\u7ebf\u548c\u5e97\u5185\u573a\u666f\u7684\u670d\u88c5\u8d2d\u7269\u4f53\u9a8c\uff0c\u5438\u5f15\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5e7f\u6cdb\u5174\u8da3\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u4fdd\u771f\u8bd5\u6234\u6027\u80fd\uff0c\u5927\u591a\u6570\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u4ecd\u7136\u4f9d\u8d56\u4e8e\u51c6\u786e\u7684\u5206\u5272\u63a9\u6a21\uff0c\u8fd9\u4e9b\u63a9\u6a21\u901a\u5e38\u662f\u7531\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u89e3\u6790\u5668\u6216\u624b\u52a8\u6807\u8bb0\u751f\u6210\u7684\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u74f6\u9888\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff08PFDM\uff09\u7684\u65e0\u89e3\u6790\u5668\u865a\u62df\u8bd5\u6234\u65b9\u6cd5\u3002\u7ed9\u5b9a\u4e24\u5f20\u56fe\u50cf\uff0cPFDM \u53ef\u4ee5\u901a\u8fc7\u9690\u5f0f\u53d8\u5f62\u5728\u6ca1\u6709\u4efb\u4f55\u5176\u4ed6\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u65e0\u7f1d\u5730\u201c\u7a7f\u7740\u201d\u76ee\u6807\u4eba\u7269\u7684\u8863\u670d\u3002\u4e3a\u4e86\u6709\u6548\u5730\u5b66\u4e60\u6a21\u578b\uff0c\u6211\u4eec\u5408\u6210\u4e86\u8bb8\u591a\u4f2a\u56fe\u50cf\u5e76\u901a\u8fc7\u5728\u4eba\u8eab\u4e0a\u7a7f\u7740\u5404\u79cd\u670d\u88c5\u6765\u6784\u5efa\u6837\u672c\u5bf9\u3002\u5728\u5927\u89c4\u6a21\u6269\u5c55\u6570\u636e\u96c6\u7684\u76d1\u7763\u4e0b\uff0c\u6211\u4eec\u4f7f\u7528\u63d0\u51fa\u7684\u670d\u88c5\u878d\u5408\u6ce8\u610f\uff08GFA\uff09\u673a\u5236\u878d\u5408\u4eba\u7269\u548c\u670d\u88c5\u7279\u5f81\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 PFDM \u53ef\u4ee5\u6210\u529f\u5904\u7406\u590d\u6742\u7684\u60c5\u51b5\uff0c\u5408\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\uff0c\u5e76\u4e14\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0\u89e3\u6790\u5668\u548c\u57fa\u4e8e\u89e3\u6790\u5668\u7684\u6a21\u578b\u3002|[2402.03047v1](http://arxiv.org/pdf/2402.03047v1)|null|\n", "2402.03040": "|**2024-02-05**|**InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions**|InteractiveVideo\uff1a\u5177\u6709\u534f\u540c\u591a\u6a21\u5f0f\u6307\u4ee4\u7684\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u53ef\u63a7\u89c6\u9891\u751f\u6210|Yiyuan Zhang, Yuhao Kang, Zhixin Zhang, Xiaohan Ding, Sanyuan Zhao, Xiangyu Yue|We introduce $\\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions until their requirements are fully met. Code, models, and demo are available at https://github.com/invictus717/InteractiveVideo|\u6211\u4eec\u5f15\u5165 $\\textit{InteractiveVideo}$\uff0c\u4e00\u4e2a\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\u3002\u4e0e\u57fa\u4e8e\u7528\u6237\u63d0\u4f9b\u7684\u56fe\u50cf\u6216\u6587\u672c\u64cd\u4f5c\u7684\u4f20\u7edf\u751f\u6210\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u6846\u67b6\u662f\u4e3a\u52a8\u6001\u4ea4\u4e92\u800c\u8bbe\u8ba1\u7684\uff0c\u5141\u8bb8\u7528\u6237\u5728\u6574\u4e2a\u751f\u6210\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u5404\u79cd\u76f4\u89c2\u7684\u673a\u5236\u6765\u6307\u5bfc\u751f\u6210\u6a21\u578b\uff0c\u4f8b\u5982\u6587\u672c\u548c\u56fe\u50cf\u63d0\u793a\u3001\u7ed8\u753b\u3001\u62d6\u653e\u7b49\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u540c\u591a\u6a21\u6001\u6307\u4ee4\u673a\u5236\uff0c\u65e8\u5728\u5c06\u7528\u6237\u7684\u591a\u6a21\u6001\u6307\u4ee4\u65e0\u7f1d\u96c6\u6210\u5230\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u4fc3\u8fdb\u7528\u6237\u8f93\u5165\u548c\u751f\u6210\u8fc7\u7a0b\u4e4b\u95f4\u7684\u534f\u4f5c\u548c\u54cd\u5e94\u4ea4\u4e92\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u7cbe\u786e\u6709\u6548\u7684\u7528\u6237\u6307\u4ee4\u5bf9\u751f\u6210\u7ed3\u679c\u8fdb\u884c\u8fed\u4ee3\u548c\u7ec6\u7c92\u5ea6\u7684\u7ec6\u5316\u3002\u901a\u8fc7 $\\textit{InteractiveVideo}$\uff0c\u7528\u6237\u53ef\u4ee5\u7075\u6d3b\u5730\u7cbe\u5fc3\u5b9a\u5236\u89c6\u9891\u7684\u5173\u952e\u65b9\u9762\u3002\u4ed6\u4eec\u53ef\u4ee5\u7ed8\u5236\u53c2\u8003\u56fe\u50cf\u3001\u7f16\u8f91\u8bed\u4e49\u5e76\u8c03\u6574\u89c6\u9891\u52a8\u4f5c\uff0c\u76f4\u5230\u5b8c\u5168\u6ee1\u8db3\u4ed6\u4eec\u7684\u8981\u6c42\u3002\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6f14\u793a\u53ef\u5728 https://github.com/invictus717/InteractiveVideo \u83b7\u53d6|[2402.03040v1](http://arxiv.org/pdf/2402.03040v1)|null|\n", "2402.02972": "|**2024-02-05**|**Retrieval-Augmented Score Distillation for Text-to-3D Generation**|\u7528\u4e8e\u6587\u672c\u8f6c 3D \u751f\u6210\u7684\u68c0\u7d22\u589e\u5f3a\u5206\u6570\u84b8\u998f|Junyoung Seo, Susung Hong, Wooseok Jang, In\u00e8s Hyeonsu Kim, Minseop Kwak, Doyup Lee, Seungryong Kim|Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency. Project page is available at https://ku-cvlab.github.io/RetDream/.|\u6587\u672c\u5230 3D \u751f\u6210\u901a\u8fc7\u7ed3\u5408\u5f3a\u5927\u7684 2D \u6269\u6563\u6a21\u578b\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u6210\u529f\uff0c\u4f46 3D \u5148\u9a8c\u77e5\u8bc6\u4e0d\u8db3\u4e5f\u5bfc\u81f4\u4e86 3D \u51e0\u4f55\u7684\u4e0d\u4e00\u81f4\u3002\u8fd1\u5e74\u6765\uff0c\u968f\u7740\u5927\u89c4\u6a21\u591a\u89c6\u56fe\u6570\u636e\u96c6\u7684\u53d1\u5e03\uff0c\u5728\u591a\u89c6\u56fe\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6269\u6563\u6a21\u578b\u6210\u4e3a\u89e3\u51b33D\u4e0d\u4e00\u81f4\u95ee\u9898\u7684\u4e3b\u6d41\u3002\u7136\u800c\uff0c\u4e0e 2D \u6570\u636e\u76f8\u6bd4\uff0c3D \u6570\u636e\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u6709\u9650\uff0c\u56e0\u6b64\u9762\u4e34\u7740\u6839\u672c\u6027\u7684\u56f0\u96be\u3002\u4e3a\u4e86\u907f\u514d\u8fd9\u4e9b\u6743\u8861\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u79cd\u4e13\u4e3a\u5206\u6570\u84b8\u998f\u800c\u8bbe\u8ba1\u7684\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\uff0c\u79f0\u4e3a RetDream\u3002\u6211\u4eec\u5047\u8bbe\u901a\u8fc7\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u4f7f\u7528\u8bed\u4e49\u76f8\u5173\u8d44\u4ea7\uff0c\u53ef\u4ee5\u5145\u5206\u5229\u7528 2D \u6269\u6563\u6a21\u578b\u7684\u8868\u73b0\u529b\u548c 3D \u8d44\u4ea7\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6587\u672c\u8f6c 3D \u751f\u6210\u4e2d\u57fa\u4e8e\u68c0\u7d22\u7684\u8d28\u91cf\u589e\u5f3a\u7684\u65b0\u9896\u6846\u67b6\u3002\u6211\u4eec\u5229\u7528\u68c0\u7d22\u5230\u7684\u8d44\u4ea7\u5c06\u5176\u51e0\u4f55\u5148\u9a8c\u5408\u5e76\u5230\u53d8\u5206\u76ee\u6807\u4e2d\uff0c\u5e76\u8c03\u6574\u6269\u6563\u6a21\u578b\u7684 2D \u5148\u9a8c\u4ee5\u5b9e\u73b0\u89c6\u56fe\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5728\u751f\u6210\u573a\u666f\u7684\u51e0\u4f55\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u5b9e\u73b0\u663e\u7740\u6539\u8fdb\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\u6765\u8bc1\u660e RetDream \u5177\u6709\u5353\u8d8a\u7684\u54c1\u8d28\u548c\u66f4\u9ad8\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u9879\u76ee\u9875\u9762\u4f4d\u4e8e https://ku-cvlab.github.io/RetDream/\u3002|[2402.02972v1](http://arxiv.org/pdf/2402.02972v1)|null|\n", "2402.02928": "|**2024-02-05**|**Instance Segmentation XXL-CT Challenge of a Historic Airplane**|\u5386\u53f2\u98de\u673a\u7684\u5b9e\u4f8b\u5206\u5272 XXL-CT \u6311\u6218|Roland Gruber, Johann Christopher Engster, Markus Michen, Nele Blum, Maik Stille, Stefan Gerth, Thomas Wittenberg|Instance segmentation of compound objects in XXL-CT imagery poses a unique challenge in non-destructive testing. This complexity arises from the lack of known reference segmentation labels, limited applicable segmentation tools, as well as partially degraded image quality. To asses recent advancements in the field of machine learning-based image segmentation, the \"Instance Segmentation XXL-CT Challenge of a Historic Airplane\" was conducted. The challenge aimed to explore automatic or interactive instance segmentation methods for an efficient delineation of the different aircraft components, such as screws, rivets, metal sheets or pressure tubes. We report the organization and outcome of this challenge and describe the capabilities and limitations of the submitted segmentation methods.|XXL-CT \u56fe\u50cf\u4e2d\u590d\u5408\u5bf9\u8c61\u7684\u5b9e\u4f8b\u5206\u5272\u5bf9\u65e0\u635f\u6d4b\u8bd5\u63d0\u51fa\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u8fd9\u79cd\u590d\u6742\u6027\u6e90\u4e8e\u7f3a\u4e4f\u5df2\u77e5\u7684\u53c2\u8003\u5206\u5272\u6807\u7b7e\u3001\u6709\u9650\u7684\u9002\u7528\u5206\u5272\u5de5\u5177\u4ee5\u53ca\u90e8\u5206\u964d\u4f4e\u7684\u56fe\u50cf\u8d28\u91cf\u3002\u4e3a\u4e86\u8bc4\u4f30\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u56fe\u50cf\u5206\u5272\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5f00\u5c55\u4e86\u201c\u5386\u53f2\u98de\u673a\u7684\u5b9e\u4f8b\u5206\u5272 XXL-CT \u6311\u6218\u201d\u3002\u8be5\u6311\u6218\u65e8\u5728\u63a2\u7d22\u81ea\u52a8\u6216\u4ea4\u4e92\u5f0f\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\uff0c\u4ee5\u6709\u6548\u63cf\u7ed8\u4e0d\u540c\u7684\u98de\u673a\u90e8\u4ef6\uff0c\u4f8b\u5982\u87ba\u9489\u3001\u94c6\u9489\u3001\u91d1\u5c5e\u677f\u6216\u538b\u529b\u7ba1\u3002\u6211\u4eec\u62a5\u544a\u4e86\u8fd9\u4e00\u6311\u6218\u7684\u7ec4\u7ec7\u548c\u7ed3\u679c\uff0c\u5e76\u63cf\u8ff0\u4e86\u6240\u63d0\u4ea4\u7684\u5206\u5272\u65b9\u6cd5\u7684\u529f\u80fd\u548c\u5c40\u9650\u6027\u3002|[2402.02928v1](http://arxiv.org/pdf/2402.02928v1)|null|\n", "2402.02906": "|**2024-02-05**|**ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis**|ViewFusion\uff1a\u5b66\u4e60\u7528\u4e8e\u65b0\u89c6\u56fe\u5408\u6210\u7684\u53ef\u7ec4\u5408\u6269\u6563\u6a21\u578b|Bernard Spiegl, Andrea Perin, St\u00e9phane Deny, Alexander Ilin|Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR. Code is available.|\u6df1\u5ea6\u5b66\u4e60\u4e3a\u89e3\u51b3\u65b0\u9896\u89c6\u56fe\u5408\u6210\u7684\u8001\u95ee\u9898\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u65b0\u65b9\u6cd5\uff0c\u4ece\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u65b9\u6cd5\u5230\u7aef\u5230\u7aef\u98ce\u683c\u67b6\u6784\u3002\u6bcf\u79cd\u65b9\u6cd5\u90fd\u6709\u7279\u5b9a\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u9002\u7528\u6027\u4e0a\u4e5f\u6709\u7279\u5b9a\u7684\u9650\u5236\u3002\u8fd9\u9879\u5de5\u4f5c\u4ecb\u7ecd\u4e86 ViewFusion\uff0c\u8fd9\u662f\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\uff0c\u5177\u6709\u65e0\u4e0e\u4f26\u6bd4\u7684\u7075\u6d3b\u6027\u3002 ViewFusion \u5305\u62ec\u540c\u65f6\u5bf9\u573a\u666f\u7684\u4efb\u610f\u6570\u91cf\u7684\u8f93\u5165\u89c6\u56fe\u5e94\u7528\u6269\u6563\u53bb\u566a\u6b65\u9aa4\uff0c\u7136\u540e\u5c06\u6bcf\u4e2a\u89c6\u56fe\u83b7\u5f97\u7684\u566a\u58f0\u68af\u5ea6\u4e0e\uff08\u63a8\u65ad\u7684\uff09\u50cf\u7d20\u52a0\u6743\u63a9\u6a21\u76f8\u7ed3\u5408\uff0c\u786e\u4fdd\u5bf9\u4e8e\u76ee\u6807\u573a\u666f\u7684\u6bcf\u4e2a\u533a\u57df\uff0c\u53ea\u6709\u6700\u8003\u8651\u5230\u4fe1\u606f\u4e30\u5bcc\u7684\u8f93\u5165\u89c6\u56fe\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u89e3\u51b3\u4e86\u4ee5\u524d\u65b9\u6cd5\u7684\u51e0\u4e2a\u5c40\u9650\u6027\uff1a\uff081\uff09\u53ef\u5728\u591a\u4e2a\u573a\u666f\u548c\u5bf9\u8c61\u7c7b\u522b\u4e2d\u8fdb\u884c\u8bad\u7ec3\u548c\u6cdb\u5316\uff0c\uff082\uff09\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u5730\u91c7\u7528\u53ef\u53d8\u6570\u91cf\u7684\u65e0\u59ff\u52bf\u89c6\u56fe\uff0c\uff083\uff09\u751f\u6210\u5408\u7406\u7684\u89c6\u56fe\u5373\u4f7f\u5728\u4e25\u91cd\u4e0d\u786e\u5b9a\u7684\u6761\u4ef6\u4e0b\uff08\u7531\u4e8e\u5176\u751f\u6210\u6027\uff09 - \u540c\u65f6\u751f\u6210\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u8d28\u91cf\u89c6\u56fe\u3002\u5c40\u9650\u6027\u5305\u62ec\u4e0d\u751f\u6210\u573a\u666f\u7684 3D \u5d4c\u5165\uff0c\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u76f8\u5bf9\u8f83\u6162\uff0c\u5e76\u4e14\u6211\u4eec\u7684\u65b9\u6cd5\u4ec5\u5728\u76f8\u5bf9\u8f83\u5c0f\u7684\u6570\u636e\u96c6 NMR \u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u4ee3\u7801\u53ef\u7528\u3002|[2402.02906v1](http://arxiv.org/pdf/2402.02906v1)|null|\n", "2402.02826": "|**2024-02-05**|**SynthVision - Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data**|SynthVision - \u4f7f\u7528\u5408\u6210\u56fe\u50cf\u6570\u636e\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4e2d\u5229\u7528\u6700\u5c0f\u8f93\u5165\u83b7\u5f97\u6700\u5927\u8f93\u51fa|Yudara Kularathne, Prathapa Janitha, Sithira Ambepitiya, Thanveer Ahamed, Dinuka Wijesundara, Prarththanan Sothyrajah|Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism. However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data. We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data. In our study, we employed a two phase experimental design using diffusion models. In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts. The second phase involved the training and testing vision model using this synthetic dataset. This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision model performance in medical image recognition. The study findings revealed significant insights into the performance of the vision model trained on synthetic images generated through diffusion models. The vision model showed exceptional performance in accurately identifying cases of genital warts. It achieved an accuracy rate of 96% underscoring its effectiveness in medical image classification. For HPV cases the model demonstrated a high precision of 99% and a recall of 94%. In normal cases the precision was 95% with an impressive recall of 99%. These metrics indicate the model capability to correctly identify true positive cases and minimize false positives. The model achieved an F1 Score of 96% for HPV cases and 97% for normal cases. The high F1 Score across both categories highlights the balanced nature of the model precision and recall ensuring reliability and robustness in its predictions.|\u75be\u75c5\u68c0\u6d4b\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u4e8e\u5e94\u5bf9\u6d41\u884c\u75c5\u6216\u751f\u7269\u6050\u6016\u4e3b\u4e49\u4e8b\u4ef6\u7b49\u7d27\u6025\u533b\u7597\u5371\u673a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u5bf9\u4e8e\u8fd9\u4e9b\u573a\u666f\u6765\u8bf4\u592a\u6162\uff0c\u9700\u8981\u521b\u65b0\u65b9\u6cd5\u4ece\u6700\u5c11\u7684\u6570\u636e\u5feb\u901f\u751f\u6210\u53ef\u9760\u7684\u6a21\u578b\u3002\u6211\u4eec\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u6765\u5c55\u793a\u6211\u4eec\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u6765\u68c0\u6d4b\u4eba\u4e73\u5934\u7624\u75c5\u6bd2\u751f\u6b96\u5668\u75a3\u3002\u5728\u6211\u4eec\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4f7f\u7528\u6269\u6563\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u5b9e\u9a8c\u8bbe\u8ba1\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4ece \u200b\u200b10 \u4e2a HPV \u5f15\u5bfc\u56fe\u50cf\u751f\u6210\u5927\u91cf\u4e0d\u540c\u7684\u5408\u6210\u56fe\u50cf\uff0c\u660e\u786e\u4e13\u6ce8\u4e8e\u51c6\u786e\u63cf\u7ed8\u5c16\u9510\u6e7f\u75a3\u3002\u7b2c\u4e8c\u9636\u6bb5\u6d89\u53ca\u4f7f\u7528\u8be5\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u548c\u6d4b\u8bd5\u89c6\u89c9\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u8bc4\u4f30\u6269\u6563\u6a21\u578b\u5728\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\u4ee5\u53ca\u968f\u540e\u5bf9\u533b\u5b66\u56fe\u50cf\u8bc6\u522b\u4e2d\u89c6\u89c9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5bf9\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u8bad\u7ec3\u7684\u89c6\u89c9\u6a21\u578b\u7684\u6027\u80fd\u7684\u91cd\u8981\u89c1\u89e3\u3002\u89c6\u89c9\u6a21\u578b\u5728\u51c6\u786e\u8bc6\u522b\u5c16\u9510\u6e7f\u75a3\u75c5\u4f8b\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002\u5b83\u8fbe\u5230\u4e86 96% \u7684\u51c6\u786e\u7387\uff0c\u7a81\u663e\u4e86\u5176\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u5bf9\u4e8e HPV \u75c5\u4f8b\uff0c\u8be5\u6a21\u578b\u8868\u73b0\u51fa 99% \u7684\u9ad8\u7cbe\u5ea6\u548c 94% \u7684\u53ec\u56de\u7387\u3002\u5728\u6b63\u5e38\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u7387\u4e3a 95%\uff0c\u53ec\u56de\u7387\u9ad8\u8fbe 99%\u3002\u8fd9\u4e9b\u6307\u6807\u8868\u660e\u6a21\u578b\u80fd\u591f\u6b63\u786e\u8bc6\u522b\u771f\u9633\u6027\u75c5\u4f8b\u5e76\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8bef\u62a5\u3002\u8be5\u6a21\u578b\u5728 HPV \u75c5\u4f8b\u4e2d\u7684 F1 \u5f97\u5206\u4e3a 96%\uff0c\u5728\u6b63\u5e38\u75c5\u4f8b\u4e2d\u7684 F1 \u5f97\u5206\u4e3a 97%\u3002\u4e24\u4e2a\u7c7b\u522b\u7684\u9ad8 F1 \u5206\u6570\u51f8\u663e\u4e86\u6a21\u578b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u7684\u5e73\u8861\u6027\u8d28\uff0c\u786e\u4fdd\u4e86\u9884\u6d4b\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002|[2402.02826v1](http://arxiv.org/pdf/2402.02826v1)|null|\n", "2402.02800": "|**2024-02-05**|**Extreme Two-View Geometry From Object Poses with Diffusion Models**|\u5177\u6709\u6269\u6563\u6a21\u578b\u7684\u7269\u4f53\u59ff\u52bf\u7684\u6781\u7aef\u4e8c\u89c6\u56fe\u51e0\u4f55|Yujing Sun, Caiyi Sun, Yuan Liu, Yuexin Ma, Siu Ming Yiu|Human has an incredible ability to effortlessly perceive the viewpoint difference between two images containing the same object, even when the viewpoint change is astonishingly vast with no co-visible regions in the images. This remarkable skill, however, has proven to be a challenge for existing camera pose estimation methods, which often fail when faced with large viewpoint differences due to the lack of overlapping local features for matching. In this paper, we aim to effectively harness the power of object priors to accurately determine two-view geometry in the face of extreme viewpoint changes. In our method, we first mathematically transform the relative camera pose estimation problem to an object pose estimation problem. Then, to estimate the object pose, we utilize the object priors learned from a diffusion model Zero123 to synthesize novel-view images of the object. The novel-view images are matched to determine the object pose and thus the two-view camera pose. In experiments, our method has demonstrated extraordinary robustness and resilience to large viewpoint changes, consistently estimating two-view poses with exceptional generalization ability across both synthetic and real-world datasets. Code will be available at https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models.|\u4eba\u7c7b\u5177\u6709\u4ee4\u4eba\u96be\u4ee5\u7f6e\u4fe1\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u6beb\u4e0d\u8d39\u529b\u5730\u611f\u77e5\u5305\u542b\u540c\u4e00\u5bf9\u8c61\u7684\u4e24\u4e2a\u56fe\u50cf\u4e4b\u95f4\u7684\u89c6\u70b9\u5dee\u5f02\uff0c\u5373\u4f7f\u89c6\u70b9\u53d8\u5316\u60ca\u4eba\u5730\u5de8\u5927\u5e76\u4e14\u56fe\u50cf\u4e2d\u6ca1\u6709\u5171\u540c\u53ef\u89c1\u7684\u533a\u57df\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u975e\u51e1\u7684\u6280\u80fd\u5df2\u88ab\u8bc1\u660e\u5bf9\u73b0\u6709\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7531\u4e8e\u7f3a\u4e4f\u7528\u4e8e\u5339\u914d\u7684\u91cd\u53e0\u5c40\u90e8\u7279\u5f81\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u9762\u4e34\u8f83\u5927\u89c6\u70b9\u5dee\u5f02\u65f6\u5e38\u5e38\u4f1a\u5931\u8d25\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u6709\u6548\u5229\u7528\u5bf9\u8c61\u5148\u9a8c\u7684\u529b\u91cf\uff0c\u5728\u9762\u5bf9\u6781\u7aef\u89c6\u70b9\u53d8\u5316\u65f6\u51c6\u786e\u786e\u5b9a\u53cc\u89c6\u56fe\u51e0\u4f55\u3002\u5728\u6211\u4eec\u7684\u65b9\u6cd5\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u5728\u6570\u5b66\u4e0a\u5c06\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\u8f6c\u6362\u4e3a\u5bf9\u8c61\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\u3002\u7136\u540e\uff0c\u4e3a\u4e86\u4f30\u8ba1\u7269\u4f53\u59ff\u6001\uff0c\u6211\u4eec\u5229\u7528\u4ece\u6269\u6563\u6a21\u578b Zero123 \u5b66\u5230\u7684\u7269\u4f53\u5148\u9a8c\u6765\u5408\u6210\u7269\u4f53\u7684\u65b0\u9896\u89c6\u56fe\u56fe\u50cf\u3002\u5339\u914d\u65b0\u89c6\u56fe\u56fe\u50cf\u4ee5\u786e\u5b9a\u5bf9\u8c61\u59ff\u52bf\uff0c\u4ece\u800c\u786e\u5b9a\u53cc\u89c6\u56fe\u76f8\u673a\u59ff\u52bf\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u4e86\u5bf9\u5927\u89c6\u70b9\u53d8\u5316\u7684\u975e\u51e1\u9c81\u68d2\u6027\u548c\u5f39\u6027\uff0c\u80fd\u591f\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4e00\u81f4\u5730\u4f30\u8ba1\u53cc\u89c6\u56fe\u59ff\u52bf\uff0c\u5e76\u5177\u6709\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4ee3\u7801\u5c06\u5728 https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models \u4e0a\u63d0\u4f9b\u3002|[2402.02800v1](http://arxiv.org/pdf/2402.02800v1)|null|\n", "2402.02739": "|**2024-02-05**|**DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models**|DisDet\uff1a\u63a2\u7d22\u6269\u6563\u6a21\u578b\u540e\u95e8\u653b\u51fb\u7684\u53ef\u68c0\u6d4b\u6027|Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, Bo Yuan|In the exciting generative AI era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical. Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique.   In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works. Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection. Based on this finding, we propose a low-cost trigger detection mechanism that can effectively identify the poisoned input noise. We then take a further step to study the same problem from the attack side, proposing a backdoor attack strategy that can learn the unnoticeable trigger to evade our proposed detection scheme.   Empirical evaluations across various diffusion models and datasets demonstrate the effectiveness of the proposed trigger detection and detection-evading attack strategy. For trigger detection, our distribution discrepancy-based solution can achieve a 100\\% detection rate for the Trojan triggers used in the existing works. For evading trigger detection, our proposed stealthy trigger design approach performs end-to-end learning to make the distribution of poisoned noise input approach that of benign noise, enabling nearly 100\\% detection pass rate with very high attack and benign performance for the backdoored diffusion models.|\u5728\u4ee4\u4eba\u5174\u594b\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\uff0c\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u4e00\u79cd\u975e\u5e38\u5f3a\u5927\u4e14\u5e7f\u6cdb\u91c7\u7528\u7684\u5404\u79cd\u6570\u636e\u6a21\u6001\u7684\u5185\u5bb9\u751f\u6210\u548c\u7f16\u8f91\u5de5\u5177\uff0c\u4f7f\u5f97\u7814\u7a76\u5176\u6f5c\u5728\u5b89\u5168\u98ce\u9669\u53d8\u5f97\u975e\u5e38\u5fc5\u8981\u548c\u5173\u952e\u3002\u6700\u8fd1\uff0c\u4e00\u4e9b\u5f00\u521b\u6027\u7684\u5de5\u4f5c\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u9488\u5bf9\u540e\u95e8\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u547c\u5401\u5bf9\u8fd9\u79cd\u6d41\u884c\u4e14\u57fa\u7840\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5b89\u5168\u6311\u6218\u8fdb\u884c\u6df1\u5165\u5206\u6790\u548c\u8c03\u67e5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u6b21\u7cfb\u7edf\u5730\u63a2\u8ba8\u4e86\u540e\u95e8\u6269\u6563\u6a21\u578b\u7684\u4e2d\u6bd2\u566a\u58f0\u8f93\u5165\u7684\u53ef\u68c0\u6d4b\u6027\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6027\u80fd\u6307\u6807\uff0c\u4f46\u5728\u73b0\u6709\u7684\u5de5\u4f5c\u4e2d\u5f88\u5c11\u8fdb\u884c\u63a2\u7d22\u3002\u6211\u4eec\u4ece\u9632\u5fa1\u8005\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u9996\u5148\u5206\u6790\u73b0\u6709\u6269\u6563\u540e\u95e8\u653b\u51fb\u4e2d\u89e6\u53d1\u6a21\u5f0f\u7684\u7279\u6027\uff0c\u53d1\u73b0\u5206\u5e03\u5dee\u5f02\u5728\u6728\u9a6c\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u89e6\u53d1\u68c0\u6d4b\u673a\u5236\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8bc6\u522b\u4e2d\u6bd2\u7684\u8f93\u5165\u566a\u58f0\u3002\u7136\u540e\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u4ece\u653b\u51fb\u65b9\u7814\u7a76\u540c\u6837\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u540e\u95e8\u653b\u51fb\u7b56\u7565\uff0c\u53ef\u4ee5\u5b66\u4e60\u4e0d\u660e\u663e\u7684\u89e6\u53d1\u56e0\u7d20\u6765\u9003\u907f\u6211\u4eec\u63d0\u51fa\u7684\u68c0\u6d4b\u65b9\u6848\u3002\u5bf9\u5404\u79cd\u6269\u6563\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u89e6\u53d1\u68c0\u6d4b\u548c\u68c0\u6d4b\u89c4\u907f\u653b\u51fb\u7b56\u7565\u7684\u6709\u6548\u6027\u3002\u5bf9\u4e8e\u89e6\u53d1\u5668\u68c0\u6d4b\uff0c\u6211\u4eec\u57fa\u4e8e\u5206\u5e03\u5dee\u5f02\u7684\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u5bf9\u73b0\u6709\u4f5c\u54c1\u4e2d\u4f7f\u7528\u7684\u6728\u9a6c\u89e6\u53d1\u5668\u5b9e\u73b0100\uff05\u7684\u68c0\u6d4b\u7387\u3002\u4e3a\u4e86\u9003\u907f\u89e6\u53d1\u68c0\u6d4b\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u9690\u5f62\u89e6\u53d1\u8bbe\u8ba1\u65b9\u6cd5\u6267\u884c\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u4f7f\u6709\u6bd2\u566a\u58f0\u8f93\u5165\u7684\u5206\u5e03\u63a5\u8fd1\u826f\u6027\u566a\u58f0\u7684\u5206\u5e03\uff0c\u4ece\u800c\u4f7f\u540e\u95e8\u7684\u68c0\u6d4b\u901a\u8fc7\u7387\u63a5\u8fd1 100%\uff0c\u5177\u6709\u975e\u5e38\u9ad8\u7684\u653b\u51fb\u548c\u826f\u6027\u6027\u80fd\u6269\u6563\u6a21\u578b\u3002|[2402.02739v1](http://arxiv.org/pdf/2402.02739v1)|null|\n", "2402.02734": "|**2024-02-05**|**InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data**|InVA\uff1a\u7528\u4e8e\u534f\u8c03\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u7684\u7efc\u5408\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668|Bowen Lei, Rajarshi Guhaniyogi, Krishnendu Chandra, Aaron Scheffler, Bani Mallick|There is a significant interest in exploring non-linear associations among multiple images derived from diverse imaging modalities. While there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing approaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image. Building on the literature of Variational Auto Encoders (VAEs), this article proposes a novel approach, referred to as Integrative Variational Autoencoder (\\texttt{InVA}) method, which borrows information from multiple images obtained from different sources to draw predictive inference of an image. The proposed approach captures complex non-linear association between the outcome image and input images, while allowing rapid computation. Numerical results demonstrate substantial advantages of \\texttt{InVA} over VAEs, which typically do not allow borrowing information between input images. The proposed framework offers highly accurate predictive inferences for costly positron emission topography (PET) from multiple measures of cortical structure in human brain scans readily available from magnetic resonance imaging (MRI).|\u4eba\u4eec\u5bf9\u63a2\u7d22\u6e90\u81ea\u4e0d\u540c\u6210\u50cf\u6a21\u5f0f\u7684\u591a\u4e2a\u56fe\u50cf\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u5173\u8054\u975e\u5e38\u611f\u5174\u8da3\u3002\u867d\u7136\u5173\u4e8e\u56fe\u50cf\u5bf9\u56fe\u50cf\u56de\u5f52\u6765\u63cf\u8ff0\u57fa\u4e8e\u591a\u4e2a\u56fe\u50cf\u7684\u56fe\u50cf\u7684\u9884\u6d4b\u63a8\u7406\u7684\u6587\u732e\u8d8a\u6765\u8d8a\u591a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u9884\u6d4b\u4e2d\u6709\u6548\u501f\u7528\u591a\u4e2a\u6210\u50cf\u6a21\u6001\u4e4b\u95f4\u7684\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u57fa\u4e8e\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668 (VAE) \u7684\u6587\u732e\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u96c6\u6210\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668 (\\texttt{InVA}) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u501f\u7528\u4ece\u4e0d\u540c\u6765\u6e90\u83b7\u5f97\u7684\u591a\u4e2a\u56fe\u50cf\u4e2d\u7684\u4fe1\u606f\u6765\u5f97\u51fa\u9884\u6d4b\u63a8\u7406\u56fe\u50cf\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6355\u200b\u200b\u83b7\u7ed3\u679c\u56fe\u50cf\u548c\u8f93\u5165\u56fe\u50cf\u4e4b\u95f4\u590d\u6742\u7684\u975e\u7ebf\u6027\u5173\u8054\uff0c\u540c\u65f6\u5141\u8bb8\u5feb\u901f\u8ba1\u7b97\u3002\u6570\u503c\u7ed3\u679c\u8bc1\u660e\u4e86 \\texttt{InVA} \u76f8\u5bf9\u4e8e VAE \u7684\u5de8\u5927\u4f18\u52bf\uff0cVAE \u901a\u5e38\u4e0d\u5141\u8bb8\u5728\u8f93\u5165\u56fe\u50cf\u4e4b\u95f4\u501f\u7528\u4fe1\u606f\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u8f7b\u677e\u83b7\u5f97\u7684\u4eba\u8111\u626b\u63cf\u4e2d\u76ae\u5c42\u7ed3\u6784\u7684\u591a\u79cd\u6d4b\u91cf\u7ed3\u679c\uff0c\u4e3a\u6602\u8d35\u7684\u6b63\u7535\u5b50\u53d1\u5c04\u5f62\u8c8c\uff08PET\uff09\u63d0\u4f9b\u9ad8\u5ea6\u51c6\u786e\u7684\u9884\u6d4b\u63a8\u8bba\u3002|[2402.02734v1](http://arxiv.org/pdf/2402.02734v1)|null|\n", "2402.02729": "|**2024-02-05**|**Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN**|GAN \u652f\u6301\u5feb\u901f\u51c6\u786e\u7684\u534f\u4f5c\u65e0\u7ebf\u7535\u5730\u56fe\u4f30\u8ba1|Zezhong Zhang, Guangxu Zhu, Junting Chen, Shuguang Cui|In the 6G era, real-time radio resource monitoring and management are urged to support diverse wireless-empowered applications. This calls for fast and accurate estimation on the distribution of the radio resources, which is usually represented by the spatial signal power strength over the geographical environment, known as a radio map. In this paper, we present a cooperative radio map estimation (CRME) approach enabled by the generative adversarial network (GAN), called as GAN-CRME, which features fast and accurate radio map estimation without the transmitters' information. The radio map is inferred by exploiting the interaction between distributed received signal strength (RSS) measurements at mobile users and the geographical map using a deep neural network estimator, resulting in low data-acquisition cost and computational complexity. Moreover, a GAN-based learning algorithm is proposed to boost the inference capability of the deep neural network estimator by exploiting the power of generative AI. Simulation results showcase that the proposed GAN-CRME is even capable of coarse error-correction when the geographical map information is inaccurate.|6G\u65f6\u4ee3\uff0c\u8feb\u5207\u9700\u8981\u5b9e\u65f6\u65e0\u7ebf\u8d44\u6e90\u76d1\u63a7\u548c\u7ba1\u7406\uff0c\u4ee5\u652f\u6301\u591a\u6837\u5316\u7684\u65e0\u7ebf\u5e94\u7528\u3002\u8fd9\u5c31\u9700\u8981\u5bf9\u65e0\u7ebf\u7535\u8d44\u6e90\u7684\u5206\u5e03\u8fdb\u884c\u5feb\u901f\u3001\u51c6\u786e\u7684\u4f30\u8ba1\uff0c\u8fd9\u901a\u5e38\u7531\u5730\u7406\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u4fe1\u53f7\u529f\u7387\u5f3a\u5ea6\u6765\u8868\u793a\uff0c\u79f0\u4e3a\u65e0\u7ebf\u7535\u5730\u56fe\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u652f\u6301\u7684\u534f\u4f5c\u65e0\u7ebf\u7535\u5730\u56fe\u4f30\u8ba1\uff08CRME\uff09\u65b9\u6cd5\uff0c\u79f0\u4e3a GAN-CRME\uff0c\u5176\u7279\u70b9\u662f\u65e0\u9700\u53d1\u5c04\u673a\u4fe1\u606f\u5373\u53ef\u5feb\u901f\u51c6\u786e\u5730\u4f30\u8ba1\u65e0\u7ebf\u7535\u5730\u56fe\u3002\u901a\u8fc7\u5229\u7528\u79fb\u52a8\u7528\u6237\u7684\u5206\u5e03\u5f0f\u63a5\u6536\u4fe1\u53f7\u5f3a\u5ea6 (RSS) \u6d4b\u91cf\u4e0e\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u5668\u7684\u5730\u7406\u5730\u56fe\u4e4b\u95f4\u7684\u4ea4\u4e92\u6765\u63a8\u65ad\u65e0\u7ebf\u7535\u5730\u56fe\uff0c\u4ece\u800c\u964d\u4f4e\u6570\u636e\u91c7\u96c6\u6210\u672c\u548c\u8ba1\u7b97\u590d\u6742\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e GAN \u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u529b\u91cf\u6765\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u5668\u7684\u63a8\u7406\u80fd\u529b\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u5730\u7406\u5730\u56fe\u4fe1\u606f\u4e0d\u51c6\u786e\u65f6\uff0c\u6240\u63d0\u51fa\u7684 GAN-CRME \u751a\u81f3\u80fd\u591f\u8fdb\u884c\u7c97\u7565\u7ea0\u9519\u3002|[2402.02729v1](http://arxiv.org/pdf/2402.02729v1)|null|\n"}, "\u591a\u6a21\u6001": {"2402.03309": "|**2024-02-05**|**AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion**|AONeuS\uff1a\u58f0\u5149\u4f20\u611f\u5668\u878d\u5408\u7684\u795e\u7ecf\u6e32\u67d3\u6846\u67b6|Mohamad Qadri, Kevin Zhang, Akshay Hinduja, Michael Kaess, Adithya Pediredla, Christopher A. Metzler|Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate that AONeuS dramatically outperforms recent RGB-only and sonar-only inverse-differentiable-rendering--based surface reconstruction methods. A website visualizing the results of our paper is located at this address: https://aoneus.github.io/|\u6c34\u4e0b\u611f\u77e5\u548c 3D \u8868\u9762\u91cd\u5efa\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5728\u5efa\u7b51\u3001\u5b89\u5168\u3001\u6d77\u6d0b\u8003\u53e4\u548c\u73af\u5883\u76d1\u6d4b\u9886\u57df\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u5371\u9669\u7684\u64cd\u4f5c\u6761\u4ef6\u3001\u8106\u5f31\u7684\u73af\u5883\u548c\u6709\u9650\u7684\u5bfc\u822a\u63a7\u5236\u901a\u5e38\u51b3\u5b9a\u6f5c\u6c34\u5668\u9650\u5236\u5176\u8fd0\u52a8\u8303\u56f4\uff0c\u4ece\u800c\u9650\u5236\u5b83\u4eec\u6355\u83b7\u6d4b\u91cf\u503c\u7684\u57fa\u7ebf\u3002\u5728 3D \u573a\u666f\u91cd\u5efa\u7684\u80cc\u666f\u4e0b\uff0c\u4f17\u6240\u5468\u77e5\uff0c\u8f83\u5c0f\u7684\u57fa\u7ebf\u4f7f\u91cd\u5efa\u66f4\u5177\u6311\u6218\u6027\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u591a\u6a21\u6001\u58f0\u5149\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u6846\u67b6\uff08AONEuS\uff09\uff0c\u80fd\u591f\u6709\u6548\u5730\u5c06\u9ad8\u5206\u8fa8\u7387 RGB \u6d4b\u91cf\u4e0e\u4f4e\u5206\u8fa8\u7387\u6df1\u5ea6\u5206\u8fa8\u6210\u50cf\u58f0\u7eb3\u6d4b\u91cf\u76f8\u96c6\u6210\u3002\u901a\u8fc7\u878d\u5408\u8fd9\u4e9b\u4e92\u8865\u6a21\u5f0f\uff0c\u6211\u4eec\u7684\u6846\u67b6\u53ef\u4ee5\u6839\u636e\u5728\u4e25\u683c\u9650\u5236\u7684\u57fa\u7ebf\u4e0a\u6355\u83b7\u7684\u6d4b\u91cf\u7ed3\u679c\u91cd\u5efa\u51c6\u786e\u7684\u9ad8\u5206\u8fa8\u7387 3D \u8868\u9762\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u5b9e\u9a8c\u5ba4\u5185\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e AONEuS \u7684\u6027\u80fd\u663e\u7740\u4f18\u4e8e\u6700\u8fd1\u7684\u4ec5 RGB \u548c\u4ec5\u58f0\u7eb3\u57fa\u4e8e\u9006\u5fae\u5206\u6e32\u67d3\u7684\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u3002\u53ef\u89c6\u5316\u6211\u4eec\u8bba\u6587\u7ed3\u679c\u7684\u7f51\u7ad9\u4f4d\u4e8e\u4ee5\u4e0b\u5730\u5740\uff1ahttps://aoneus.github.io/|[2402.03309v1](http://arxiv.org/pdf/2402.03309v1)|null|\n", "2402.03235": "|**2024-02-05**|**ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D Object Detection**|ActiveAnno3D - \u7528\u4e8e\u591a\u6a21\u6001 3D \u5bf9\u8c61\u68c0\u6d4b\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6|Ahmed Ghita, Bj\u00f8rk Antoniussen, Walter Zimmer, Ross Greer, Christian Cre\u00df, Andreas M\u00f8gelmose, Mohan M. Trivedi, Alois C. Knoll|The curation of large-scale datasets is still costly and requires much time and resources. Data is often manually labeled, and the challenge of creating high-quality datasets remains. In this work, we fill the research gap using active learning for multi-modal 3D object detection. We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training. We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance. Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset. We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset. BEVFusion achieved an mAP of 64.31 when using half of the training data and 75.0 mAP when using the complete nuScenes dataset. We integrate our active learning framework into the proAnno labeling tool to enable AI-assisted data selection and labeling and minimize the labeling costs. Finally, we provide code, weights, and visualization results on our website: https://active3d-framework.github.io/active3d-framework.|\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u7ba1\u7406\u4ecd\u7136\u6210\u672c\u9ad8\u6602\uff0c\u5e76\u4e14\u9700\u8981\u5927\u91cf\u65f6\u95f4\u548c\u8d44\u6e90\u3002\u6570\u636e\u901a\u5e38\u662f\u624b\u52a8\u6807\u8bb0\u7684\uff0c\u521b\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u6311\u6218\u4ecd\u7136\u5b58\u5728\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u4e3b\u52a8\u5b66\u4e60\u8fdb\u884c\u591a\u6a21\u6001 3D \u5bf9\u8c61\u68c0\u6d4b\u6765\u586b\u8865\u7814\u7a76\u7a7a\u767d\u200b\u200b\u3002\u6211\u4eec\u63d0\u51fa\u4e86 ActiveAnno3D\uff0c\u8fd9\u662f\u4e00\u79cd\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9009\u62e9\u4e3a\u8bad\u7ec3\u63d0\u4f9b\u6700\u5927\u4fe1\u606f\u91cf\u7684\u6807\u8bb0\u6570\u636e\u6837\u672c\u3002\u6211\u4eec\u63a2\u7d22\u5404\u79cd\u8fde\u7eed\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u6574\u5408\u5173\u4e8e\u8ba1\u7b97\u9700\u6c42\u548c\u68c0\u6d4b\u6027\u80fd\u7684\u6700\u6709\u6548\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528 BEVFusion \u548c PV-RCNN \u5728 nuScenes \u548c TUM Traffic Intersection \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u3002\u6211\u4eec\u8868\u660e\uff0c\u5f53\u4ec5\u4f7f\u7528 TUM \u4ea4\u901a\u4ea4\u53c9\u53e3\u6570\u636e\u96c6\u7684\u4e00\u534a\u8bad\u7ec3\u6570\u636e\uff0877.25 mAP \u4e0e 83.50 mAP\uff09\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 PV-RCNN \u548c\u57fa\u4e8e\u71b5\u7684\u67e5\u8be2\u7b56\u7565\u5b9e\u73b0\u51e0\u4e4e\u76f8\u540c\u7684\u6027\u80fd\u3002\u5f53\u4f7f\u7528\u4e00\u534a\u7684\u8bad\u7ec3\u6570\u636e\u65f6\uff0cBEVFusion \u7684 mAP \u4e3a 64.31\uff1b\u5f53\u4f7f\u7528\u5b8c\u6574\u7684 nuScenes \u6570\u636e\u96c6\u65f6\uff0cBEVFusion \u7684 mAP \u4e3a 75.0\u3002\u6211\u4eec\u5c06\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u96c6\u6210\u5230 proAnno \u6807\u8bb0\u5de5\u5177\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u6570\u636e\u9009\u62e9\u548c\u6807\u8bb0\uff0c\u5e76\u6700\u5927\u9650\u5ea6\u5730\u964d\u4f4e\u6807\u8bb0\u6210\u672c\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u6211\u4eec\u7684\u7f51\u7ad9\u4e0a\u63d0\u4f9b\u4ee3\u7801\u3001\u6743\u91cd\u548c\u53ef\u89c6\u5316\u7ed3\u679c\uff1ahttps://active3d-framework.github.io/active3d-framework\u3002|[2402.03235v1](http://arxiv.org/pdf/2402.03235v1)|null|\n", "2402.03173": "|**2024-02-05**|**Multi: Multimodal Understanding Leaderboard with Text and Images**|\u591a\uff1a\u5e26\u6709\u6587\u672c\u548c\u56fe\u50cf\u7684\u591a\u6a21\u5f0f\u7406\u89e3\u6392\u884c\u699c|Zichen Zhu, Yang Xu, Lu Chen, Jingkai Yang, Yichuan Ma, Yiming Sun, Hailin Wen, Jiaqi Liu, Jinyu Cai, Yingzi Ma, et.al.|Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4,500 knowledge pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and 53.7%. Multi serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5feb\u901f\u8fdb\u5c55\u51f8\u663e\u4e86\u5411\u5b66\u672f\u754c\u5f15\u5165\u5177\u6709\u6311\u6218\u6027\u4f46\u73b0\u5b9e\u7684\u57fa\u51c6\u7684\u5fc5\u8981\u6027\u3002\u73b0\u6709\u7684\u57fa\u51c6\u4e3b\u8981\u4fa7\u91cd\u4e8e\u7b80\u5355\u7684\u81ea\u7136\u56fe\u50cf\u7406\u89e3\uff0c\u4f46 Multi \u6210\u4e3a MLLM \u7684\u524d\u6cbf\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30 MLLM \u662f\u5426\u7406\u89e3\u590d\u6742\u7684\u56fe\u5f62\u548c\u8868\u683c\u4ee5\u53ca\u79d1\u5b66\u95ee\u9898\u3002\u8be5\u57fa\u51c6\u53cd\u6620\u4e86\u5f53\u524d\u73b0\u5b9e\u7684\u8003\u8bd5\u98ce\u683c\uff0c\u63d0\u4f9b\u4e86\u591a\u6a21\u5f0f\u8f93\u5165\uff0c\u5e76\u8981\u6c42\u7cbe\u786e\u6216\u5f00\u653e\u5f0f\u7684\u56de\u7b54\uff0c\u7c7b\u4f3c\u4e8e\u73b0\u5b9e\u751f\u6d3b\u4e2d\u7684\u5b66\u6821\u6d4b\u8bd5\u3002\u5b83\u5411 MLLM \u63d0\u51fa\u5404\u79cd\u4efb\u52a1\u7684\u6311\u6218\uff0c\u4ece\u516c\u5f0f\u63a8\u5bfc\u5230\u56fe\u50cf\u7ec6\u8282\u5206\u6790\uff0c\u518d\u5230\u8de8\u6a21\u6001\u63a8\u7406\u3002 Multi \u5305\u542b 18,000 \u591a\u4e2a\u95ee\u9898\uff0c\u91cd\u70b9\u662f\u591a\u79cd\u5f62\u5f0f\u7684\u57fa\u4e8e\u79d1\u5b66\u7684 QA\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86 Multi-Elite\uff08\u4e00\u4e2a\u5305\u542b 500 \u4e2a\u95ee\u9898\u7684\u5b50\u96c6\uff0c\u7528\u4e8e\u6d4b\u8bd5 MLLM \u7684\u6781\u9650\uff09\u548c Multi-Extend\uff08\u5b83\u901a\u8fc7 4,500 \u591a\u4e2a\u77e5\u8bc6\u7247\u6bb5\u589e\u5f3a\u4e86\u60c5\u5883\u5b66\u4e60\u7814\u7a76\uff09\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e MLLM \u5177\u6709\u5de8\u5927\u7684\u8fdb\u6b65\u6f5c\u529b\uff0cGPT-4V \u5728 Multi \u4e0a\u5b9e\u73b0\u4e86 63.7% \u7684\u51c6\u786e\u7387\uff0c\u800c\u5176\u4ed6 MLLM \u7684\u5f97\u5206\u5728 31.3% \u5230 53.7% \u4e4b\u95f4\u3002 Multi\u4e0d\u4ec5\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u800c\u4e14\u4e3a\u4e13\u5bb6\u7ea7\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2402.03173v1](http://arxiv.org/pdf/2402.03173v1)|null|\n", "2402.03161": "|**2024-02-05**|**Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization**|Video-LaVIT\uff1a\u5177\u6709\u89e3\u8026\u89c6\u89c9\u8fd0\u52a8\u6807\u8bb0\u5316\u7684\u7edf\u4e00\u89c6\u9891\u8bed\u8a00\u9884\u8bad\u7ec3|Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, et.al.|In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models will be available at https://video-lavit.github.io.|\u9274\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u5c06\u5176\u4ece\u56fe\u50cf\u6587\u672c\u6570\u636e\u6269\u5c55\u5230\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u3002\u4e0e\u9759\u6001\u56fe\u50cf\u76f8\u6bd4\uff0c\u89c6\u9891\u7531\u4e8e\u5176\u65f6\u7a7a\u52a8\u6001\u5efa\u6a21\uff0c\u5bf9\u6709\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u63d0\u51fa\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u6709\u6548\u7684\u89c6\u9891\u5206\u89e3\u6765\u89e3\u51b3\u89c6\u9891\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u7684\u6b64\u7c7b\u5c40\u9650\u6027\uff0c\u5c06\u6bcf\u4e2a\u89c6\u9891\u8868\u793a\u4e3a\u5173\u952e\u5e27\u548c\u65f6\u95f4\u8fd0\u52a8\u3002\u7136\u540e\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6807\u8bb0\u5668\u5c06\u5b83\u4eec\u9002\u5e94\u6cd5\u5b66\u7855\u58eb\uff0c\u5c06\u89c6\u89c9\u548c\u65f6\u95f4\u4fe1\u606f\u79bb\u6563\u5316\u4e3a\u51e0\u4e2a\u6807\u8bb0\uff0c\u4ece\u800c\u5b9e\u73b0\u89c6\u9891\u3001\u56fe\u50cf\u548c\u6587\u672c\u7684\u7edf\u4e00\u751f\u6210\u9884\u8bad\u7ec3\u3002\u63a8\u7406\u65f6\uff0cLLM \u751f\u6210\u7684\u4ee4\u724c\u88ab\u5c0f\u5fc3\u5730\u6062\u590d\u5230\u539f\u59cb\u7684\u8fde\u7eed\u50cf\u7d20\u7a7a\u95f4\uff0c\u4ee5\u521b\u5efa\u5404\u79cd\u89c6\u9891\u5185\u5bb9\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u56fe\u50cf\u548c\u89c6\u9891\u5185\u5bb9\uff0c\u6b63\u5982\u5176\u5728\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u5728 13 \u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u4e2d\u7684\u7ade\u4e89\u8868\u73b0\u6240\u8bc1\u660e\u7684\u90a3\u6837\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728 https://video-lavit.github.io \u4e0a\u63d0\u4f9b\u3002|[2402.03161v1](http://arxiv.org/pdf/2402.03161v1)|null|\n", "2402.02996": "|**2024-02-05**|**Text-Guided Image Clustering**|\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u805a\u7c7b|Andreas Stephan, Lukas Miklautz, Kevin Sidak, Jan Philip Wahle, Bela Gipp, Claudia Plant, Benjamin Roth|Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the question of using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, this research challenges traditional approaches and paves the way for a paradigm shift in image clustering, using generated text.|\u56fe\u50cf\u805a\u7c7b\u5c06\u56fe\u50cf\u96c6\u5408\u5212\u5206\u4e3a\u6709\u610f\u4e49\u7684\u7ec4\uff0c\u901a\u5e38\u901a\u8fc7\u4eba\u7c7b\u7ed9\u51fa\u7684\u6ce8\u91ca\u8fdb\u884c\u4e8b\u540e\u89e3\u91ca\u3002\u8fd9\u4e9b\u901a\u5e38\u91c7\u7528\u6587\u672c\u5f62\u5f0f\uff0c\u8fd9\u5c31\u5f15\u51fa\u4e86\u4f7f\u7528\u6587\u672c\u4f5c\u4e3a\u56fe\u50cf\u805a\u7c7b\u62bd\u8c61\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u56fe\u50cf\u805a\u7c7b\u65b9\u6cd5\u5ffd\u7565\u4e86\u751f\u6210\u7684\u6587\u672c\u63cf\u8ff0\u7684\u4f7f\u7528\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u805a\u7c7b\uff0c\u5373\u4f7f\u7528\u56fe\u50cf\u5b57\u5e55\u548c\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6a21\u578b\u751f\u6210\u6587\u672c\uff0c\u7136\u540e\u5bf9\u751f\u6210\u7684\u6587\u672c\u8fdb\u884c\u805a\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a VQA \u6a21\u578b\u6765\u6ce8\u5165\u4efb\u52a1\u6216\u9886\u57df\u77e5\u8bc6\u4ee5\u8fdb\u884c\u805a\u7c7b\u3002\u5728\u516b\u4e2a\u4e0d\u540c\u7684\u56fe\u50cf\u805a\u7c7b\u6570\u636e\u96c6\u4e2d\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u83b7\u5f97\u7684\u6587\u672c\u8868\u793a\u901a\u5e38\u4f18\u4e8e\u56fe\u50cf\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u6570\u7684\u805a\u7c7b\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6d3e\u751f\u7684\u57fa\u4e8e\u5173\u952e\u5b57\u7684\u89e3\u91ca\u6bd4\u76f8\u5e94\u7684\u805a\u7c7b\u51c6\u786e\u5ea6\u6240\u5efa\u8bae\u7684\u66f4\u597d\u5730\u63cf\u8ff0\u4e86\u805a\u7c7b\u3002\u603b\u7684\u6765\u8bf4\uff0c\u8fd9\u9879\u7814\u7a76\u6311\u6218\u4e86\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u4e3a\u4f7f\u7528\u751f\u6210\u6587\u672c\u7684\u56fe\u50cf\u805a\u7c7b\u8303\u5f0f\u8f6c\u53d8\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2402.02996v1](http://arxiv.org/pdf/2402.02996v1)|null|\n", "2402.02968": "|**2024-02-05**|**Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives**|\u6df1\u5165\u7814\u7a76\u9053\u8def\u573a\u666f\u7406\u89e3\u7684\u591a\u6a21\u6001\u591a\u4efb\u52a1\u57fa\u7840\u6a21\u578b\uff1a\u4ece\u5b66\u4e60\u8303\u5f0f\u7684\u89d2\u5ea6|Sheng Luo, Wei Chen, Wanxin Tian, Rui Liu, Luanxuan Hou, Xiubao Zhang, Haifeng Shen, Ruiqi Wu, Shuyi Geng, Yi Zhou, et.al.|Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms. These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability. Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models. To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS|\u57fa\u7840\u6a21\u578b\u786e\u5b9e\u5bf9\u5404\u4e2a\u9886\u57df\u4ea7\u751f\u4e86\u6df1\u8fdc\u7684\u5f71\u54cd\uff0c\u6210\u4e3a\u663e\u7740\u5851\u9020\u667a\u80fd\u7cfb\u7edf\u529f\u80fd\u7684\u5173\u952e\u7ec4\u4ef6\u3002\u5728\u667a\u80fd\u6c7d\u8f66\u7684\u80cc\u666f\u4e0b\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u529b\u91cf\u5df2\u88ab\u8bc1\u660e\u662f\u5177\u6709\u53d8\u9769\u6027\u7684\uff0c\u53ef\u4ee5\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u5e26\u6765\u663e\u7740\u7684\u8fdb\u6b65\u3002\u591a\u6a21\u6001\u591a\u4efb\u52a1\u89c6\u89c9\u7406\u89e3\u57fa\u7840\u6a21\u578b\uff08MM-VUFM\uff09\u5177\u5907\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u5b66\u4e60\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u548c\u878d\u5408\u591a\u79cd\u6a21\u6001\u7684\u6570\u636e\uff0c\u540c\u65f6\u5904\u7406\u5404\u79cd\u4e0e\u9a7e\u9a76\u76f8\u5173\u7684\u4efb\u52a1\uff0c\u5177\u6709\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u6709\u52a9\u4e8e\u5bf9\u5468\u56f4\u573a\u666f\u6709\u66f4\u5168\u9762\u7684\u4e86\u89e3\u3002\u5728\u672c\u6b21\u8c03\u67e5\u4e2d\uff0c\u6211\u4eec\u5bf9\u4e13\u4e3a\u9053\u8def\u573a\u666f\u8bbe\u8ba1\u7684 MM-VUFM \u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u6790\u3002\u6211\u4eec\u7684\u76ee\u6807\u4e0d\u4ec5\u662f\u63d0\u4f9b\u5e38\u89c1\u5b9e\u8df5\u7684\u5168\u9762\u6982\u8ff0\uff0c\u53c2\u8003\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u3001\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u3001\u7edf\u4e00\u591a\u4efb\u52a1\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u63d0\u793a\u6280\u672f\uff0c\u800c\u4e14\u8fd8\u5f3a\u8c03\u5b83\u4eec\u5728\u4e0d\u540c\u5b66\u4e60\u8303\u5f0f\u4e2d\u7684\u5148\u8fdb\u80fd\u529b\u3002\u8fd9\u4e9b\u8303\u4f8b\u5305\u62ec\u5f00\u653e\u4e16\u754c\u7684\u7406\u89e3\u3001\u9053\u8def\u573a\u666f\u7684\u9ad8\u6548\u4f20\u8f93\u3001\u6301\u7eed\u5b66\u4e60\u3001\u4ea4\u4e92\u548c\u751f\u6210\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u5bf9\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u8d8b\u52bf\u7684\u89c1\u89e3\uff0c\u4f8b\u5982\u95ed\u73af\u9a7e\u9a76\u7cfb\u7edf\u3001\u53ef\u89e3\u91ca\u6027\u3001\u5177\u4f53\u9a7e\u9a76\u4ee3\u7406\u548c\u4e16\u754c\u6a21\u578b\u3002\u4e3a\u4e86\u65b9\u4fbf\u7814\u7a76\u4eba\u5458\u53ca\u65f6\u4e86\u89e3\u9053\u8def\u573a\u666f MM-VUFM \u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6211\u4eec\u5728 https://github.com/rolsheng/MM-VUFM4DS \u5efa\u7acb\u4e86\u4e00\u4e2a\u6301\u7eed\u66f4\u65b0\u7684\u5b58\u50a8\u5e93|[2402.02968v1](http://arxiv.org/pdf/2402.02968v1)|null|\n"}, "Nerf": {}, "3DGS": {"2402.03307": "|**2024-02-05**|**4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes**|4D \u9ad8\u65af\u6cfc\u6e85\uff1a\u5b9e\u73b0\u52a8\u6001\u573a\u666f\u7684\u9ad8\u6548\u65b0\u9896\u89c6\u56fe\u5408\u6210|Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen|We consider the problem of novel view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively.|\u6211\u4eec\u8003\u8651\u52a8\u6001\u573a\u666f\u7684\u65b0\u9896\u89c6\u56fe\u5408\u6210\uff08NVS\uff09\u95ee\u9898\u3002\u6700\u8fd1\u7684\u795e\u7ecf\u65b9\u6cd5\u5df2\u7ecf\u5728\u9759\u6001 3D \u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u51fa\u8272\u7684 NVS \u7ed3\u679c\uff0c\u4f46\u6269\u5c55\u5230 4D \u65f6\u53d8\u573a\u666f\u4ecd\u7136\u5f88\u91cd\u8981\u3002\u5148\u524d\u7684\u52aa\u529b\u901a\u5e38\u901a\u8fc7\u5b66\u4e60\u89c4\u8303\u7a7a\u95f4\u52a0\u4e0a\u9690\u5f0f\u6216\u663e\u5f0f\u53d8\u5f62\u573a\u6765\u7f16\u7801\u52a8\u6001\uff0c\u8fd9\u5728\u7a81\u7136\u8fd0\u52a8\u6216\u6355\u83b7\u9ad8\u4fdd\u771f\u6e32\u67d3\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 4D \u9ad8\u65af\u5206\u5e03 (4DGS)\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u5404\u5411\u5f02\u6027 4D XYZT \u9ad8\u65af\u8868\u793a\u52a8\u6001\u573a\u666f\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u5176\u7075\u611f\u6765\u81ea\u4e8e 3D \u9ad8\u65af\u5206\u5e03\u5728\u9759\u6001\u573a\u666f\u4e2d\u7684\u6210\u529f\u3002\u6211\u4eec\u901a\u8fc7\u5bf9 4D \u9ad8\u65af\u8fdb\u884c\u65f6\u95f4\u5207\u7247\u6765\u5bf9\u6bcf\u4e2a\u65f6\u95f4\u6233\u7684\u52a8\u6001\u8fdb\u884c\u5efa\u6a21\uff0c\u8fd9\u81ea\u7136\u5730\u7ec4\u6210\u4e86\u52a8\u6001 3D \u9ad8\u65af\uff0c\u5e76\u4e14\u53ef\u4ee5\u65e0\u7f1d\u6295\u5f71\u5230\u56fe\u50cf\u4e2d\u3002\u4f5c\u4e3a\u4e00\u79cd\u660e\u786e\u7684\u65f6\u7a7a\u8868\u793a\uff0c4DGS \u5c55\u793a\u4e86\u5bf9\u590d\u6742\u52a8\u6001\u548c\u7cbe\u7ec6\u7ec6\u8282\u8fdb\u884c\u5efa\u6a21\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u8fd0\u52a8\u7a81\u7136\u7684\u573a\u666f\u3002\u6211\u4eec\u5728\u9ad8\u5ea6\u4f18\u5316\u7684 CUDA \u52a0\u901f\u6846\u67b6\u4e2d\u8fdb\u4e00\u6b65\u5b9e\u73b0\u4e86\u65f6\u95f4\u5207\u7247\u548c\u6cfc\u6e85\u6280\u672f\uff0c\u5728 RTX 3090 GPU \u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 277 FPS \u7684\u5b9e\u65f6\u63a8\u7406\u6e32\u67d3\u901f\u5ea6\uff0c\u5728 RTX 4090 GPU \u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 583 FPS \u7684\u5b9e\u65f6\u63a8\u7406\u6e32\u67d3\u901f\u5ea6\u3002\u5bf9\u5177\u6709\u4e0d\u540c\u8fd0\u52a8\u7684\u573a\u666f\u7684\u4e25\u683c\u8bc4\u4f30\u5c55\u793a\u4e86 4DGS \u7684\u5353\u8d8a\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u5b83\u5728\u6570\u91cf\u548c\u8d28\u91cf\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002|[2402.03307v1](http://arxiv.org/pdf/2402.03307v1)|null|\n", "2402.03246": "|**2024-02-05**|**SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM**|SGS-SLAM\uff1a\u795e\u7ecf\u5bc6\u96c6 SLAM \u7684\u8bed\u4e49\u9ad8\u65af\u6cfc\u6e85|Mingrui Li, Shuhong Liu, Heng Zhou|Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaus- sian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability.|\u8bed\u4e49\u7406\u89e3\u5728\u5bc6\u96c6\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5efa\u56fe\uff08SLAM\uff09\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u6709\u52a9\u4e8e\u5168\u9762\u7684\u573a\u666f\u89e3\u91ca\u3002\u6700\u8fd1\u5c06\u9ad8\u65af\u6cfc\u6e85\u96c6\u6210\u5230 SLAM \u7cfb\u7edf\u4e2d\u7684\u8fdb\u5c55\u5df2\u7ecf\u8bc1\u660e\u4e86\u5176\u901a\u8fc7\u4f7f\u7528\u663e\u5f0f 3D \u9ad8\u65af\u8868\u793a\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u6709\u6548\u6027\u3002\u5728\u6b64\u8fdb\u5c55\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 SGS-SLAM\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e 3D \u9ad8\u65af\u7684\u8bed\u4e49\u5bc6\u96c6\u89c6\u89c9 SLAM \u7cfb\u7edf\uff0c\u5b83\u63d0\u4f9b\u7cbe\u786e\u7684 3D \u8bed\u4e49\u5206\u5272\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5efa\u8bae\u5728\u6620\u5c04\u8fc7\u7a0b\u4e2d\u91c7\u7528\u591a\u901a\u9053\u4f18\u5316\uff0c\u5c06\u5916\u89c2\u3001\u51e0\u4f55\u548c\u8bed\u4e49\u7ea6\u675f\u4e0e\u5173\u952e\u5e27\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSGS-SLAM \u5728\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3001\u5730\u56fe\u91cd\u5efa\u548c\u8bed\u4e49\u5206\u5272\u65b9\u9762\u63d0\u4f9b\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5b9e\u65f6\u6e32\u67d3\u80fd\u529b\u3002|[2402.03246v1](http://arxiv.org/pdf/2402.03246v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.03241": "|**2024-02-05**|**FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition**|FROSTER\uff1aFrozen CLIP \u662f\u5f00\u653e\u8bcd\u6c47\u52a8\u4f5c\u8bc6\u522b\u7684\u5f3a\u5927\u8001\u5e08|Xiaohu Huang, Hao Zhou, Kun Yao, Kai Han|In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos. Meanwhile, it uses a residual sub-network for feature distillation to reach a balance between the two distinct objectives of learning generalizable and video-specific features.   We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings. FROSTER consistently achieves state-of-the-art performance on all datasets across the board. Project page: https://visual-ai.github.io/froster.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 FROSTER\uff0c\u4e00\u4e2a\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u52a8\u4f5c\u8bc6\u522b\u7684\u6709\u6548\u6846\u67b6\u3002 CLIP \u6a21\u578b\u5728\u4e00\u7cfb\u5217\u57fa\u4e8e\u56fe\u50cf\u7684\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u529f\uff0c\u8fd9\u5f97\u76ca\u4e8e\u5176\u9884\u4fdd\u7559\u5927\u91cf\u56fe\u50cf\u6587\u672c\u5bf9\u6240\u5e26\u6765\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8e CLIP \u9884\u8bad\u7ec3\u4e2d\u7f3a\u4e4f\u65f6\u95f4\u4fe1\u606f\uff0c\u5c06 CLIP \u76f4\u63a5\u5e94\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002\u6b64\u5916\uff0c\u5728\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u5fae\u8c03 CLIP \u53ef\u80fd\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u62df\u5408\u5e76\u963b\u788d\u5176\u6cdb\u5316\u6027\uff0c\u4ece\u800c\u5728\u5904\u7406\u770b\u4e0d\u89c1\u7684\u52a8\u4f5c\u65f6\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4ee4\u4eba\u6ee1\u610f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0cFROSTER\u91c7\u7528\u6b8b\u5dee\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\u6765\u786e\u4fddCLIP\u4fdd\u7559\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u6709\u6548\u9002\u5e94\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6b8b\u4f59\u7279\u5f81\u84b8\u998f\u5c06\u51bb\u7ed3\u7684 CLIP \u6a21\u578b\u89c6\u4e3a\u8001\u5e08\uff0c\u4ee5\u4fdd\u6301\u539f\u59cb CLIP \u6240\u8868\u73b0\u51fa\u7684\u901a\u7528\u6027\uff0c\u5e76\u76d1\u7763\u7279\u5f81\u5b66\u4e60\u4ee5\u63d0\u53d6\u89c6\u9891\u7279\u5b9a\u7279\u5f81\uff0c\u4ee5\u5f25\u5408\u56fe\u50cf\u548c\u89c6\u9891\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u540c\u65f6\uff0c\u5b83\u4f7f\u7528\u6b8b\u5dee\u5b50\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u84b8\u998f\uff0c\u4ee5\u5728\u5b66\u4e60\u53ef\u6cdb\u5316\u7279\u5f81\u548c\u89c6\u9891\u7279\u5b9a\u7279\u5f81\u8fd9\u4e24\u4e2a\u4e0d\u540c\u76ee\u6807\u4e4b\u95f4\u8fbe\u5230\u5e73\u8861\u3002\u6211\u4eec\u5728\u57fa\u7840\u5230\u5c0f\u8bf4\u548c\u8de8\u6570\u636e\u96c6\u8bbe\u7f6e\u4e0b\uff0c\u5728\u5f00\u653e\u8bcd\u6c47\u52a8\u4f5c\u8bc6\u522b\u57fa\u51c6\u4e0a\u5e7f\u6cdb\u8bc4\u4f30 FROSTER\u3002 FROSTER \u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://visual-ai.github.io/froster\u3002|[2402.03241v1](http://arxiv.org/pdf/2402.03241v1)|null|\n", "2402.03119": "|**2024-02-05**|**Good Teachers Explain: Explanation-Enhanced Knowledge Distillation**|\u597d\u8001\u5e08\u8bb2\u89e3\uff1a\u8bb2\u89e3\u589e\u5f3a\u77e5\u8bc6\u84b8\u998f|Amin Parchami-Araghi, Moritz B\u00f6hle, Sukrut Rao, Bernt Schiele|Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and to give similar explanations, and (3) is robust with respect to the model architectures, the amount of training data, and even works with 'approximate', pre-computed explanations.|\u77e5\u8bc6\u84b8\u998f (KD) \u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u5927\u578b\u6559\u5e08\u6a21\u578b\u538b\u7f29\u4e3a\u8f83\u5c0f\u7684\u5b66\u751f\u6a21\u578b\u3002\u4f17\u6240\u5468\u77e5\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u4ee5\u8fbe\u5230\u4e0e\u6559\u5e08\u76f8\u4f3c\u7684\u51c6\u786e\u5ea6\uff0c\u4f46\u4e5f\u6709\u7814\u7a76\u8868\u660e\uff0c\u4ed6\u4eec\u901a\u5e38\u65e0\u6cd5\u5b66\u4e60\u76f8\u540c\u7684\u529f\u80fd\u3002\u7136\u800c\uff0c\u901a\u5e38\u975e\u5e38\u5e0c\u671b\u5b66\u751f\u548c\u6559\u5e08\u7684\u51fd\u6570\u5177\u6709\u76f8\u4f3c\u7684\u5c5e\u6027\uff0c\u4f8b\u5982\u57fa\u4e8e\u76f8\u540c\u7684\u8f93\u5165\u7279\u5f81\u8fdb\u884c\u9884\u6d4b\uff0c\u56e0\u4e3a\u8fd9\u53ef\u4ee5\u786e\u4fdd\u5b66\u751f\u4ece\u6559\u5e08\u90a3\u91cc\u5b66\u4e60\u201c\u6b63\u786e\u7684\u7279\u5f81\u201d\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u4f18\u5316\u7ecf\u5178\u7684 KD \u635f\u5931\u4ee5\u53ca\u6559\u5e08\u548c\u5b66\u751f\u751f\u6210\u7684\u89e3\u91ca\u7684\u76f8\u4f3c\u6027\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u5c3d\u7ba1\u8fd9\u4e2a\u60f3\u6cd5\u7b80\u5355\u76f4\u89c2\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u6211\u4eec\u63d0\u51fa\u7684\u201c\u89e3\u91ca\u589e\u5f3a\u201dKD (e$^2$KD) (1) \u5728\u51c6\u786e\u6027\u548c\u5e08\u751f\u4e00\u81f4\u6027\u65b9\u9762\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u6536\u76ca\uff0c(2) \u786e\u4fdd\u4e86\u5b66\u751f\u4ece\u8001\u5e08\u90a3\u91cc\u5b66\u5230\u6b63\u786e\u7684\u7406\u7531\u5e76\u7ed9\u51fa\u7c7b\u4f3c\u7684\u89e3\u91ca\uff0c\u5e76\u4e14\uff083\uff09\u5728\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u6570\u636e\u91cf\u65b9\u9762\u662f\u7a33\u5065\u7684\uff0c\u751a\u81f3\u53ef\u4ee5\u4f7f\u7528\u201c\u8fd1\u4f3c\u201d\u7684\u9884\u5148\u8ba1\u7b97\u7684\u89e3\u91ca\u3002|[2402.03119v1](http://arxiv.org/pdf/2402.03119v1)|**[link](https://github.com/m-parchami/goodteachersexplain)**|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.03311": "|**2024-02-05**|**HASSOD: Hierarchical Adaptive Self-Supervised Object Detection**|HASSOD\uff1a\u5206\u5c42\u81ea\u9002\u5e94\u81ea\u76d1\u7763\u76ee\u6807\u68c0\u6d4b|Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, Yu-Xiong Wang|The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process. Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in self-supervised object detection. Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: https://HASSOD-NeurIPS23.github.io.|\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u5728\u6ca1\u6709\u660e\u786e\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u975e\u51e1\u7684\u5b66\u4e60\u80fd\u529b\u548c\u7406\u89e3\u7269\u4f53\u7684\u90e8\u5206\u5230\u6574\u4f53\u7ec4\u6210\u7684\u80fd\u529b\u3002\u4ece\u8fd9\u4e24\u79cd\u80fd\u529b\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u5c42\u81ea\u9002\u5e94\u81ea\u76d1\u7763\u5bf9\u8c61\u68c0\u6d4b\uff08HASSOD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u4eba\u7c7b\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u68c0\u6d4b\u5bf9\u8c61\u5e76\u7406\u89e3\u5176\u7ec4\u6210\u3002 HASSOD \u91c7\u7528\u5206\u5c42\u81ea\u9002\u5e94\u805a\u7c7b\u7b56\u7565\uff0c\u6839\u636e\u81ea\u6211\u76d1\u7763\u7684\u89c6\u89c9\u8868\u793a\u5c06\u533a\u57df\u5206\u7ec4\u4e3a\u5bf9\u8c61\u63a9\u6a21\uff0c\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u6bcf\u4e2a\u56fe\u50cf\u7684\u5bf9\u8c61\u6570\u91cf\u3002\u6b64\u5916\uff0cHASSOD\u901a\u8fc7\u5206\u6790\u63a9\u6a21\u4e4b\u95f4\u7684\u8986\u76d6\u5173\u7cfb\u5e76\u6784\u5efa\u6811\u7ed3\u6784\u6765\u8bc6\u522b\u5bf9\u8c61\u5728\u7ec4\u6210\u65b9\u9762\u7684\u5c42\u6b21\u7ea7\u522b\u3002\u8fd9\u79cd\u989d\u5916\u7684\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u53ef\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u653e\u5f03\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u4f4e\u6548\u7684\u591a\u8f6e\u81ea\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u800c\u662f\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u7684 Mean Teacher \u6846\u67b6\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u987a\u7545\u3001\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002\u901a\u8fc7\u5bf9\u6d41\u884c\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 HASSOD \u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u81ea\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u5c06 LVIS \u4e0a\u7684 Mask AR \u4ece 20.2 \u63d0\u9ad8\u5230 22.5\uff0c\u5c06 SA-1B \u4e0a\u7684 Mask AR \u4ece 17.0 \u63d0\u9ad8\u5230 26.0\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://HASSOD-NeurIPS23.github.io\u3002|[2402.03311v1](http://arxiv.org/pdf/2402.03311v1)|null|\n", "2402.03302": "|**2024-02-05**|**Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining**|Swin-UMamba\uff1a\u57fa\u4e8e Mamba \u7684 UNet \u548c\u57fa\u4e8e ImageNet \u7684\u9884\u8bad\u7ec3|Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, et.al.|Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed specifically for medical image segmentation tasks, leveraging the advantages of ImageNet-based pretraining. Our experimental results reveal the vital role of ImageNet-based training in enhancing the performance of Mamba-based models. Swin-UMamba demonstrates superior performance with a large margin compared to CNNs, ViTs, and latest Mamba-based models. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba outperforms its closest counterpart U-Mamba by an average score of 3.58%. The code and models of Swin-UMamba are publicly available at: https://github.com/JiarunLiu/Swin-UMamba|\u51c6\u786e\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9700\u8981\u96c6\u6210\u591a\u5c3a\u5ea6\u4fe1\u606f\uff0c\u4ece\u5c40\u90e8\u7279\u5f81\u5230\u5168\u5c40\u4f9d\u8d56\u6027\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u8fdc\u7a0b\u5168\u5c40\u4fe1\u606f\u8fdb\u884c\u5efa\u6a21\u5177\u6709\u6311\u6218\u6027\uff0c\u5176\u4e2d\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u53d7\u5230\u5176\u5c40\u90e8\u611f\u53d7\u91ce\u7684\u9650\u5236\uff0c\u800c\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u5219\u53d7\u5230\u5176\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u5f71\u54cd\u3002\u6700\u8fd1\uff0c\u57fa\u4e8e Mamba \u7684\u6a21\u578b\u56e0\u5176\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u65b9\u9762\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u800c\u53d7\u5230\u6781\u5927\u5173\u6ce8\u3002\u591a\u9879\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u90fd\u53ef\u4ee5\u4f18\u4e8e\u6d41\u884c\u7684\u89c6\u89c9\u6a21\u578b\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u66f4\u4f4e\u7684\u5185\u5b58\u6d88\u8017\u548c\u66f4\u5c11\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e Mamba \u7684\u6a21\u578b\u5927\u591a\u662f\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684\uff0c\u5e76\u6ca1\u6709\u63a2\u7d22\u9884\u8bad\u7ec3\u7684\u529b\u91cf\uff0c\u800c\u9884\u8bad\u7ec3\u5df2\u88ab\u8bc1\u660e\u5bf9\u4e8e\u6570\u636e\u9ad8\u6548\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u975e\u5e38\u6709\u6548\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e Mamba \u7684\u65b0\u578b\u6a21\u578b Swin-UMamba\uff0c\u8be5\u6a21\u578b\u4e13\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u800c\u8bbe\u8ba1\uff0c\u5229\u7528\u4e86\u57fa\u4e8e ImageNet \u7684\u9884\u8bad\u7ec3\u7684\u4f18\u52bf\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u57fa\u4e8e ImageNet \u7684\u8bad\u7ec3\u5728\u589e\u5f3a\u57fa\u4e8e Mamba \u7684\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u91cd\u8981\u4f5c\u7528\u3002\u4e0e CNN\u3001ViT \u548c\u6700\u65b0\u7684\u57fa\u4e8e Mamba \u7684\u6a21\u578b\u76f8\u6bd4\uff0cSwin-UMamba \u8868\u73b0\u51fa\u4e86\u5de8\u5927\u7684\u4f18\u8d8a\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u8179\u90e8 MRI\u3001\u80a0\u955c\u68c0\u67e5\u548c\u663e\u5fae\u955c\u68c0\u67e5\u6570\u636e\u96c6\u4e0a\uff0cSwin-UMamba \u7684\u5e73\u5747\u5f97\u5206\u6bd4\u6700\u63a5\u8fd1\u7684 U-Mamba \u597d 3.58%\u3002 Swin-UMamba\u7684\u4ee3\u7801\u548c\u6a21\u578b\u516c\u5f00\u4e8e\uff1ahttps://github.com/JiarunLiu/Swin-UMamba|[2402.03302v1](http://arxiv.org/pdf/2402.03302v1)|**[link](https://github.com/jiarunliu/swin-umamba)**|\n", "2402.03230": "|**2024-02-05**|**CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models**|\u57fa\u4e8e CT \u7684\u80f8\u90e8\u624b\u672f\u89c4\u5212\u89e3\u5256\u5206\u5272\uff1a3D U \u5f62\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u57fa\u51c6\u7814\u7a76|Arash Harirpoush, Amirhossein Rasoulian, Marta Kersten-Oertel, Yiming Xiao|Recent rising interests in patient-specific thoracic surgical planning and simulation require efficient and robust creation of digital anatomical models from automatic medical image segmentation algorithms. Deep learning (DL) is now state-of-the-art in various radiological tasks, and U-shaped DL models have particularly excelled in medical image segmentation since the inception of the 2D UNet. To date, many variants of U-shaped models have been proposed by the integration of different attention mechanisms and network configurations. Leveraging the recent development of large multi-label databases, systematic benchmark studies for these models can provide valuable insights for clinical deployment and future model designs, but such studies are still rare. We conduct the first benchmark study for variants of 3D U-shaped models (3DUNet, STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet with four variants) with a focus on CT-based anatomical segmentation for thoracic surgery. Our study systematically examines the impact of different attention mechanisms, number of resolution stages, and network configurations on segmentation accuracy and computational complexity. To allow cross-reference with other recent benchmarking studies, we also included a performance assessment of the BTCV abdominal structural segmentation. With the STUNet ranking at the top, our study demonstrated the value of CNN-based U-shaped models for the investigated tasks and the benefit of residual blocks in network configuration designs to boost segmentation performance.|\u6700\u8fd1\u4eba\u4eec\u5bf9\u9488\u5bf9\u7279\u5b9a\u60a3\u8005\u7684\u80f8\u90e8\u624b\u672f\u89c4\u5212\u548c\u6a21\u62df\u7684\u5174\u8da3\u65e5\u76ca\u6d53\u539a\uff0c\u9700\u8981\u901a\u8fc7\u81ea\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7b97\u6cd5\u9ad8\u6548\u3001\u7a33\u5065\u5730\u521b\u5efa\u6570\u5b57\u89e3\u5256\u6a21\u578b\u3002\u6df1\u5ea6\u5b66\u4e60 (DL) \u73b0\u5728\u5728\u5404\u79cd\u653e\u5c04\u5b66\u4efb\u52a1\u4e2d\u90fd\u662f\u6700\u5148\u8fdb\u7684\uff0c\u81ea 2D UNet \u8bde\u751f\u4ee5\u6765\uff0cU \u5f62 DL \u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u9762\u5c24\u5176\u51fa\u8272\u3002\u8fc4\u4eca\u4e3a\u6b62\uff0c\u901a\u8fc7\u6574\u5408\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u7f51\u7edc\u914d\u7f6e\uff0c\u5df2\u7ecf\u63d0\u51fa\u4e86\u8bb8\u591a U \u5f62\u6a21\u578b\u7684\u53d8\u4f53\u3002\u5229\u7528\u5927\u578b\u591a\u6807\u7b7e\u6570\u636e\u5e93\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u7cfb\u7edf\u57fa\u51c6\u7814\u7a76\u53ef\u4ee5\u4e3a\u4e34\u5e8a\u90e8\u7f72\u548c\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u4f46\u6b64\u7c7b\u7814\u7a76\u4ecd\u7136\u5f88\u5c11\u3002\u6211\u4eec\u5bf9 3D U \u5f62\u6a21\u578b\u7684\u53d8\u4f53\uff083DUNet\u3001STUNet\u3001AttentionUNet\u3001SwinUNETR\u3001FocalSegNet \u548c\u5177\u6709\u56db\u79cd\u53d8\u4f53\u7684\u65b0\u578b 3D SwinUnet\uff09\u8fdb\u884c\u4e86\u9996\u6b21\u57fa\u51c6\u7814\u7a76\uff0c\u91cd\u70b9\u662f\u80f8\u5916\u79d1\u624b\u672f\u4e2d\u57fa\u4e8e CT \u7684\u89e3\u5256\u5206\u5272\u3002\u6211\u4eec\u7684\u7814\u7a76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u673a\u5236\u3001\u5206\u8fa8\u7387\u9636\u6bb5\u7684\u6570\u91cf\u548c\u7f51\u7edc\u914d\u7f6e\u5bf9\u5206\u5272\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u590d\u6742\u6027\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u4e0e\u5176\u4ed6\u6700\u8fd1\u7684\u57fa\u51c6\u7814\u7a76\u8fdb\u884c\u4ea4\u53c9\u5f15\u7528\uff0c\u6211\u4eec\u8fd8\u5bf9 BTCV \u8179\u90e8\u7ed3\u6784\u5206\u5272\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002 STUNet \u6392\u540d\u9760\u524d\uff0c\u6211\u4eec\u7684\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e CNN \u7684 U \u5f62\u6a21\u578b\u5bf9\u4e8e\u6240\u7814\u7a76\u4efb\u52a1\u7684\u4ef7\u503c\uff0c\u4ee5\u53ca\u7f51\u7edc\u914d\u7f6e\u8bbe\u8ba1\u4e2d\u6b8b\u5dee\u5757\u5bf9\u63d0\u9ad8\u5206\u5272\u6027\u80fd\u7684\u597d\u5904\u3002|[2402.03230v1](http://arxiv.org/pdf/2402.03230v1)|**[link](https://github.com/healthx-lab/deepsegthoracic)**|\n", "2402.03188": "|**2024-02-05**|**Towards mitigating uncann(eye)ness in face swaps via gaze-centric loss terms**|\u901a\u8fc7\u4ee5\u51dd\u89c6\u4e3a\u4e2d\u5fc3\u7684\u635f\u5931\u9879\u6765\u51cf\u8f7b\u9762\u90e8\u4ea4\u6362\u4e2d\u7684\u4e0d\u53ef\u601d\u8bae\uff08\u773c\u775b\uff09|Ethan Wilson, Frederick Shic, Sophie J\u00f6rg, Eakta Jain|Advances in face swapping have enabled the automatic generation of highly realistic faces. Yet face swaps are perceived differently than when looking at real faces, with key differences in viewer behavior surrounding the eyes. Face swapping algorithms generally place no emphasis on the eyes, relying on pixel or feature matching losses that consider the entire face to guide the training process. We further investigate viewer perception of face swaps, focusing our analysis on the presence of an uncanny valley effect. We additionally propose a novel loss equation for the training of face swapping models, leveraging a pretrained gaze estimation network to directly improve representation of the eyes. We confirm that viewed face swaps do elicit uncanny responses from viewers. Our proposed improvements significant reduce viewing angle errors between face swaps and their source material. Our method additionally reduces the prevalence of the eyes as a deciding factor when viewers perform deepfake detection tasks. Our findings have implications on face swapping for special effects, as digital avatars, as privacy mechanisms, and more; negative responses from users could limit effectiveness in said applications. Our gaze improvements are a first step towards alleviating negative viewer perceptions via a targeted approach.|\u9762\u90e8\u4ea4\u6362\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5f97\u81ea\u52a8\u751f\u6210\u9ad8\u5ea6\u771f\u5b9e\u7684\u9762\u90e8\u6210\u4e3a\u53ef\u80fd\u3002\u7136\u800c\uff0c\u6362\u8138\u7684\u611f\u89c9\u4e0e\u89c2\u770b\u771f\u5b9e\u9762\u5b54\u65f6\u4e0d\u540c\uff0c\u5176\u4e2d\u89c2\u770b\u8005\u773c\u775b\u5468\u56f4\u7684\u884c\u4e3a\u5b58\u5728\u91cd\u5927\u5dee\u5f02\u3002\u9762\u90e8\u4ea4\u6362\u7b97\u6cd5\u901a\u5e38\u4e0d\u5f3a\u8c03\u773c\u775b\uff0c\u800c\u662f\u4f9d\u9760\u8003\u8651\u6574\u4e2a\u9762\u90e8\u7684\u50cf\u7d20\u6216\u7279\u5f81\u5339\u914d\u635f\u5931\u6765\u6307\u5bfc\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8c03\u67e5\u89c2\u4f17\u5bf9\u6362\u8138\u7684\u611f\u77e5\uff0c\u5c06\u5206\u6790\u91cd\u70b9\u653e\u5728\u6050\u6016\u8c37\u6548\u5e94\u7684\u5b58\u5728\u4e0a\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u635f\u5931\u65b9\u7a0b\uff0c\u7528\u4e8e\u8bad\u7ec3\u9762\u90e8\u4ea4\u6362\u6a21\u578b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6ce8\u89c6\u4f30\u8ba1\u7f51\u7edc\u6765\u76f4\u63a5\u6539\u5584\u773c\u775b\u7684\u8868\u793a\u3002\u6211\u4eec\u786e\u8ba4\uff0c\u89c2\u770b\u7684\u8138\u90e8\u4e92\u6362\u786e\u5b9e\u4f1a\u5f15\u8d77\u89c2\u4f17\u7684\u4e0d\u53ef\u601d\u8bae\u7684\u53cd\u5e94\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6539\u8fdb\u663e\u7740\u51cf\u5c11\u4e86\u9762\u90e8\u4ea4\u6362\u4e0e\u5176\u6e90\u6750\u6599\u4e4b\u95f4\u7684\u89c6\u89d2\u8bef\u5dee\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u8fd8\u51cf\u5c11\u4e86\u89c2\u770b\u8005\u6267\u884c\u6df1\u5ea6\u6362\u8138\u68c0\u6d4b\u4efb\u52a1\u65f6\u773c\u775b\u4f5c\u4e3a\u51b3\u5b9a\u56e0\u7d20\u7684\u53d1\u751f\u7387\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5bf9\u9762\u90e8\u4ea4\u6362\u7684\u7279\u6b8a\u6548\u679c\u3001\u6570\u5b57\u5316\u8eab\u3001\u9690\u79c1\u673a\u5236\u7b49\u6709\u5f71\u54cd\uff1b\u7528\u6237\u7684\u8d1f\u9762\u53cd\u5e94\u53ef\u80fd\u4f1a\u9650\u5236\u6240\u8ff0\u5e94\u7528\u7a0b\u5e8f\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u89c6\u7ebf\u6539\u8fdb\u662f\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u65b9\u6cd5\u51cf\u8f7b\u89c2\u4f17\u8d1f\u9762\u770b\u6cd5\u7684\u7b2c\u4e00\u6b65\u3002|[2402.03188v1](http://arxiv.org/pdf/2402.03188v1)|null|\n", "2402.03166": "|**2024-02-05**|**RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification**|RRWNet\uff1a\u7528\u4e8e\u6709\u6548\u89c6\u7f51\u819c\u52a8\u8109/\u9759\u8109\u5206\u5272\u548c\u5206\u7c7b\u7684\u9012\u5f52\u7ec6\u5316\u7f51\u7edc|Jos\u00e9 Morano, Guilherme Aresta, Hrvoje Bogunovi\u0107|The caliber and configuration of retinal blood vessels serve as important biomarkers for various diseases and medical conditions. A thorough analysis of the retinal vasculature requires the segmentation of blood vessels and their classification into arteries and veins, which is typically performed on color fundus images obtained by retinography, a widely used imaging technique. Nonetheless, manually performing these tasks is labor-intensive and prone to human error. Various automated methods have been proposed to address this problem. However, the current state of art in artery/vein segmentation and classification faces challenges due to manifest classification errors that affect the topological consistency of segmentation maps. This study presents an innovative end-to-end framework, RRWNet, designed to recursively refine semantic segmentation maps and correct manifest classification errors. The framework consists of a fully convolutional neural network with a Base subnetwork that generates base segmentation maps from input images, and a Recursive Refinement subnetwork that iteratively and recursively improves these maps. Evaluation on public datasets demonstrates the state-of-the-art performance of the proposed method, yielding more topologically consistent segmentation maps with fewer manifest classification errors than existing approaches. In addition, the Recursive Refinement module proves effective in post-processing segmentation maps from other methods, automatically correcting classification errors and improving topological consistency. The model code, weights, and predictions are publicly available at https://github.com/j-morano/rrwnet.|\u89c6\u7f51\u819c\u8840\u7ba1\u7684\u53e3\u5f84\u548c\u7ed3\u6784\u662f\u5404\u79cd\u75be\u75c5\u548c\u533b\u7597\u72b6\u51b5\u7684\u91cd\u8981\u751f\u7269\u6807\u5fd7\u7269\u3002\u5bf9\u89c6\u7f51\u819c\u8109\u7ba1\u7cfb\u7edf\u7684\u5f7b\u5e95\u5206\u6790\u9700\u8981\u5bf9\u8840\u7ba1\u8fdb\u884c\u5206\u5272\u5e76\u5c06\u5176\u5206\u7c7b\u4e3a\u52a8\u8109\u548c\u9759\u8109\uff0c\u8fd9\u901a\u5e38\u662f\u5728\u901a\u8fc7\u89c6\u7f51\u819c\u6210\u50cf\uff08\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6210\u50cf\u6280\u672f\uff09\u83b7\u5f97\u7684\u5f69\u8272\u773c\u5e95\u56fe\u50cf\u4e0a\u8fdb\u884c\u7684\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u624b\u52a8\u6267\u884c\u8fd9\u4e9b\u4efb\u52a1\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\uff0c\u5e76\u4e14\u5bb9\u6613\u51fa\u73b0\u4eba\u4e3a\u9519\u8bef\u3002\u5df2\u7ecf\u63d0\u51fa\u4e86\u5404\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u7136\u800c\uff0c\u7531\u4e8e\u660e\u663e\u7684\u5206\u7c7b\u9519\u8bef\u5f71\u54cd\u4e86\u5206\u5272\u56fe\u7684\u62d3\u6251\u4e00\u81f4\u6027\uff0c\u5f53\u524d\u52a8\u8109/\u9759\u8109\u5206\u5272\u548c\u5206\u7c7b\u7684\u6280\u672f\u6c34\u5e73\u9762\u4e34\u7740\u6311\u6218\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u7aef\u5230\u7aef\u6846\u67b6 RRWNet\uff0c\u65e8\u5728\u9012\u5f52\u5730\u7ec6\u5316\u8bed\u4e49\u5206\u5272\u56fe\u5e76\u7ea0\u6b63\u660e\u663e\u7684\u5206\u7c7b\u9519\u8bef\u3002\u8be5\u6846\u67b6\u7531\u4e00\u4e2a\u5b8c\u5168\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7ec4\u6210\uff0c\u8be5\u7f51\u7edc\u5177\u6709\u4e00\u4e2a\u4ece\u8f93\u5165\u56fe\u50cf\u751f\u6210\u57fa\u672c\u5206\u5272\u56fe\u7684\u57fa\u672c\u5b50\u7f51\uff0c\u4ee5\u53ca\u4e00\u4e2a\u8fed\u4ee3\u548c\u9012\u5f52\u5730\u6539\u8fdb\u8fd9\u4e9b\u56fe\u7684\u9012\u5f52\u7ec6\u5316\u5b50\u7f51\u3002\u5bf9\u516c\u5171\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4ea7\u751f\u4e86\u62d3\u6251\u66f4\u52a0\u4e00\u81f4\u7684\u5206\u5272\u56fe\uff0c\u5e76\u4e14\u660e\u663e\u7684\u5206\u7c7b\u9519\u8bef\u66f4\u5c11\u3002\u6b64\u5916\uff0c\u9012\u5f52\u7ec6\u5316\u6a21\u5757\u5728\u5bf9\u5176\u4ed6\u65b9\u6cd5\u7684\u5206\u5272\u56fe\u8fdb\u884c\u540e\u5904\u7406\u3001\u81ea\u52a8\u7ea0\u6b63\u5206\u7c7b\u9519\u8bef\u5e76\u63d0\u9ad8\u62d3\u6251\u4e00\u81f4\u6027\u65b9\u9762\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\u3002\u6a21\u578b\u4ee3\u7801\u3001\u6743\u91cd\u548c\u9884\u6d4b\u53ef\u5728 https://github.com/j-morano/rrwnet \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2402.03166v1](http://arxiv.org/pdf/2402.03166v1)|null|\n", "2402.03124": "|**2024-02-05**|**Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks**|\u6d88\u9664\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u4e2d\u7684\u786c\u6807\u7b7e\u7ea6\u675f|Yanbo Wang, Jian Liang, Ran He|Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints. Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels. In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process. In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods. Extensive experiments testify to the label recovery accuracy, as well as the benefits to the following image reconstruction. We believe soft labels in classification tasks are worth further attention in gradient inversion attacks.|\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u65e8\u5728\u4ece\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\u66b4\u9732\u7684\u4e2d\u95f4\u68af\u5ea6\u91cd\u5efa\u672c\u5730\u8bad\u7ec3\u6570\u636e\u3002\u5c3d\u7ba1\u653b\u51fb\u6210\u529f\uff0c\u4f46\u4e4b\u524d\u7684\u6240\u6709\u65b9\u6cd5\uff0c\u4ece\u91cd\u5efa\u5355\u4e2a\u6570\u636e\u70b9\u5f00\u59cb\uff0c\u7136\u540e\u5c06\u5355\u56fe\u50cf\u9650\u5236\u653e\u5bbd\u5230\u6279\u91cf\u7ea7\u522b\uff0c\u90fd\u4ec5\u5728\u786c\u6807\u7b7e\u7ea6\u675f\u4e0b\u8fdb\u884c\u6d4b\u8bd5\u3002\u5373\u4f7f\u5bf9\u4e8e\u5355\u56fe\u50cf\u91cd\u5efa\uff0c\u6211\u4eec\u4ecd\u7136\u7f3a\u4e4f\u57fa\u4e8e\u5206\u6790\u7684\u7b97\u6cd5\u6765\u6062\u590d\u589e\u5f3a\u7684\u8f6f\u6807\u7b7e\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06\u91cd\u70b9\u4ece\u6269\u5927\u6279\u91cf\u5927\u5c0f\u8f6c\u79fb\u5230\u7814\u7a76\u786c\u6807\u7b7e\u7ea6\u675f\uff0c\u8003\u8651\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u6807\u7b7e\u5e73\u6ed1\u548c\u6df7\u5408\u6280\u672f\u7684\u66f4\u73b0\u5b9e\u7684\u60c5\u51b5\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7b2c\u4e00\u4e2a\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b97\u6cd5\uff0c\u53ef\u4ee5\u4ece\u5355\u8f93\u5165\u68af\u5ea6\u4e2d\u540c\u65f6\u6062\u590d\u771f\u5b9e\u589e\u5f3a\u6807\u7b7e\u548c\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u5165\u7279\u5f81\uff0c\u5e76\u4e3a\u4efb\u4f55\u57fa\u4e8e\u5206\u6790\u7684\u6807\u7b7e\u6062\u590d\u63d0\u4f9b\u5fc5\u8981\u6761\u4ef6\u65b9\u6cd5\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6807\u7b7e\u6062\u590d\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u5bf9\u540e\u7eed\u56fe\u50cf\u91cd\u5efa\u7684\u597d\u5904\u3002\u6211\u4eec\u8ba4\u4e3a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8f6f\u6807\u7b7e\u5728\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u4e2d\u503c\u5f97\u8fdb\u4e00\u6b65\u5173\u6ce8\u3002|[2402.03124v1](http://arxiv.org/pdf/2402.03124v1)|**[link](https://github.com/ybwang119/label_recovery)**|\n", "2402.03094": "|**2024-02-05**|**Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector**|\u901a\u8fc7\u589e\u5f3a\u578b\u5f00\u653e\u96c6\u5bf9\u8c61\u68c0\u6d4b\u5668\u8fdb\u884c\u8de8\u57df\u5c11\u6837\u672c\u5bf9\u8c61\u68c0\u6d4b|Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Lingjie Kong, Yanwei Fu, Luc Van Gool, et.al.|This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting open-set detectors directly for CD-FSOD. Sequentially, to overcome the performance degradation issue and also to answer the second proposed question, we endeavor to enhance the vanilla DE-ViT. With several novel components including finetuning, a learnable prototype module, and a lightweight attention module, we present an improved Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO). Experiments show that our CD-ViTO achieves impressive results on both out-of-domain and in-domain target datasets, establishing new SOTAs for both CD-FSOD and FSOD. All the datasets, codes, and models will be released to the community.|\u672c\u6587\u89e3\u51b3\u4e86\u8de8\u57df\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff08CD-FSOD\uff09\u7684\u6311\u6218\uff0c\u65e8\u5728\u4e3a\u5177\u6709\u6700\u5c11\u6807\u8bb0\u793a\u4f8b\u7684\u65b0\u9886\u57df\u5f00\u53d1\u51c6\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u3002\u867d\u7136\u57fa\u4e8e Transformer \u7684\u5f00\u653e\u96c6\u68c0\u6d4b\u5668\uff08\u4f8b\u5982 DE-ViT~\\cite{zhang2023Detect}\uff09\u5728\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u548c\u4f20\u7edf\u7684\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u68c0\u6d4b\u7684\u7c7b\u522b\u8d85\u51fa\u4e86\u8bad\u7ec3\u671f\u95f4\u770b\u5230\u7684\u7c7b\u522b\uff0c\u56e0\u6b64\u6211\u4eec\u81ea\u7136\u5730\u63d0\u51fa\u4e86\u4e24\u4e2a\u5173\u952e\u95ee\u9898\u95ee\u9898\uff1a1\uff09\u8fd9\u79cd\u5f00\u96c6\u68c0\u6d4b\u65b9\u6cd5\u53ef\u4ee5\u8f7b\u677e\u63a8\u5e7f\u5230 CD-FSOD \u5417\uff1f 2\uff09\u5982\u679c\u4e0d\u662f\uff0c\u5f53\u9762\u4e34\u663e\u7740\u7684\u57df\u5dee\u8ddd\u65f6\u5982\u4f55\u589e\u5f3a\u5f00\u653e\u96c6\u65b9\u6cd5\u7684\u7ed3\u679c\uff1f\u4e3a\u4e86\u89e3\u51b3\u7b2c\u4e00\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u51e0\u4e2a\u6307\u6807\u6765\u91cf\u5316\u57df\u5dee\u5f02\uff0c\u5e76\u5efa\u7acb\u5177\u6709\u4e0d\u540c\u57df\u6307\u6807\u503c\u7684\u65b0 CD-FSOD \u57fa\u51c6\u3002\u4e00\u4e9b\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u5f00\u653e\u96c6\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\u5728\u6b64\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728\u57df\u5916\u6570\u636e\u96c6\u4e0a\u89c2\u5bdf\u5230\u660e\u663e\u7684\u6027\u80fd\u4e0b\u964d\u3002\u8fd9\u8868\u660e\u76f4\u63a5\u91c7\u7528\u5f00\u96c6\u63a2\u6d4b\u5668\u8fdb\u884cCD-FSOD\u662f\u5931\u8d25\u7684\u3002\u63a5\u4e0b\u6765\uff0c\u4e3a\u4e86\u514b\u670d\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u5e76\u56de\u7b54\u7b2c\u4e8c\u4e2a\u63d0\u51fa\u7684\u95ee\u9898\uff0c\u6211\u4eec\u52aa\u529b\u589e\u5f3a vanilla DE-ViT\u3002\u901a\u8fc7\u5fae\u8c03\u3001\u53ef\u5b66\u4e60\u539f\u578b\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u6a21\u5757\u7b49\u51e0\u4e2a\u65b0\u9896\u7684\u7ec4\u4ef6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684 CD-FSOD (CD-ViTO) \u8de8\u57df\u89c6\u89c9\u8f6c\u6362\u5668\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 CD-ViTO \u5728\u57df\u5916\u548c\u57df\u5185\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4e3a CD-FSOD \u548c FSOD \u5efa\u7acb\u4e86\u65b0\u7684 SOTA\u3002\u6240\u6709\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u6a21\u578b\u90fd\u5c06\u53d1\u5e03\u5230\u793e\u533a\u3002|[2402.03094v1](http://arxiv.org/pdf/2402.03094v1)|null|\n", "2402.03019": "|**2024-02-05**|**Taylor Videos for Action Recognition**|\u7528\u4e8e\u52a8\u4f5c\u8bc6\u522b\u7684\u6cf0\u52d2\u89c6\u9891|Lei Wang, Xiuyuan Yuan, Tom Gedeon, Liang Zheng|Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us dominant motion patterns, where static objects, small and unstable motions are removed. Experimentally we show that Taylor videos are effective inputs to popular architectures including 2D CNNs, 3D CNNs, and transformers. When used individually, Taylor videos yield competitive action recognition accuracy compared to RGB videos and optical flow. When fused with RGB or optical flow videos, further accuracy improvement is achieved.|\u4ece\u89c6\u9891\u4e2d\u6709\u6548\u5730\u63d0\u53d6\u52a8\u4f5c\u662f\u52a8\u4f5c\u8bc6\u522b\u7684\u4e00\u4e2a\u5173\u952e\u4e14\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\u3002\u8fd9\u4e2a\u95ee\u9898\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u8fd0\u52a8\uff08i\uff09\u6ca1\u6709\u660e\u786e\u7684\u5f62\u5f0f\uff0c\uff08ii\uff09\u5177\u6709\u4f4d\u79fb\u3001\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7b49\u5404\u79cd\u6982\u5ff5\uff0c\u5e76\u4e14\uff08iii\uff09\u901a\u5e38\u5305\u542b\u7531\u4e0d\u7a33\u5b9a\u50cf\u7d20\u5f15\u8d77\u7684\u566a\u58f0\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6cf0\u52d2\u89c6\u9891\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u683c\u5f0f\uff0c\u7a81\u51fa\u663e\u793a\u6bcf\u4e2a\u5e27\u4e2d\u7684\u4e3b\u5bfc\u8fd0\u52a8\uff08\u4f8b\u5982\uff0c\u6325\u624b\uff09\uff0c\u79f0\u4e3a\u6cf0\u52d2\u5e27\u3002\u6cf0\u52d2\u89c6\u9891\u4ee5\u6cf0\u52d2\u7ea7\u6570\u547d\u540d\uff0c\u6cf0\u52d2\u7ea7\u6570\u4f7f\u7528\u91cd\u8981\u672f\u8bed\u5728\u7ed9\u5b9a\u70b9\u903c\u8fd1\u51fd\u6570\u3002\u5728\u89c6\u9891\u573a\u666f\u4e2d\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u9690\u5f0f\u8fd0\u52a8\u63d0\u53d6\u51fd\u6570\uff0c\u65e8\u5728\u4ece\u89c6\u9891\u65f6\u95f4\u5757\u4e2d\u63d0\u53d6\u8fd0\u52a8\u3002\u5728\u6b64\u5757\u4e2d\uff0c\u4f7f\u7528\u5e27\u3001\u5dee\u5f02\u5e27\u548c\u9ad8\u9636\u5dee\u5f02\u5e27\uff0c\u6211\u4eec\u6267\u884c\u6cf0\u52d2\u5c55\u5f00\u4ee5\u5728\u8d77\u59cb\u5e27\u5904\u8fd1\u4f3c\u8be5\u51fd\u6570\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6cf0\u52d2\u7ea7\u6570\u4e2d\u9ad8\u9636\u9879\u7684\u603b\u548c\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e3b\u5bfc\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5176\u4e2d\u9759\u6001\u7269\u4f53\u3001\u5c0f\u4e14\u4e0d\u7a33\u5b9a\u7684\u8fd0\u52a8\u88ab\u79fb\u9664\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e Taylor \u89c6\u9891\u662f\u6d41\u884c\u67b6\u6784\uff08\u5305\u62ec 2D CNN\u30013D CNN \u548c Transformer\uff09\u7684\u6709\u6548\u8f93\u5165\u3002\u5355\u72ec\u4f7f\u7528\u65f6\uff0c\u4e0e RGB \u89c6\u9891\u548c\u5149\u6d41\u76f8\u6bd4\uff0c\u6cf0\u52d2\u89c6\u9891\u7684\u52a8\u4f5c\u8bc6\u522b\u7cbe\u5ea6\u5177\u6709\u7ade\u4e89\u529b\u3002\u5f53\u4e0e RGB \u6216\u5149\u6d41\u89c6\u9891\u878d\u5408\u65f6\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u3002|[2402.03019v1](http://arxiv.org/pdf/2402.03019v1)|null|\n", "2402.03003": "|**2024-02-05**|**[Citation needed] Data usage and citation practices in medical imaging conferences**|[\u9700\u8981\u5f15\u7528]\u533b\u5b66\u5f71\u50cf\u4f1a\u8bae\u4e2d\u7684\u6570\u636e\u4f7f\u7528\u548c\u5f15\u7528\u5b9e\u8df5|Th\u00e9o Sourget, Ahmet Akko\u00e7, Stinna Winther, Christine Lyngbye Galsgaard, Amelia Jim\u00e9nez-S\u00e1nchez, Dovile Juodelyte, Caroline Petitjean, Veronika Cheplygina|Medical imaging papers often focus on methodology, but the quality of the algorithms and the validity of the conclusions are highly dependent on the datasets used. As creating datasets requires a lot of effort, researchers often use publicly available datasets, there is however no adopted standard for citing the datasets used in scientific papers, leading to difficulty in tracking dataset usage. In this work, we present two open-source tools we created that could help with the detection of dataset usage, a pipeline \\url{https://github.com/TheoSourget/Public_Medical_Datasets_References} using OpenAlex and full-text analysis, and a PDF annotation software \\url{https://github.com/TheoSourget/pdf_annotator} used in our study to manually label the presence of datasets. We applied both tools on a study of the usage of 20 publicly available medical datasets in papers from MICCAI and MIDL. We compute the proportion and the evolution between 2013 and 2023 of 3 types of presence in a paper: cited, mentioned in the full text, cited and mentioned. Our findings demonstrate the concentration of the usage of a limited set of datasets. We also highlight different citing practices, making the automation of tracking difficult.|\u533b\u5b66\u6210\u50cf\u8bba\u6587\u901a\u5e38\u5173\u6ce8\u65b9\u6cd5\u8bba\uff0c\u4f46\u7b97\u6cd5\u7684\u8d28\u91cf\u548c\u7ed3\u8bba\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6240\u4f7f\u7528\u7684\u6570\u636e\u96c6\u3002\u7531\u4e8e\u521b\u5efa\u6570\u636e\u96c6\u9700\u8981\u5927\u91cf\u7684\u5de5\u4f5c\uff0c\u7814\u7a76\u4eba\u5458\u7ecf\u5e38\u4f7f\u7528\u516c\u5f00\u7684\u6570\u636e\u96c6\uff0c\u4f46\u662f\u6ca1\u6709\u91c7\u7528\u7684\u6807\u51c6\u6765\u5f15\u7528\u79d1\u5b66\u8bba\u6587\u4e2d\u4f7f\u7528\u7684\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u96be\u4ee5\u8ddf\u8e2a\u6570\u636e\u96c6\u7684\u4f7f\u7528\u60c5\u51b5\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u6211\u4eec\u521b\u5efa\u7684\u4e24\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u53ef\u4ee5\u5e2e\u52a9\u68c0\u6d4b\u6570\u636e\u96c6\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u4e00\u4e2a\u4f7f\u7528 OpenAlex \u548c\u5168\u6587\u5206\u6790\u7684\u7ba1\u9053 \\url{https://github.com/TheoSourget/Public_Medical_Datasets_References}\uff0c\u4ee5\u53ca\u4e00\u4e2a\u6211\u4eec\u7684\u7814\u7a76\u4e2d\u4f7f\u7528 PDF \u6ce8\u91ca\u8f6f\u4ef6 \\url{https://github.com/TheoSourget/pdf_annotator} \u6765\u624b\u52a8\u6807\u8bb0\u6570\u636e\u96c6\u7684\u5b58\u5728\u3002\u6211\u4eec\u5e94\u7528\u8fd9\u4e24\u79cd\u5de5\u5177\u6765\u7814\u7a76 MICCAI \u548c MIDL \u8bba\u6587\u4e2d 20 \u4e2a\u516c\u5f00\u53ef\u7528\u7684\u533b\u5b66\u6570\u636e\u96c6\u7684\u4f7f\u7528\u60c5\u51b5\u3002\u6211\u4eec\u8ba1\u7b97\u4e86 2013 \u5e74\u81f3 2023 \u5e74\u95f4\u8bba\u6587\u4e2d 3 \u79cd\u5b58\u5728\u7c7b\u578b\u7684\u6bd4\u4f8b\u548c\u6f14\u53d8\uff1a\u88ab\u5f15\u7528\u3001\u5168\u6587\u63d0\u53ca\u3001\u88ab\u5f15\u7528\u548c\u63d0\u53ca\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u6709\u9650\u6570\u636e\u96c6\u7684\u4f7f\u7528\u96c6\u4e2d\u3002\u6211\u4eec\u8fd8\u5f3a\u8c03\u4e86\u4e0d\u540c\u7684\u5f15\u7528\u5b9e\u8df5\uff0c\u8fd9\u4f7f\u5f97\u8ddf\u8e2a\u7684\u81ea\u52a8\u5316\u53d8\u5f97\u56f0\u96be\u3002|[2402.03003v1](http://arxiv.org/pdf/2402.03003v1)|**[link](https://github.com/theosourget/public_medical_datasets_references)**|\n", "2402.02986": "|**2024-02-05**|**A Safety-Adapted Loss for Pedestrian Detection in Automated Driving**|\u81ea\u52a8\u9a7e\u9a76\u4e2d\u884c\u4eba\u68c0\u6d4b\u7684\u5b89\u5168\u81ea\u9002\u5e94\u635f\u5931|Maria Lyssenko, Piyush Pimplikar, Maarten Bieshaar, Farzad Nozarian, Rudolph Triebel|In safety-critical domains like automated driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU). As common evaluation metrics are not an adequate safety indicator, recent works employ approaches to identify safety-critical VRU and back-annotate the risk to the object detector. However, those approaches do not consider the safety factor in the deep neural network (DNN) training process. Thus, state-of-the-art DNN penalizes all misdetections equally irrespective of their criticality. Subsequently, to mitigate the occurrence of critical failure cases, i.e., false negatives, a safety-aware training strategy might be required to enhance the detection performance for critical pedestrians. In this paper, we propose a novel safety-aware loss variation that leverages the estimated per-pedestrian criticality scores during training. We exploit the reachability set-based time-to-collision (TTC-RSB) metric from the motion domain along with distance information to account for the worst-case threat quantifying the criticality. Our evaluation results using RetinaNet and FCOS on the nuScenes dataset demonstrate that training the models with our safety-aware loss function mitigates the misdetection of critical pedestrians without sacrificing performance for the general case, i.e., pedestrians outside the safety-critical zone.|\u5728\u81ea\u52a8\u9a7e\u9a76 (AD) \u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u7269\u4f53\u68c0\u6d4b\u5668\u7684\u9519\u8bef\u53ef\u80fd\u4f1a\u5371\u53ca\u884c\u4eba\u548c\u5176\u4ed6\u6613\u53d7\u4f24\u5bb3\u7684\u9053\u8def\u4f7f\u7528\u8005 (VRU)\u3002\u7531\u4e8e\u5e38\u89c1\u7684\u8bc4\u4f30\u6307\u6807\u5e76\u4e0d\u662f\u8db3\u591f\u7684\u5b89\u5168\u6307\u6807\uff0c\u56e0\u6b64\u6700\u8fd1\u7684\u5de5\u4f5c\u91c7\u7528\u4e86\u4e00\u4e9b\u65b9\u6cd5\u6765\u8bc6\u522b\u5b89\u5168\u5173\u952e\u7684 VRU \u5e76\u53cd\u5411\u6ce8\u91ca\u5bf9\u8c61\u68c0\u6d4b\u5668\u7684\u98ce\u9669\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6ca1\u6709\u8003\u8651\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u56e0\u7d20\u3002\u56e0\u6b64\uff0c\u6700\u5148\u8fdb\u7684 DNN \u4f1a\u540c\u7b49\u5730\u60e9\u7f5a\u6240\u6709\u8bef\u68c0\uff0c\u65e0\u8bba\u5176\u4e25\u91cd\u6027\u5982\u4f55\u3002\u968f\u540e\uff0c\u4e3a\u4e86\u51cf\u5c11\u5173\u952e\u6545\u969c\u6848\u4f8b\uff08\u5373\u6f0f\u62a5\uff09\u7684\u53d1\u751f\uff0c\u53ef\u80fd\u9700\u8981\u5b89\u5168\u610f\u8bc6\u8bad\u7ec3\u7b56\u7565\u6765\u589e\u5f3a\u5173\u952e\u884c\u4eba\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b89\u5168\u610f\u8bc6\u635f\u5931\u53d8\u5316\uff0c\u8be5\u53d8\u5316\u5229\u7528\u8bad\u7ec3\u671f\u95f4\u4f30\u8ba1\u7684\u6bcf\u4e2a\u884c\u4eba\u5173\u952e\u6027\u5206\u6570\u3002\u6211\u4eec\u5229\u7528\u8fd0\u52a8\u57df\u4e2d\u57fa\u4e8e\u53ef\u8fbe\u6027\u96c6\u7684\u78b0\u649e\u65f6\u95f4\uff08TTC-RSB\uff09\u5ea6\u91cf\u4ee5\u53ca\u8ddd\u79bb\u4fe1\u606f\u6765\u8003\u8651\u6700\u574f\u60c5\u51b5\u7684\u5a01\u80c1\uff0c\u4ece\u800c\u91cf\u5316\u5173\u952e\u6027\u3002\u6211\u4eec\u5728 nuScenes \u6570\u636e\u96c6\u4e0a\u4f7f\u7528 RetinaNet \u548c FCOS \u8fdb\u884c\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u6211\u4eec\u7684\u5b89\u5168\u611f\u77e5\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u51cf\u5c11\u5bf9\u5173\u952e\u884c\u4eba\u7684\u8bef\u68c0\u6d4b\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u4e00\u822c\u60c5\u51b5\uff08\u5373\u5b89\u5168\u5173\u952e\u533a\u57df\u5916\u7684\u884c\u4eba\uff09\u7684\u6027\u80fd\u3002|[2402.02986v1](http://arxiv.org/pdf/2402.02986v1)|null|\n", "2402.02985": "|**2024-02-05**|**Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing**|\u7528\u4e8e\u9053\u8def\u573a\u666f\u89e3\u6790\u7684\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u56fe\u50cf\u7684\u65e0\u76d1\u7763\u8bed\u4e49\u5206\u5272|Zihan Ma, Yongshang Li, Ronggui Ma, Chen Liang|Two challenges are presented when parsing road scenes in UAV images. First, the high resolution of UAV images makes processing difficult. Second, supervised deep learning methods require a large amount of manual annotations to train robust and accurate models. In this paper, an unsupervised road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images. Subsequently, the vision foundation model SAM is utilized to generate masks for the road regions without category information. Following that, a self-supervised representation learning network extracts feature representations from all masked regions. Finally, an unsupervised clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster. The masked regions are combined with the corresponding IDs to generate initial pseudo-labels, which initiate an iterative self-training process for regular semantic segmentation. The proposed method achieves an impressive 89.96% mIoU on the development dataset without relying on any manual annotation. Particularly noteworthy is the extraordinary flexibility of the proposed method, which even goes beyond the limitations of human-defined categories and is able to acquire knowledge of new categories from the dataset itself.|\u89e3\u6790\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u9053\u8def\u573a\u666f\u65f6\u5b58\u5728\u4e24\u4e2a\u6311\u6218\u3002\u9996\u5148\uff0c\u65e0\u4eba\u673a\u56fe\u50cf\u7684\u9ad8\u5206\u8fa8\u7387\u5bfc\u81f4\u5904\u7406\u56f0\u96be\u3002\u5176\u6b21\uff0c\u6709\u76d1\u7763\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u624b\u52a8\u6ce8\u91ca\u6765\u8bad\u7ec3\u7a33\u5065\u4e14\u51c6\u786e\u7684\u6a21\u578b\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u672c\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u7684\u65e0\u76d1\u7763\u9053\u8def\u89e3\u6790\u6846\u67b6\u3002\u9996\u5148\uff0c\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u6709\u6548\u5904\u7406\u8d85\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u56fe\u50cf\uff0c\u4ee5\u5feb\u901f\u68c0\u6d4b\u611f\u5174\u8da3\u7684\u9053\u8def\u533a\u57df\u56fe\u50cf\u3002\u968f\u540e\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578bSAM\u4e3a\u6ca1\u6709\u7c7b\u522b\u4fe1\u606f\u7684\u9053\u8def\u533a\u57df\u751f\u6210\u63a9\u6a21\u3002\u63a5\u4e0b\u6765\uff0c\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7f51\u7edc\u4ece\u6240\u6709\u5c4f\u853d\u533a\u57df\u4e2d\u63d0\u53d6\u7279\u5f81\u8868\u793a\u3002\u6700\u540e\uff0c\u5e94\u7528\u65e0\u76d1\u7763\u805a\u7c7b\u7b97\u6cd5\u5bf9\u8fd9\u4e9b\u7279\u5f81\u8868\u793a\u8fdb\u884c\u805a\u7c7b\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u5206\u914d ID\u3002\u5c4f\u853d\u533a\u57df\u4e0e\u76f8\u5e94\u7684 ID \u76f8\u7ed3\u5408\uff0c\u751f\u6210\u521d\u59cb\u4f2a\u6807\u7b7e\uff0c\u4ece\u800c\u542f\u52a8\u5e38\u89c4\u8bed\u4e49\u5206\u5272\u7684\u8fed\u4ee3\u81ea\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5f00\u53d1\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684 89.96% mIoU\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4efb\u4f55\u624b\u52a8\u6ce8\u91ca\u3002\u7279\u522b\u503c\u5f97\u6ce8\u610f\u7684\u662f\u8be5\u65b9\u6cd5\u5177\u6709\u975e\u51e1\u7684\u7075\u6d3b\u6027\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u4eba\u7c7b\u5b9a\u4e49\u7c7b\u522b\u7684\u9650\u5236\uff0c\u80fd\u591f\u4ece\u6570\u636e\u96c6\u672c\u8eab\u83b7\u53d6\u65b0\u7c7b\u522b\u7684\u77e5\u8bc6\u3002|[2402.02985v1](http://arxiv.org/pdf/2402.02985v1)|null|\n", "2402.02963": "|**2024-02-05**|**One-class anomaly detection through color-to-thermal AI for building envelope inspection**|\u901a\u8fc7\u989c\u8272\u5230\u70ed\u4eba\u5de5\u667a\u80fd\u8fdb\u884c\u4e00\u7ea7\u5f02\u5e38\u68c0\u6d4b\uff0c\u7528\u4e8e\u5efa\u7b51\u56f4\u62a4\u7ed3\u6784\u68c0\u67e5|Polina Kurtser, Kailun Feng, Thomas Olofsson, Aitor De Andres|We present a label-free method for detecting anomalies during thermographic inspection of building envelopes. It is based on the AI-driven prediction of thermal distributions from color images. Effectively the method performs as a one-class classifier of the thermal image regions with high mismatch between the predicted and actual thermal distributions. The algorithm can learn to identify certain features as normal or anomalous by selecting the target sample used for training. We demonstrated this principle by training the algorithm with data collected at different outdoors temperature, which lead to the detection of thermal bridges. The method can be implemented to assist human professionals during routine building inspections or combined with mobile platforms for automating examination of large areas.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6807\u7b7e\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5efa\u7b51\u56f4\u62a4\u7ed3\u6784\u70ed\u6210\u50cf\u68c0\u67e5\u8fc7\u7a0b\u4e2d\u68c0\u6d4b\u5f02\u5e38\u60c5\u51b5\u3002\u5b83\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u5f69\u8272\u56fe\u50cf\u70ed\u5206\u5e03\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u5145\u5f53\u70ed\u56fe\u50cf\u533a\u57df\u7684\u4e00\u7c7b\u5206\u7c7b\u5668\uff0c\u9884\u6d4b\u70ed\u5206\u5e03\u4e0e\u5b9e\u9645\u70ed\u5206\u5e03\u4e4b\u95f4\u5b58\u5728\u9ad8\u5ea6\u4e0d\u5339\u914d\u3002\u8be5\u7b97\u6cd5\u53ef\u4ee5\u901a\u8fc7\u9009\u62e9\u7528\u4e8e\u8bad\u7ec3\u7684\u76ee\u6807\u6837\u672c\u6765\u5b66\u4e60\u5c06\u67d0\u4e9b\u7279\u5f81\u8bc6\u522b\u4e3a\u6b63\u5e38\u6216\u5f02\u5e38\u3002\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u5728\u4e0d\u540c\u5ba4\u5916\u6e29\u5ea6\u4e0b\u6536\u96c6\u7684\u6570\u636e\u8bad\u7ec3\u7b97\u6cd5\u6765\u8bc1\u660e\u8fd9\u4e00\u539f\u7406\uff0c\u4ece\u800c\u68c0\u6d4b\u5230\u70ed\u6865\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u65e5\u5e38\u5efa\u7b51\u68c0\u67e5\u671f\u95f4\u534f\u52a9\u4eba\u7c7b\u4e13\u4e1a\u4eba\u5458\uff0c\u6216\u4e0e\u79fb\u52a8\u5e73\u53f0\u7ed3\u5408\u4ee5\u81ea\u52a8\u68c0\u67e5\u5927\u9762\u79ef\u533a\u57df\u3002|[2402.02963v1](http://arxiv.org/pdf/2402.02963v1)|null|\n", "2402.02946": "|**2024-02-05**|**HoughToRadon Transform: New Neural Network Layer for Features Improvement in Projection Space**|HoughToRadon \u53d8\u6362\uff1a\u7528\u4e8e\u6295\u5f71\u7a7a\u95f4\u7279\u5f81\u6539\u8fdb\u7684\u65b0\u795e\u7ecf\u7f51\u7edc\u5c42|Alexandra Zhabitskaya, Alexander Sheshkus, Vladimir L. Arlazarov|In this paper, we introduce HoughToRadon Transform layer, a novel layer designed to improve the speed of neural networks incorporated with Hough Transform to solve semantic image segmentation problems. By placing it after a Hough Transform layer, \"inner\" convolutions receive modified feature maps with new beneficial properties, such as a smaller area of processed images and parameter space linearity by angle and shift. These properties were not presented in Hough Transform alone. Furthermore, HoughToRadon Transform layer allows us to adjust the size of intermediate feature maps using two new parameters, thus allowing us to balance the speed and quality of the resulting neural network. Our experiments on the open MIDV-500 dataset show that this new approach leads to time savings in document segmentation tasks and achieves state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger computational complexity.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 HoughToRadon \u53d8\u6362\u5c42\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5c42\uff0c\u65e8\u5728\u63d0\u9ad8\u4e0e\u970d\u592b\u53d8\u6362\u7ed3\u5408\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u901f\u5ea6\uff0c\u4ee5\u89e3\u51b3\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u95ee\u9898\u3002\u901a\u8fc7\u5c06\u5176\u653e\u7f6e\u5728\u970d\u592b\u53d8\u6362\u5c42\u4e4b\u540e\uff0c\u201c\u5185\u90e8\u201d\u5377\u79ef\u63a5\u6536\u5177\u6709\u65b0\u7684\u6709\u76ca\u5c5e\u6027\u7684\u4fee\u6539\u540e\u7684\u7279\u5f81\u56fe\uff0c\u4f8b\u5982\u5904\u7406\u56fe\u50cf\u7684\u8f83\u5c0f\u533a\u57df\u4ee5\u53ca\u901a\u8fc7\u89d2\u5ea6\u548c\u79fb\u4f4d\u7684\u53c2\u6570\u7a7a\u95f4\u7ebf\u6027\u3002\u8fd9\u4e9b\u5c5e\u6027\u5e76\u6ca1\u6709\u5355\u72ec\u5728\u970d\u592b\u53d8\u6362\u4e2d\u5448\u73b0\u3002\u6b64\u5916\uff0cHoughToRadon \u53d8\u6362\u5c42\u5141\u8bb8\u6211\u4eec\u4f7f\u7528\u4e24\u4e2a\u65b0\u53c2\u6570\u8c03\u6574\u4e2d\u95f4\u7279\u5f81\u56fe\u7684\u5927\u5c0f\uff0c\u4ece\u800c\u4f7f\u6211\u4eec\u80fd\u591f\u5e73\u8861\u6240\u5f97\u795e\u7ecf\u7f51\u7edc\u7684\u901f\u5ea6\u548c\u8d28\u91cf\u3002\u6211\u4eec\u5728\u5f00\u653e MIDV-500 \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b0\u65b9\u6cd5\u53ef\u4ee5\u8282\u7701\u6587\u6863\u5206\u5272\u4efb\u52a1\u7684\u65f6\u95f4\uff0c\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u7684 97.7% \u51c6\u786e\u7387\uff0c\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u9ad8\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e HoughEncoder\u3002|[2402.02946v1](http://arxiv.org/pdf/2402.02946v1)|null|\n", "2402.02887": "|**2024-02-05**|**Time-, Memory- and Parameter-Efficient Visual Adaptation**|\u65f6\u95f4\u3001\u5185\u5b58\u548c\u53c2\u6570\u9ad8\u6548\u7684\u89c6\u89c9\u9002\u5e94|Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, Anurag Arnab|As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly.   We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the training efficiency and scalability of our method by adapting a vision transformer backbone of 4 billion parameters for the computationally demanding task of video classification, without any intricate model parallelism. Here, we outperform a prior adaptor-based method which could only scale to a 1 billion parameter backbone, or fully-finetuning a smaller backbone, with the same GPU and less training time.|\u968f\u7740\u57fa\u7840\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u6d41\u884c\uff0c\u8d8a\u6765\u8d8a\u9700\u8981\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6709\u6548\u5730\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\u3002\u5c3d\u7ba1\u5df2\u7ecf\u63d0\u51fa\u4e86\u8bb8\u591a\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u7684\u8bbe\u8ba1\u76ee\u7684\u53ea\u662f\u5728\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u65b9\u9762\u6709\u6548\u3002\u7136\u800c\uff0c\u5b83\u4eec\u901a\u5e38\u4ecd\u7136\u9700\u8981\u5728\u6574\u4e2a\u6a21\u578b\u4e2d\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4eec\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u6210\u672c\u4e0d\u4f1a\u663e\u7740\u51cf\u5c11\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4f1a\u901a\u8fc7\u4e3b\u5e72\u7f51\u7edc\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u3002\u6211\u4eec\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u5e76\u884c\u7684\u8f7b\u91cf\u7ea7\u7f51\u7edc\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u8be5\u7f51\u7edc\u5bf9\u6765\u81ea\u51bb\u7ed3\u7684\u3001\u9884\u8bad\u7ec3\u7684\u4e3b\u5e72\u7f51\u7684\u7279\u5f81\u8fdb\u884c\u64cd\u4f5c\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u5728\u53c2\u6570\u65b9\u9762\u9ad8\u6548\uff0c\u800c\u4e14\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u4e5f\u9ad8\u6548\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6d41\u884c\u7684 VTAB \u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u53c2\u6570\u6743\u8861\uff0c\u5e76\u4e14\u6211\u4eec\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u6211\u4eec\u5982\u4f55\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u8d85\u8d8a\u5148\u524d\u7684\u5de5\u4f5c\u3002\u6211\u4eec\u901a\u8fc7\u91c7\u7528 40 \u4ebf\u4e2a\u53c2\u6570\u7684\u89c6\u89c9\u8f6c\u6362\u5668\u4e3b\u5e72\u6765\u9002\u5e94\u8ba1\u7b97\u8981\u6c42\u8f83\u9ad8\u7684\u89c6\u9891\u5206\u7c7b\u4efb\u52a1\uff0c\u800c\u4e0d\u9700\u8981\u4efb\u4f55\u590d\u6742\u7684\u6a21\u578b\u5e76\u884c\u6027\uff0c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f18\u4e8e\u4e4b\u524d\u57fa\u4e8e\u9002\u914d\u5668\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ea\u80fd\u6269\u5c55\u5230 10 \u4ebf\u4e2a\u53c2\u6570\u7684\u9aa8\u5e72\u7f51\uff0c\u6216\u8005\u5728\u76f8\u540c\u7684 GPU \u548c\u66f4\u5c11\u7684\u8bad\u7ec3\u65f6\u95f4\u4e0b\u5b8c\u5168\u5fae\u8c03\u66f4\u5c0f\u7684\u9aa8\u5e72\u7f51\u3002|[2402.02887v1](http://arxiv.org/pdf/2402.02887v1)|null|\n", "2402.02811": "|**2024-02-05**|**Multi-scale fMRI time series analysis for understanding neurodegeneration in MCI**|\u591a\u5c3a\u5ea6\u529f\u80fd\u78c1\u5171\u632f\u6210\u50cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7528\u4e8e\u4e86\u89e3 MCI \u4e2d\u7684\u795e\u7ecf\u9000\u884c\u6027\u53d8|Ammu R., Debanjali Bhattacharya, Ameiy Acharya, Ninad Aithal, Neelam Sinha|In this study, we present a technique that spans multi-scale views (global scale -- meaning brain network-level and local scale -- examining each individual ROI that constitutes the network) applied to resting-state fMRI volumes. Deep learning based classification is utilized in understanding neurodegeneration. The novelty of the proposed approach lies in utilizing two extreme scales of analysis. One branch considers the entire network within graph-analysis framework. Concurrently, the second branch scrutinizes each ROI within a network independently, focusing on evolution of dynamics. For each subject, graph-based approach employs partial correlation to profile the subject in a single graph where each ROI is a node, providing insights into differences in levels of participation. In contrast, non-linear analysis employs recurrence plots to profile a subject as a multichannel 2D image, revealing distinctions in underlying dynamics. The proposed approach is employed for classification of a cohort of 50 healthy control (HC) and 50 Mild Cognitive Impairment (MCI), sourced from ADNI dataset. Results point to: (1) reduced activity in ROIs such as PCC in MCI (2) greater activity in occipital in MCI, which is not seen in HC (3) when analysed for dynamics, all ROIs in MCI show greater predictability in time-series.|\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u8d8a\u591a\u5c3a\u5ea6\u89c6\u56fe\uff08\u5168\u5c40\u5c3a\u5ea6\u2014\u2014\u610f\u5473\u7740\u5927\u8111\u7f51\u7edc\u6c34\u5e73\u548c\u5c40\u90e8\u5c3a\u5ea6\u2014\u2014\u68c0\u67e5\u6784\u6210\u7f51\u7edc\u7684\u6bcf\u4e2a\u5355\u72ec\u7684 ROI\uff09\u7684\u6280\u672f\uff0c\u5e94\u7528\u4e8e\u9759\u606f\u6001 fMRI \u4f53\u79ef\u3002\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5206\u7c7b\u7528\u4e8e\u7406\u89e3\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u3002\u8be5\u65b9\u6cd5\u7684\u65b0\u9896\u4e4b\u5904\u5728\u4e8e\u5229\u7528\u4e86\u4e24\u79cd\u6781\u7aef\u7684\u5206\u6790\u5c3a\u5ea6\u3002\u4e00\u4e2a\u5206\u652f\u5728\u56fe\u5206\u6790\u6846\u67b6\u5185\u8003\u8651\u6574\u4e2a\u7f51\u7edc\u3002\u540c\u65f6\uff0c\u7b2c\u4e8c\u4e2a\u5206\u652f\u72ec\u7acb\u5ba1\u67e5\u7f51\u7edc\u5185\u7684\u6bcf\u4e2a\u6295\u8d44\u56de\u62a5\u7387\uff0c\u91cd\u70b9\u5173\u6ce8\u52a8\u6001\u7684\u6f14\u53d8\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u4e3b\u9898\uff0c\u57fa\u4e8e\u56fe\u8868\u7684\u65b9\u6cd5\u91c7\u7528\u90e8\u5206\u76f8\u5173\u6027\u5728\u5355\u4e2a\u56fe\u8868\u4e2d\u63cf\u8ff0\u4e3b\u9898\uff0c\u5176\u4e2d\u6bcf\u4e2a ROI \u90fd\u662f\u4e00\u4e2a\u8282\u70b9\uff0c\u4ece\u800c\u63d0\u4f9b\u5bf9\u53c2\u4e0e\u6c34\u5e73\u5dee\u5f02\u7684\u6d1e\u5bdf\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u975e\u7ebf\u6027\u5206\u6790\u91c7\u7528\u9012\u5f52\u56fe\u5c06\u5bf9\u8c61\u63cf\u7ed8\u4e3a\u591a\u901a\u9053 2D \u56fe\u50cf\uff0c\u63ed\u793a\u6f5c\u5728\u52a8\u6001\u7684\u5dee\u5f02\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7528\u4e8e\u5bf9\u6e90\u81ea ADNI \u6570\u636e\u96c6\u7684 50 \u540d\u5065\u5eb7\u5bf9\u7167 (HC) \u548c 50 \u540d\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d (MCI) \u4eba\u7fa4\u8fdb\u884c\u5206\u7c7b\u3002\u7ed3\u679c\u8868\u660e\uff1a(1) ROI \u4e2d\u7684\u6d3b\u52a8\u51cf\u5c11\uff0c\u4f8b\u5982 MCI \u4e2d\u7684 PCC (2) MCI \u4e2d\u6795\u9aa8\u7684\u6d3b\u52a8\u589e\u52a0\uff0c\u8fd9\u5728 HC \u4e2d\u672a\u89c1 (3) \u5f53\u8fdb\u884c\u52a8\u6001\u5206\u6790\u65f6\uff0cMCI \u4e2d\u7684\u6240\u6709 ROI \u5728\u65f6\u95f4\u4e0a\u90fd\u663e\u793a\u51fa\u66f4\u5927\u7684\u53ef\u9884\u6d4b\u6027 -\u7cfb\u5217\u3002|[2402.02811v1](http://arxiv.org/pdf/2402.02811v1)|null|\n", "2402.02797": "|**2024-02-05**|**Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects**|\u7528\u4e8e\u8868\u9762\u7f3a\u9677\u663e\u7740\u6027\u68c0\u6d4b\u7684\u8054\u5408\u6ce8\u610f\u529b\u5f15\u5bfc\u7279\u5f81\u878d\u5408\u7f51\u7edc|Xiaoheng Jiang, Feng Yan, Yang Lu, Ke Wang, Shuai Guo, Tianzhu Zhang, Yanwei Pang, Jianwei Niu, Mingliang Xu|Surface defect inspection plays an important role in the process of industrial manufacture and production. Though Convolutional Neural Network (CNN) based defect inspection methods have made huge leaps, they still confront a lot of challenges such as defect scale variation, complex background, low contrast, and so on. To address these issues, we propose a joint attention-guided feature fusion network (JAFFNet) for saliency detection of surface defects based on the encoder-decoder network. JAFFNet mainly incorporates a joint attention-guided feature fusion (JAFF) module into decoding stages to adaptively fuse low-level and high-level features. The JAFF module learns to emphasize defect features and suppress background noise during feature fusion, which is beneficial for detecting low-contrast defects. In addition, JAFFNet introduces a dense receptive field (DRF) module following the encoder to capture features with rich context information, which helps detect defects of different scales. The JAFF module mainly utilizes a learned joint channel-spatial attention map provided by high-level semantic features to guide feature fusion. The attention map makes the model pay more attention to defect features. The DRF module utilizes a sequence of multi-receptive-field (MRF) units with each taking as inputs all the preceding MRF feature maps and the original input. The obtained DRF features capture rich context information with a large range of receptive fields. Extensive experiments conducted on SD-saliency-900, Magnetic tile, and DAGM 2007 indicate that our method achieves promising performance in comparison with other state-of-the-art methods. Meanwhile, our method reaches a real-time defect detection speed of 66 FPS.|\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u5728\u5de5\u4e1a\u5236\u9020\u548c\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\u3002\u5c3d\u7ba1\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u7f3a\u9677\u68c0\u6d4b\u65b9\u6cd5\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u98de\u8dc3\uff0c\u4f46\u4ecd\u7136\u9762\u4e34\u7740\u7f3a\u9677\u5c3a\u5ea6\u53d8\u5316\u3001\u80cc\u666f\u590d\u6742\u3001\u5bf9\u6bd4\u5ea6\u4f4e\u7b49\u8bf8\u591a\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u7684\u8054\u5408\u6ce8\u610f\u529b\u5f15\u5bfc\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08JAFFNet\uff09\uff0c\u7528\u4e8e\u8868\u9762\u7f3a\u9677\u7684\u663e\u7740\u6027\u68c0\u6d4b\u3002 JAFFNet \u4e3b\u8981\u5c06\u8054\u5408\u6ce8\u610f\u529b\u5f15\u5bfc\u7279\u5f81\u878d\u5408\uff08JAFF\uff09\u6a21\u5757\u7eb3\u5165\u89e3\u7801\u9636\u6bb5\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u878d\u5408\u4f4e\u7ea7\u548c\u9ad8\u7ea7\u7279\u5f81\u3002 JAFF\u6a21\u5757\u5728\u7279\u5f81\u878d\u5408\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u5f3a\u8c03\u7f3a\u9677\u7279\u5f81\u5e76\u6291\u5236\u80cc\u666f\u566a\u58f0\uff0c\u8fd9\u6709\u5229\u4e8e\u68c0\u6d4b\u4f4e\u5bf9\u6bd4\u5ea6\u7f3a\u9677\u3002\u6b64\u5916\uff0cJAFFNet\u5728\u7f16\u7801\u5668\u540e\u9762\u5f15\u5165\u4e86\u5bc6\u96c6\u611f\u53d7\u91ce\uff08DRF\uff09\u6a21\u5757\u6765\u6355\u83b7\u5177\u6709\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u7279\u5f81\uff0c\u8fd9\u6709\u52a9\u4e8e\u68c0\u6d4b\u4e0d\u540c\u5c3a\u5ea6\u7684\u7f3a\u9677\u3002 JAFF\u6a21\u5757\u4e3b\u8981\u5229\u7528\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\u63d0\u4f9b\u7684\u5b66\u4e60\u8054\u5408\u901a\u9053\u7a7a\u95f4\u6ce8\u610f\u529b\u56fe\u6765\u6307\u5bfc\u7279\u5f81\u878d\u5408\u3002\u6ce8\u610f\u529b\u56fe\u4f7f\u5f97\u6a21\u578b\u66f4\u52a0\u5173\u6ce8\u7f3a\u9677\u7279\u5f81\u3002 DRF \u6a21\u5757\u5229\u7528\u4e00\u7cfb\u5217\u591a\u611f\u53d7\u91ce (MRF) \u5355\u5143\uff0c\u6bcf\u4e2a\u5355\u5143\u5c06\u6240\u6709\u524d\u9762\u7684 MRF \u7279\u5f81\u56fe\u548c\u539f\u59cb\u8f93\u5165\u4f5c\u4e3a\u8f93\u5165\u3002\u83b7\u5f97\u7684 DRF \u7279\u5f81\u6355\u83b7\u4e86\u5177\u6709\u5927\u8303\u56f4\u611f\u53d7\u91ce\u7684\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u5728 SD-saliency-900\u3001Magnetictile \u548c DAGM 2007 \u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86 66 FPS \u7684\u5b9e\u65f6\u7f3a\u9677\u68c0\u6d4b\u901f\u5ea6\u3002|[2402.02797v1](http://arxiv.org/pdf/2402.02797v1)|null|\n", "2402.02761": "|**2024-02-05**|**Transmission Line Detection Based on Improved Hough Transform**|\u57fa\u4e8e\u6539\u8fdbHough\u53d8\u6362\u7684\u8f93\u7535\u7ebf\u8def\u68c0\u6d4b|Wei Song, Pei Li, Man Wang|To address the challenges of low detection accuracy and high false positive rates of transmission lines in UAV (Unmanned Aerial Vehicle) images, we explore the linear features and spatial distribution. We introduce an enhanced stochastic Hough transform technique tailored for detecting transmission lines in complex backgrounds. By employing the Hessian matrix for initial preprocessing of transmission lines, and utilizing boundary search and pixel row segmentation, our approach distinguishes transmission line areas from the background. We significantly reduce both false positives and missed detections, thereby improving the accuracy of transmission line identification. Experiments demonstrate that our method not only processes images more rapidly, but also yields superior detection results compared to conventional and random Hough transform methods.|\u4e3a\u4e86\u89e3\u51b3\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u4f20\u8f93\u7ebf\u68c0\u6d4b\u7cbe\u5ea6\u4f4e\u548c\u8bef\u62a5\u7387\u9ad8\u7684\u6311\u6218\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u7ebf\u6027\u7279\u5f81\u548c\u7a7a\u95f4\u5206\u5e03\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u968f\u673a\u970d\u592b\u53d8\u6362\u6280\u672f\uff0c\u4e13\u95e8\u7528\u4e8e\u68c0\u6d4b\u590d\u6742\u80cc\u666f\u4e2d\u7684\u4f20\u8f93\u7ebf\u3002\u901a\u8fc7\u91c7\u7528 Hessian \u77e9\u9635\u5bf9\u4f20\u8f93\u7ebf\u8fdb\u884c\u521d\u59cb\u9884\u5904\u7406\uff0c\u5e76\u5229\u7528\u8fb9\u754c\u641c\u7d22\u548c\u50cf\u7d20\u884c\u5206\u5272\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u4f20\u8f93\u7ebf\u533a\u57df\u4e0e\u80cc\u666f\u533a\u5206\u5f00\u6765\u3002\u6211\u4eec\u663e\u7740\u51cf\u5c11\u4e86\u8bef\u62a5\u548c\u6f0f\u68c0\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4f20\u8f93\u7ebf\u8def\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u548c\u968f\u673a\u970d\u592b\u53d8\u6362\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u4ee5\u66f4\u5feb\u5730\u5904\u7406\u56fe\u50cf\uff0c\u800c\u4e14\u53ef\u4ee5\u4ea7\u751f\u66f4\u597d\u7684\u68c0\u6d4b\u7ed3\u679c\u3002|[2402.02761v1](http://arxiv.org/pdf/2402.02761v1)|null|\n", "2402.02738": "|**2024-02-05**|**Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective**|\u4ece\u878d\u5408\u7b56\u7565\u7684\u89d2\u5ea6\u63d0\u9ad8\u6fc0\u5149\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\u6a21\u578b\u5bf9\u6297\u5929\u6c14\u8150\u8680\u7684\u9c81\u68d2\u6027|Yihao Huang, Kaiyuan Yu, Qing Guo, Felix Juefei-Xu, Xiaojun Jia, Tianlin Li, Geguang Pu, Yang Liu|In recent years, LiDAR-camera fusion models have markedly advanced 3D object detection tasks in autonomous driving. However, their robustness against common weather corruption such as fog, rain, snow, and sunlight in the intricate physical world remains underexplored. In this paper, we evaluate the robustness of fusion models from the perspective of fusion strategies on the corrupted dataset. Based on the evaluation, we further propose a concise yet practical fusion strategy to enhance the robustness of the fusion models, namely flexibly weighted fusing features from LiDAR and camera sources to adapt to varying weather scenarios. Experiments conducted on four types of fusion models, each with two distinct lightweight implementations, confirm the broad applicability and effectiveness of the approach.|\u8fd1\u5e74\u6765\uff0cLiDAR-\u76f8\u673a\u878d\u5408\u6a21\u578b\u663e\u7740\u63a8\u8fdb\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684 3D \u7269\u4f53\u68c0\u6d4b\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5bf9\u590d\u6742\u7269\u7406\u4e16\u754c\u4e2d\u96fe\u3001\u96e8\u3001\u96ea\u548c\u9633\u5149\u7b49\u5e38\u89c1\u5929\u6c14\u7834\u574f\u7684\u7a33\u5065\u6027\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u635f\u574f\u6570\u636e\u96c6\u7684\u878d\u5408\u7b56\u7565\u7684\u89d2\u5ea6\u8bc4\u4f30\u878d\u5408\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u57fa\u4e8e\u8bc4\u4f30\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u6d01\u800c\u5b9e\u7528\u7684\u878d\u5408\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u878d\u5408\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5373\u7075\u6d3b\u52a0\u6743\u878d\u5408\u6fc0\u5149\u96f7\u8fbe\u548c\u76f8\u673a\u6e90\u7684\u7279\u5f81\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u5929\u6c14\u573a\u666f\u3002\u5bf9\u56db\u79cd\u7c7b\u578b\u7684\u878d\u5408\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u9a8c\uff0c\u6bcf\u79cd\u6a21\u578b\u90fd\u6709\u4e24\u79cd\u4e0d\u540c\u7684\u8f7b\u91cf\u7ea7\u5b9e\u73b0\uff0c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u6709\u6548\u6027\u3002|[2402.02738v1](http://arxiv.org/pdf/2402.02738v1)|null|\n", "2402.02724": "|**2024-02-05**|**FDNet: Frequency Domain Denoising Network For Cell Segmentation in Astrocytes Derived From Induced Pluripotent Stem Cells**|FDNet\uff1a\u7528\u4e8e\u8bf1\u5bfc\u591a\u80fd\u5e72\u7ec6\u80de\u884d\u751f\u7684\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u5206\u5272\u7684\u9891\u57df\u53bb\u566a\u7f51\u7edc|Haoran Li, Jiahua Shi, Huaming Chen, Bo Du, Simon Maksour, Gabrielle Phillips, Mirella Dottori, Jun Shen|Artificially generated induced pluripotent stem cells (iPSCs) from somatic cells play an important role for disease modeling and drug screening of neurodegenerative diseases. Astrocytes differentiated from iPSCs are important targets to investigate neuronal metabolism. The astrocyte differentiation progress can be monitored through the variations of morphology observed from microscopy images at different differentiation stages, then determined by molecular biology techniques upon maturation. However, the astrocytes usually ``perfectly'' blend into the background and some of them are covered by interference information (i.e., dead cells, media sediments, and cell debris), which makes astrocytes difficult to observe. Due to the lack of annotated datasets, the existing state-of-the-art deep learning approaches cannot be used to address this issue. In this paper, we introduce a new task named astrocyte segmentation with a novel dataset, called IAI704, which contains 704 images and their corresponding pixel-level annotation masks. Moreover, a novel frequency domain denoising network, named FDNet, is proposed for astrocyte segmentation. In detail, our FDNet consists of a contextual information fusion module (CIF), an attention block (AB), and a Fourier transform block (FTB). CIF and AB fuse multi-scale feature embeddings to localize the astrocytes. FTB transforms feature embeddings into the frequency domain and conducts a high-pass filter to eliminate interference information. Experimental results demonstrate the superiority of our proposed FDNet over the state-of-the-art substitutes in astrocyte segmentation, shedding insights for iPSC differentiation progress prediction.|\u4ece\u4f53\u7ec6\u80de\u4eba\u5de5\u4ea7\u751f\u7684\u8bf1\u5bfc\u591a\u80fd\u5e72\u7ec6\u80de\uff08iPSC\uff09\u5728\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u75be\u75c5\u5efa\u6a21\u548c\u836f\u7269\u7b5b\u9009\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\u3002\u7531 iPSC \u5206\u5316\u800c\u6765\u7684\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u662f\u7814\u7a76\u795e\u7ecf\u5143\u4ee3\u8c22\u7684\u91cd\u8981\u9776\u6807\u3002\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u7684\u5206\u5316\u8fdb\u7a0b\u53ef\u4ee5\u901a\u8fc7\u5728\u4e0d\u540c\u5206\u5316\u9636\u6bb5\u7684\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u89c2\u5bdf\u5230\u7684\u5f62\u6001\u53d8\u5316\u6765\u76d1\u6d4b\uff0c\u7136\u540e\u5728\u6210\u719f\u65f6\u901a\u8fc7\u5206\u5b50\u751f\u7269\u5b66\u6280\u672f\u6765\u786e\u5b9a\u3002\u7136\u800c\uff0c\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u901a\u5e38\u201c\u5b8c\u7f8e\u201d\u5730\u878d\u5165\u80cc\u666f\uff0c\u5e76\u4e14\u5176\u4e2d\u4e00\u4e9b\u88ab\u5e72\u6270\u4fe1\u606f\uff08\u5373\u6b7b\u7ec6\u80de\u3001\u57f9\u517b\u57fa\u6c89\u79ef\u7269\u548c\u7ec6\u80de\u788e\u7247\uff09\u8986\u76d6\uff0c\u8fd9\u4f7f\u5f97\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u96be\u4ee5\u89c2\u5bdf\u3002\u7531\u4e8e\u7f3a\u4e4f\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u96c6\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u7528\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u9879\u540d\u4e3a\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u5206\u5272\u7684\u65b0\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u4f7f\u7528\u4e00\u4e2a\u540d\u4e3a IAI704 \u7684\u65b0\u9896\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 704 \u5f20\u56fe\u50cf\u53ca\u5176\u76f8\u5e94\u7684\u50cf\u7d20\u7ea7\u6ce8\u91ca\u63a9\u6a21\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9891\u57df\u53bb\u566a\u7f51\u7edc\uff0c\u79f0\u4e3a FDNet\uff0c\u7528\u4e8e\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u5206\u5272\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684 FDNet \u7531\u4e0a\u4e0b\u6587\u4fe1\u606f\u878d\u5408\u6a21\u5757\uff08CIF\uff09\u3001\u6ce8\u610f\u529b\u5757\uff08AB\uff09\u548c\u5085\u91cc\u53f6\u53d8\u6362\u5757\uff08FTB\uff09\u7ec4\u6210\u3002 CIF \u548c AB \u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u5d4c\u5165\u6765\u5b9a\u4f4d\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u3002 FTB\u5c06\u7279\u5f81\u5d4c\u5165\u53d8\u6362\u5230\u9891\u57df\u5e76\u8fdb\u884c\u9ad8\u901a\u6ee4\u6ce2\u5668\u4ee5\u6d88\u9664\u5e72\u6270\u4fe1\u606f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 FDNet \u5728\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u5206\u5272\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u66ff\u4ee3\u54c1\uff0c\u4e3a iPSC \u5206\u5316\u8fdb\u5c55\u9884\u6d4b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002|[2402.02724v1](http://arxiv.org/pdf/2402.02724v1)|null|\n", "2402.02662": "|**2024-02-05**|**Image-Caption Encoding for Improving Zero-Shot Generalization**|\u7528\u4e8e\u6539\u8fdb\u96f6\u6837\u672c\u6cdb\u5316\u7684\u56fe\u50cf\u6807\u9898\u7f16\u7801|Eric Yang Yu, Christopher Liao, Sathvik Ravi, Theodoros Tsiligkaridis, Brian Kulis|Recent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification. However, a persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities. We first show that when an OOD data point is misclassified, the correct class can be typically found in the Top-K predicted classes. In order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only. Intuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes. We show that our method can be easily combined with other SOTA methods to enhance Top-1 OOD accuracies by 0.5% on average and up to 3% on challenging datasets. Our code: https://github.com/Chris210634/ice|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5c06\u5bf9\u6bd4\u65b9\u6cd5\u4e0e\u751f\u6210\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u4ee5\u5728\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u7b49\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684 (SOTA)\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u95ee\u9898\u662f\u5b83\u4eec\u7684\u5206\u5e03\u5916\uff08OOD\uff09\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u9996\u5148\u8868\u660e\uff0c\u5f53 OOD \u6570\u636e\u70b9\u88ab\u9519\u8bef\u5206\u7c7b\u65f6\uff0c\u901a\u5e38\u53ef\u4ee5\u5728 Top-K \u9884\u6d4b\u7c7b\u4e2d\u627e\u5230\u6b63\u786e\u7684\u7c7b\u3002\u4e3a\u4e86\u5c06\u6a21\u578b\u9884\u6d4b\u5f15\u5bfc\u5230\u9876\u7ea7\u9884\u6d4b\u7c7b\u522b\u4e2d\u7684\u6b63\u786e\u7c7b\u522b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u56fe\u50cf\u6807\u9898\u7f16\u7801\uff08ICE\uff09\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u76f4\u63a5\u5f3a\u5236\u4ec5\u5728\u8bc4\u4f30\u65f6\u56fe\u50cf\u6761\u4ef6\u9884\u6d4b\u548c\u6807\u9898\u6761\u4ef6\u9884\u6d4b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u7684\u7b80\u5355\u65b9\u6cd5\u3002\u76f4\u89c2\u4e0a\uff0c\u6211\u4eec\u5229\u7528\u751f\u6210\u7684\u6807\u9898\u7684\u72ec\u7279\u5c5e\u6027\u6765\u6307\u5bfc\u6211\u4eec\u5728 Top-K \u9884\u6d4b\u7c7b\u4e2d\u672c\u5730\u641c\u7d22\u6b63\u786e\u7684\u7c7b\u6807\u7b7e\u3002\u6211\u4eec\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u8f7b\u677e\u5730\u4e0e\u5176\u4ed6 SOTA \u65b9\u6cd5\u7ed3\u5408\uff0c\u5c06 Top-1 OOD \u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad8 0.5%\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u9ad8\u8fbe 3%\u3002\u6211\u4eec\u7684\u4ee3\u7801\uff1ahttps://github.com/Chris210634/ice|[2402.02662v1](http://arxiv.org/pdf/2402.02662v1)|**[link](https://github.com/chris210634/ice)**|\n", "2402.02653": "|**2024-02-05**|**Learning with Mixture of Prototypes for Out-of-Distribution Detection**|\u6df7\u5408\u539f\u578b\u5b66\u4e60\u4ee5\u8fdb\u884c\u5206\u5e03\u5916\u68c0\u6d4b|Haodong Lu, Dong Gong, Shuo Wang, Jason Xue, Lina Yao, Kristen Moore|Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation learning. They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes. However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data. Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance. To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) which models each class with multiple prototypes to capture the sample diversities, and learns more faithful and compact samples embeddings to enhance OOD detection. Our method automatically identifies and dynamically updates prototypes, assigning each sample to a subset of prototypes via reciprocal neighbor soft assignment weights. PALM optimizes a maximum likelihood estimation (MLE) loss to encourage the sample embeddings to be compact around the associated prototypes, as well as a contrastive loss on all prototypes to enhance intra-class compactness and inter-class discrimination at the prototype level. Moreover, the automatic estimation of prototypes enables our approach to be extended to the challenging OOD detection task with unlabelled ID data. Extensive experiments demonstrate the superiority of PALM, achieving state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark. Code is available at https://github.com/jeff024/PALM.|\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u65e8\u5728\u68c0\u6d4b\u8fdc\u79bb\u5206\u5e03\u5185\uff08ID\uff09\u8bad\u7ec3\u6570\u636e\u7684\u6d4b\u8bd5\u6837\u672c\uff0c\u8fd9\u5bf9\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u968f\u7740\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u7684\u589e\u5f3a\uff0c\u57fa\u4e8e\u8ddd\u79bb\u7684 OOD \u68c0\u6d4b\u65b9\u6cd5\u5e94\u8fd0\u800c\u751f\u3002\u4ed6\u4eec\u901a\u8fc7\u6d4b\u91cf\u4e0e ID \u7c7b\u8d28\u5fc3\u6216\u539f\u578b\u7684\u8ddd\u79bb\u6765\u8bc6\u522b\u770b\u4e0d\u89c1\u7684 OOD \u6837\u672c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8fc7\u4e8e\u7b80\u5316\u7684\u6570\u636e\u5047\u8bbe\u6765\u5b66\u4e60\u8868\u793a\uff0c\u4f8b\u5982\uff0c\u4f7f\u7528\u4e00\u4e2a\u8d28\u5fc3\u7c7b\u539f\u578b\u5bf9\u6bcf\u4e00\u7c7b\u7684 ID \u6570\u636e\u8fdb\u884c\u5efa\u6a21\uff0c\u6216\u8005\u4f7f\u7528\u5e76\u975e\u4e3a OOD \u68c0\u6d4b\u800c\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u5ffd\u7565\u4e86\u6570\u636e\u5185\u7684\u81ea\u7136\u591a\u6837\u6027\u3002\u5929\u771f\u5730\u5f3a\u5236\u6bcf\u4e00\u7c7b\u7684\u6570\u636e\u6837\u672c\u4ec5\u56f4\u7ed5\u4e00\u4e2a\u539f\u578b\u7d27\u51d1\uff0c\u5bfc\u81f4\u5bf9\u5b9e\u9645\u6570\u636e\u7684\u5efa\u6a21\u4e0d\u5145\u5206\u5e76\u4e14\u6027\u80fd\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u539f\u578b\u6df7\u5408\u5b66\u4e60\uff08PALM\uff09\uff0c\u5b83\u7528\u591a\u4e2a\u539f\u578b\u5bf9\u6bcf\u4e2a\u7c7b\u8fdb\u884c\u5efa\u6a21\u4ee5\u6355\u83b7\u6837\u672c\u591a\u6837\u6027\uff0c\u5e76\u5b66\u4e60\u66f4\u5fe0\u5b9e\u548c\u7d27\u51d1\u7684\u6837\u672c\u5d4c\u5165\u4ee5\u589e\u5f3a OOD \u68c0\u6d4b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u81ea\u52a8\u8bc6\u522b\u5e76\u52a8\u6001\u66f4\u65b0\u539f\u578b\uff0c\u901a\u8fc7\u4e92\u90bb\u8f6f\u5206\u914d\u6743\u91cd\u5c06\u6bcf\u4e2a\u6837\u672c\u5206\u914d\u7ed9\u539f\u578b\u7684\u5b50\u96c6\u3002 PALM \u4f18\u5316\u4e86\u6700\u5927\u4f3c\u7136\u4f30\u8ba1 (MLE) \u635f\u5931\uff0c\u4ee5\u9f13\u52b1\u6837\u672c\u5d4c\u5165\u5728\u76f8\u5173\u539f\u578b\u5468\u56f4\u7d27\u51d1\uff0c\u5e76\u4f18\u5316\u6240\u6709\u539f\u578b\u4e0a\u7684\u5bf9\u6bd4\u635f\u5931\uff0c\u4ee5\u589e\u5f3a\u539f\u578b\u7ea7\u522b\u7684\u7c7b\u5185\u7d27\u51d1\u6027\u548c\u7c7b\u95f4\u533a\u5206\u5ea6\u3002\u6b64\u5916\uff0c\u539f\u578b\u7684\u81ea\u52a8\u4f30\u8ba1\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6269\u5c55\u5230\u5177\u6709\u672a\u6807\u8bb0 ID \u6570\u636e\u7684\u5177\u6709\u6311\u6218\u6027\u7684 OOD \u68c0\u6d4b\u4efb\u52a1\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86 PALM \u7684\u4f18\u8d8a\u6027\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684 CIFAR-100 \u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5e73\u5747 AUROC \u6027\u80fd 93.82\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/jeff024/PALM \u83b7\u53d6\u3002|[2402.02653v1](http://arxiv.org/pdf/2402.02653v1)|null|\n", "2402.02649": "|**2024-02-05**|**Densely Decoded Networks with Adaptive Deep Supervision for Medical Image Segmentation**|\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u5177\u6709\u81ea\u9002\u5e94\u6df1\u5ea6\u76d1\u7763\u7684\u5bc6\u96c6\u89e3\u7801\u7f51\u7edc|Suraj Mishra|Medical image segmentation using deep neural networks has been highly successful. However, the effectiveness of these networks is often limited by inadequate dense prediction and inability to extract robust features. To achieve refined dense prediction, we propose densely decoded networks (ddn), by selectively introducing 'crutch' network connections. Such 'crutch' connections in each upsampling stage of the network decoder (1) enhance target localization by incorporating high resolution features from the encoder, and (2) improve segmentation by facilitating multi-stage contextual information flow. Further, we present a training strategy based on adaptive deep supervision (ads), which exploits and adapts specific attributes of input dataset, for robust feature extraction. In particular, ads strategically locates and deploys auxiliary supervision, by matching the average input object size with the layer-wise effective receptive fields (lerf) of a network, resulting in a class of ddns. Such inclusion of 'companion objective' from a specific hidden layer, helps the model pay close attention to some distinct input-dependent features, which the network might otherwise 'ignore' during training. Our new networks and training strategy are validated on 4 diverse datasets of different modalities, demonstrating their effectiveness.|\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u975e\u5e38\u6210\u529f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u7f51\u7edc\u7684\u6709\u6548\u6027\u5f80\u5f80\u53d7\u5230\u5bc6\u96c6\u9884\u6d4b\u4e0d\u8db3\u548c\u65e0\u6cd5\u63d0\u53d6\u9c81\u68d2\u7279\u5f81\u7684\u9650\u5236\u3002\u4e3a\u4e86\u5b9e\u73b0\u7cbe\u7ec6\u7684\u5bc6\u96c6\u9884\u6d4b\uff0c\u6211\u4eec\u901a\u8fc7\u9009\u62e9\u6027\u5730\u5f15\u5165\u201c\u62d0\u6756\u201d\u7f51\u7edc\u8fde\u63a5\u6765\u63d0\u51fa\u5bc6\u96c6\u89e3\u7801\u7f51\u7edc\uff08ddn\uff09\u3002\u7f51\u7edc\u89e3\u7801\u5668\u7684\u6bcf\u4e2a\u4e0a\u91c7\u6837\u9636\u6bb5\u4e2d\u7684\u8fd9\u79cd\u201c\u62d0\u6756\u201d\u8fde\u63a5\uff081\uff09\u901a\u8fc7\u5408\u5e76\u6765\u81ea\u7f16\u7801\u5668\u7684\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u6765\u589e\u5f3a\u76ee\u6807\u5b9a\u4f4d\uff0c\u4ee5\u53ca\uff082\uff09\u901a\u8fc7\u4fc3\u8fdb\u591a\u7ea7\u4e0a\u4e0b\u6587\u4fe1\u606f\u6d41\u6765\u6539\u8fdb\u5206\u5272\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u6df1\u5ea6\u76d1\u7763\uff08ads\uff09\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u548c\u9002\u5e94\u8f93\u5165\u6570\u636e\u96c6\u7684\u7279\u5b9a\u5c5e\u6027\uff0c\u4ee5\u8fdb\u884c\u7a33\u5065\u7684\u7279\u5f81\u63d0\u53d6\u3002\u7279\u522b\u662f\uff0c\u5e7f\u544a\u901a\u8fc7\u5c06\u5e73\u5747\u8f93\u5165\u5bf9\u8c61\u5927\u5c0f\u4e0e\u7f51\u7edc\u7684\u5206\u5c42\u6709\u6548\u611f\u53d7\u91ce\uff08lerf\uff09\u76f8\u5339\u914d\u6765\u6218\u7565\u6027\u5730\u5b9a\u4f4d\u548c\u90e8\u7f72\u8f85\u52a9\u76d1\u7763\uff0c\u4ece\u800c\u4ea7\u751f\u4e00\u7c7b ddns\u3002\u8fd9\u79cd\u5305\u542b\u6765\u81ea\u7279\u5b9a\u9690\u85cf\u5c42\u7684\u201c\u4f34\u968f\u76ee\u6807\u201d\u6709\u52a9\u4e8e\u6a21\u578b\u5bc6\u5207\u5173\u6ce8\u4e00\u4e9b\u4e0d\u540c\u7684\u4f9d\u8d56\u4e8e\u8f93\u5165\u7684\u7279\u5f81\uff0c\u5426\u5219\u7f51\u7edc\u53ef\u80fd\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u201c\u5ffd\u7565\u201d\u8fd9\u4e9b\u7279\u5f81\u3002\u6211\u4eec\u7684\u65b0\u7f51\u7edc\u548c\u8bad\u7ec3\u7b56\u7565\u5728 4 \u4e2a\u4e0d\u540c\u6a21\u5f0f\u7684\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002|[2402.02649v1](http://arxiv.org/pdf/2402.02649v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2402.03251": "|**2024-02-05**|**CLIP Can Understand Depth**|CLIP \u53ef\u4ee5\u7406\u89e3\u6df1\u5ea6|Dunam Kim, Seokju Lee|Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way. This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging. We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies.|\u6700\u8fd1\u5173\u4e8e\u5c06 CLIP \u63a8\u5e7f\u5230\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5728\u7f51\u7edc\u722c\u884c\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u7684 CLIP \u5bf9\u4e8e\u63a8\u5bfc\u56fe\u50cf\u5757\u548c\u6df1\u5ea6\u76f8\u5173\u63d0\u793a\u4e4b\u95f4\u7684\u9002\u5f53\u76f8\u4f3c\u6027\u6548\u7387\u5f88\u4f4e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91c7\u7528 CLIP \u6765\u5b9e\u73b0\u5177\u6709\u5bc6\u96c6\u9884\u6d4b\u7684\u6709\u610f\u4e49\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u8d28\u91cf\uff0c\u800c\u65e0\u9700\u5fae\u8c03\u5176\u539f\u59cb\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u3002\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u4e00\u4e2a\u7d27\u51d1\u7684\u53cd\u5377\u79ef\u89e3\u7801\u5668\u548c\u4e00\u4e2a\u540d\u4e3amirror\u7684\u5fae\u5c0f\u53ef\u5b66\u4e60\u5d4c\u5165\u77e9\u9635\uff0c\u4f5c\u4e3a\u5176\u6587\u672c\u7f16\u7801\u5668\u7684\u9759\u6001\u63d0\u793a\uff0cCLIP\u80fd\u591f\u7406\u89e3\u6df1\u5ea6\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728 NYU Depth v2 \u548c KITTI \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u4e0e\u4e4b\u524d\u51e0\u4e2a\u6700\u5148\u8fdb\u7684\u4ec5\u89c6\u89c9\u6a21\u578b\u76f8\u5339\u914d\uff0c\u5927\u5927\u4f18\u4e8e\u6bcf\u4e2a\u57fa\u4e8e CLIP \u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u3002\u65f6\u95f4\u6df1\u5ea6\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u8fde\u7eed\u6027\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u7ec6\u5316 CLIP \u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u5bf9\u955c\u5b50\u7684\u6d88\u878d\u7814\u7a76\u8bc1\u660e\uff0c\u5c3d\u7ba1\u6ca1\u6709\u7ed9\u51fa\u4efb\u4f55\u4ee5\u4eba\u7c7b\u65b9\u5f0f\u7f16\u5199\u7684\u63d0\u793a\uff0c\u4f46\u6240\u5f97\u6a21\u578b\u4e0d\u4ec5\u5229\u7528\u56fe\u50cf\u7f16\u7801\u5668\u7684\u77e5\u8bc6\uff0c\u800c\u4e14\u8fd8\u5229\u7528\u6587\u672c\u7f16\u7801\u5668\u7684\u77e5\u8bc6\u6765\u4f30\u8ba1\u6df1\u5ea6\u3002\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u6700\u5c0f\u7684\u8c03\u6574\uff0c\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08\u4f8b\u5982 CLIP\uff09\u7684\u5148\u9a8c\u77e5\u8bc6\u751a\u81f3\u53ef\u4ee5\u63a8\u5e7f\u5230\u9884\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u5177\u6709\u6311\u6218\u6027\u7684\u9886\u57df\u3002\u6211\u4eec\u4fc3\u8fdb\u672a\u6765\u7684\u5de5\u4f5c\u91cd\u70b9\u5173\u6ce8\u4f7f\u7528\u975e\u4eba\u7c7b\u8bed\u8a00\u63d0\u793a\u8c03\u6574\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6b21\u4f18\u5148\u9a8c\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u5b9e\u73b0\u4e0e\u7279\u5b9a\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002|[2402.03251v1](http://arxiv.org/pdf/2402.03251v1)|null|\n", "2402.02836": "|**2024-02-05**|**Perceptual Learned Image Compression via End-to-End JND-Based Optimization**|\u901a\u8fc7\u57fa\u4e8e JND \u7684\u7aef\u5230\u7aef\u4f18\u5316\u8fdb\u884c\u611f\u77e5\u5b66\u4e60\u56fe\u50cf\u538b\u7f29|Farhad Pakdaman, Sanaz Nami, Moncef Gabbouj|Emerging Learned image Compression (LC) achieves significant improvements in coding efficiency by end-to-end training of neural networks for compression. An important benefit of this approach over traditional codecs is that any optimization criteria can be directly applied to the encoder-decoder networks during training. Perceptual optimization of LC to comply with the Human Visual System (HVS) is among such criteria, which has not been fully explored yet. This paper addresses this gap by proposing a novel framework to integrate Just Noticeable Distortion (JND) principles into LC. Leveraging existing JND datasets, three perceptual optimization methods are proposed to integrate JND into the LC training process: (1) Pixel-Wise JND Loss (PWL) prioritizes pixel-by-pixel fidelity in reproducing JND characteristics, (2) Image-Wise JND Loss (IWL) emphasizes on overall imperceptible degradation levels, and (3) Feature-Wise JND Loss (FWL) aligns the reconstructed image features with perceptually significant features. Experimental evaluations demonstrate the effectiveness of JND integration, highlighting improvements in rate-distortion performance and visual quality, compared to baseline methods. The proposed methods add no extra complexity after training.|\u65b0\u5174\u7684\u5b66\u4e60\u56fe\u50cf\u538b\u7f29\uff08LC\uff09\u901a\u8fc7\u5bf9\u538b\u7f29\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u7f16\u7801\u6548\u7387\u3002\u4e0e\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u76f8\u6bd4\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7684\u4e00\u4e2a\u91cd\u8981\u597d\u5904\u662f\uff0c\u4efb\u4f55\u4f18\u5316\u6807\u51c6\u90fd\u53ef\u4ee5\u5728\u8bad\u7ec3\u671f\u95f4\u76f4\u63a5\u5e94\u7528\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u3002 LC \u7684\u611f\u77e5\u4f18\u5316\u4ee5\u7b26\u5408\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf (HVS) \u662f\u6b64\u7c7b\u6807\u51c6\u4e4b\u4e00\uff0c\u4f46\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u5c06\u53ef\u5bdf\u89c9\u5931\u771f (JND) \u539f\u7406\u96c6\u6210\u5230 LC \u4e2d\u7684\u65b0\u9896\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002\u5229\u7528\u73b0\u6709\u7684 JND \u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u611f\u77e5\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06 JND \u96c6\u6210\u5230 LC \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff1a\uff081\uff09\u9010\u50cf\u7d20 JND \u635f\u5931\uff08PWL\uff09\u5728\u518d\u73b0 JND \u7279\u5f81\u65f6\u4f18\u5148\u8003\u8651\u9010\u50cf\u7d20\u4fdd\u771f\u5ea6\uff0c\uff082\uff09\u9010\u56fe\u50cf JND\u635f\u5931\uff08IWL\uff09\u5f3a\u8c03\u6574\u4f53\u4e0d\u53ef\u5bdf\u89c9\u7684\u9000\u5316\u6c34\u5e73\uff0c\uff083\uff09\u7279\u5f81\u7ea7JND\u635f\u5931\uff08FWL\uff09\u5c06\u91cd\u5efa\u7684\u56fe\u50cf\u7279\u5f81\u4e0e\u611f\u77e5\u663e\u7740\u7279\u5f81\u5bf9\u9f50\u3002\u5b9e\u9a8c\u8bc4\u4f30\u8bc1\u660e\u4e86 JND \u96c6\u6210\u7684\u6709\u6548\u6027\uff0c\u7a81\u51fa\u4e86\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u5728\u7387\u5931\u771f\u6027\u80fd\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u7684\u6539\u8fdb\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u540e\u4e0d\u4f1a\u589e\u52a0\u989d\u5916\u7684\u590d\u6742\u6027\u3002|[2402.02836v1](http://arxiv.org/pdf/2402.02836v1)|null|\n"}, "LLM": {}, "Transformer": {"2402.03286": "|**2024-02-05**|**Training-Free Consistent Text-to-Image Generation**|\u514d\u8bad\u7ec3\u4e00\u81f4\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210|Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, Yuval Atzmon|Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.|\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u65b0\u6c34\u5e73\u7684\u521b\u610f\u7075\u6d3b\u6027\u3002\u7136\u800c\uff0c\u4f7f\u7528\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0d\u540c\u7684\u63d0\u793a\u4e0b\u4e00\u81f4\u5730\u63cf\u7ed8\u540c\u4e00\u4e3b\u9898\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6559\u5176\u63cf\u8ff0\u7279\u5b9a\u7528\u6237\u63d0\u4f9b\u7684\u4e3b\u9898\u7684\u65b0\u5355\u8bcd\u6216\u5411\u6a21\u578b\u6dfb\u52a0\u56fe\u50cf\u8c03\u8282\u3002\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u5bf9\u8c61\u8fdb\u884c\u957f\u65f6\u95f4\u7684\u4f18\u5316\u6216\u5927\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u5f88\u96be\u5c06\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u6587\u672c\u63d0\u793a\u5bf9\u9f50\uff0c\u5e76\u4e14\u5728\u63cf\u7ed8\u591a\u4e2a\u4e3b\u9898\u65f6\u9762\u4e34\u56f0\u96be\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ConsiStory\uff0c\u8fd9\u662f\u4e00\u79cd\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\u6765\u5b9e\u73b0\u4e00\u81f4\u7684\u4e3b\u9898\u751f\u6210\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e3b\u9898\u9a71\u52a8\u7684\u5171\u4eab\u6ce8\u610f\u529b\u5757\u548c\u57fa\u4e8e\u5bf9\u5e94\u7684\u7279\u5f81\u6ce8\u5165\uff0c\u4ee5\u4fc3\u8fdb\u56fe\u50cf\u4e4b\u95f4\u7684\u4e3b\u9898\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5236\u5b9a\u7b56\u7565\u6765\u9f13\u52b1\u5e03\u5c40\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e3b\u9898\u4e00\u81f4\u6027\u3002\u6211\u4eec\u5c06 ConsiStory \u4e0e\u4e00\u7cfb\u5217\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u4f18\u5316\u6b65\u9aa4\u3002\u6700\u540e\uff0cConsiStory \u53ef\u4ee5\u81ea\u7136\u5730\u6269\u5c55\u5230\u591a\u4e3b\u4f53\u573a\u666f\uff0c\u751a\u81f3\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u5e38\u89c1\u5bf9\u8c61\u7684\u514d\u8bad\u7ec3\u4e2a\u6027\u5316\u3002|[2402.03286v1](http://arxiv.org/pdf/2402.03286v1)|null|\n", "2402.02956": "|**2024-02-05**|**AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image**|AdaTreeFormer\uff1a\u4ece\u5355\u4e2a\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u8fdb\u884c\u6811\u6728\u8ba1\u6570\u7684\u5c11\u91cf\u955c\u5934\u57df\u9002\u5e94|Hamed Amini Amirkolaee, Miaojing Shi, Lianghua He, Mark Mulligan|The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different domains while generating tree density maps; a hierarchical cross-domain feature alignment scheme is proposed that progressively aligns the features from the source and target domains. We also adopt adversarial learning into the framework to further reduce the gap between source and target domains. Our AdaTreeFormer is evaluated on six designed domain adaptation tasks using three tree counting datasets, ie Jiangsu, Yosemite, and London; and outperforms the state of the art methods significantly.|\u5728\u6444\u5f71\u6d4b\u91cf\u548c\u9065\u611f\u9886\u57df\uff0c\u4ec5\u4f7f\u7528\u5355\u4e2a\u822a\u7a7a\u6216\u536b\u661f\u56fe\u50cf\u6765\u4f30\u8ba1\u548c\u8ba1\u7b97\u6811\u6728\u5bc6\u5ea6\u7684\u8fc7\u7a0b\u662f\u4e00\u9879\u8270\u5de8\u7684\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5b83\u5728\u68ee\u6797\u7ba1\u7406\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5730\u5f62\u5404\u5f02\u7684\u6811\u6728\u79cd\u7c7b\u7e41\u591a\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u6811\u6728\u8ba1\u6570\u6a21\u578b\u7684\u826f\u597d\u8868\u73b0\u3002\u672c\u6587\u7684\u76ee\u7684\u662f\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4ece\u5177\u6709\u8db3\u591f\u6807\u8bb0\u6811\u7684\u6e90\u57df\u4e2d\u5b66\u4e60\uff0c\u5e76\u9002\u5e94\u4ec5\u5177\u6709\u6709\u9650\u6570\u91cf\u6807\u8bb0\u6811\u7684\u76ee\u6807\u57df\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3a AdaTreeFormer\uff0c\u5305\u542b\u4e00\u4e2a\u5177\u6709\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u65b9\u6848\u7684\u5171\u4eab\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u4ece\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e2d\u63d0\u53d6\u7a33\u5065\u7684\u7279\u5f81\u3002\u5b83\u8fd8\u7531\u4e09\u4e2a\u5b50\u7f51\u7ec4\u6210\uff1a\u4e24\u4e2a\u5b50\u7f51\u5206\u522b\u7528\u4e8e\u4ece\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e2d\u63d0\u53d6\u81ea\u57df\u6ce8\u610f\u529b\u56fe\uff0c\u4e00\u4e2a\u7528\u4e8e\u63d0\u53d6\u8de8\u57df\u6ce8\u610f\u529b\u56fe\u3002\u5bf9\u4e8e\u540e\u8005\uff0c\u5f15\u5165\u6ce8\u610f\u9002\u5e94\u673a\u5236\uff0c\u5728\u751f\u6210\u6811\u5bc6\u5ea6\u56fe\u7684\u540c\u65f6\u4ece\u4e0d\u540c\u9886\u57df\u63d0\u53d6\u76f8\u5173\u4fe1\u606f\uff1b\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u65b9\u6848\uff0c\u9010\u6b65\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u7279\u5f81\u3002\u6211\u4eec\u8fd8\u5728\u6846\u67b6\u4e2d\u91c7\u7528\u5bf9\u6297\u6027\u5b66\u4e60\uff0c\u4ee5\u8fdb\u4e00\u6b65\u7f29\u5c0f\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u6211\u4eec\u7684 AdaTreeFormer \u4f7f\u7528\u4e09\u4e2a\u6811\u6728\u8ba1\u6570\u6570\u636e\u96c6\uff08\u5373\u6c5f\u82cf\u3001\u4f18\u80dc\u7f8e\u5730\u548c\u4f26\u6566\uff09\u5bf9\u516d\u4e2a\u8bbe\u8ba1\u7684\u57df\u9002\u5e94\u4efb\u52a1\u8fdb\u884c\u4e86\u8bc4\u4f30\uff1b\u5e76\u4e14\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2402.02956v1](http://arxiv.org/pdf/2402.02956v1)|null|\n", "2402.02941": "|**2024-02-05**|**Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey**|\u63a2\u7d22\u8ba1\u7b97\u673a\u89c6\u89c9\u6df7\u5408 CNN \u548c ViT \u67b6\u6784\u7684\u534f\u540c\u4f5c\u7528\uff1a\u4e00\u9879\u8c03\u67e5|Haruna Yunusa, Shiyin Qin, Abdulrahman Hamman Adama Chukkol, Abdulganiyu Abdu Yusuf, Isah Bello, Adamu Lawan|The hybrid of Convolutional Neural Network (CNN) and Vision Transformers (ViT) architectures has emerged as a groundbreaking approach, pushing the boundaries of computer vision (CV). This comprehensive review provides a thorough examination of the literature on state-of-the-art hybrid CNN-ViT architectures, exploring the synergies between these two approaches. The main content of this survey includes: (1) a background on the vanilla CNN and ViT, (2) systematic review of various taxonomic hybrid designs to explore the synergy achieved through merging CNNs and ViTs models, (3) comparative analysis and application task-specific synergy between different hybrid architectures, (4) challenges and future directions for hybrid models, (5) lastly, the survey concludes with a summary of key findings and recommendations. Through this exploration of hybrid CV architectures, the survey aims to serve as a guiding resource, fostering a deeper understanding of the intricate dynamics between CNNs and ViTs and their collective impact on shaping the future of CV architectures.|\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u548c\u89c6\u89c9\u53d8\u6362\u5668 (ViT) \u67b6\u6784\u7684\u6df7\u5408\u5df2\u7ecf\u6210\u4e3a\u4e00\u79cd\u7a81\u7834\u6027\u7684\u65b9\u6cd5\uff0c\u7a81\u7834\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9 (CV) \u7684\u754c\u9650\u3002\u8fd9\u7bc7\u5168\u9762\u7684\u7efc\u8ff0\u5bf9\u6700\u5148\u8fdb\u7684\u6df7\u5408 CNN-ViT \u67b6\u6784\u7684\u6587\u732e\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5ba1\u67e5\uff0c\u63a2\u7d22\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002\u672c\u6b21\u8c03\u67e5\u7684\u4e3b\u8981\u5185\u5bb9\u5305\u62ec\uff1a\uff081\uff09\u666e\u901aCNN\u548cViT\u7684\u80cc\u666f\uff0c\uff082\uff09\u7cfb\u7edf\u56de\u987e\u5404\u79cd\u5206\u7c7b\u6df7\u5408\u8bbe\u8ba1\uff0c\u63a2\u7d22\u901a\u8fc7\u5408\u5e76CNN\u548cViT\u6a21\u578b\u6240\u5b9e\u73b0\u7684\u534f\u540c\u4f5c\u7528\uff0c\uff083\uff09\u6bd4\u8f83\u5206\u6790\u548c\u5e94\u7528\u4efb\u52a1-\u4e0d\u540c\u6df7\u5408\u67b6\u6784\u4e4b\u95f4\u7684\u5177\u4f53\u534f\u540c\u4f5c\u7528\uff0c(4)\u6df7\u5408\u6a21\u578b\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c(5)\u6700\u540e\uff0c\u8c03\u67e5\u603b\u7ed3\u4e86\u4e3b\u8981\u53d1\u73b0\u548c\u5efa\u8bae\u3002\u901a\u8fc7\u5bf9\u6df7\u5408 CV \u67b6\u6784\u7684\u63a2\u7d22\uff0c\u8be5\u8c03\u67e5\u65e8\u5728\u4f5c\u4e3a\u6307\u5bfc\u8d44\u6e90\uff0c\u4fc3\u8fdb\u4eba\u4eec\u66f4\u6df1\u5165\u5730\u4e86\u89e3 CNN \u548c ViT \u4e4b\u95f4\u7684\u590d\u6742\u52a8\u6001\u53ca\u5176\u5bf9\u5851\u9020 CV \u67b6\u6784\u672a\u6765\u7684\u96c6\u4f53\u5f71\u54cd\u3002|[2402.02941v1](http://arxiv.org/pdf/2402.02941v1)|null|\n"}, "3D/CG": {"2402.03135": "|**2024-02-05**|**GPU-Accelerated 3D Polygon Visibility Volumes for Synergistic Perception and Navigation**|GPU \u52a0\u901f\u7684 3D \u591a\u8fb9\u5f62\u53ef\u89c1\u4f53\u79ef\u53ef\u5b9e\u73b0\u534f\u540c\u611f\u77e5\u548c\u5bfc\u822a|Andrew Willis, Collin Hague, Artur Wolek, Kevin Brink|UAV missions often require specific geometric constraints to be satisfied between ground locations and the vehicle location. Such requirements are typical for contexts where line-of-sight must be maintained between the vehicle location and the ground control location and are also important in surveillance applications where the UAV wishes to be able to sense, e.g., with a camera sensor, a specific region within a complex geometric environment. This problem is further complicated when the ground location is generalized to a convex 2D polygonal region. This article describes the theory and implementation of a system which can quickly calculate the 3D volume that encloses all 3D coordinates from which a 2D convex planar region can be entirely viewed; referred to as a visibility volume. The proposed approach computes visibility volumes using a combination of depth map computation using GPU-acceleration and geometric boolean operations. Solutions to this problem require complex 3D geometric analysis techniques that must execute using arbitrary precision arithmetic on a collection of discontinuous and non-analytic surfaces. Post-processing steps incorporate navigational constraints to further restrict the enclosed coordinates to include both visibility and navigation constraints. Integration of sensing visibility constraints with navigational constraints yields a range of navigable space where a vehicle will satisfy both perceptual sensing and navigational needs of the mission. This algorithm then provides a synergistic perception and navigation sensitive solution yielding a volume of coordinates in 3D that satisfy both the mission path and sensing needs.|\u65e0\u4eba\u673a\u4efb\u52a1\u901a\u5e38\u9700\u8981\u5730\u9762\u4f4d\u7f6e\u548c\u8f66\u8f86\u4f4d\u7f6e\u4e4b\u95f4\u6ee1\u8db3\u7279\u5b9a\u7684\u51e0\u4f55\u7ea6\u675f\u3002\u6b64\u7c7b\u8981\u6c42\u5bf9\u4e8e\u5fc5\u987b\u5728\u8f66\u8f86\u4f4d\u7f6e\u548c\u5730\u9762\u63a7\u5236\u4f4d\u7f6e\u4e4b\u95f4\u4fdd\u6301\u89c6\u7ebf\u7684\u73af\u5883\u6765\u8bf4\u662f\u5178\u578b\u7684\uff0c\u5e76\u4e14\u5728\u65e0\u4eba\u673a\u5e0c\u671b\u80fd\u591f\uff08\u4f8b\u5982\u4f7f\u7528\u6444\u50cf\u5934\u4f20\u611f\u5668\uff09\u611f\u77e5\u7279\u5b9a\u7684\u76d1\u89c6\u5e94\u7528\u4e2d\u4e5f\u5f88\u91cd\u8981\u3002\u590d\u6742\u51e0\u4f55\u73af\u5883\u4e2d\u7684\u533a\u57df\u3002\u5f53\u5730\u9762\u4f4d\u7f6e\u63a8\u5e7f\u5230\u51f8\u4e8c\u7ef4\u591a\u8fb9\u5f62\u533a\u57df\u65f6\uff0c\u8fd9\u4e2a\u95ee\u9898\u4f1a\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u3002\u672c\u6587\u63cf\u8ff0\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7684\u7406\u8bba\u548c\u5b9e\u73b0\uff0c\u8be5\u7cfb\u7edf\u53ef\u4ee5\u5feb\u901f\u8ba1\u7b97\u5305\u542b\u6240\u6709 3D \u5750\u6807\u7684 3D \u4f53\u79ef\uff0c\u4ece\u4e2d\u53ef\u4ee5\u5b8c\u6574\u5730\u67e5\u770b 2D \u51f8\u5e73\u9762\u533a\u57df\uff1b\u79f0\u4e3a\u53ef\u89c1\u4f53\u79ef\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528 GPU \u52a0\u901f\u548c\u51e0\u4f55\u5e03\u5c14\u8fd0\u7b97\u7684\u6df1\u5ea6\u56fe\u8ba1\u7b97\u6765\u8ba1\u7b97\u53ef\u89c1\u6027\u4f53\u79ef\u3002\u6b64\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u9700\u8981\u590d\u6742\u7684 3D \u51e0\u4f55\u5206\u6790\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u5fc5\u987b\u5728\u4e00\u7ec4\u4e0d\u8fde\u7eed\u548c\u975e\u5206\u6790\u66f2\u9762\u4e0a\u4f7f\u7528\u4efb\u610f\u7cbe\u5ea6\u7b97\u672f\u6765\u6267\u884c\u3002\u540e\u5904\u7406\u6b65\u9aa4\u5408\u5e76\u5bfc\u822a\u7ea6\u675f\uff0c\u4ee5\u8fdb\u4e00\u6b65\u9650\u5236\u5c01\u95ed\u7684\u5750\u6807\u4ee5\u5305\u62ec\u53ef\u89c1\u6027\u548c\u5bfc\u822a\u7ea6\u675f\u3002\u4f20\u611f\u53ef\u89c1\u5ea6\u7ea6\u675f\u4e0e\u5bfc\u822a\u7ea6\u675f\u7684\u96c6\u6210\u4ea7\u751f\u4e86\u4e00\u7cfb\u5217\u53ef\u5bfc\u822a\u7a7a\u95f4\uff0c\u8f66\u8f86\u5c06\u5728\u5176\u4e2d\u6ee1\u8db3\u4efb\u52a1\u7684\u611f\u77e5\u4f20\u611f\u548c\u5bfc\u822a\u9700\u6c42\u3002\u7136\u540e\uff0c\u8be5\u7b97\u6cd5\u63d0\u4f9b\u534f\u540c\u611f\u77e5\u548c\u5bfc\u822a\u654f\u611f\u89e3\u51b3\u65b9\u6848\uff0c\u4ea7\u751f\u5927\u91cf 3D \u5750\u6807\uff0c\u6ee1\u8db3\u4efb\u52a1\u8def\u5f84\u548c\u4f20\u611f\u9700\u6c42\u3002|[2402.03135v1](http://arxiv.org/pdf/2402.03135v1)|null|\n", "2402.03093": "|**2024-02-05**|**AI-Enhanced Virtual Reality in Medicine: A Comprehensive Survey**|\u4eba\u5de5\u667a\u80fd\u589e\u5f3a\u865a\u62df\u73b0\u5b9e\u5728\u533b\u5b66\u4e2d\u7684\u5e94\u7528\uff1a\u7efc\u5408\u8c03\u67e5|Yixuan Wu, Kaiyuan Hu, Danny Z. Chen, Jian Wu|With the rapid advance of computer graphics and artificial intelligence technologies, the ways we interact with the world have undergone a transformative shift. Virtual Reality (VR) technology, aided by artificial intelligence (AI), has emerged as a dominant interaction media in multiple application areas, thanks to its advantage of providing users with immersive experiences. Among those applications, medicine is considered one of the most promising areas. In this paper, we present a comprehensive examination of the burgeoning field of AI-enhanced VR applications in medical care and services. By introducing a systematic taxonomy, we meticulously classify the pertinent techniques and applications into three well-defined categories based on different phases of medical diagnosis and treatment: Visualization Enhancement, VR-related Medical Data Processing, and VR-assisted Intervention. This categorization enables a structured exploration of the diverse roles that AI-powered VR plays in the medical domain, providing a framework for a more comprehensive understanding and evaluation of these technologies. To our best knowledge, this is the first systematic survey of AI-powered VR systems in medical settings, laying a foundation for future research in this interdisciplinary domain.|\u968f\u7740\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6211\u4eec\u4e0e\u4e16\u754c\u4e92\u52a8\u7684\u65b9\u5f0f\u53d1\u751f\u4e86\u53d8\u9769\u3002\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u6280\u672f\u5728\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u8f85\u52a9\u4e0b\uff0c\u51ed\u501f\u5176\u4e3a\u7528\u6237\u63d0\u4f9b\u6c89\u6d78\u5f0f\u4f53\u9a8c\u7684\u4f18\u52bf\uff0c\u5df2\u6210\u4e3a\u591a\u4e2a\u5e94\u7528\u9886\u57df\u7684\u4e3b\u5bfc\u4ea4\u4e92\u5a92\u4f53\u3002\u5728\u8fd9\u4e9b\u5e94\u7528\u4e2d\uff0c\u533b\u5b66\u88ab\u8ba4\u4e3a\u662f\u6700\u6709\u524d\u9014\u7684\u9886\u57df\u4e4b\u4e00\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u533b\u7597\u4fdd\u5065\u548c\u670d\u52a1\u4e2d\u4eba\u5de5\u667a\u80fd\u589e\u5f3a\u578b VR \u5e94\u7528\u7684\u65b0\u5174\u9886\u57df\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\u3002\u901a\u8fc7\u5f15\u5165\u7cfb\u7edf\u7684\u5206\u7c7b\u6cd5\uff0c\u6211\u4eec\u6839\u636e\u533b\u7597\u8bca\u65ad\u548c\u6cbb\u7597\u7684\u4e0d\u540c\u9636\u6bb5\uff0c\u5c06\u76f8\u5173\u6280\u672f\u548c\u5e94\u7528\u7ec6\u81f4\u5730\u5206\u4e3a\u4e09\u4e2a\u660e\u786e\u7684\u7c7b\u522b\uff1a\u53ef\u89c6\u5316\u589e\u5f3a\u3001VR\u76f8\u5173\u7684\u533b\u7597\u6570\u636e\u5904\u7406\u548cVR\u8f85\u52a9\u5e72\u9884\u3002\u8fd9\u79cd\u5206\u7c7b\u53ef\u4ee5\u5bf9\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u865a\u62df\u73b0\u5b9e\u5728\u533b\u7597\u9886\u57df\u53d1\u6325\u7684\u4e0d\u540c\u4f5c\u7528\u8fdb\u884c\u7ed3\u6784\u5316\u63a2\u7d22\uff0c\u4e3a\u66f4\u5168\u9762\u5730\u7406\u89e3\u548c\u8bc4\u4f30\u8fd9\u4e9b\u6280\u672f\u63d0\u4f9b\u4e00\u4e2a\u6846\u67b6\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u5bf9\u533b\u7597\u73af\u5883\u4e2d\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684 VR \u7cfb\u7edf\u7684\u9996\u6b21\u7cfb\u7edf\u8c03\u67e5\uff0c\u4e3a\u8fd9\u4e00\u8de8\u5b66\u79d1\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002|[2402.03093v1](http://arxiv.org/pdf/2402.03093v1)|null|\n", "2402.02892": "|**2024-02-05**|**Motion-Aware Video Frame Interpolation**|\u8fd0\u52a8\u611f\u77e5\u89c6\u9891\u5e27\u63d2\u503c|Pengfei Han, Fuhua Zhang, Bin Zhao, Xuelong Li|Video frame interpolation methodologies endeavor to create novel frames betwixt extant ones, with the intent of augmenting the video's frame frequency. However, current methods are prone to image blurring and spurious artifacts in challenging scenarios involving occlusions and discontinuous motion. Moreover, they typically rely on optical flow estimation, which adds complexity to modeling and computational costs. To address these issues, we introduce a Motion-Aware Video Frame Interpolation (MA-VFI) network, which directly estimates intermediate optical flow from consecutive frames by introducing a novel hierarchical pyramid module. It not only extracts global semantic relationships and spatial details from input frames with different receptive fields, enabling the model to capture intricate motion patterns, but also effectively reduces the required computational cost and complexity. Subsequently, a cross-scale motion structure is presented to estimate and refine intermediate flow maps by the extracted features. This approach facilitates the interplay between input frame features and flow maps during the frame interpolation process and markedly heightens the precision of the intervening flow delineations. Finally, a discerningly fashioned loss centered around an intermediate flow is meticulously contrived, serving as a deft rudder to skillfully guide the prognostication of said intermediate flow, thereby substantially refining the precision of the intervening flow mappings. Experiments illustrate that MA-VFI surpasses several representative VFI methods across various datasets, and can enhance efficiency while maintaining commendable efficacy.|\u89c6\u9891\u5e27\u63d2\u503c\u65b9\u6cd5\u81f4\u529b\u4e8e\u5728\u73b0\u6709\u5e27\u4e4b\u95f4\u521b\u5efa\u65b0\u9896\u7684\u5e27\uff0c\u76ee\u7684\u662f\u589e\u5f3a\u89c6\u9891\u7684\u5e27\u9891\u7387\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u5728\u6d89\u53ca\u906e\u6321\u548c\u4e0d\u8fde\u7eed\u8fd0\u52a8\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u5bb9\u6613\u51fa\u73b0\u56fe\u50cf\u6a21\u7cca\u548c\u865a\u5047\u4f2a\u5f71\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u901a\u5e38\u4f9d\u8d56\u4e8e\u5149\u6d41\u4f30\u8ba1\uff0c\u8fd9\u589e\u52a0\u4e86\u5efa\u6a21\u7684\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6210\u672c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8fd0\u52a8\u611f\u77e5\u89c6\u9891\u5e27\u63d2\u503c\uff08MA-VFI\uff09\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u5206\u5c42\u91d1\u5b57\u5854\u6a21\u5757\u76f4\u63a5\u4f30\u8ba1\u8fde\u7eed\u5e27\u7684\u4e2d\u95f4\u5149\u6d41\u3002\u5b83\u4e0d\u4ec5\u4ece\u5177\u6709\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u8f93\u5165\u5e27\u4e2d\u63d0\u53d6\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\u548c\u7a7a\u95f4\u7ec6\u8282\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u83b7\u590d\u6742\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u800c\u4e14\u8fd8\u6709\u6548\u964d\u4f4e\u4e86\u6240\u9700\u7684\u8ba1\u7b97\u6210\u672c\u548c\u590d\u6742\u6027\u3002\u968f\u540e\uff0c\u63d0\u51fa\u4e86\u8de8\u5c3a\u5ea6\u8fd0\u52a8\u7ed3\u6784\uff0c\u4ee5\u901a\u8fc7\u63d0\u53d6\u7684\u7279\u5f81\u6765\u4f30\u8ba1\u548c\u7ec6\u5316\u4e2d\u95f4\u6d41\u56fe\u3002\u8fd9\u79cd\u65b9\u6cd5\u4fc3\u8fdb\u4e86\u5e27\u63d2\u503c\u8fc7\u7a0b\u4e2d\u8f93\u5165\u5e27\u7279\u5f81\u548c\u6d41\u56fe\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u663e\u7740\u63d0\u9ad8\u4e86\u4ecb\u5165\u6d41\u63cf\u7ed8\u7684\u7cbe\u5ea6\u3002\u6700\u540e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4ee5\u4e2d\u95f4\u6d41\u4e3a\u4e2d\u5fc3\u7684\u72ec\u7279\u635f\u5931\uff0c\u4f5c\u4e3a\u7075\u5de7\u7684\u8235\u6765\u5de7\u5999\u5730\u6307\u5bfc\u6240\u8ff0\u4e2d\u95f4\u6d41\u7684\u9884\u6d4b\uff0c\u4ece\u800c\u5927\u5927\u63d0\u9ad8\u4e86\u4e2d\u95f4\u6d41\u6620\u5c04\u7684\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0cMA-VFI \u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u51e0\u79cd\u4ee3\u8868\u6027\u7684 VFI \u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u4fdd\u6301\u4ee4\u4eba\u79f0\u8d5e\u7684\u529f\u6548\u7684\u540c\u65f6\u63d0\u9ad8\u6548\u7387\u3002|[2402.02892v1](http://arxiv.org/pdf/2402.02892v1)|null|\n", "2402.02733": "|**2024-02-05**|**ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer**|ToonAging\uff1a\u827a\u672f\u8096\u50cf\u98ce\u683c\u8fc1\u79fb\u4e0b\u7684\u9762\u90e8\u518d\u8001\u5316|Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo|Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attributes and NPR appearance. Adopting an exemplar-based approach, our method offers greater flexibility than domain-level fine-tuning approaches, which typically require separate training or fine-tuning for each domain. This effectively addresses the limitation of requiring paired datasets for re-aging and domain-level, data-driven approaches for stylization. Our experiments show that our model can effortlessly generate re-aged images while simultaneously transferring the style of examples, maintaining both natural appearance and controllability.|\u4eba\u8138\u518d\u8001\u5316\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u5f62\u9886\u57df\u7684\u4e00\u4e2a\u7a81\u51fa\u9886\u57df\uff0c\u5728\u7535\u5f71\u3001\u5e7f\u544a\u548c\u76f4\u64ad\u7b49\u771f\u5b9e\u611f\u9886\u57df\u6709\u7740\u91cd\u8981\u7684\u5e94\u7528\u3002\u6700\u8fd1\uff0c\u5c06\u9762\u90e8\u91cd\u65b0\u8001\u5316\u5e94\u7528\u4e8e\u6f2b\u753b\u3001\u63d2\u56fe\u548c\u52a8\u753b\u7b49\u975e\u771f\u5b9e\u611f\u56fe\u50cf\u7684\u9700\u6c42\u5df2\u6210\u4e3a\u5404\u4e2a\u5a31\u4e50\u9886\u57df\u7684\u5ef6\u4f38\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u80fd\u591f\u65e0\u7f1d\u7f16\u8f91 NPR \u56fe\u50cf\u4e0a\u7684\u8868\u89c2\u5e74\u9f84\u7684\u7f51\u7edc\uff0c\u8fd9\u610f\u5473\u7740\u8fd9\u4e9b\u4efb\u52a1\u4ec5\u9650\u4e8e\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u5373\u6309\u987a\u5e8f\u5e94\u7528\u6bcf\u4e2a\u4efb\u52a1\u3002\u7531\u4e8e\u57df\u5dee\u5f02\uff0c\u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u4ee4\u4eba\u4e0d\u5feb\u7684\u4f2a\u5f71\u548c\u9762\u90e8\u5c5e\u6027\u4e22\u5931\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5355\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u9762\u90e8\u91cd\u65b0\u8001\u5316\u4e0e\u8096\u50cf\u98ce\u683c\u8fc1\u79fb\u76f8\u7ed3\u5408\uff0c\u5e76\u5728\u5355\u4e2a\u751f\u6210\u6b65\u9aa4\u4e2d\u6267\u884c\u3002\u6211\u4eec\u5229\u7528\u73b0\u6709\u7684\u9762\u90e8\u518d\u8001\u5316\u548c\u98ce\u683c\u8f6c\u79fb\u7f51\u7edc\uff0c\u4e24\u8005\u90fd\u5728\u540c\u4e00\u516c\u5173\u9886\u57df\u5185\u8fdb\u884c\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u72ec\u7279\u5730\u878d\u5408\u4e86\u4e0d\u540c\u7684\u6f5c\u5728\u5411\u91cf\uff0c\u6bcf\u4e2a\u6f5c\u5728\u5411\u91cf\u8d1f\u8d23\u7ba1\u7406\u4e0e\u8870\u8001\u76f8\u5173\u7684\u5c5e\u6027\u548c NPR \u5916\u89c2\u3002\u91c7\u7528\u57fa\u4e8e\u793a\u4f8b\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u57df\u7ea7\u5fae\u8c03\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u57df\u7ea7\u5fae\u8c03\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u57df\u8fdb\u884c\u5355\u72ec\u7684\u8bad\u7ec3\u6216\u5fae\u8c03\u3002\u8fd9\u6709\u6548\u5730\u89e3\u51b3\u4e86\u9700\u8981\u914d\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u91cd\u65b0\u8001\u5316\u548c\u57df\u7ea7\u6570\u636e\u9a71\u52a8\u7684\u98ce\u683c\u5316\u65b9\u6cd5\u7684\u9650\u5236\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u6beb\u4e0d\u8d39\u529b\u5730\u751f\u6210\u91cd\u65b0\u8001\u5316\u7684\u56fe\u50cf\uff0c\u540c\u65f6\u8f6c\u79fb\u793a\u4f8b\u7684\u98ce\u683c\uff0c\u4fdd\u6301\u81ea\u7136\u7684\u5916\u89c2\u548c\u53ef\u63a7\u6027\u3002|[2402.02733v1](http://arxiv.org/pdf/2402.02733v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.02705": "|**2024-02-05**|**Representation Surgery for Multi-Task Model Merging**|\u591a\u4efb\u52a1\u6a21\u578b\u5408\u5e76\u7684\u8868\u793a\u624b\u672f|Enneng Yang, Li Shen, Zhenyi Wang, Guibing Guo, Xiaojun Chen, Xingwei Wang, Dacheng Tao|Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called \"Surgery\" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation from the merged model. We then designed an unsupervised optimization objective that updates the Surgery module by minimizing the distance between the merged model's representation and the individual model's representation. Extensive experiments demonstrate significant MTL performance improvements when our Surgery module is applied to state-of-the-art (SOTA) model merging schemes.|\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u5c06\u591a\u4e2a\u4efb\u52a1\u7684\u4fe1\u606f\u538b\u7f29\u5230\u7edf\u4e00\u7684\u4e3b\u5e72\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6700\u8fd1\u7684\u5de5\u4f5c\u76f4\u63a5\u5408\u5e76\u591a\u4e2a\u72ec\u7acb\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u6267\u884cMTL\uff0c\u800c\u4e0d\u662f\u6536\u96c6\u5b83\u4eec\u7684\u539f\u59cb\u6570\u636e\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0c\u6781\u5927\u5730\u6269\u5c55\u4e86MTL\u7684\u5e94\u7528\u573a\u666f\u3002\u7136\u800c\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6848\u7684\u8868\u793a\u5206\u5e03\uff0c\u6211\u4eec\u53d1\u73b0\u5408\u5e76\u540e\u7684\u6a21\u578b\u7ecf\u5e38\u906d\u53d7\u8868\u793a\u504f\u5dee\u7684\u56f0\u5883\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u5408\u5e76\u6a21\u578b\u548c\u5355\u72ec\u6a21\u578b\u4e4b\u95f4\u7684\u8868\u793a\u5206\u5e03\u5b58\u5728\u663e\u7740\u5dee\u5f02\uff0c\u5bfc\u81f4\u5408\u5e76 MTL \u7684\u6027\u80fd\u8f83\u5dee\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201cSurgery\u201d\u7684\u8868\u5f81\u624b\u672f\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u51cf\u5c11\u5408\u5e76\u6a21\u578b\u4e2d\u7684\u8868\u5f81\u504f\u5dee\u3002\u5177\u4f53\u6765\u8bf4\uff0cSurgery \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u6a21\u5757\uff0c\u5b83\u5c06\u5408\u5e76\u6a21\u578b\u7684\u8868\u793a\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u5c1d\u8bd5\u8f93\u51fa\u5408\u5e76\u6a21\u578b\u7684\u8868\u793a\u4e2d\u5305\u542b\u7684\u504f\u5dee\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65e0\u76d1\u7763\u4f18\u5316\u76ee\u6807\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5408\u5e76\u6a21\u578b\u8868\u793a\u4e0e\u5355\u4e2a\u6a21\u578b\u8868\u793a\u4e4b\u95f4\u7684\u8ddd\u79bb\u6765\u66f4\u65b0\u624b\u672f\u6a21\u5757\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u6211\u4eec\u7684\u5916\u79d1\u624b\u672f\u6a21\u5757\u5e94\u7528\u4e8e\u6700\u5148\u8fdb\u7684 (SOTA) \u6a21\u578b\u5408\u5e76\u65b9\u6848\u65f6\uff0cMTL \u6027\u80fd\u5f97\u5230\u663e\u7740\u63d0\u9ad8\u3002|[2402.02705v1](http://arxiv.org/pdf/2402.02705v1)|**[link](https://github.com/ennengyang/representationsurgery)**|\n"}, "\u5176\u4ed6": {"2402.03312": "|**2024-02-05**|**Test-Time Adaptation for Depth Completion**|\u6df1\u5ea6\u5b8c\u6210\u7684\u6d4b\u8bd5\u65f6\u95f4\u8c03\u6574|Hyoungseob Park, Anjali Gupta, Alex Wong|It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain. We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%.|\u5f53\u5c06\u5728\u67d0\u4e9b\uff08\u6e90\uff09\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8f6c\u79fb\u5230\u76ee\u6807\u6d4b\u8bd5\u6570\u636e\u65f6\uff0c\u7531\u4e8e\u5b83\u4eec\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\uff0c\u901a\u5e38\u4f1a\u89c2\u5bdf\u5230\u6027\u80fd\u4e0b\u964d\u3002\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u4f8b\u5982\u57df\u9002\u5e94\uff08DA\uff09\uff0c\u53ef\u80fd\u9700\u8981\u8bad\u7ec3\u6a21\u578b\u6240\u4f9d\u636e\u7684\u6e90\u6570\u636e\uff08\u901a\u5e38\u4e0d\u53ef\u7528\uff09\uff0c\u800c\u5176\u4ed6\u65b9\u6cd5\uff0c\u5373\u65e0\u6e90 DA\uff0c\u5219\u9700\u8981\u591a\u6b21\u901a\u8fc7\u6d4b\u8bd5\u6570\u636e\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6df1\u5ea6\u5b8c\u6210\u7684\u5728\u7ebf\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u5355\u4e2a\u56fe\u50cf\u548c\u76f8\u5173\u7684\u7a00\u758f\u6df1\u5ea6\u56fe\u63a8\u65ad\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u4ece\u800c\u7f29\u5c0f\u5355\u6b21\u4f20\u9012\u4e2d\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u6211\u4eec\u9996\u5148\u7814\u7a76\u4e86\u6bcf\u79cd\u6570\u636e\u6a21\u6001\u7684\u57df\u8f6c\u79fb\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u6839\u636e\u6211\u4eec\u7684\u89c2\u5bdf\uff0c\u7a00\u758f\u6df1\u5ea6\u6a21\u6001\u8868\u73b0\u51fa\u6bd4\u56fe\u50cf\u5c0f\u5f97\u591a\u7684\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5728\u6e90\u57df\u4e2d\u8bad\u7ec3\u7684\u5d4c\u5165\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u4fdd\u7559\u4ece\u4ec5\u7f16\u7801\u7a00\u758f\u6df1\u5ea6\u7684\u7279\u5f81\u5230\u7f16\u7801\u56fe\u50cf\u548c\u7a00\u758f\u6df1\u5ea6\u7684\u7279\u5f81\u7684\u6620\u5c04\u3002\u5728\u6d4b\u8bd5\u671f\u95f4\uff0c\u4f7f\u7528\u8be5\u56fe\u4f5c\u4e3a\u6e90\u57df\u7279\u5f81\u7684\u4ee3\u7406\u6765\u6295\u5f71\u7a00\u758f\u6df1\u5ea6\u7279\u5f81\uff0c\u5e76\u7528\u4f5c\u8bad\u7ec3\u4e00\u7ec4\u8f85\u52a9\u53c2\u6570\uff08\u5373\u9002\u5e94\u5c42\uff09\u7684\u6307\u5bfc\uff0c\u4ee5\u5c06\u76ee\u6807\u6d4b\u8bd5\u57df\u4e2d\u7684\u56fe\u50cf\u548c\u7a00\u758f\u6df1\u5ea6\u7279\u5f81\u5bf9\u9f50\u5230\u6e90\u57df\u7684\u90a3\u4e2a\u3002\u6211\u4eec\u5728\u5ba4\u5185\u548c\u5ba4\u5916\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u6bd4\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad8\u4e86 21.1%\u3002|[2402.03312v1](http://arxiv.org/pdf/2402.03312v1)|null|\n", "2402.03310": "|**2024-02-05**|**V-IRL: Grounding Virtual Intelligence in Real Life**|V-IRL\uff1a\u5c06\u865a\u62df\u667a\u80fd\u878d\u5165\u73b0\u5b9e\u751f\u6d3b|Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, Saining Xie|There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.|\u4eba\u7c7b\u5c45\u4f4f\u7684\u5730\u7403\u4e0e\u521b\u5efa\u73b0\u4ee3\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u6570\u5b57\u9886\u57df\u4e4b\u95f4\u5b58\u5728\u7740\u611f\u5b98\u9e3f\u6c9f\u3002\u4e3a\u4e86\u5f00\u53d1\u80fd\u591f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u50cf\u4eba\u7c7b\u4e00\u6837\u7075\u6d3b\u611f\u77e5\u3001\u601d\u8003\u548c\u884c\u52a8\u7684\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\uff0c\u5fc5\u987b\u5f25\u5408\u6570\u5b57\u4e16\u754c\u548c\u7269\u7406\u4e16\u754c\u4e4b\u95f4\u7684\u73b0\u5b9e\u5dee\u8ddd\u3002\u6211\u4eec\u5982\u4f55\u624d\u80fd\u5728\u50cf\u6211\u4eec\u6240\u5c45\u4f4f\u7684\u73af\u5883\u4e00\u6837\u4e30\u5bcc\u591a\u6837\u7684\u73af\u5883\u4e2d\u4f53\u73b0\u4ee3\u7406\uff0c\u800c\u4e0d\u53d7\u771f\u5b9e\u786c\u4ef6\u548c\u63a7\u5236\u7684\u9650\u5236\uff1f\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 V-IRL\uff1a\u4e00\u4e2a\u5e73\u53f0\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5728\u865a\u62df\u4f46\u73b0\u5b9e\u7684\u73af\u5883\u4e2d\u4e0e\u73b0\u5b9e\u4e16\u754c\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u4ea4\u4e92\u3002\u6211\u4eec\u7684\u5e73\u53f0\u662f\u5f00\u53d1\u80fd\u591f\u5b8c\u6210\u5404\u79cd\u5b9e\u9645\u4efb\u52a1\u7684\u4ee3\u7406\u7684\u6e38\u4e50\u573a\uff0c\u4e5f\u662f\u8861\u91cf\u611f\u77e5\u3001\u51b3\u7b56\u4ee5\u53ca\u4e0e\u5168\u7403\u771f\u5b9e\u6570\u636e\u4ea4\u4e92\u7b49\u80fd\u529b\u8fdb\u5c55\u7684\u5de8\u5927\u6d4b\u8bd5\u5e73\u53f0\u3002|[2402.03310v1](http://arxiv.org/pdf/2402.03310v1)|null|\n", "2402.03283": "|**2024-02-05**|**Towards a Flexible Scale-out Framework for Efficient Visual Data Query Processing**|\u9762\u5411\u9ad8\u6548\u53ef\u89c6\u6570\u636e\u67e5\u8be2\u5904\u7406\u7684\u7075\u6d3b\u6a2a\u5411\u6269\u5c55\u6846\u67b6|Rohit Verma, Arun Raghunath|There is growing interest in visual data management systems that support queries with specialized operations ranging from resizing an image to running complex machine learning models. With a plethora of such operations, the basic need to receive query responses in minimal time takes a hit, especially when the client desires to run multiple such operations in a single query. Existing systems provide an ad-hoc approach where different solutions are clubbed together to provide an end-to-end visual data management system. Unlike such solutions, the Visual Data Management System (VDMS) natively executes queries with multiple operations, thus providing an end-to-end solution. However, a fixed subset of native operations and a synchronous threading architecture limit its generality and scalability.   In this paper, we develop VDMS-Async that adds the capability to run user-defined operations with VDMS and execute operations within a query on a remote server. VDMS-Async utilizes an event-driven architecture to create an efficient pipeline for executing operations within a query. Our experiments have shown that VDMS-Async reduces the query execution time by 2-3X compared to existing state-of-the-art systems. Further, remote operations coupled with an event-driven architecture enables VDMS-Async to scale query execution time linearly with the addition of every new remote server. We demonstrate a 64X reduction in query execution time when adding 64 remote servers.|\u4eba\u4eec\u5bf9\u53ef\u89c6\u5316\u6570\u636e\u7ba1\u7406\u7cfb\u7edf\u8d8a\u6765\u8d8a\u611f\u5174\u8da3\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u652f\u6301\u4ece\u8c03\u6574\u56fe\u50cf\u5927\u5c0f\u5230\u8fd0\u884c\u590d\u6742\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7b49\u4e13\u95e8\u64cd\u4f5c\u7684\u67e5\u8be2\u3002\u7531\u4e8e\u6b64\u7c7b\u64cd\u4f5c\u8fc7\u591a\uff0c\u5728\u6700\u77ed\u7684\u65f6\u95f4\u5185\u63a5\u6536\u67e5\u8be2\u54cd\u5e94\u7684\u57fa\u672c\u9700\u6c42\u5c31\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5f53\u5ba2\u6237\u7aef\u5e0c\u671b\u5728\u5355\u4e2a\u67e5\u8be2\u4e2d\u8fd0\u884c\u591a\u4e2a\u6b64\u7c7b\u64cd\u4f5c\u65f6\u3002\u73b0\u6709\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e34\u65f6\u65b9\u6cd5\uff0c\u5c06\u4e0d\u540c\u7684\u89e3\u51b3\u65b9\u6848\u7ec4\u5408\u5728\u4e00\u8d77\u4ee5\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u53ef\u89c6\u5316\u6570\u636e\u7ba1\u7406\u7cfb\u7edf\u3002\u4e0e\u6b64\u7c7b\u89e3\u51b3\u65b9\u6848\u4e0d\u540c\uff0c\u53ef\u89c6\u5316\u6570\u636e\u7ba1\u7406\u7cfb\u7edf (VDMS) \u672c\u673a\u6267\u884c\u5177\u6709\u591a\u4e2a\u64cd\u4f5c\u7684\u67e5\u8be2\uff0c\u4ece\u800c\u63d0\u4f9b\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u672c\u673a\u64cd\u4f5c\u7684\u56fa\u5b9a\u5b50\u96c6\u548c\u540c\u6b65\u7ebf\u7a0b\u67b6\u6784\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 VDMS-Async\uff0c\u5b83\u6dfb\u52a0\u4e86\u4f7f\u7528 VDMS \u8fd0\u884c\u7528\u6237\u5b9a\u4e49\u7684\u64cd\u4f5c\u4ee5\u53ca\u5728\u8fdc\u7a0b\u670d\u52a1\u5668\u4e0a\u7684\u67e5\u8be2\u4e2d\u6267\u884c\u64cd\u4f5c\u7684\u529f\u80fd\u3002 VDMS-Async \u5229\u7528\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u521b\u5efa\u9ad8\u6548\u7684\u7ba1\u9053\u6765\u6267\u884c\u67e5\u8be2\u4e2d\u7684\u64cd\u4f5c\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u76f8\u6bd4\uff0cVDMS-Async \u5c06\u67e5\u8be2\u6267\u884c\u65f6\u95f4\u7f29\u77ed\u4e86 2-3 \u500d\u3002\u6b64\u5916\uff0c\u8fdc\u7a0b\u64cd\u4f5c\u4e0e\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u76f8\u7ed3\u5408\uff0c\u4f7f VDMS-Async \u80fd\u591f\u968f\u7740\u6bcf\u4e2a\u65b0\u8fdc\u7a0b\u670d\u52a1\u5668\u7684\u6dfb\u52a0\u800c\u7ebf\u6027\u6269\u5c55\u67e5\u8be2\u6267\u884c\u65f6\u95f4\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u6dfb\u52a0 64 \u4e2a\u8fdc\u7a0b\u670d\u52a1\u5668\u540e\uff0c\u67e5\u8be2\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u4e86 64 \u500d\u3002|[2402.03283v1](http://arxiv.org/pdf/2402.03283v1)|null|\n", "2402.02936": "|**2024-02-05**|**Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss**|\u4f7f\u7528\u95e8\u63a7\u5377\u79ef\u548c\u4e0a\u4e0b\u6587\u91cd\u5efa\u635f\u5931\u7684\u5168\u666f\u56fe\u50cf\u4fee\u590d|Li Yu, Yanjun Gao, Farhad Pakdaman, Moncef Gabbouj|Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting. However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results. In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP) format as network input. The generator employs gated convolutions to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the generators to find the most suitable reference patch for inpainting the missing region. The proposed method is compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in terms of PSNR and SSIM. Experimental results and ablation study demonstrate that the proposed method outperforms SOTA both quantitatively and qualitatively.|\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u5168\u666f\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5f88\u96be\u533a\u5206\u6709\u6548\u50cf\u7d20\u548c\u65e0\u6548\u50cf\u7d20\u5e76\u4e3a\u635f\u574f\u533a\u57df\u627e\u5230\u5408\u9002\u7684\u53c2\u8003\uff0c\u4ece\u800c\u5bfc\u81f4\u4fee\u590d\u7ed3\u679c\u4e2d\u51fa\u73b0\u4f2a\u5f71\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u666f\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7531\u4e00\u4e2a\u4eba\u8138\u751f\u6210\u5668\u3001\u4e00\u4e2a\u7acb\u65b9\u4f53\u751f\u6210\u5668\u3001\u4e00\u4e2a\u4fa7\u5206\u652f\u548c\u4e24\u4e2a\u9274\u522b\u5668\u7ec4\u6210\u3002\u6211\u4eec\u4f7f\u7528\u7acb\u65b9\u4f53\u8d34\u56fe\u6295\u5f71\uff08CMP\uff09\u683c\u5f0f\u4f5c\u4e3a\u7f51\u7edc\u8f93\u5165\u3002\u751f\u6210\u5668\u91c7\u7528\u95e8\u63a7\u5377\u79ef\u6765\u533a\u5206\u6709\u6548\u50cf\u7d20\u548c\u65e0\u6548\u50cf\u7d20\uff0c\u800c\u4fa7\u5206\u652f\u7684\u8bbe\u8ba1\u5229\u7528\u4e0a\u4e0b\u6587\u91cd\u5efa\uff08CR\uff09\u635f\u5931\u6765\u5f15\u5bfc\u751f\u6210\u5668\u627e\u5230\u6700\u5408\u9002\u7684\u53c2\u8003\u8865\u4e01\u6765\u4fee\u590d\u7f3a\u5931\u533a\u57df\u3002\u5728 PSNR \u548c SSIM \u65b9\u9762\uff0c\u5c06\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0e SUN360 \u8857\u666f\u6570\u636e\u96c6\u4e0a\u6700\u5148\u8fdb\u7684 (SOTA) \u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6570\u91cf\u548c\u8d28\u91cf\u4e0a\u90fd\u4f18\u4e8e SOTA\u3002|[2402.02936v1](http://arxiv.org/pdf/2402.02936v1)|null|\n", "2402.02922": "|**2024-02-05**|**Pixel-Wise Color Constancy via Smoothness Techniques in Multi-Illuminant Scenes**|\u901a\u8fc7\u591a\u5149\u6e90\u573a\u666f\u4e2d\u7684\u5e73\u6ed1\u6280\u672f\u5b9e\u73b0\u9010\u50cf\u7d20\u989c\u8272\u6052\u5b9a|Umut Cem Entok, Firas Laakom, Farhad Pakdaman, Moncef Gabbouj|Most scenes are illuminated by several light sources, where the traditional assumption of uniform illumination is invalid. This issue is ignored in most color constancy methods, primarily due to the complex spatial impact of multiple light sources on the image. Moreover, most existing multi-illuminant methods fail to preserve the smooth change of illumination, which stems from spatial dependencies in natural images. Motivated by this, we propose a novel multi-illuminant color constancy method, by learning pixel-wise illumination maps caused by multiple light sources. The proposed method enforces smoothness within neighboring pixels, by regularizing the training with the total variation loss. Moreover, a bilateral filter is provisioned further to enhance the natural appearance of the estimated images, while preserving the edges. Additionally, we propose a label-smoothing technique that enables the model to generalize well despite the uncertainties in ground truth. Quantitative and qualitative experiments demonstrate that the proposed method outperforms the state-of-the-art.|\u5927\u591a\u6570\u573a\u666f\u7531\u591a\u4e2a\u5149\u6e90\u7167\u660e\uff0c\u4f20\u7edf\u7684\u5747\u5300\u7167\u660e\u5047\u8bbe\u662f\u65e0\u6548\u7684\u3002\u5927\u591a\u6570\u989c\u8272\u6052\u5e38\u6027\u65b9\u6cd5\u90fd\u5ffd\u7565\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u591a\u4e2a\u5149\u6e90\u5bf9\u56fe\u50cf\u7684\u590d\u6742\u7a7a\u95f4\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u591a\u5149\u6e90\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u6301\u7167\u660e\u7684\u5e73\u6ed1\u53d8\u5316\uff0c\u8fd9\u6e90\u4e8e\u81ea\u7136\u56fe\u50cf\u4e2d\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5149\u6e90\u989c\u8272\u6052\u5e38\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7531\u591a\u4e2a\u5149\u6e90\u5f15\u8d77\u7684\u50cf\u7d20\u7ea7\u7167\u660e\u56fe\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7528\u603b\u53d8\u5316\u635f\u5931\u6765\u89c4\u8303\u8bad\u7ec3\u6765\u589e\u5f3a\u76f8\u90bb\u50cf\u7d20\u5185\u7684\u5e73\u6ed1\u5ea6\u3002\u6b64\u5916\uff0c\u8fdb\u4e00\u6b65\u63d0\u4f9b\u53cc\u8fb9\u6ee4\u6ce2\u5668\u4ee5\u589e\u5f3a\u4f30\u8ba1\u56fe\u50cf\u7684\u81ea\u7136\u5916\u89c2\uff0c\u540c\u65f6\u4fdd\u7559\u8fb9\u7f18\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u7b7e\u5e73\u6ed1\u6280\u672f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5f88\u597d\u5730\u6982\u62ec\uff0c\u5c3d\u7ba1\u5730\u9762\u4e8b\u5b9e\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2402.02922v1](http://arxiv.org/pdf/2402.02922v1)|null|\n", "2402.02889": "|**2024-02-05**|**Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding**|\u63a2\u7d22\u901a\u7528\u97f3\u9891\u7406\u89e3\u7684\u8054\u5408\u81ea\u76d1\u7763\u5b66\u4e60|Yasar Abbas Ur Rehman, Kin Wai Lau, Yuyang Xie, Lan Ma, Jiajun Shen|The integration of Federated Learning (FL) and Self-supervised Learning (SSL) offers a unique and synergetic combination to exploit the audio data for general-purpose audio understanding, without compromising user data privacy. However, rare efforts have been made to investigate the SSL models in the FL regime for general-purpose audio understanding, especially when the training data is generated by large-scale heterogeneous audio sources. In this paper, we evaluate the performance of feature-matching and predictive audio-SSL techniques when integrated into large-scale FL settings simulated with non-independently identically distributed (non-iid) data. We propose a novel Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning intermediate feature representations from large-scale decentralized heterogeneous clients, holding unlabelled audio data. Our study has found that audio F-SSL approaches perform on par with the centralized audio-SSL approaches on the audio-retrieval task. Extensive experiments demonstrate the effectiveness and significance of FASSL as it assists in obtaining the optimal global model for state-of-the-art FL aggregation methods.|\u8054\u90a6\u5b66\u4e60 (FL) \u548c\u81ea\u76d1\u7763\u5b66\u4e60 (SSL) \u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u534f\u540c\u7ec4\u5408\uff0c\u53ef\u5229\u7528\u97f3\u9891\u6570\u636e\u8fdb\u884c\u901a\u7528\u97f3\u9891\u7406\u89e3\uff0c\u800c\u4e0d\u4f1a\u635f\u5bb3\u7528\u6237\u6570\u636e\u9690\u79c1\u3002\u7136\u800c\uff0c\u4eba\u4eec\u5f88\u5c11\u52aa\u529b\u7814\u7a76 FL \u4f53\u7cfb\u4e2d\u7684 SSL \u6a21\u578b\u4ee5\u5b9e\u73b0\u901a\u7528\u97f3\u9891\u7406\u89e3\uff0c\u7279\u522b\u662f\u5f53\u8bad\u7ec3\u6570\u636e\u7531\u5927\u89c4\u6a21\u5f02\u6784\u97f3\u9891\u6e90\u751f\u6210\u65f6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u7279\u5f81\u5339\u914d\u548c\u9884\u6d4b\u97f3\u9891 SSL \u6280\u672f\u96c6\u6210\u5230\u4f7f\u7528\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff09\u6570\u636e\u6a21\u62df\u7684\u5927\u89c4\u6a21 FL \u8bbe\u7f6e\u4e2d\u65f6\u7684\u6027\u80fd\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u5408 SSL (F-SSL) \u6846\u67b6\uff0c\u79f0\u4e3a FASSL\uff0c\u5b83\u80fd\u591f\u4ece\u5927\u89c4\u6a21\u53bb\u4e2d\u5fc3\u5316\u5f02\u6784\u5ba2\u6237\u7aef\u5b66\u4e60\u4e2d\u95f4\u7279\u5f81\u8868\u793a\uff0c\u5e76\u4fdd\u5b58\u672a\u6807\u8bb0\u7684\u97f3\u9891\u6570\u636e\u3002\u6211\u4eec\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u97f3\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0c\u97f3\u9891 F-SSL \u65b9\u6cd5\u7684\u8868\u73b0\u4e0e\u96c6\u4e2d\u5f0f\u97f3\u9891 SSL \u65b9\u6cd5\u76f8\u5f53\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86 FASSL \u7684\u6709\u6548\u6027\u548c\u91cd\u8981\u6027\uff0c\u56e0\u4e3a\u5b83\u6709\u52a9\u4e8e\u83b7\u5f97\u6700\u5148\u8fdb\u7684 FL \u805a\u5408\u65b9\u6cd5\u7684\u6700\u4f73\u5168\u5c40\u6a21\u578b\u3002|[2402.02889v1](http://arxiv.org/pdf/2402.02889v1)|null|\n", "2402.02886": "|**2024-02-05**|**Time-Distributed Backdoor Attacks on Federated Spiking Learning**|\u5bf9\u8054\u90a6\u5c16\u5cf0\u5b66\u4e60\u7684\u65f6\u95f4\u5206\u5e03\u5f0f\u540e\u95e8\u653b\u51fb|Gorka Abad, Stjepan Picek, Aitor Urbieta|This paper investigates the vulnerability of spiking neural networks (SNNs) and federated learning (FL) to backdoor attacks using neuromorphic data. Despite the efficiency of SNNs and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks. We first assess the viability of using FL with SNNs using neuromorphic data, showing its potential usage. Then, we evaluate the transferability of known FL attack methods to SNNs, finding that these lead to suboptimal attack performance. Therefore, we explore backdoor attacks involving single and multiple attackers to improve the attack performance. Our primary contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious devices, enhancing the attack's effectiveness and stealthiness. In the best case, we achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM. Moreover, we adapt and evaluate an existing defense against backdoor attacks, revealing its inadequacy in protecting SNNs. This study underscores the need for robust security measures in deploying SNNs and FL, particularly in the context of backdoor attacks.|\u672c\u6587\u7814\u7a76\u4e86\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc (SNN) \u548c\u8054\u90a6\u5b66\u4e60 (FL) \u5bf9\u4e8e\u4f7f\u7528\u795e\u7ecf\u5f62\u6001\u6570\u636e\u7684\u540e\u95e8\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002\u5c3d\u7ba1 SNN \u5177\u6709\u9ad8\u6548\u6027\u5e76\u4e14 FL \u5177\u6709\u9690\u79c1\u4f18\u52bf\uff08\u5c24\u5176\u662f\u5728\u4f4e\u529f\u8017\u8bbe\u5907\u4e2d\uff09\uff0c\u4f46\u6211\u4eec\u8bc1\u660e\u8fd9\u4e9b\u7cfb\u7edf\u5f88\u5bb9\u6613\u53d7\u5230\u6b64\u7c7b\u653b\u51fb\u3002\u6211\u4eec\u9996\u5148\u4f7f\u7528\u795e\u7ecf\u5f62\u6001\u6570\u636e\u8bc4\u4f30\u5c06 FL \u4e0e SNN \u7ed3\u5408\u4f7f\u7528\u7684\u53ef\u884c\u6027\uff0c\u5c55\u793a\u5176\u6f5c\u5728\u7528\u9014\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5df2\u77e5 FL \u653b\u51fb\u65b9\u6cd5\u5230 SNN \u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u653b\u51fb\u6027\u80fd\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63a2\u7d22\u6d89\u53ca\u5355\u4e2a\u548c\u591a\u4e2a\u653b\u51fb\u8005\u7684\u540e\u95e8\u653b\u51fb\uff0c\u4ee5\u63d0\u9ad8\u653b\u51fb\u6027\u80fd\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u662f\u5f00\u53d1\u4e00\u79cd\u9488\u5bf9 SNN \u548c FL \u7684\u65b0\u578b\u653b\u51fb\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u53ef\u4ee5\u5728\u6076\u610f\u8bbe\u5907\u4e4b\u95f4\u4e34\u65f6\u5206\u53d1\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u4ece\u800c\u589e\u5f3a\u653b\u51fb\u7684\u6709\u6548\u6027\u548c\u9690\u853d\u6027\u3002\u5728\u6700\u597d\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86 100 \u7684\u653b\u51fb\u6210\u529f\u7387\u30010.13 MSE \u548c 98.9 SSIM\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8c03\u6574\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u7684\u540e\u95e8\u653b\u51fb\u9632\u5fa1\u63aa\u65bd\uff0c\u63ed\u793a\u4e86\u5176\u5728\u4fdd\u62a4 SNN \u65b9\u9762\u7684\u4e0d\u8db3\u3002\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u90e8\u7f72 SNN \u548c FL \u65f6\u9700\u8981\u91c7\u53d6\u5f3a\u6709\u529b\u7684\u5b89\u5168\u63aa\u65bd\uff0c\u7279\u522b\u662f\u5728\u540e\u95e8\u653b\u51fb\u7684\u60c5\u51b5\u4e0b\u3002|[2402.02886v1](http://arxiv.org/pdf/2402.02886v1)|null|\n", "2402.02851": "|**2024-02-05**|**Enhancing Compositional Generalization via Compositional Feature Alignment**|\u901a\u8fc7\u7ec4\u5408\u7279\u5f81\u5bf9\u9f50\u589e\u5f3a\u7ec4\u5408\u6cdb\u5316|Haoxiang Wang, Haozhe Si, Huajie Shao, Han Zhao|Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen. We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models. We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning.|\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u7ecf\u5e38\u9762\u4e34\u6570\u636e\u5206\u5e03\u53d8\u5316\uff0c\u5176\u4e2d\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u3002\u5728\u5e38\u89c1\u7684\u591a\u57df\u591a\u7c7b\u8bbe\u7f6e\u4e2d\uff0c\u968f\u7740\u7c7b\u548c\u57df\u6570\u91cf\u7684\u589e\u52a0\uff0c\u6536\u96c6\u6bcf\u4e2a\u57df\u7c7b\u7ec4\u5408\u7684\u8bad\u7ec3\u6570\u636e\u53d8\u5f97\u4e0d\u53ef\u884c\u3002\u8fd9\u4e00\u6311\u6218\u81ea\u7136\u5bfc\u81f4\u4e86\u5bf9\u5177\u6709\u7ec4\u5408\u6cdb\u5316\uff08CG\uff09\u80fd\u529b\u7684\u6a21\u578b\u7684\u8ffd\u6c42\uff0c\u5176\u4e2d\u6a21\u578b\u53ef\u4ee5\u6cdb\u5316\u5230\u770b\u4e0d\u89c1\u7684\u9886\u57df\u7c7b\u7ec4\u5408\u3002\u4e3a\u4e86\u6df1\u5165\u7814\u7a76 CG \u6311\u6218\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 CG-Bench\uff0c\u8fd9\u662f\u4e00\u5957\u6e90\u81ea\u73b0\u6709\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u96c6\u7684 CG \u57fa\u51c6\uff0c\u5e76\u89c2\u5bdf\u5230\u57fa\u7840\u6a21\u578b\uff08\u4f8b\u5982 CLIP \u548c DINOv2\uff09\u4e0a\u6d41\u884c\u7684\u9884\u8bad\u7ec3\u5fae\u8c03\u8303\u5f0f\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7ec4\u5408\u7279\u5f81\u5bf9\u9f50\uff08CFA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u7684\u4e24\u9636\u6bb5\u5fae\u8c03\u6280\u672f\uff0ci\uff09\u5728\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4e0a\u5b66\u4e60\u5173\u4e8e\u7c7b\u548c\u57df\u6807\u7b7e\u7684\u4e24\u4e2a\u6b63\u4ea4\u7ebf\u6027\u5934\uff0cii\uff09\u4f7f\u7528\u65b0\u5b66\u7684\u5934\u50f5\u4f4f\u4e86\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u548c\u7ecf\u9a8c\u4e0a\u8bc1\u660e CFA \u9f13\u52b1\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7ec4\u5408\u7279\u5f81\u5b66\u4e60\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5728 CG-Bench \u4e0a\u9488\u5bf9 CLIP \u548c DINOv2 \u8fd9\u4e24\u4e2a\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCFA \u5728\u6210\u5206\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u5e38\u89c1\u7684\u5fae\u8c03\u6280\u672f\uff0c\u8bc1\u5b9e\u4e86 CFA \u5728\u6210\u5206\u7279\u5f81\u5b66\u4e60\u65b9\u9762\u7684\u529f\u6548\u3002|[2402.02851v1](http://arxiv.org/pdf/2402.02851v1)|**[link](https://github.com/haoxiang-wang/compositional-feature-alignment)**|\n", "2402.02736": "|**2024-02-05**|**Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes**|\u4f7f\u7528\u8fd0\u52a8\u63d0\u793a\u6765\u76d1\u7763\u4f4e\u6570\u636e\u72b6\u6001\u4e0b\u7684\u5355\u5e27\u8eab\u4f53\u59ff\u52bf\u548c\u5f62\u72b6\u4f30\u8ba1|Andrey Davydov, Alexey Sidnev, Artsiom Sanakoyeu, Yuhua Chen, Mathieu Salzmann, Pascal Fua|When enough annotated training data is available, supervised deep-learning algorithms excel at estimating human body pose and shape using a single camera. The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors. Unfortunately, such sources are not always available either. We show that, in such cases, easy-to-obtain unannotated videos can be used instead to provide the required supervisory signals. Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next. This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data.|\u5f53\u6709\u8db3\u591f\u7684\u5e26\u6ce8\u91ca\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u7528\u65f6\uff0c\u6709\u76d1\u7763\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u64c5\u957f\u4f7f\u7528\u5355\u4e2a\u76f8\u673a\u6765\u4f30\u8ba1\u4eba\u4f53\u59ff\u52bf\u548c\u5f62\u72b6\u3002\u901a\u8fc7\u4f7f\u7528\u5176\u4ed6\u4fe1\u606f\u6e90\uff08\u4f8b\u5982\u8eab\u4f53\u5f62\u72b6\u6570\u636e\u5e93\uff09\u6765\u4e86\u89e3\u5148\u9a8c\u77e5\u8bc6\uff0c\u53ef\u4ee5\u51cf\u8f7b\u53ef\u7528\u6570\u636e\u592a\u5c11\u7684\u5f71\u54cd\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u6b64\u7c7b\u6765\u6e90\u5e76\u4e0d\u603b\u662f\u53ef\u7528\u3002\u6211\u4eec\u8868\u660e\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f7f\u7528\u6613\u4e8e\u83b7\u53d6\u7684\u672a\u6ce8\u91ca\u89c6\u9891\u6765\u63d0\u4f9b\u6240\u9700\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u7ed9\u5b9a\u4f7f\u7528\u592a\u5c11\u6ce8\u91ca\u6570\u636e\u7684\u8bad\u7ec3\u6a21\u578b\uff0c\u6211\u4eec\u8ba1\u7b97\u8fde\u7eed\u5e27\u4e2d\u7684\u59ff\u52bf\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u7684\u5149\u6d41\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f3a\u5236\u56fe\u50cf\u5149\u6d41\u4e0e\u4ece\u4e00\u5e27\u5230\u4e0b\u4e00\u5e27\u7684\u59ff\u52bf\u53d8\u5316\u63a8\u65ad\u51fa\u7684\u5149\u6d41\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u8fd9\u63d0\u4f9b\u4e86\u8db3\u591f\u7684\u989d\u5916\u76d1\u7763\uff0c\u4ee5\u6709\u6548\u5730\u7ec6\u5316\u7f51\u7edc\u6743\u91cd\uff0c\u5e76\u4e0e\u4f7f\u7528\u66f4\u591a\u6ce8\u91ca\u6570\u636e\u8bad\u7ec3\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002|[2402.02736v1](http://arxiv.org/pdf/2402.02736v1)|null|\n"}}