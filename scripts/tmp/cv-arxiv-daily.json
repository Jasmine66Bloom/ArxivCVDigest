{"\u751f\u6210\u6a21\u578b": {"2402.09368": "|**2024-02-14**|**Magic-Me: Identity-Specific Video Customized Diffusion**|Magic-Me\uff1a\u9488\u5bf9\u7279\u5b9a\u8eab\u4efd\u7684\u89c6\u9891\u5b9a\u5236\u6269\u6563|Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng|Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution.   Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability. The codes are available at https://github.com/Zhen-Dong/Magic-Me.|\u4e3a\u7279\u5b9a\u8eab\u4efd (ID) \u521b\u5efa\u5185\u5bb9\u5df2\u5f15\u8d77\u4eba\u4eec\u5bf9\u751f\u6210\u6a21\u578b\u9886\u57df\u7684\u6d53\u539a\u5174\u8da3\u3002\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff08T2I\uff09\u9886\u57df\uff0c\u4e3b\u9898\u9a71\u52a8\u7684\u5185\u5bb9\u751f\u6210\u53d6\u5f97\u4e86\u957f\u8db3\u7684\u8fdb\u6b65\uff0c\u5e76\u4e14\u56fe\u50cf\u4e2d\u7684ID\u53ef\u63a7\u3002\u7136\u800c\uff0c\u5c06\u5176\u6269\u5c55\u5230\u89c6\u9891\u751f\u6210\u5c1a\u672a\u5f97\u5230\u5f88\u597d\u7684\u63a2\u7d22\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u4e3b\u9898\u8eab\u4efd\u53ef\u63a7\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u79f0\u4e3a\u89c6\u9891\u81ea\u5b9a\u4e49\u6269\u6563\uff08VCD\uff09\u3002 VCD\u5229\u7528\u7531\u51e0\u5f20\u56fe\u50cf\u5b9a\u4e49\u7684\u6307\u5b9a\u4e3b\u4f53ID\uff0c\u52a0\u5f3a\u4e86\u8eab\u4efd\u4fe1\u606f\u63d0\u53d6\uff0c\u5e76\u5728\u521d\u59cb\u5316\u9636\u6bb5\u6ce8\u5165\u9010\u5e27\u76f8\u5173\u6027\uff0c\u4ee5\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4fdd\u7559\u8eab\u4efd\u7684\u7a33\u5b9a\u89c6\u9891\u8f93\u51fa\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u4e2a\u5bf9\u4e8e\u9ad8\u8d28\u91cf ID \u4fdd\u5b58\u81f3\u5173\u91cd\u8981\u7684\u65b0\u9896\u7ec4\u4ef6\uff1a1\uff09\u4e00\u4e2a ID \u6a21\u5757\uff0c\u901a\u8fc7\u63d0\u793a\u5206\u5272\u6765\u4f7f\u7528\u88c1\u526a\u540e\u7684\u8eab\u4efd\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u89e3\u5f00 ID \u4fe1\u606f\u548c\u80cc\u666f\u566a\u58f0\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684 ID \u4ee4\u724c\u5b66\u4e60; 2) \u6587\u672c\u8f6c\u89c6\u9891 (T2V) VCD \u6a21\u5757\uff0c\u5177\u6709 3D \u9ad8\u65af\u566a\u58f0\u5148\u9a8c\uff0c\u53ef\u5b9e\u73b0\u66f4\u597d\u7684\u5e27\u95f4\u4e00\u81f4\u6027\uff1b3) \u89c6\u9891\u8f6c\u89c6\u9891 (V2V) \u8138\u90e8 VCD \u548c\u5e73\u94fa VCD \u6a21\u5757\uff0c\u53ef\u5bf9\u8138\u90e8\u8fdb\u884c\u53bb\u6a21\u7cca\u5904\u7406\u5e76\u5347\u7ea7\u89c6\u9891\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u5e27\u95f4\u4e00\u81f4\u6027\u3002\u66f4\u9ad8\u7684\u5206\u8fa8\u7387\u3002\u5c3d\u7ba1\u5b83\u5f88\u7b80\u5355\uff0c\u6211\u4eec\u8fd8\u662f\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6765\u9a8c\u8bc1 VCD \u80fd\u591f\u751f\u6210\u7a33\u5b9a\u4e14\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\uff0c\u5176 ID \u4f18\u4e8e\u6240\u9009\u7684\u5f3a\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u7531\u4e8eID\u6a21\u5757\u7684\u53ef\u79fb\u690d\u6027\uff0cVCD\u8fd8\u53ef\u4ee5\u4e0e\u516c\u5f00\u7684\u7ecf\u8fc7\u5fae\u8c03\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u914d\u5408\u826f\u597d\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5176\u53ef\u7528\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/Zhen-Dong/Magic-Me \u83b7\u53d6\u3002|[2402.09368v1](http://arxiv.org/pdf/2402.09368v1)|null|\n", "2402.09242": "|**2024-02-14**|**Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food Detection**|\u7efc\u5408\u77e5\u8bc6\u589e\u5f3a\u7279\u5f81\u4ee5\u5b9e\u73b0\u73b0\u5b9e\u4e16\u754c\u7684\u96f6\u6837\u672c\u98df\u54c1\u68c0\u6d4b|Pengfei Zhou, Weiqing Min, Jiajun Song, Yang Zhang, Shuqiang Jiang|Food computing brings various perspectives to computer vision like vision-based food analysis for nutrition and health. As a fundamental task in food computing, food detection needs Zero-Shot Detection (ZSD) on novel unseen food objects to support real-world scenarios, such as intelligent kitchens and smart restaurants. Therefore, we first benchmark the task of Zero-Shot Food Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations. Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make synthesized features inseparable. The complexity of food semantic attributes further makes it more difficult for current ZSD methods to distinguish various food categories. To address these problems, we propose a novel framework ZSFDet to tackle fine-grained problems by exploiting the interaction between complex attributes. Specifically, we model the correlation between food categories and attributes in ZSFDet by multi-source graphs to provide prior knowledge for distinguishing fine-grained features. Within ZSFDet, Knowledge-Enhanced Feature Synthesizer (KEFS) learns knowledge representation from multiple sources (e.g., ingredients correlation from knowledge graph) via the multi-source graph fusion. Conditioned on the fusion of semantic knowledge representation, the region feature diffusion model in KEFS can generate fine-grained features for training the effective zero-shot detector. Extensive evaluations demonstrate the superior performance of our method ZSFDet on FOWA and the widely-used food dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP compared with the strong baseline RRFS. Further experiments on PASCAL VOC and MS COCO prove that enhancement of the semantic knowledge can also improve the performance on general ZSD. Code and dataset are available at https://github.com/LanceZPF/KEFS.|\u98df\u54c1\u8ba1\u7b97\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u5e26\u6765\u4e86\u591a\u79cd\u89c6\u89d2\uff0c\u4f8b\u5982\u57fa\u4e8e\u89c6\u89c9\u7684\u8425\u517b\u548c\u5065\u5eb7\u98df\u54c1\u5206\u6790\u3002\u4f5c\u4e3a\u98df\u54c1\u8ba1\u7b97\u7684\u4e00\u9879\u57fa\u672c\u4efb\u52a1\uff0c\u98df\u54c1\u68c0\u6d4b\u9700\u8981\u5bf9\u65b0\u9896\u7684\u770b\u4e0d\u89c1\u7684\u98df\u54c1\u5bf9\u8c61\u8fdb\u884c\u96f6\u6837\u672c\u68c0\u6d4b\uff08ZSD\uff09\uff0c\u4ee5\u652f\u6301\u73b0\u5b9e\u4e16\u754c\u7684\u573a\u666f\uff0c\u4f8b\u5982\u667a\u80fd\u53a8\u623f\u548c\u667a\u80fd\u9910\u5385\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u5f15\u5165\u5177\u6709\u4e30\u5bcc\u5c5e\u6027\u6ce8\u91ca\u7684 FOWA \u6570\u636e\u96c6\u6765\u5bf9\u96f6\u6837\u672c\u98df\u7269\u68c0\u6d4b\uff08ZSFD\uff09\u4efb\u52a1\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u4e0e ZSD \u4e0d\u540c\uff0cZSFD \u4e2d\u7684\u7ec6\u7c92\u5ea6\u95ee\u9898\uff08\u4f8b\u5982\u7c7b\u95f4\u76f8\u4f3c\u6027\uff09\u4f7f\u5f97\u5408\u6210\u7279\u5f81\u4e0d\u53ef\u5206\u5272\u3002\u98df\u7269\u8bed\u4e49\u5c5e\u6027\u7684\u590d\u6742\u6027\u8fdb\u4e00\u6b65\u4f7f\u5f97\u5f53\u524d\u7684ZSD\u65b9\u6cd5\u533a\u5206\u5404\u79cd\u98df\u7269\u7c7b\u522b\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6 ZSFDet\uff0c\u901a\u8fc7\u5229\u7528\u590d\u6742\u5c5e\u6027\u4e4b\u95f4\u7684\u4ea4\u4e92\u6765\u89e3\u51b3\u7ec6\u7c92\u5ea6\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u901a\u8fc7\u591a\u6e90\u56fe\u5bf9 ZSFDet \u4e2d\u98df\u7269\u7c7b\u522b\u548c\u5c5e\u6027\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u8fdb\u884c\u5efa\u6a21\uff0c\u4e3a\u533a\u5206\u7ec6\u7c92\u5ea6\u7279\u5f81\u63d0\u4f9b\u5148\u9a8c\u77e5\u8bc6\u3002\u5728 ZSFDet \u4e2d\uff0c\u77e5\u8bc6\u589e\u5f3a\u7279\u5f81\u5408\u6210\u5668 (KEFS) \u901a\u8fc7\u591a\u6e90\u56fe\u878d\u5408\u4ece\u591a\u4e2a\u6e90\u5b66\u4e60\u77e5\u8bc6\u8868\u793a\uff08\u4f8b\u5982\uff0c\u77e5\u8bc6\u56fe\u7684\u6210\u5206\u76f8\u5173\u6027\uff09\u3002\u4ee5\u8bed\u4e49\u77e5\u8bc6\u8868\u793a\u7684\u878d\u5408\u4e3a\u6761\u4ef6\uff0cKEFS\u4e2d\u7684\u533a\u57df\u7279\u5f81\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81\u6765\u8bad\u7ec3\u6709\u6548\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u5668\u3002\u5e7f\u6cdb\u7684\u8bc4\u4f30\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5 ZSFDet \u5728 FOWA \u548c\u5e7f\u6cdb\u4f7f\u7528\u7684\u98df\u54c1\u6570\u636e\u96c6 UECFOOD-256 \u4e0a\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u4e0e\u5f3a\u57fa\u7ebf RRFS \u76f8\u6bd4\uff0cZSD mAP \u663e\u7740\u63d0\u9ad8\u4e86 1.8% \u548c 3.7%\u3002\u5728 PASCAL VOC \u548c MS COCO \u4e0a\u7684\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8bed\u4e49\u77e5\u8bc6\u7684\u589e\u5f3a\u4e5f\u53ef\u4ee5\u63d0\u9ad8\u901a\u7528 ZSD \u7684\u6027\u80fd\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u5728 https://github.com/LanceZPF/KEFS \u83b7\u53d6\u3002|[2402.09242v1](http://arxiv.org/pdf/2402.09242v1)|**[link](https://github.com/lancezpf/kefs)**|\n", "2402.09137": "|**2024-02-14**|**Semi-Supervised Diffusion Model for Brain Age Prediction**|\u7528\u4e8e\u8111\u5e74\u9f84\u9884\u6d4b\u7684\u534a\u76d1\u7763\u6269\u6563\u6a21\u578b|Ayodeji Ijishakin, Sophie Martin, Florence Townend, Federica Agosta, Edoardo Gioele Spinelli, Silvia Basaia, Paride Schito, Yuri Falzone, Massimo Filippi, James Cole, et.al.|Brain age prediction models have succeeded in predicting clinical outcomes in neurodegenerative diseases, but can struggle with tasks involving faster progressing diseases and low quality data. To enhance their performance, we employ a semi-supervised diffusion model, obtaining a 0.83(p<0.01) correlation between chronological and predicted age on low quality T1w MR images. This was competitive with state-of-the-art non-generative methods. Furthermore, the predictions produced by our model were significantly associated with survival length (r=0.24, p<0.05) in Amyotrophic Lateral Sclerosis. Thus, our approach demonstrates the value of diffusion-based architectures for the task of brain age prediction.|\u8111\u5e74\u9f84\u9884\u6d4b\u6a21\u578b\u5df2\u6210\u529f\u9884\u6d4b\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u4e34\u5e8a\u7ed3\u679c\uff0c\u4f46\u5728\u5904\u7406\u6d89\u53ca\u8fdb\u5c55\u8f83\u5feb\u7684\u75be\u75c5\u548c\u4f4e\u8d28\u91cf\u6570\u636e\u7684\u4efb\u52a1\u65f6\u53ef\u80fd\u4f1a\u9047\u5230\u56f0\u96be\u3002\u4e3a\u4e86\u63d0\u9ad8\u5176\u6027\u80fd\uff0c\u6211\u4eec\u91c7\u7528\u534a\u76d1\u7763\u6269\u6563\u6a21\u578b\uff0c\u5728\u4f4e\u8d28\u91cf T1w MR \u56fe\u50cf\u4e0a\u83b7\u5f97\u5b9e\u9645\u5e74\u9f84\u548c\u9884\u6d4b\u5e74\u9f84\u4e4b\u95f4\u7684 0.83\uff08p<0.01\uff09\u76f8\u5173\u6027\u3002\u8fd9\u4e0e\u6700\u5148\u8fdb\u7684\u975e\u751f\u6210\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4ea7\u751f\u7684\u9884\u6d4b\u4e0e\u808c\u840e\u7f29\u4fa7\u7d22\u786c\u5316\u75c7\u7684\u751f\u5b58\u957f\u5ea6\u663e\u7740\u76f8\u5173\uff08r=0.24\uff0cp<0.05\uff09\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8bc1\u660e\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u67b6\u6784\u5bf9\u4e8e\u5927\u8111\u5e74\u9f84\u9884\u6d4b\u4efb\u52a1\u7684\u4ef7\u503c\u3002|[2402.09137v1](http://arxiv.org/pdf/2402.09137v1)|null|\n", "2402.09101": "|**2024-02-14**|**DestripeCycleGAN: Stripe Simulation CycleGAN for Unsupervised Infrared Image Destriping**|DestripeCycleGAN\uff1a\u7528\u4e8e\u65e0\u76d1\u7763\u7ea2\u5916\u56fe\u50cf\u53bb\u6761\u7eb9\u7684\u6761\u7eb9\u6a21\u62df CycleGAN|Shiqi Yang, Hanlin Qin, Shuai Yuan, Xiang Yan, Hossein Rahmani|CycleGAN has been proven to be an advanced approach for unsupervised image restoration. This framework consists of two generators: a denoising one for inference and an auxiliary one for modeling noise to fulfill cycle-consistency constraints. However, when applied to the infrared destriping task, it becomes challenging for the vanilla auxiliary generator to consistently produce vertical noise under unsupervised constraints. This poses a threat to the effectiveness of the cycle-consistency loss, leading to stripe noise residual in the denoised image. To address the above issue, we present a novel framework for single-frame infrared image destriping, named DestripeCycleGAN. In this model, the conventional auxiliary generator is replaced with a priori stripe generation model (SGM) to introduce vertical stripe noise in the clean data, and the gradient map is employed to re-establish cycle-consistency. Meanwhile, a Haar wavelet background guidance module (HBGM) has been designed to minimize the divergence of background details between the different domains. To preserve vertical edges, a multi-level wavelet U-Net (MWUNet) is proposed as the denoising generator, which utilizes the Haar wavelet transform as the sampler to decline directional information loss. Moreover, it incorporates the group fusion block (GFB) into skip connections to fuse the multi-scale features and build the context of long-distance dependencies. Extensive experiments on real and synthetic data demonstrate that our DestripeCycleGAN surpasses the state-of-the-art methods in terms of visual quality and quantitative evaluation. Our code will be made public at https://github.com/0wuji/DestripeCycleGAN.|CycleGAN \u5df2\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u7528\u4e8e\u65e0\u76d1\u7763\u56fe\u50cf\u6062\u590d\u7684\u5148\u8fdb\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u7531\u4e24\u4e2a\u751f\u6210\u5668\u7ec4\u6210\uff1a\u4e00\u4e2a\u7528\u4e8e\u63a8\u7406\u7684\u53bb\u566a\u751f\u6210\u5668\u548c\u4e00\u4e2a\u7528\u4e8e\u5bf9\u566a\u58f0\u8fdb\u884c\u5efa\u6a21\u4ee5\u6ee1\u8db3\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\u7684\u8f85\u52a9\u751f\u6210\u5668\u3002\u7136\u800c\uff0c\u5f53\u5e94\u7528\u4e8e\u7ea2\u5916\u53bb\u6761\u7eb9\u4efb\u52a1\u65f6\uff0c\u666e\u901a\u8f85\u52a9\u53d1\u751f\u5668\u5728\u65e0\u4eba\u76d1\u7763\u7684\u7ea6\u675f\u4e0b\u6301\u7eed\u4ea7\u751f\u5782\u76f4\u566a\u58f0\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\u3002\u8fd9\u5bf9\u5faa\u73af\u4e00\u81f4\u6027\u635f\u5931\u7684\u6709\u6548\u6027\u6784\u6210\u4e86\u5a01\u80c1\uff0c\u5bfc\u81f4\u53bb\u566a\u56fe\u50cf\u4e2d\u6b8b\u7559\u6761\u7eb9\u566a\u58f0\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5355\u5e27\u7ea2\u5916\u56fe\u50cf\u53bb\u6761\u7eb9\u6846\u67b6\uff0c\u540d\u4e3a DestripeCycleGAN\u3002\u5728\u8be5\u6a21\u578b\u4e2d\uff0c\u4f20\u7edf\u7684\u8f85\u52a9\u751f\u6210\u5668\u88ab\u66ff\u6362\u4e3a\u5148\u9a8c\u6761\u7eb9\u751f\u6210\u6a21\u578b\uff08SGM\uff09\uff0c\u4ee5\u5728\u5e72\u51c0\u7684\u6570\u636e\u4e2d\u5f15\u5165\u5782\u76f4\u6761\u7eb9\u566a\u58f0\uff0c\u5e76\u5229\u7528\u68af\u5ea6\u56fe\u6765\u91cd\u65b0\u5efa\u7acb\u5faa\u73af\u4e00\u81f4\u6027\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u54c8\u5c14\u5c0f\u6ce2\u80cc\u666f\u5f15\u5bfc\u6a21\u5757\uff08HBGM\uff09\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e0d\u540c\u57df\u4e4b\u95f4\u80cc\u666f\u7ec6\u8282\u7684\u5dee\u5f02\u3002\u4e3a\u4e86\u4fdd\u7559\u5782\u76f4\u8fb9\u7f18\uff0c\u63d0\u51fa\u4e86\u591a\u7ea7\u5c0f\u6ce2 U-Net\uff08MWUNet\uff09\u4f5c\u4e3a\u53bb\u566a\u751f\u6210\u5668\uff0c\u5b83\u5229\u7528 Haar \u5c0f\u6ce2\u53d8\u6362\u4f5c\u4e3a\u91c7\u6837\u5668\u6765\u51cf\u5c11\u65b9\u5411\u4fe1\u606f\u635f\u5931\u3002\u6b64\u5916\uff0c\u5b83\u5c06\u7ec4\u878d\u5408\u5757\uff08GFB\uff09\u5408\u5e76\u5230\u8df3\u8dc3\u8fde\u63a5\u4e2d\uff0c\u4ee5\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u5e76\u6784\u5efa\u957f\u8ddd\u79bb\u4f9d\u8d56\u7684\u4e0a\u4e0b\u6587\u3002\u5bf9\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 DestripeCycleGAN \u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u8bc4\u4f30\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728 https://github.com/0wuji/DestripeCycleGAN \u516c\u5f00\u3002|[2402.09101v1](http://arxiv.org/pdf/2402.09101v1)|null|\n", "2402.09100": "|**2024-02-14**|**Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs**|\u57fa\u4e8e GAN \u7684\u903c\u771f\u5730\u6807\u5f15\u5bfc\u9762\u90e8\u89c6\u9891\u4fee\u590d|Fatemeh Ghorbani Lohesara, Karen Egiazarian, Sebastian Knorr|Facial video inpainting plays a crucial role in a wide range of applications, including but not limited to the removal of obstructions in video conferencing and telemedicine, enhancement of facial expression analysis, privacy protection, integration of graphical overlays, and virtual makeup. This domain presents serious challenges due to the intricate nature of facial features and the inherent human familiarity with faces, heightening the need for accurate and persuasive completions. In addressing challenges specifically related to occlusion removal in this context, our focus is on the progressive task of generating complete images from facial data covered by masks, ensuring both spatial and temporal coherence. Our study introduces a network designed for expression-based video inpainting, employing generative adversarial networks (GANs) to handle static and moving occlusions across all frames. By utilizing facial landmarks and an occlusion-free reference image, our model maintains the user's identity consistently across frames. We further enhance emotional preservation through a customized facial expression recognition (FER) loss function, ensuring detailed inpainted outputs. Our proposed framework exhibits proficiency in eliminating occlusions from facial videos in an adaptive form, whether appearing static or dynamic on the frames, while providing realistic and coherent results.|\u9762\u90e8\u89c6\u9891\u4fee\u590d\u5728\u5e7f\u6cdb\u7684\u5e94\u7528\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u89c6\u9891\u4f1a\u8bae\u548c\u8fdc\u7a0b\u533b\u7597\u4e2d\u7684\u969c\u788d\u7269\u6e05\u9664\u3001\u9762\u90e8\u8868\u60c5\u5206\u6790\u7684\u589e\u5f3a\u3001\u9690\u79c1\u4fdd\u62a4\u3001\u56fe\u5f62\u53e0\u52a0\u96c6\u6210\u548c\u865a\u62df\u5316\u5986\u3002\u7531\u4e8e\u9762\u90e8\u7279\u5f81\u7684\u590d\u6742\u6027\u548c\u4eba\u7c7b\u5bf9\u9762\u90e8\u56fa\u6709\u7684\u719f\u6089\u6027\uff0c\u8be5\u9886\u57df\u63d0\u51fa\u4e86\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5bf9\u51c6\u786e\u548c\u6709\u8bf4\u670d\u529b\u7684\u5b8c\u6210\u7684\u9700\u6c42\u3002\u5728\u89e3\u51b3\u4e0e\u8fd9\u79cd\u60c5\u51b5\u4e0b\u7684\u906e\u6321\u53bb\u9664\u76f8\u5173\u7684\u6311\u6218\u65f6\uff0c\u6211\u4eec\u7684\u91cd\u70b9\u662f\u4ece\u63a9\u6a21\u8986\u76d6\u7684\u9762\u90e8\u6570\u636e\u751f\u6210\u5b8c\u6574\u56fe\u50cf\u7684\u6e10\u8fdb\u4efb\u52a1\uff0c\u786e\u4fdd\u7a7a\u95f4\u548c\u65f6\u95f4\u7684\u4e00\u81f4\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u4e13\u4e3a\u57fa\u4e8e\u8868\u8fbe\u7684\u89c6\u9891\u4fee\u590d\u800c\u8bbe\u8ba1\u7684\u7f51\u7edc\uff0c\u91c7\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u6765\u5904\u7406\u6240\u6709\u5e27\u4e2d\u7684\u9759\u6001\u548c\u79fb\u52a8\u906e\u6321\u3002\u901a\u8fc7\u5229\u7528\u9762\u90e8\u6807\u5fd7\u548c\u65e0\u906e\u6321\u53c2\u8003\u56fe\u50cf\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u5728\u5e27\u4e4b\u95f4\u4fdd\u6301\u7528\u6237\u8eab\u4efd\u7684\u4e00\u81f4\u6027\u3002\u6211\u4eec\u901a\u8fc7\u5b9a\u5236\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u635f\u5931\u51fd\u6570\u8fdb\u4e00\u6b65\u589e\u5f3a\u60c5\u611f\u4fdd\u5b58\uff0c\u786e\u4fdd\u8be6\u7ec6\u7684\u4fee\u590d\u8f93\u51fa\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u5c55\u793a\u4e86\u4ee5\u81ea\u9002\u5e94\u5f62\u5f0f\u719f\u7ec3\u6d88\u9664\u9762\u90e8\u89c6\u9891\u906e\u6321\u7684\u80fd\u529b\uff0c\u65e0\u8bba\u662f\u5728\u5e27\u4e0a\u663e\u793a\u9759\u6001\u8fd8\u662f\u52a8\u6001\uff0c\u540c\u65f6\u63d0\u4f9b\u771f\u5b9e\u4e14\u8fde\u8d2f\u7684\u7ed3\u679c\u3002|[2402.09100v1](http://arxiv.org/pdf/2402.09100v1)|null|\n", "2402.08934": "|**2024-02-14**|**Extreme Video Compression with Pre-trained Diffusion Models**|\u4f7f\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6781\u9650\u89c6\u9891\u538b\u7f29|Bohan Li, Yiming Liu, Xueyan Niu, Bo Bai, Lei Deng, Deniz G\u00fcnd\u00fcz|Diffusion models have achieved remarkable success in generating high quality image and video data. More recently, they have also been used for image compression with high perceptual quality. In this paper, we present a novel approach to extreme video compression leveraging the predictive power of diffusion-based generative models at the decoder. The conditional diffusion model takes several neural compressed frames and generates subsequent frames. When the reconstruction quality drops below the desired level, new frames are encoded to restart prediction. The entire video is sequentially encoded to achieve a visually pleasing reconstruction, considering perceptual quality metrics such as the learned perceptual image patch similarity (LPIPS) and the Frechet video distance (FVD), at bit rates as low as 0.02 bits per pixel (bpp). Experimental results demonstrate the effectiveness of the proposed scheme compared to standard codecs such as H.264 and H.265 in the low bpp regime. The results showcase the potential of exploiting the temporal relations in video data using generative models. Code is available at: https://github.com/ElesionKyrie/Extreme-Video-Compression-With-Prediction-Using-Pre-trainded-Diffusion-Models-|\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u529f\u3002\u6700\u8fd1\uff0c\u5b83\u4eec\u8fd8\u88ab\u7528\u4e8e\u5177\u6709\u9ad8\u611f\u77e5\u8d28\u91cf\u7684\u56fe\u50cf\u538b\u7f29\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89e3\u7801\u5668\u4e2d\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u6765\u8fdb\u884c\u6781\u7aef\u89c6\u9891\u538b\u7f29\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u6761\u4ef6\u6269\u6563\u6a21\u578b\u91c7\u7528\u591a\u4e2a\u795e\u7ecf\u538b\u7f29\u5e27\u5e76\u751f\u6210\u540e\u7eed\u5e27\u3002\u5f53\u91cd\u5efa\u8d28\u91cf\u4f4e\u4e8e\u6240\u9700\u6c34\u5e73\u65f6\uff0c\u5bf9\u65b0\u5e27\u8fdb\u884c\u7f16\u7801\u4ee5\u91cd\u65b0\u5f00\u59cb\u9884\u6d4b\u3002\u6574\u4e2a\u89c6\u9891\u6309\u987a\u5e8f\u7f16\u7801\uff0c\u4ee5\u5b9e\u73b0\u89c6\u89c9\u4e0a\u4ee4\u4eba\u6109\u60a6\u7684\u91cd\u5efa\uff0c\u8003\u8651\u611f\u77e5\u8d28\u91cf\u6307\u6807\uff0c\u4f8b\u5982\u5b66\u4e60\u7684\u611f\u77e5\u56fe\u50cf\u5757\u76f8\u4f3c\u6027 (LPIPS) \u548c Frechet \u89c6\u9891\u8ddd\u79bb (FVD)\uff0c\u6bd4\u7279\u7387\u4f4e\u81f3\u6bcf\u50cf\u7d20 0.02 \u4f4d (bpp) \u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u4e0e\u4f4e bpp \u673a\u5236\u4e2d\u7684 H.264 \u548c H.265 \u7b49\u6807\u51c6\u7f16\u89e3\u7801\u5668\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6848\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u5c55\u793a\u4e86\u4f7f\u7528\u751f\u6210\u6a21\u578b\u5229\u7528\u89c6\u9891\u6570\u636e\u4e2d\u7684\u65f6\u95f4\u5173\u7cfb\u7684\u6f5c\u529b\u3002\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/ElesionKyrie/Extreme-Video-Compression-With-Prediction-Using-Pre-trainded-Diffusion-Models-|[2402.08934v1](http://arxiv.org/pdf/2402.08934v1)|**[link](https://github.com/elesionkyrie/extreme-video-compression-with-prediction-using-pre-trainded-diffusion-models-)**|\n"}, "\u591a\u6a21\u6001": {"2402.09262": "|**2024-02-14**|**MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models**|MultiMedEval\uff1a\u8bc4\u4f30\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u51c6\u548c\u5de5\u5177\u5305|Corentin Royer, Bjoern Menze, Anjany Sekuboyina|We introduce MultiMedEval, an open-source toolkit for fair and reproducible evaluation of large, medical vision-language models (VLM). MultiMedEval comprehensively assesses the models' performance on a broad array of six multi-modal tasks, conducted over 23 datasets, and spanning over 11 medical domains. The chosen tasks and performance metrics are based on their widespread adoption in the community and their diversity, ensuring a thorough evaluation of the model's overall generalizability. We open-source a Python toolkit (github.com/corentin-ryr/MultiMedEval) with a simple interface and setup process, enabling the evaluation of any VLM in just a few lines of code. Our goal is to simplify the intricate landscape of VLM evaluation, thus promoting fair and uniform benchmarking of future models.|\u6211\u4eec\u63a8\u51fa\u4e86 MultiMedEval\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u5bf9\u5927\u578b\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u8fdb\u884c\u516c\u5e73\u4e14\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u3002 MultiMedEval \u5168\u9762\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u516d\u79cd\u591a\u6a21\u5f0f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6d89\u53ca\u8d85\u8fc7 23 \u4e2a\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u8d85\u8fc7 11 \u4e2a\u533b\u5b66\u9886\u57df\u3002\u6240\u9009\u62e9\u7684\u4efb\u52a1\u548c\u6027\u80fd\u6307\u6807\u57fa\u4e8e\u5176\u5728\u793e\u533a\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\u53ca\u5176\u591a\u6837\u6027\uff0c\u786e\u4fdd\u5bf9\u6a21\u578b\u7684\u6574\u4f53\u901a\u7528\u6027\u8fdb\u884c\u5f7b\u5e95\u8bc4\u4f30\u3002\u6211\u4eec\u5f00\u6e90\u4e86\u4e00\u4e2a Python \u5de5\u5177\u5305 (github.com/corentin-ryr/MultiMedEval)\uff0c\u5177\u6709\u7b80\u5355\u7684\u754c\u9762\u548c\u8bbe\u7f6e\u8fc7\u7a0b\uff0c\u53ea\u9700\u51e0\u884c\u4ee3\u7801\u5373\u53ef\u8bc4\u4f30\u4efb\u4f55 VLM\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u7b80\u5316 VLM \u8bc4\u4f30\u7684\u590d\u6742\u60c5\u51b5\uff0c\u4ece\u800c\u4fc3\u8fdb\u672a\u6765\u6a21\u578b\u7684\u516c\u5e73\u548c\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002|[2402.09262v1](http://arxiv.org/pdf/2402.09262v1)|null|\n", "2402.09181": "|**2024-02-14**|**OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM**|OmniMedVQA\uff1a\u533b\u7597 LVLM \u7684\u65b0\u578b\u5927\u89c4\u6a21\u7efc\u5408\u8bc4\u4f30\u57fa\u51c6|Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo|Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in various multimodal tasks. However, their potential in the medical domain remains largely unexplored. A significant challenge arises from the scarcity of diverse medical images spanning various modalities and anatomical regions, which is essential in real-world medical applications. To solve this problem, in this paper, we introduce OmniMedVQA, a novel comprehensive medical Visual Question Answering (VQA) benchmark. This benchmark is collected from 75 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions. Importantly, all images in this benchmark are sourced from authentic medical scenarios, ensuring alignment with the requirements of the medical field and suitability for evaluating LVLMs. Through our extensive experiments, we have found that existing LVLMs struggle to address these medical VQA problems effectively. Moreover, what surprises us is that medical-specialized LVLMs even exhibit inferior performance to those general-domain models, calling for a more versatile and robust LVLM in the biomedical field. The evaluation results not only reveal the current limitations of LVLM in understanding real medical images but also highlight our dataset's significance. Our dataset will be made publicly available.|\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u5404\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u533b\u5b66\u9886\u57df\u7684\u6f5c\u529b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u88ab\u5f00\u53d1\u3002\u8de8\u8d8a\u5404\u79cd\u6a21\u5f0f\u548c\u89e3\u5256\u533a\u57df\u7684\u5404\u79cd\u533b\u5b66\u56fe\u50cf\u7684\u7a00\u7f3a\u5e26\u6765\u4e86\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u533b\u5b66\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 OmniMedVQA\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u7efc\u5408\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u6536\u96c6\u81ea 75 \u4e2a\u4e0d\u540c\u7684\u533b\u5b66\u6570\u636e\u96c6\uff0c\u5305\u62ec 12 \u79cd\u4e0d\u540c\u7684\u6a21\u5f0f\uff0c\u8986\u76d6 20 \u591a\u4e2a\u4e0d\u540c\u7684\u89e3\u5256\u533a\u57df\u3002\u91cd\u8981\u7684\u662f\uff0c\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6240\u6709\u56fe\u50cf\u5747\u6765\u81ea\u771f\u5b9e\u7684\u533b\u7597\u573a\u666f\uff0c\u786e\u4fdd\u7b26\u5408\u533b\u7597\u9886\u57df\u7684\u8981\u6c42\u4ee5\u53ca\u8bc4\u4f30 LVLM \u7684\u9002\u7528\u6027\u3002\u901a\u8fc7\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u73b0\u6709\u7684 LVLM \u5f88\u96be\u6709\u6548\u5730\u89e3\u51b3\u8fd9\u4e9b\u533b\u5b66 VQA \u95ee\u9898\u3002\u6b64\u5916\uff0c\u4ee4\u6211\u4eec\u60ca\u8bb6\u7684\u662f\uff0c\u533b\u5b66\u4e13\u7528\u7684 LVLM \u751a\u81f3\u8868\u73b0\u51fa\u6bd4\u90a3\u4e9b\u901a\u7528\u9886\u57df\u6a21\u578b\u5dee\u7684\u6027\u80fd\uff0c\u8fd9\u9700\u8981\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u63d0\u4f9b\u66f4\u901a\u7528\u3001\u66f4\u5f3a\u5927\u7684 LVLM\u3002\u8bc4\u4f30\u7ed3\u679c\u4e0d\u4ec5\u63ed\u793a\u4e86 LVLM \u76ee\u524d\u5728\u7406\u89e3\u771f\u5b9e\u533b\u5b66\u56fe\u50cf\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u800c\u4e14\u51f8\u663e\u4e86\u6211\u4eec\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002|[2402.09181v1](http://arxiv.org/pdf/2402.09181v1)|null|\n", "2402.09107": "|**2024-02-14**|**Headset: Human emotion awareness under partial occlusions multimodal dataset**|\u8033\u673a\uff1a\u90e8\u5206\u906e\u6321\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0b\u7684\u4eba\u7c7b\u60c5\u611f\u610f\u8bc6|Fatemeh Ghorbani Lohesara, Davi Rabbouni Freitas, Christine Guillemot, Karen Eguiazarian, Sebastian Knorr|The volumetric representation of human interactions is one of the fundamental domains in the development of immersive media productions and telecommunication applications. Particularly in the context of the rapid advancement of Extended Reality (XR) applications, this volumetric data has proven to be an essential technology for future XR elaboration. In this work, we present a new multimodal database to help advance the development of immersive technologies. Our proposed database provides ethically compliant and diverse volumetric data, in particular 27 participants displaying posed facial expressions and subtle body movements while speaking, plus 11 participants wearing head-mounted displays (HMDs). The recording system consists of a volumetric capture (VoCap) studio, including 31 synchronized modules with 62 RGB cameras and 31 depth cameras. In addition to textured meshes, point clouds, and multi-view RGB-D data, we use one Lytro Illum camera for providing light field (LF) data simultaneously. Finally, we also provide an evaluation of our dataset employment with regard to the tasks of facial expression classification, HMDs removal, and point cloud reconstruction. The dataset can be helpful in the evaluation and performance testing of various XR algorithms, including but not limited to facial expression recognition and reconstruction, facial reenactment, and volumetric video. HEADSET and its all associated raw data and license agreement will be publicly available for research purposes.|\u4eba\u7c7b\u4ea4\u4e92\u7684\u4f53\u79ef\u8868\u793a\u662f\u6c89\u6d78\u5f0f\u5a92\u4f53\u5236\u4f5c\u548c\u7535\u4fe1\u5e94\u7528\u5f00\u53d1\u7684\u57fa\u672c\u9886\u57df\u4e4b\u4e00\u3002\u7279\u522b\u662f\u5728\u6269\u5c55\u73b0\u5b9e (XR) \u5e94\u7528\u5feb\u901f\u53d1\u5c55\u7684\u80cc\u666f\u4e0b\uff0c\u8fd9\u79cd\u4f53\u79ef\u6570\u636e\u5df2\u88ab\u8bc1\u660e\u662f\u672a\u6765 XR \u9610\u8ff0\u7684\u4e00\u9879\u91cd\u8981\u6280\u672f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u5f0f\u6570\u636e\u5e93\uff0c\u4ee5\u5e2e\u52a9\u63a8\u8fdb\u6c89\u6d78\u5f0f\u6280\u672f\u7684\u53d1\u5c55\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6570\u636e\u5e93\u63d0\u4f9b\u4e86\u7b26\u5408\u9053\u5fb7\u89c4\u8303\u548c\u591a\u6837\u5316\u7684\u4f53\u79ef\u6570\u636e\uff0c\u7279\u522b\u662f 27 \u540d\u53c2\u4e0e\u8005\u5728\u8bf4\u8bdd\u65f6\u5c55\u793a\u4e86\u9762\u90e8\u8868\u60c5\u548c\u5fae\u5999\u7684\u8eab\u4f53\u52a8\u4f5c\uff0c\u4ee5\u53ca 11 \u540d\u4f69\u6234\u5934\u6234\u5f0f\u663e\u793a\u5668 (HMD) \u7684\u53c2\u4e0e\u8005\u3002\u8bb0\u5f55\u7cfb\u7edf\u7531\u4f53\u79ef\u6355\u6349 (VoCap) \u5de5\u4f5c\u5ba4\u7ec4\u6210\uff0c\u5305\u62ec 31 \u4e2a\u540c\u6b65\u6a21\u5757\u300162 \u4e2a RGB \u6444\u50cf\u5934\u548c 31 \u4e2a\u6df1\u5ea6\u6444\u50cf\u5934\u3002\u9664\u4e86\u7eb9\u7406\u7f51\u683c\u3001\u70b9\u4e91\u548c\u591a\u89c6\u56fe RGB-D \u6570\u636e\u5916\uff0c\u6211\u4eec\u8fd8\u4f7f\u7528\u4e00\u53f0 Lytro Illum \u76f8\u673a\u540c\u65f6\u63d0\u4f9b\u5149\u573a (LF) \u6570\u636e\u3002\u6700\u540e\uff0c\u6211\u4eec\u8fd8\u5bf9\u9762\u90e8\u8868\u60c5\u5206\u7c7b\u3001\u5934\u663e\u79fb\u9664\u548c\u70b9\u4e91\u91cd\u5efa\u4efb\u52a1\u7684\u6570\u636e\u96c6\u4f7f\u7528\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u8be5\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u5404\u79cd XR \u7b97\u6cd5\u7684\u8bc4\u4f30\u548c\u6027\u80fd\u6d4b\u8bd5\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u548c\u91cd\u5efa\u3001\u9762\u90e8\u91cd\u6f14\u548c\u4f53\u79ef\u89c6\u9891\u3002 HEADSET \u53ca\u5176\u6240\u6709\u76f8\u5173\u539f\u59cb\u6570\u636e\u548c\u8bb8\u53ef\u534f\u8bae\u5c06\u516c\u5f00\u7528\u4e8e\u7814\u7a76\u76ee\u7684\u3002|[2402.09107v1](http://arxiv.org/pdf/2402.09107v1)|null|\n", "2402.09055": "|**2024-02-14**|**Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection**|\u901a\u8fc7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u8fdb\u884c\u77ed\u89c6\u9891\u5e7d\u9ed8\u68c0\u6d4b\u7684\u8bc4\u8bba\u8f85\u52a9\u89c6\u9891\u8bed\u8a00\u5bf9\u9f50|Yang Liu, Tongfei Shen, Dong Zhang, Qingying Sun, Shoushan Li, Guodong Zhou|The growing importance of multi-modal humor detection within affective computing correlates with the expanding influence of short-form video sharing on social media platforms. In this paper, we propose a novel two-branch hierarchical model for short-form video humor detection (SVHD), named Comment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal contrastive pre-training. Notably, our CVLA not only operates on raw signals across various modal channels but also yields an appropriate multi-modal representation by aligning the video and language components within a consistent semantic space. The experimental results on two humor detection datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically outperforms state-of-the-art and several competitive baseline approaches. Our dataset, code and model release at https://github.com/yliu-cs/CVLA.|\u60c5\u611f\u8ba1\u7b97\u4e2d\u591a\u6a21\u5f0f\u5e7d\u9ed8\u68c0\u6d4b\u7684\u91cd\u8981\u6027\u4e0e\u65e5\u4ff1\u589e\uff0c\u8fd9\u4e0e\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u77ed\u89c6\u9891\u5171\u4eab\u7684\u5f71\u54cd\u529b\u4e0d\u65ad\u6269\u5927\u76f8\u5173\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u77ed\u89c6\u9891\u5e7d\u9ed8\u68c0\u6d4b\uff08SVHD\uff09\u7684\u65b0\u578b\u4e24\u5206\u652f\u5206\u5c42\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u591a\u6a21\u6001\u5bf9\u6bd4\u9884\u8bad\u7ec3\uff0c\u79f0\u4e3a\u8bc4\u8bba\u8f85\u52a9\u89c6\u9891\u8bed\u8a00\u5bf9\u9f50\uff08CVLA\uff09\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684 CVLA \u4e0d\u4ec5\u5bf9\u5404\u79cd\u6a21\u6001\u901a\u9053\u7684\u539f\u59cb\u4fe1\u53f7\u8fdb\u884c\u64cd\u4f5c\uff0c\u800c\u4e14\u8fd8\u901a\u8fc7\u5728\u4e00\u81f4\u7684\u8bed\u4e49\u7a7a\u95f4\u5185\u5bf9\u9f50\u89c6\u9891\u548c\u8bed\u8a00\u7ec4\u4ef6\u6765\u4ea7\u751f\u9002\u5f53\u7684\u591a\u6a21\u6001\u8868\u793a\u3002\u5305\u62ec DY11k \u548c UR-FUNNY \u5728\u5185\u7684\u4e24\u4e2a\u5e7d\u9ed8\u68c0\u6d4b\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCVLA \u7684\u6027\u80fd\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u548c\u51e0\u79cd\u6709\u7ade\u4e89\u529b\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u6a21\u578b\u53d1\u5e03\u4e8e https://github.com/yliu-cs/CVLA\u3002|[2402.09055v1](http://arxiv.org/pdf/2402.09055v1)|null|\n", "2402.09036": "|**2024-02-14**|**Can Text-to-image Model Assist Multi-modal Learning for Visual Recognition with Visual Modality Missing?**|\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u80fd\u5426\u8f85\u52a9\u89c6\u89c9\u6a21\u6001\u7f3a\u5931\u7684\u89c6\u89c9\u8bc6\u522b\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff1f|Tiantian Feng, Daniel Yang, Digbalay Bose, Shrikanth Narayanan|Multi-modal learning has emerged as an increasingly promising avenue in vision recognition, driving innovations across diverse domains ranging from media and education to healthcare and transportation. Despite its success, the robustness of multi-modal learning for visual recognition is often challenged by the unavailability of a subset of modalities, especially the visual modality. Conventional approaches to mitigate missing modalities in multi-modal learning rely heavily on algorithms and modality fusion schemes. In contrast, this paper explores the use of text-to-image models to assist multi-modal learning. Specifically, we propose a simple but effective multi-modal learning framework GTI-MM to enhance the data efficiency and model robustness against missing visual modality by imputing the missing data with generative transformers. Using multiple multi-modal datasets with visual recognition tasks, we present a comprehensive analysis of diverse conditions involving missing visual modality in data, including model training. Our findings reveal that synthetic images benefit training data efficiency with visual data missing in training and improve model robustness with visual data missing involving training and testing. Moreover, we demonstrate GTI-MM is effective with lower generation quantity and simple prompt techniques.|\u591a\u6a21\u5f0f\u5b66\u4e60\u5df2\u6210\u4e3a\u89c6\u89c9\u8bc6\u522b\u9886\u57df\u65e5\u76ca\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u63a8\u52a8\u4ece\u5a92\u4f53\u548c\u6559\u80b2\u5230\u533b\u7597\u4fdd\u5065\u548c\u4ea4\u901a\u7b49\u4e0d\u540c\u9886\u57df\u7684\u521b\u65b0\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u89c6\u89c9\u8bc6\u522b\u7684\u591a\u6a21\u6001\u5b66\u4e60\u7684\u7a33\u5065\u6027\u7ecf\u5e38\u53d7\u5230\u6a21\u6001\u5b50\u96c6\uff08\u5c24\u5176\u662f\u89c6\u89c9\u6a21\u6001\uff09\u4e0d\u53ef\u7528\u7684\u6311\u6218\u3002\u51cf\u5c11\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7f3a\u5931\u6a21\u6001\u7684\u4f20\u7edf\u65b9\u6cd5\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u7b97\u6cd5\u548c\u6a21\u6001\u878d\u5408\u65b9\u6848\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u6765\u8f85\u52a9\u591a\u6a21\u6001\u5b66\u4e60\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6 GTI-MM\uff0c\u901a\u8fc7\u4f7f\u7528\u751f\u6210\u53d8\u538b\u5668\u6765\u586b\u5145\u7f3a\u5931\u7684\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u9488\u5bf9\u7f3a\u5931\u89c6\u89c9\u6a21\u6001\u7684\u6a21\u578b\u9c81\u68d2\u6027\u3002\u4f7f\u7528\u5177\u6709\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u7684\u591a\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5bf9\u6d89\u53ca\u6570\u636e\u4e2d\u7f3a\u5931\u89c6\u89c9\u6a21\u6001\u7684\u5404\u79cd\u6761\u4ef6\uff08\u5305\u62ec\u6a21\u578b\u8bad\u7ec3\uff09\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8bad\u7ec3\u4e2d\u7f3a\u5c11\u89c6\u89c9\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5408\u6210\u56fe\u50cf\u6709\u5229\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u6548\u7387\uff0c\u5e76\u5728\u6d89\u53ca\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7684\u89c6\u89c9\u6570\u636e\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6a21\u578b\u7684\u7a33\u5065\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 GTI-MM \u5177\u6709\u8f83\u4f4e\u7684\u751f\u6210\u6570\u91cf\u548c\u7b80\u5355\u7684\u63d0\u793a\u6280\u672f\u7684\u6709\u6548\u6027\u3002|[2402.09036v1](http://arxiv.org/pdf/2402.09036v1)|null|\n", "2402.08987": "|**2024-02-14**|**Multi-modality transrectal ultrasound vudei classification for identification of clinically significant prostate cancer**|\u591a\u6a21\u6001\u7ecf\u76f4\u80a0\u8d85\u58f0 vudei \u5206\u7c7b\u7528\u4e8e\u8bc6\u522b\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u524d\u5217\u817a\u764c|Hong Wu, Juan Fu, Hongsheng Ye, Yuming Zhong, Xuebin Zhou, Jianhua Zhou, Yi Wang|Prostate cancer is the most common noncutaneous cancer in the world. Recently, multi-modality transrectal ultrasound (TRUS) has increasingly become an effective tool for the guidance of prostate biopsies. With the aim of effectively identifying prostate cancer, we propose a framework for the classification of clinically significant prostate cancer (csPCa) from multi-modality TRUS videos. The framework utilizes two 3D ResNet-50 models to extract features from B-mode images and shear wave elastography images, respectively. An adaptive spatial fusion module is introduced to aggregate two modalities' features. An orthogonal regularized loss is further used to mitigate feature redundancy. The proposed framework is evaluated on an in-house dataset containing 512 TRUS videos, and achieves favorable performance in identifying csPCa with an area under curve (AUC) of 0.84. Furthermore, the visualized class activation mapping (CAM) images generated from the proposed framework may provide valuable guidance for the localization of csPCa, thus facilitating the TRUS-guided targeted biopsy. Our code is publicly available at https://github.com/2313595986/ProstateTRUS.|\u524d\u5217\u817a\u764c\u662f\u4e16\u754c\u4e0a\u6700\u5e38\u89c1\u7684\u975e\u76ae\u80a4\u764c\u3002\u6700\u8fd1\uff0c\u591a\u6a21\u6001\u7ecf\u76f4\u80a0\u8d85\u58f0\uff08TRUS\uff09\u5df2\u65e5\u76ca\u6210\u4e3a\u6307\u5bfc\u524d\u5217\u817a\u6d3b\u68c0\u7684\u6709\u6548\u5de5\u5177\u3002\u4e3a\u4e86\u6709\u6548\u8bc6\u522b\u524d\u5217\u817a\u764c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u591a\u6a21\u6001 TRUS \u89c6\u9891\u4e2d\u5bf9\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u524d\u5217\u817a\u764c (csPCa) \u8fdb\u884c\u5206\u7c7b\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u4e24\u4e2a 3D ResNet-50 \u6a21\u578b\u5206\u522b\u4ece B \u6a21\u5f0f\u56fe\u50cf\u548c\u526a\u5207\u6ce2\u5f39\u6027\u6210\u50cf\u56fe\u50cf\u4e2d\u63d0\u53d6\u7279\u5f81\u3002\u5f15\u5165\u81ea\u9002\u5e94\u7a7a\u95f4\u878d\u5408\u6a21\u5757\u6765\u805a\u5408\u4e24\u79cd\u6a21\u6001\u7684\u7279\u5f81\u3002\u6b63\u4ea4\u6b63\u5219\u5316\u635f\u5931\u8fdb\u4e00\u6b65\u7528\u4e8e\u51cf\u8f7b\u7279\u5f81\u5197\u4f59\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5305\u542b 512 \u4e2a TRUS \u89c6\u9891\u7684\u5185\u200b\u200b\u90e8\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5728\u8bc6\u522b\u66f2\u7ebf\u4e0b\u9762\u79ef (AUC) \u4e3a 0.84 \u7684 csPCa \u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4ece\u6240\u63d0\u51fa\u7684\u6846\u67b6\u751f\u6210\u7684\u53ef\u89c6\u5316\u7c7b\u6fc0\u6d3b\u6620\u5c04\uff08CAM\uff09\u56fe\u50cf\u53ef\u4ee5\u4e3a csPCa \u7684\u5b9a\u4f4d\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6307\u5bfc\uff0c\u4ece\u800c\u4fc3\u8fdb TRUS \u5f15\u5bfc\u7684\u9776\u5411\u6d3b\u68c0\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/2313595986/ProstateTRUS \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2402.08987v1](http://arxiv.org/pdf/2402.08987v1)|**[link](https://github.com/2313595986/prostatetrus)**|\n", "2402.08966": "|**2024-02-14**|**Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays**|\u7eb5\u5411\u80f8\u90e8 X \u5c04\u7ebf\u5dee\u5f02\u89c6\u89c9\u95ee\u7b54\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b|Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin|Difference visual question answering (diff-VQA) is a challenging task that requires answering complex questions based on differences between a pair of images. This task is particularly important in reading chest X-ray images because radiologists often compare multiple images of the same patient taken at different times to track disease progression and changes in its severity in their clinical practice. However, previous works focused on designing specific network architectures for the diff-VQA task, missing opportunities to enhance the model's performance using a pretrained vision-language model (VLM). Here, we introduce a novel VLM called PLURAL, which is pretrained on natural and longitudinal chest X-ray data for the diff-VQA task. The model is developed using a step-by-step approach, starting with being pretrained on natural images and texts, followed by being trained using longitudinal chest X-ray data. The longitudinal data consist of pairs of X-ray images, along with question-answer sets and radiologist's reports that describe the changes in lung abnormalities and diseases over time. Our experimental results show that the PLURAL model outperforms state-of-the-art methods not only in diff-VQA for longitudinal X-rays but also in conventional VQA for a single X-ray image. Through extensive experiments, we demonstrate the effectiveness of the proposed VLM architecture and pretraining method in improving the model's performance.|\u5dee\u5f02\u89c6\u89c9\u95ee\u7b54\uff08diff-VQA\uff09\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u6839\u636e\u4e00\u5bf9\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u56de\u7b54\u590d\u6742\u7684\u95ee\u9898\u3002\u8fd9\u9879\u4efb\u52a1\u5728\u8bfb\u53d6\u80f8\u90e8 X \u5c04\u7ebf\u56fe\u50cf\u65f6\u5c24\u5176\u91cd\u8981\uff0c\u56e0\u4e3a\u653e\u5c04\u79d1\u533b\u751f\u7ecf\u5e38\u6bd4\u8f83\u540c\u4e00\u60a3\u8005\u5728\u4e0d\u540c\u65f6\u95f4\u62cd\u6444\u7684\u591a\u5f20\u56fe\u50cf\uff0c\u4ee5\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u8ddf\u8e2a\u75be\u75c5\u8fdb\u5c55\u53ca\u5176\u4e25\u91cd\u7a0b\u5ea6\u7684\u53d8\u5316\u3002\u7136\u800c\uff0c\u4e4b\u524d\u7684\u5de5\u4f5c\u91cd\u70b9\u662f\u4e3a diff-VQA \u4efb\u52a1\u8bbe\u8ba1\u7279\u5b9a\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u9519\u8fc7\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u589e\u5f3a\u6a21\u578b\u6027\u80fd\u7684\u673a\u4f1a\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a PLURAL \u7684\u65b0\u9896 VLM\uff0c\u5b83\u9488\u5bf9 diff-VQA \u4efb\u52a1\u5bf9\u81ea\u7136\u548c\u7eb5\u5411\u80f8\u90e8 X \u5c04\u7ebf\u6570\u636e\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002\u8be5\u6a21\u578b\u91c7\u7528\u5206\u6b65\u65b9\u6cd5\u5f00\u53d1\uff0c\u9996\u5148\u5bf9\u81ea\u7136\u56fe\u50cf\u548c\u6587\u672c\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u4f7f\u7528\u7eb5\u5411\u80f8\u90e8 X \u5c04\u7ebf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u7eb5\u5411\u6570\u636e\u7531\u6210\u5bf9\u7684 X \u5c04\u7ebf\u56fe\u50cf\u3001\u95ee\u9898\u89e3\u7b54\u96c6\u548c\u63cf\u8ff0\u80ba\u90e8\u5f02\u5e38\u548c\u75be\u75c5\u968f\u65f6\u95f4\u53d8\u5316\u7684\u653e\u5c04\u79d1\u533b\u751f\u62a5\u544a\u7ec4\u6210\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPLURAL \u6a21\u578b\u4e0d\u4ec5\u5728\u7eb5\u5411 X \u5c04\u7ebf\u7684 diff-VQA \u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u5355\u4e2a X \u5c04\u7ebf\u56fe\u50cf\u7684\u4f20\u7edf VQA \u4e2d\u4e5f\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 VLM \u67b6\u6784\u548c\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002|[2402.08966v1](http://arxiv.org/pdf/2402.08966v1)|null|\n", "2402.08919": "|**2024-02-14**|**Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding**|\u901a\u8fc7\u590d\u6742\u6027\u7ea6\u675f\u7684\u63cf\u8ff0\u6027\u81ea\u52a8\u7f16\u7801\u6765\u6d4b\u91cf\u6982\u5ff5\u76f8\u4f3c\u6027\u7684\u53ef\u89e3\u91ca\u6027|Alessandro Achille, Greg Ver Steeg, Tian Yu Liu, Matthew Trager, Carson Klingenberg, Stefano Soatto|Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to define and compute a notion of \"conceptual similarity\" among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate \"explanations\" (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two highly dissimilar images can be discriminated early in their description, whereas conceptually dissimilar ones will need more detail to be distinguished. We operationalize this definition and show that it correlates with subjective (averaged human evaluation) assessment, and beats existing baselines on both image-to-image and text-to-text similarity benchmarks. Beyond just providing a number, our method also offers interpretability by pointing to the specific level of granularity of the description where the source data are differentiated.|\u91cf\u5316\u56fe\u50cf\u4e4b\u95f4\u7684\u76f8\u4f3c\u7a0b\u5ea6\u662f\u57fa\u4e8e\u56fe\u50cf\u7684\u673a\u5668\u5b66\u4e60\u7684\u5173\u952e\u7248\u6743\u95ee\u9898\u3002\u7136\u800c\uff0c\u5728\u6cd5\u5f8b\u539f\u5219\u4e2d\uff0c\u786e\u5b9a\u4f5c\u54c1\u4e4b\u95f4\u7684\u76f8\u4f3c\u7a0b\u5ea6\u9700\u8981\u4e3b\u89c2\u5206\u6790\uff0c\u800c\u4e8b\u5b9e\u8ba4\u5b9a\u8005\uff08\u6cd5\u5b98\u548c\u966a\u5ba1\u56e2\uff09\u53ef\u4ee5\u8bc1\u660e\u8fd9\u4e9b\u4e3b\u89c2\u5224\u65ad\u5b58\u5728\u76f8\u5f53\u5927\u7684\u53ef\u53d8\u6027\u3002\u7ed3\u6784\u76f8\u4f3c\u7684\u56fe\u50cf\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e0d\u76f8\u4f3c\uff0c\u800c\u573a\u666f\u5b8c\u5168\u4e0d\u540c\u7684\u56fe\u50cf\u53ef\u4ee5\u88ab\u89c6\u4e3a\u8db3\u591f\u76f8\u4f3c\u4ee5\u652f\u6301\u590d\u5236\u7684\u4e3b\u5f20\u3002\u6211\u4eec\u8bd5\u56fe\u5b9a\u4e49\u548c\u8ba1\u7b97\u56fe\u50cf\u4e4b\u95f4\u7684\u201c\u6982\u5ff5\u76f8\u4f3c\u6027\u201d\u6982\u5ff5\uff0c\u5373\u4f7f\u5728\u4e0d\u5171\u4eab\u91cd\u590d\u5143\u7d20\u6216\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7ec4\u4ef6\u7684\u56fe\u50cf\u4e4b\u95f4\u4e5f\u80fd\u6355\u83b7\u9ad8\u7ea7\u5173\u7cfb\u3002\u8fd9\u4e2a\u60f3\u6cd5\u662f\u4f7f\u7528\u57fa\u672c\u7684\u591a\u6a21\u6001\u6a21\u578b\u6765\u751f\u6210\u590d\u6742\u7a0b\u5ea6\u4e0d\u65ad\u589e\u52a0\u7684\u89c6\u89c9\u6570\u636e\u7684\u201c\u89e3\u91ca\u201d\uff08\u6807\u9898\uff09\u3002\u7136\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7\u533a\u5206\u4e24\u4e2a\u56fe\u50cf\u6240\u9700\u7684\u6807\u9898\u957f\u5ea6\u6765\u8861\u91cf\u76f8\u4f3c\u6027\uff1a\u53ef\u4ee5\u5728\u63cf\u8ff0\u7684\u65e9\u671f\u533a\u5206\u4e24\u4e2a\u9ad8\u5ea6\u4e0d\u540c\u7684\u56fe\u50cf\uff0c\u800c\u6982\u5ff5\u4e0a\u4e0d\u540c\u7684\u56fe\u50cf\u5219\u9700\u8981\u66f4\u591a\u7ec6\u8282\u6765\u533a\u5206\u3002\u6211\u4eec\u5bf9\u8fd9\u4e2a\u5b9a\u4e49\u8fdb\u884c\u4e86\u64cd\u4f5c\uff0c\u5e76\u8868\u660e\u5b83\u4e0e\u4e3b\u89c2\uff08\u5e73\u5747\u4eba\u7c7b\u8bc4\u4f30\uff09\u8bc4\u4f30\u76f8\u5173\uff0c\u5e76\u4e14\u5728\u56fe\u50cf\u5230\u56fe\u50cf\u548c\u6587\u672c\u5230\u6587\u672c\u76f8\u4f3c\u6027\u57fa\u51c6\u4e0a\u90fd\u51fb\u8d25\u4e86\u73b0\u6709\u57fa\u7ebf\u3002\u9664\u4e86\u63d0\u4f9b\u6570\u5b57\u4e4b\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fd8\u901a\u8fc7\u6307\u51fa\u533a\u5206\u6e90\u6570\u636e\u7684\u63cf\u8ff0\u7684\u7279\u5b9a\u7c92\u5ea6\u7ea7\u522b\u6765\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002|[2402.08919v1](http://arxiv.org/pdf/2402.08919v1)|null|\n"}, "Nerf": {"2402.09325": "|**2024-02-14**|**PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames in Autonomous Driving Environments**|PC-NeRF\uff1a\u5728\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e2d\u4f7f\u7528\u7a00\u758f LiDAR \u5e27\u7684\u4eb2\u5b50\u795e\u7ecf\u8f90\u5c04\u573a|Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma|Large-scale 3D scene reconstruction and novel view synthesis are vital for autonomous vehicles, especially utilizing temporally sparse LiDAR frames. However, conventional explicit representations remain a significant bottleneck towards representing the reconstructed and synthetic scenes at unlimited resolution. Although the recently developed neural radiance fields (NeRF) have shown compelling results in implicit representations, the problem of large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR frames remains unexplored. To bridge this gap, we propose a 3D scene reconstruction and novel view synthesis framework called parent-child neural radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF, the framework implements hierarchical spatial partitioning and multi-level scene representation, including scene, segment, and point levels. The multi-level scene representation enhances the efficient utilization of sparse LiDAR point cloud data and enables the rapid acquisition of an approximate volumetric scene representation. With extensive experiments, PC-NeRF is proven to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in large-scale scenes. Moreover, PC-NeRF can effectively handle situations with sparse LiDAR frames and demonstrate high deployment efficiency with limited training epochs. Our approach implementation and the pre-trained models are available at https://github.com/biter0088/pc-nerf.|\u5927\u89c4\u6a21 3D \u573a\u666f\u91cd\u5efa\u548c\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5229\u7528\u65f6\u95f4\u7a00\u758f\u7684 LiDAR \u5e27\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u663e\u5f0f\u8868\u793a\u4ecd\u7136\u662f\u4ee5\u65e0\u9650\u5206\u8fa8\u7387\u8868\u793a\u91cd\u5efa\u548c\u5408\u6210\u573a\u666f\u7684\u91cd\u5927\u74f6\u9888\u3002\u5c3d\u7ba1\u6700\u8fd1\u5f00\u53d1\u7684\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u5728\u9690\u5f0f\u8868\u793a\u65b9\u9762\u663e\u793a\u51fa\u4e86\u5f15\u4eba\u6ce8\u76ee\u7684\u7ed3\u679c\uff0c\u4f46\u4f7f\u7528\u7a00\u758f LiDAR \u5e27\u8fdb\u884c\u5927\u89c4\u6a21 3D \u573a\u666f\u91cd\u5efa\u548c\u65b0\u9896\u89c6\u56fe\u5408\u6210\u7684\u95ee\u9898\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd 3D \u573a\u666f\u91cd\u5efa\u548c\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u6846\u67b6\uff0c\u79f0\u4e3a\u7236\u5b50\u795e\u7ecf\u8f90\u5c04\u573a (PC-NeRF)\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u7236NeRF\u548c\u5b50NeRF\u4e24\u4e2a\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5c42\u6b21\u7a7a\u95f4\u5206\u533a\u548c\u591a\u5c42\u6b21\u573a\u666f\u8868\u793a\uff0c\u5305\u62ec\u573a\u666f\u3001\u7247\u6bb5\u548c\u70b9\u7ea7\u522b\u3002\u591a\u7ea7\u573a\u666f\u8868\u793a\u589e\u5f3a\u4e86\u7a00\u758f\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u6570\u636e\u7684\u6709\u6548\u5229\u7528\uff0c\u5e76\u80fd\u591f\u5feb\u901f\u83b7\u53d6\u8fd1\u4f3c\u4f53\u79ef\u573a\u666f\u8868\u793a\u3002\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cPC-NeRF\u88ab\u8bc1\u660e\u53ef\u4ee5\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u65b0\u9896\u7684LiDAR\u89c6\u56fe\u5408\u6210\u548c3D\u91cd\u5efa\u3002\u6b64\u5916\uff0cPC-NeRF \u53ef\u4ee5\u6709\u6548\u5904\u7406\u7a00\u758f LiDAR \u5e27\u7684\u60c5\u51b5\uff0c\u5e76\u5728\u6709\u9650\u7684\u8bad\u7ec3\u5468\u671f\u5185\u5c55\u73b0\u51fa\u8f83\u9ad8\u7684\u90e8\u7f72\u6548\u7387\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728 https://github.com/biter0088/pc-nerf \u4e0a\u627e\u5230\u3002|[2402.09325v1](http://arxiv.org/pdf/2402.09325v1)|**[link](https://github.com/biter0088/pc-nerf)**|\n"}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.09359": "|**2024-02-14**|**Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D Ultrasound Localization Microscopy**|\u4fee\u526a\u7a00\u758f\u5f20\u91cf\u795e\u7ecf\u7f51\u7edc\u652f\u6301 3D \u8d85\u58f0\u5b9a\u4f4d\u663e\u5fae\u955c\u7684\u6df1\u5ea6\u5b66\u4e60|Brice Rauby, Paul Xing, Jonathan Por\u00e9e, Maxime Gasse, Jean Provost|Ultrasound Localization Microscopy (ULM) is a non-invasive technique that allows for the imaging of micro-vessels in vivo, at depth and with a resolution on the order of ten microns. ULM is based on the sub-resolution localization of individual microbubbles injected in the bloodstream. Mapping the whole angioarchitecture requires the accumulation of microbubbles trajectories from thousands of frames, typically acquired over a few minutes. ULM acquisition times can be reduced by increasing the microbubble concentration, but requires more advanced algorithms to detect them individually. Several deep learning approaches have been proposed for this task, but they remain limited to 2D imaging, in part due to the associated large memory requirements. Herein, we propose to use sparse tensor neural networks to reduce memory usage in 2D and to improve the scaling of the memory requirement for the extension of deep learning architecture to 3D. We study several approaches to efficiently convert ultrasound data into a sparse format and study the impact of the associated loss of information. When applied in 2D, the sparse formulation reduces the memory requirements by a factor 2 at the cost of a small reduction of performance when compared against dense networks. In 3D, the proposed approach reduces memory requirements by two order of magnitude while largely outperforming conventional ULM in high concentration settings. We show that Sparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense deep learning based method in 2D ULM i.e. the use of higher concentration in silico and reduced acquisition time.|\u8d85\u58f0\u5b9a\u4f4d\u663e\u5fae\u955c (ULM) \u662f\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u6280\u672f\uff0c\u53ef\u5bf9\u4f53\u5185\u5fae\u8840\u7ba1\u8fdb\u884c\u6df1\u5ea6\u6210\u50cf\uff0c\u5206\u8fa8\u7387\u7ea6\u4e3a 10 \u5fae\u7c73\u3002 ULM \u57fa\u4e8e\u6ce8\u5c04\u5230\u8840\u6d41\u4e2d\u7684\u5355\u4e2a\u5fae\u6ce1\u7684\u4e9a\u5206\u8fa8\u7387\u5b9a\u4f4d\u3002\u7ed8\u5236\u6574\u4e2a\u8840\u7ba1\u7ed3\u6784\u9700\u8981\u4ece\u6570\u5343\u5e27\u4e2d\u79ef\u7d2f\u5fae\u6ce1\u8f68\u8ff9\uff0c\u901a\u5e38\u9700\u8981\u51e0\u5206\u949f\u7684\u65f6\u95f4\u624d\u80fd\u83b7\u5f97\u3002\u901a\u8fc7\u589e\u52a0\u5fae\u6ce1\u6d53\u5ea6\u53ef\u4ee5\u51cf\u5c11 ULM \u91c7\u96c6\u65f6\u95f4\uff0c\u4f46\u9700\u8981\u66f4\u5148\u8fdb\u7684\u7b97\u6cd5\u6765\u5355\u72ec\u68c0\u6d4b\u5b83\u4eec\u3002\u5df2\u7ecf\u9488\u5bf9\u6b64\u4efb\u52a1\u63d0\u51fa\u4e86\u51e0\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u4ec5\u9650\u4e8e 2D \u6210\u50cf\uff0c\u90e8\u5206\u539f\u56e0\u662f\u76f8\u5173\u7684\u5927\u5185\u5b58\u9700\u6c42\u3002\u5728\u6b64\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u7a00\u758f\u5f20\u91cf\u795e\u7ecf\u7f51\u7edc\u6765\u51cf\u5c11 2D \u4e2d\u7684\u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u5e76\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u6269\u5c55\u5230 3D \u65f6\u7684\u5185\u5b58\u9700\u6c42\u89c4\u6a21\u3002\u6211\u4eec\u7814\u7a76\u4e86\u51e0\u79cd\u6709\u6548\u5730\u5c06\u8d85\u58f0\u6570\u636e\u8f6c\u6362\u4e3a\u7a00\u758f\u683c\u5f0f\u7684\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u76f8\u5173\u4fe1\u606f\u4e22\u5931\u7684\u5f71\u54cd\u3002\u5f53\u5e94\u7528\u4e8e 2D \u65f6\uff0c\u4e0e\u5bc6\u96c6\u7f51\u7edc\u76f8\u6bd4\uff0c\u7a00\u758f\u516c\u5f0f\u5c06\u5185\u5b58\u9700\u6c42\u51cf\u5c11\u4e86 2 \u500d\uff0c\u4f46\u4ee3\u4ef7\u662f\u6027\u80fd\u7565\u6709\u4e0b\u964d\u3002\u5728 3D \u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06\u5185\u5b58\u9700\u6c42\u964d\u4f4e\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u5728\u9ad8\u6d53\u5ea6\u8bbe\u7f6e\u4e2d\u5927\u5927\u4f18\u4e8e\u4f20\u7edf\u7684 ULM\u3002\u6211\u4eec\u8868\u660e\uff0c3D ULM \u4e2d\u7684\u7a00\u758f\u5f20\u91cf\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u4e0e 2D ULM \u4e2d\u57fa\u4e8e\u5bc6\u96c6\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u76f8\u540c\u7684\u4f18\u70b9\uff0c\u5373\u5728\u8ba1\u7b97\u673a\u4e2d\u4f7f\u7528\u66f4\u9ad8\u7684\u6d53\u5ea6\u5e76\u51cf\u5c11\u91c7\u96c6\u65f6\u95f4\u3002|[2402.09359v1](http://arxiv.org/pdf/2402.09359v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.09372": "|**2024-02-14**|**Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge**|RibFrac \u6311\u6218\u8d5b\u4e2d CT \u7684\u6df1\u90e8\u808b\u9aa8\u9aa8\u6298\u5b9e\u4f8b\u5206\u5272\u548c\u5206\u7c7b|Jiancheng Yang, Rui Shi, Liang Jin, Xiaoyang Huang, Kaiming Kuang, Donglai Wei, Shixuan Gu, Jianying Liu, Pengfei Liu, Zhizhong Chai, et.al.|Rib fractures are a common and potentially severe injury that can be challenging and labor-intensive to detect in CT scans. While there have been efforts to address this field, the lack of large-scale annotated datasets and evaluation benchmarks has hindered the development and validation of deep learning algorithms. To address this issue, the RibFrac Challenge was introduced, providing a benchmark dataset of over 5,000 rib fractures from 660 CT scans, with voxel-level instance mask annotations and diagnosis labels for four clinical categories (buckle, nondisplaced, displaced, or segmental). The challenge includes two tracks: a detection (instance segmentation) track evaluated by an FROC-style metric and a classification track evaluated by an F1-style metric. During the MICCAI 2020 challenge period, 243 results were evaluated, and seven teams were invited to participate in the challenge summary. The analysis revealed that several top rib fracture detection solutions achieved performance comparable or even better than human experts. Nevertheless, the current rib fracture classification solutions are hardly clinically applicable, which can be an interesting area in the future. As an active benchmark and research resource, the data and online evaluation of the RibFrac Challenge are available at the challenge website. As an independent contribution, we have also extended our previous internal baseline by incorporating recent advancements in large-scale pretrained networks and point-based rib segmentation techniques. The resulting FracNet+ demonstrates competitive performance in rib fracture detection, which lays a foundation for further research and development in AI-assisted rib fracture detection and diagnosis.|\u808b\u9aa8\u9aa8\u6298\u662f\u4e00\u79cd\u5e38\u89c1\u4e14\u53ef\u80fd\u4e25\u91cd\u7684\u635f\u4f24\uff0c\u5728 CT \u626b\u63cf\u4e2d\u68c0\u6d4b\u8d77\u6765\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\u4e14\u8d39\u529b\u3002\u5c3d\u7ba1\u4eba\u4eec\u4e00\u76f4\u5728\u52aa\u529b\u89e3\u51b3\u8fd9\u4e00\u9886\u57df\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u6ce8\u91ca\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\u963b\u788d\u4e86\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u9a8c\u8bc1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5f15\u5165\u4e86 RibFrac \u6311\u6218\u8d5b\uff0c\u5b83\u63d0\u4f9b\u4e86\u6765\u81ea 660 \u6b21 CT \u626b\u63cf\u7684 5,000 \u591a\u4f8b\u808b\u9aa8\u9aa8\u6298\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u56db\u79cd\u4e34\u5e8a\u7c7b\u522b\uff08\u5e26\u72b6\u3001\u975e\u79fb\u4f4d\u3001\u79fb\u4f4d\u6216\u8282\u6bb5\u6027\uff09\u7684\u4f53\u7d20\u7ea7\u5b9e\u4f8b\u63a9\u6a21\u6ce8\u91ca\u548c\u8bca\u65ad\u6807\u7b7e\u3002\u8be5\u6311\u6218\u5305\u62ec\u4e24\u4e2a\u8f68\u9053\uff1a\u7531 FROC \u5f0f\u5ea6\u91cf\u8bc4\u4f30\u7684\u68c0\u6d4b\uff08\u5b9e\u4f8b\u5206\u5272\uff09\u8f68\u9053\u548c\u7531 F1 \u5f0f\u5ea6\u91cf\u8bc4\u4f30\u7684\u5206\u7c7b\u8f68\u9053\u3002 MICCAI 2020\u6311\u6218\u8d5b\u671f\u95f4\uff0c\u8bc4\u4f30\u4e86243\u4e2a\u7ed3\u679c\uff0c\u5e76\u9080\u8bf7\u4e867\u652f\u961f\u4f0d\u53c2\u52a0\u6311\u6218\u603b\u7ed3\u3002\u5206\u6790\u663e\u793a\uff0c\u51e0\u79cd\u9876\u7ea7\u808b\u9aa8\u9aa8\u6298\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u808b\u9aa8\u9aa8\u6298\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\u5f88\u96be\u5e94\u7528\u4e8e\u4e34\u5e8a\uff0c\u8fd9\u53ef\u80fd\u662f\u672a\u6765\u4e00\u4e2a\u6709\u8da3\u7684\u9886\u57df\u3002\u4f5c\u4e3a\u6d3b\u8dc3\u7684\u57fa\u51c6\u548c\u7814\u7a76\u8d44\u6e90\uff0cRibFrac \u6311\u6218\u8d5b\u7684\u6570\u636e\u548c\u5728\u7ebf\u8bc4\u4f30\u53ef\u5728\u6311\u6218\u8d5b\u7f51\u7ad9\u4e0a\u83b7\u53d6\u3002\u4f5c\u4e3a\u4e00\u9879\u72ec\u7acb\u8d21\u732e\uff0c\u6211\u4eec\u8fd8\u901a\u8fc7\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7f51\u7edc\u548c\u57fa\u4e8e\u70b9\u7684\u808b\u9aa8\u5206\u5272\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6269\u5c55\u4e86\u4e4b\u524d\u7684\u5185\u90e8\u57fa\u7ebf\u3002\u7531\u6b64\u4ea7\u751f\u7684FracNet+\u5728\u808b\u9aa8\u9aa8\u6298\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u8fd9\u4e3a\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u808b\u9aa8\u9aa8\u6298\u68c0\u6d4b\u548c\u8bca\u65ad\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002|[2402.09372v1](http://arxiv.org/pdf/2402.09372v1)|null|\n", "2402.09329": "|**2024-02-14**|**YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture Detection**|YOLOv8-AM\uff1a\u5e26\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684 YOLOv8\uff0c\u7528\u4e8e\u513f\u7ae5\u624b\u8155\u9aa8\u6298\u68c0\u6d4b|Chun-Tse Chien, Rui-Yang Ju, Kuang-Yi Chou, Chien-Sheng Lin, Jen-Shiun Chiang|Wrist trauma and even fractures occur frequently in daily life, particularly among children who account for a significant proportion of fracture cases. Before performing surgery, surgeons often request patients to undergo X-ray imaging first and prepare for it based on the analysis of the radiologist. With the development of neural networks, You Only Look Once (YOLO) series models have been widely used in fracture detection as computer-assisted diagnosis (CAD). In 2023, Ultralytics presented the latest version of the YOLO models, which has been employed for detecting fractures across various parts of the body. Attention mechanism is one of the hottest methods to improve the model performance. This research work proposes YOLOv8-AM, which incorporates the attention mechanism into the original YOLOv8 architecture. Specifically, we respectively employ four attention modules, Convolutional Block Attention Module (CBAM), Global Attention Mechanism (GAM), Efficient Channel Attention (ECA), and Shuffle Attention (SA), to design the improved models and train them on GRAZPEDWRI-DX dataset. Experimental results demonstrate that the mean Average Precision at IoU 50 (mAP 50) of the YOLOv8-AM model based on ResBlock + CBAM (ResCBAM) increased from 63.6% to 65.8%, which achieves the state-of-the-art (SOTA) performance. Conversely, YOLOv8-AM model incorporating GAM obtains the mAP 50 value of 64.2%, which is not a satisfactory enhancement. Therefore, we combine ResBlock and GAM, introducing ResGAM to design another new YOLOv8-AM model, whose mAP 50 value is increased to 65.0%.|\u65e5\u5e38\u751f\u6d3b\u4e2d\u624b\u8155\u5916\u4f24\u751a\u81f3\u9aa8\u6298\u7684\u60c5\u51b5\u65f6\u6709\u53d1\u751f\uff0c\u5c24\u5176\u662f\u513f\u7ae5\uff0c\u9aa8\u6298\u75c5\u4f8b\u4e2d\u6240\u5360\u6bd4\u4f8b\u76f8\u5f53\u5927\u3002\u5728\u8fdb\u884c\u624b\u672f\u4e4b\u524d\uff0c\u5916\u79d1\u533b\u751f\u7ecf\u5e38\u8981\u6c42\u60a3\u8005\u5148\u8fdb\u884cX\u5c04\u7ebf\u6210\u50cf\uff0c\u5e76\u6839\u636e\u653e\u5c04\u79d1\u533b\u751f\u7684\u5206\u6790\u505a\u597d\u51c6\u5907\u3002\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u5c55\uff0cYOLO\u7cfb\u5217\u6a21\u578b\u5df2\u4f5c\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\uff08CAD\uff09\u5e7f\u6cdb\u5e94\u7528\u4e8e\u9aa8\u6298\u68c0\u6d4b\u3002 2023 \u5e74\uff0cUltralytics \u63a8\u51fa\u4e86\u6700\u65b0\u7248\u672c\u7684 YOLO \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5df2\u7528\u4e8e\u68c0\u6d4b\u8eab\u4f53\u5404\u4e2a\u90e8\u4f4d\u7684\u9aa8\u6298\u3002\u6ce8\u610f\u529b\u673a\u5236\u662f\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u6700\u70ed\u95e8\u7684\u65b9\u6cd5\u4e4b\u4e00\u3002\u8fd9\u9879\u7814\u7a76\u5de5\u4f5c\u63d0\u51fa\u4e86YOLOv8-AM\uff0c\u5b83\u5c06\u6ce8\u610f\u529b\u673a\u5236\u878d\u5165\u5230\u539f\u59cb\u7684YOLOv8\u67b6\u6784\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5206\u522b\u91c7\u7528\u56db\u4e2a\u6ce8\u610f\u529b\u6a21\u5757\uff1a\u5377\u79ef\u5757\u6ce8\u610f\u529b\u6a21\u5757\uff08CBAM\uff09\u3001\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff08GAM\uff09\u3001\u9ad8\u6548\u901a\u9053\u6ce8\u610f\u529b\uff08ECA\uff09\u548c\u968f\u673a\u6ce8\u610f\u529b\uff08SA\uff09\u6765\u8bbe\u8ba1\u6539\u8fdb\u6a21\u578b\u5e76\u5728GRAZPEDWRI-DX\u4e0a\u8fdb\u884c\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eResBlock + CBAM\uff08ResCBAM\uff09\u7684YOLOv8-AM\u6a21\u578b\u5728IoU 50\uff08mAP 50\uff09\u4e0b\u7684\u5e73\u5747\u7cbe\u5ea6\u4ece63.6%\u63d0\u9ad8\u523065.8%\uff0c\u8fbe\u5230\u4e86state-of-the-art\uff08SOTA\uff09\u8868\u73b0\u3002\u76f8\u53cd\uff0c\u7ed3\u5408GAM\u7684YOLOv8-AM\u6a21\u578b\u83b7\u5f97\u4e8664.2%\u7684mAP 50\u503c\uff0c\u8fd9\u5e76\u4e0d\u662f\u4e00\u4e2a\u4ee4\u4eba\u6ee1\u610f\u7684\u589e\u5f3a\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06ResBlock\u548cGAM\u7ed3\u5408\u8d77\u6765\uff0c\u5f15\u5165ResGAM\u8bbe\u8ba1\u53e6\u4e00\u4e2a\u65b0\u7684YOLOv8-AM\u6a21\u578b\uff0c\u5176mAP 50\u503c\u63d0\u9ad8\u523065.0%\u3002|[2402.09329v1](http://arxiv.org/pdf/2402.09329v1)|null|\n", "2402.09316": "|**2024-02-14**|**Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models**|\u53ea\u6709\u6211\u7684\u6a21\u578b\u5728\u6211\u7684\u6570\u636e\u4e0a\uff1a\u4e00\u79cd\u4fdd\u62a4\u4e00\u4e2a\u6a21\u578b\u5e76\u6b3a\u9a97\u672a\u7ecf\u6388\u6743\u7684\u9ed1\u76d2\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5|Weiheng Chai, Brian Testa, Huantao Ren, Asif Salekin, Senem Velipasalar|Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical. Image data, if not protected, can be exploited to infer personal or contextual information. Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans. Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation. This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps. The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for identity classification, and AffectNet, for emotion classification. Our results show that the generated images can successfully maintain the accuracy of a protected model and degrade the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and 55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.|\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e7f\u6cdb\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u4eba\u8138\u8bc6\u522b\u548c\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\uff0c\u5176\u4e2d\u9690\u79c1\u548c\u6570\u636e\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\u3002\u56fe\u50cf\u6570\u636e\u5982\u679c\u4e0d\u53d7\u4fdd\u62a4\uff0c\u53ef\u80fd\u4f1a\u88ab\u7528\u6765\u63a8\u65ad\u4e2a\u4eba\u6216\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u73b0\u6709\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff08\u4f8b\u5982\u52a0\u5bc6\uff09\u4f1a\u751f\u6210\u751a\u81f3\u4eba\u7c7b\u90fd\u65e0\u6cd5\u8bc6\u522b\u7684\u6270\u52a8\u56fe\u50cf\u3002\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\u751a\u81f3\u7981\u6b62\u6388\u6743\u5229\u76ca\u76f8\u5173\u8005\u8fdb\u884c\u81ea\u52a8\u63a8\u7406\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5546\u4e1a\u548c\u5e7f\u6cdb\u9002\u5e94\u7684\u5b9e\u9645\u6fc0\u52b1\u3002\u8fd9\u9879\u5f00\u521b\u6027\u7684\u7814\u7a76\u901a\u8fc7\u751f\u6210\u4eba\u7c7b\u53ef\u611f\u77e5\u7684\u56fe\u50cf\u6765\u89e3\u51b3\u672a\u7ecf\u63a2\u7d22\u7684\u5b9e\u9645\u9690\u79c1\u4fdd\u62a4\u7528\u4f8b\uff0c\u8fd9\u4e9b\u56fe\u50cf\u4fdd\u6301\u6388\u6743\u6a21\u578b\u7684\u51c6\u786e\u63a8\u7406\uff0c\u540c\u65f6\u907f\u5f00\u5176\u4ed6\u5177\u6709\u76f8\u4f3c\u6216\u4e0d\u540c\u76ee\u6807\u7684\u672a\u7ecf\u6388\u6743\u7684\u9ed1\u76d2\u6a21\u578b\uff0c\u5e76\u89e3\u51b3\u4e86\u4e4b\u524d\u7684\u7814\u7a76\u7a7a\u767d\u3002\u4f7f\u7528\u7684\u6570\u636e\u96c6\u662f\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u7684 ImageNet\u3001\u7528\u4e8e\u8eab\u4efd\u5206\u7c7b\u7684 Celeba-HQ \u6570\u636e\u96c6\u548c\u7528\u4e8e\u60c5\u611f\u5206\u7c7b\u7684 AffectNet\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u7684\u56fe\u50cf\u53ef\u4ee5\u6210\u529f\u4fdd\u6301\u53d7\u4fdd\u62a4\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5c06\u672a\u7ecf\u6388\u6743\u7684\u9ed1\u76d2\u6a21\u578b\u7684\u5e73\u5747\u51c6\u786e\u6027\u5728 ImageNet\u3001Celeba-HQ \u548c AffectNet \u6570\u636e\u96c6\u4e0a\u5206\u522b\u964d\u4f4e\u81f3 11.97%\u30016.63% \u548c 55.51% \u3002|[2402.09316v1](http://arxiv.org/pdf/2402.09316v1)|null|\n", "2402.09315": "|**2024-02-14**|**Few-Shot Object Detection with Sparse Context Transformers**|\u4f7f\u7528\u7a00\u758f\u4e0a\u4e0b\u6587\u8f6c\u6362\u5668\u8fdb\u884c\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b|Jie Mei, Mingyuan Jiu, Hichem Sahbi, Xiaoheng Jiang, Mingliang Xu|Few-shot detection is a major task in pattern recognition which seeks to localize objects using models trained with few labeled data. One of the mainstream few-shot methods is transfer learning which consists in pretraining a detection model in a source domain prior to its fine-tuning in a target domain. However, it is challenging for fine-tuned models to effectively identify new classes in the target domain, particularly when the underlying labeled training data are scarce. In this paper, we devise a novel sparse context transformer (SCT) that effectively leverages object knowledge in the source domain, and automatically learns a sparse context from only few training images in the target domain. As a result, it combines different relevant clues in order to enhance the discrimination power of the learned detectors and reduce class confusion. We evaluate the proposed method on two challenging few-shot object detection benchmarks, and empirical results show that the proposed method obtains competitive performance compared to the related state-of-the-art.|\u5c11\u955c\u5934\u68c0\u6d4b\u662f\u6a21\u5f0f\u8bc6\u522b\u4e2d\u7684\u4e00\u9879\u4e3b\u8981\u4efb\u52a1\uff0c\u65e8\u5728\u4f7f\u7528\u7528\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u5b9a\u4f4d\u5bf9\u8c61\u3002\u4e3b\u6d41\u7684\u5c0f\u6837\u672c\u65b9\u6cd5\u4e4b\u4e00\u662f\u8fc1\u79fb\u5b66\u4e60\uff0c\u5b83\u5305\u62ec\u5728\u76ee\u6807\u57df\u4e2d\u5fae\u8c03\u68c0\u6d4b\u6a21\u578b\u4e4b\u524d\u5728\u6e90\u57df\u4e2d\u9884\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b\u3002\u7136\u800c\uff0c\u5fae\u8c03\u6a21\u578b\u6709\u6548\u8bc6\u522b\u76ee\u6807\u57df\u4e2d\u7684\u65b0\u7c7b\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5f53\u5e95\u5c42\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u65f6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a00\u758f\u4e0a\u4e0b\u6587\u53d8\u6362\u5668\uff08SCT\uff09\uff0c\u5b83\u6709\u6548\u5730\u5229\u7528\u6e90\u57df\u4e2d\u7684\u5bf9\u8c61\u77e5\u8bc6\uff0c\u5e76\u4ece\u76ee\u6807\u57df\u4e2d\u7684\u5c11\u91cf\u8bad\u7ec3\u56fe\u50cf\u4e2d\u81ea\u52a8\u5b66\u4e60\u7a00\u758f\u4e0a\u4e0b\u6587\u3002\u56e0\u6b64\uff0c\u5b83\u7ed3\u5408\u4e86\u4e0d\u540c\u7684\u76f8\u5173\u7ebf\u7d22\uff0c\u4ee5\u589e\u5f3a\u5b66\u4e60\u68c0\u6d4b\u5668\u7684\u8fa8\u522b\u80fd\u529b\u5e76\u51cf\u5c11\u7c7b\u522b\u6df7\u4e71\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u76f8\u5173\u7684\u6700\u65b0\u6280\u672f\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u83b7\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002|[2402.09315v1](http://arxiv.org/pdf/2402.09315v1)|null|\n", "2402.09303": "|**2024-02-14**|**Immediate generalisation in humans but a generalisation lag in deep neural networks$\\unicode{x2014}$evidence for representational divergence?**|\u5728\u4eba\u7c7b\u4e2d\u53ef\u4ee5\u7acb\u5373\u6cdb\u5316\uff0c\u4f46\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u6cdb\u5316\u6ede\u540e$\\unicode{x2014}$\u8868\u5f81\u5206\u6b67\u7684\u8bc1\u636e\uff1f|Lukas S. Huber, Fred W. Mast, Felix A. Wichmann|Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate and compare how well learned representations can be generalized to previously unseen test data.   Our findings indicate that in terms of absolute classification performance DNNs demonstrate a level of data efficiency comparable to$\\unicode{x2014}$and sometimes even exceeding that$\\unicode{x2014}$of human learners, challenging some prevailing assumptions in the field. However, comparisons across the entire learning process reveal significant representational differences: while DNNs' learning is characterized by a pronounced generalisation lag, humans appear to immediately acquire generalizable representations without a preliminary phase of learning training set-specific information that is only later transferred to novel data.|\u6700\u8fd1\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u9886\u57df\uff0c\u4eba\u7c7b\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u4e4b\u95f4\u5b58\u5728\u8bb8\u591a\u884c\u4e3a\u6bd4\u8f83\u3002\u901a\u5e38\uff0c\u6bd4\u8f83\u7814\u7a76\u901a\u8fc7\u6d4b\u91cf\u548c\u6bd4\u8f83\u5bf9\u8c61\u7c7b\u522b\u5f62\u6210\u540e\u7684\u8868\u793a\u7684\u76f8\u4f3c\u6027\u6765\u5173\u6ce8\u5b66\u4e60\u8fc7\u7a0b\u7684\u6700\u7ec8\u7ed3\u679c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u8868\u5f81\u5982\u4f55\u51fa\u73b0\u7684\u8fc7\u7a0b$\\unicode{x2014}$\uff0c\u5373\u5728\u83b7\u53d6$\\unicode{x2014}$\u8fc7\u7a0b\u4e2d\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u53d8\u5316\u548c\u4e2d\u95f4\u9636\u6bb5\uff0c\u5f88\u5c11\u8fdb\u884c\u76f4\u63a5\u548c\u7ecf\u9a8c\u6027\u7684\u6bd4\u8f83\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u62a5\u544a\u4e86\u5173\u4e8e\u5982\u4f55\u5728\u4eba\u7c7b\u89c2\u5bdf\u8005\u548c\u5404\u79cd\u7ecf\u5178\u548c\u6700\u5148\u8fdb\u7684 DNN \u4e2d\u83b7\u53d6\u53ef\u8f6c\u79fb\u8868\u5f81\u7684\u8be6\u7ec6\u7814\u7a76\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u53d7\u7ea6\u675f\u7684\u76d1\u7763\u5b66\u4e60\u73af\u5883\uff0c\u5728\u5176\u4e2d\u8c03\u6574\u4e0e\u5b66\u4e60\u76f8\u5173\u7684\u53c2\u6570\uff0c\u4f8b\u5982\u8d77\u70b9\u3001\u8f93\u5165\u6a21\u5f0f\u3001\u53ef\u7528\u8f93\u5165\u6570\u636e\u548c\u63d0\u4f9b\u7684\u53cd\u9988\u3002\u5728\u6574\u4e2a\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u8bc4\u4f30\u548c\u6bd4\u8f83\u5982\u4f55\u5c06\u5b66\u4e60\u5230\u7684\u8868\u5f81\u63a8\u5e7f\u5230\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u6d4b\u8bd5\u6570\u636e\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c31\u7edd\u5bf9\u5206\u7c7b\u6027\u80fd\u800c\u8a00\uff0cDNN \u8868\u73b0\u51fa\u7684\u6570\u636e\u6548\u7387\u6c34\u5e73\u4e0e$\\unicode{x2014}$\u76f8\u5f53\uff0c\u6709\u65f6\u751a\u81f3\u8d85\u8fc7\u4e86$\\unicode{x2014}$\u4eba\u7c7b\u5b66\u4e60\u8005\uff0c\u6311\u6218\u4e86\u8be5\u9886\u57df\u7684\u4e00\u4e9b\u666e\u904d\u5047\u8bbe\u3002\u7136\u800c\uff0c\u6574\u4e2a\u5b66\u4e60\u8fc7\u7a0b\u7684\u6bd4\u8f83\u63ed\u793a\u4e86\u663e\u7740\u7684\u8868\u5f81\u5dee\u5f02\uff1a\u867d\u7136 DNN \u7684\u5b66\u4e60\u5177\u6709\u660e\u663e\u7684\u6cdb\u5316\u6ede\u540e\u7684\u7279\u70b9\uff0c\u4f46\u4eba\u7c7b\u4f3c\u4e4e\u7acb\u5373\u83b7\u5f97\u4e86\u53ef\u6cdb\u5316\u7684\u8868\u5f81\uff0c\u800c\u65e0\u9700\u5b66\u4e60\u8bad\u7ec3\u96c6\u7279\u5b9a\u4fe1\u606f\u7684\u521d\u6b65\u9636\u6bb5\uff0c\u8fd9\u4e9b\u4fe1\u606f\u968f\u540e\u624d\u4f1a\u8f6c\u79fb\u5230\u65b0\u7684\u6a21\u578b\u4e2d\u3002\u6570\u636e\u3002|[2402.09303v1](http://arxiv.org/pdf/2402.09303v1)|null|\n", "2402.09257": "|**2024-02-14**|**TDViT: Temporal Dilated Video Transformer for Dense Video Tasks**|TDViT\uff1a\u7528\u4e8e\u5bc6\u96c6\u89c6\u9891\u4efb\u52a1\u7684\u65f6\u95f4\u6269\u5f20\u89c6\u9891\u8f6c\u6362\u5668|Guanxiong Sun, Yang Hua, Guosheng Hu, Neil Robertson|Deep video models, for example, 3D CNNs or video transformers, have achieved promising performance on sparse video tasks, i.e., predicting one result per video. However, challenges arise when adapting existing deep video models to dense video tasks, i.e., predicting one result per frame. Specifically, these models are expensive for deployment, less effective when handling redundant frames, and difficult to capture long-range temporal correlations. To overcome these issues, we propose a Temporal Dilated Video Transformer (TDViT) that consists of carefully designed temporal dilated transformer blocks (TDTB). TDTB can efficiently extract spatiotemporal representations and effectively alleviate the negative effect of temporal redundancy. Furthermore, by using hierarchical TDTBs, our approach obtains an exponentially expanded temporal receptive field and therefore can model long-range dynamics. Extensive experiments are conducted on two different dense video benchmarks, i.e., ImageNet VID for video object detection and YouTube VIS for video instance segmentation. Excellent experimental results demonstrate the superior efficiency, effectiveness, and compatibility of our method. The code is available at https://github.com/guanxiongsun/vfe.pytorch.|\u6df1\u5ea6\u89c6\u9891\u6a21\u578b\uff08\u4f8b\u5982 3D CNN \u6216\u89c6\u9891\u8f6c\u6362\u5668\uff09\u5728\u7a00\u758f\u89c6\u9891\u4efb\u52a1\uff08\u5373\u9884\u6d4b\u6bcf\u4e2a\u89c6\u9891\u7684\u4e00\u4e2a\u7ed3\u679c\uff09\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5f53\u5c06\u73b0\u6709\u7684\u6df1\u5ea6\u89c6\u9891\u6a21\u578b\u5e94\u7528\u4e8e\u5bc6\u96c6\u89c6\u9891\u4efb\u52a1\u65f6\uff0c\u5373\u9884\u6d4b\u6bcf\u5e27\u4e00\u4e2a\u7ed3\u679c\u65f6\uff0c\u5c31\u4f1a\u51fa\u73b0\u6311\u6218\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u90e8\u7f72\u6210\u672c\u9ad8\u6602\uff0c\u5728\u5904\u7406\u5197\u4f59\u5e27\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u5e76\u4e14\u96be\u4ee5\u6355\u83b7\u8fdc\u7a0b\u65f6\u95f4\u76f8\u5173\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u6269\u5f20\u89c6\u9891\u53d8\u6362\u5668\uff08TDViT\uff09\uff0c\u5b83\u7531\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u65f6\u95f4\u6269\u5f20\u53d8\u6362\u5668\u5757\uff08TDTB\uff09\u7ec4\u6210\u3002 TDTB\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u53d6\u65f6\u7a7a\u8868\u793a\uff0c\u5e76\u6709\u6548\u51cf\u8f7b\u65f6\u95f4\u5197\u4f59\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4f7f\u7528\u5206\u5c42 TDTB\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u83b7\u5f97\u4e86\u6307\u6570\u6269\u5c55\u7684\u65f6\u95f4\u611f\u53d7\u91ce\uff0c\u56e0\u6b64\u53ef\u4ee5\u5bf9\u8fdc\u7a0b\u52a8\u6001\u8fdb\u884c\u5efa\u6a21\u3002\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u5bc6\u96c6\u89c6\u9891\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u5373\u7528\u4e8e\u89c6\u9891\u5bf9\u8c61\u68c0\u6d4b\u7684 ImageNet VID \u548c\u7528\u4e8e\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u7684 YouTube VIS\u3002\u51fa\u8272\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u5353\u8d8a\u6548\u7387\u3001\u6709\u6548\u6027\u548c\u517c\u5bb9\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/guanxiongsun/vfe.pytorch \u83b7\u53d6\u3002|[2402.09257v1](http://arxiv.org/pdf/2402.09257v1)|**[link](https://github.com/guanxiongsun/vfe.pytorch)**|\n", "2402.09241": "|**2024-02-14**|**Efficient One-stage Video Object Detection by Exploiting Temporal Consistency**|\u5229\u7528\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u9ad8\u6548\u5355\u9636\u6bb5\u89c6\u9891\u76ee\u6807\u68c0\u6d4b|Guanxiong Sun, Yang Hua, Guosheng Hu, Neil Robertson|Recently, one-stage detectors have achieved competitive accuracy and faster speed compared with traditional two-stage detectors on image data. However, in the field of video object detection (VOD), most existing VOD methods are still based on two-stage detectors. Moreover, directly adapting existing VOD methods to one-stage detectors introduces unaffordable computational costs. In this paper, we first analyse the computational bottlenecks of using one-stage detectors for VOD. Based on the analysis, we present a simple yet efficient framework to address the computational bottlenecks and achieve efficient one-stage VOD by exploiting the temporal consistency in video frames. Specifically, our method consists of a location-prior network to filter out background regions and a size-prior network to skip unnecessary computations on low-level feature maps for specific frames. We test our method on various modern one-stage detectors and conduct extensive experiments on the ImageNet VID dataset. Excellent experimental results demonstrate the superior effectiveness, efficiency, and compatibility of our method. The code is available at https://github.com/guanxiongsun/vfe.pytorch.|\u6700\u8fd1\uff0c\u4e0e\u4f20\u7edf\u7684\u4e24\u7ea7\u68c0\u6d4b\u5668\u76f8\u6bd4\uff0c\u4e00\u7ea7\u68c0\u6d4b\u5668\u5728\u56fe\u50cf\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7cbe\u5ea6\u548c\u66f4\u5feb\u7684\u901f\u5ea6\u3002\u7136\u800c\uff0c\u5728\u89c6\u9891\u5bf9\u8c61\u68c0\u6d4b\uff08VOD\uff09\u9886\u57df\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684VOD\u65b9\u6cd5\u4ecd\u7136\u57fa\u4e8e\u4e24\u7ea7\u68c0\u6d4b\u5668\u3002\u6b64\u5916\uff0c\u76f4\u63a5\u5c06\u73b0\u6709\u7684 VOD \u65b9\u6cd5\u5e94\u7528\u4e8e\u4e00\u7ea7\u68c0\u6d4b\u5668\u4f1a\u5e26\u6765\u96be\u4ee5\u627f\u53d7\u7684\u8ba1\u7b97\u6210\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u5206\u6790\u4e86\u4f7f\u7528\u5355\u7ea7\u68c0\u6d4b\u5668\u8fdb\u884c VOD \u7684\u8ba1\u7b97\u74f6\u9888\u3002\u57fa\u4e8e\u5206\u6790\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8ba1\u7b97\u74f6\u9888\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u89c6\u9891\u5e27\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u5355\u9636\u6bb5 VOD\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u7531\u4e00\u4e2a\u7528\u4e8e\u8fc7\u6ee4\u80cc\u666f\u533a\u57df\u7684\u4f4d\u7f6e\u5148\u9a8c\u7f51\u7edc\u548c\u4e00\u4e2a\u7528\u4e8e\u8df3\u8fc7\u7279\u5b9a\u5e27\u7684\u4f4e\u7ea7\u7279\u5f81\u56fe\u4e0a\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u7684\u5c3a\u5bf8\u5148\u9a8c\u7f51\u7edc\u7ec4\u6210\u3002\u6211\u4eec\u5728\u5404\u79cd\u73b0\u4ee3\u4e00\u7ea7\u68c0\u6d4b\u5668\u4e0a\u6d4b\u8bd5\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u5728 ImageNet VID \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u51fa\u8272\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5353\u8d8a\u7684\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u517c\u5bb9\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/guanxiongsun/vfe.pytorch \u83b7\u53d6\u3002|[2402.09241v1](http://arxiv.org/pdf/2402.09241v1)|**[link](https://github.com/guanxiongsun/vfe.pytorch)**|\n", "2402.09240": "|**2024-02-14**|**Switch EMA: A Free Lunch for Better Flatness and Sharpness**|Switch EMA\uff1a\u66f4\u597d\u7684\u5e73\u5766\u5ea6\u548c\u6e05\u6670\u5ea6\u7684\u514d\u8d39\u5348\u9910|Siyuan Li, Zicheng Liu, Juanxi Tian, Ge Wang, Zedong Wang, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, et.al.|Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling. Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds.|\u6307\u6570\u79fb\u52a8\u5e73\u5747 (EMA) \u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6743\u91cd\u5e73\u5747 (WA) \u6b63\u5219\u5316\uff0c\u7528\u4e8e\u5b66\u4e60\u5e73\u5766\u6700\u4f18\u503c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\uff0c\u800c\u65e0\u9700\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u4f18\u5316\u4e2d\u4ea7\u751f\u989d\u5916\u6210\u672c\u3002\u5c3d\u7ba1\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u5766\u5ea6\uff0c\u73b0\u6709\u7684 WA \u65b9\u6cd5\u53ef\u80fd\u4f1a\u9677\u5165\u66f4\u5dee\u7684\u6700\u7ec8\u6027\u80fd\u6216\u9700\u8981\u989d\u5916\u7684\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\u3002\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5355\u884c\u4fee\u6539\u63ed\u793a\u4e86 EMA \u7684\u5168\u90e8\u6f5c\u529b\uff0c\u5373\u5728\u6bcf\u4e2a epoch \u540e\u5c06 EMA \u53c2\u6570\u5207\u6362\u5230\u539f\u59cb\u6a21\u578b\uff0c\u79f0\u4e3a Switch EMA (SEMA)\u3002\u4ece\u7406\u8bba\u548c\u5b9e\u8bc1\u65b9\u9762\uff0c\u6211\u4eec\u8bc1\u660e SEMA \u53ef\u4ee5\u5e2e\u52a9 DNN \u8fbe\u5230\u6cdb\u5316\u6700\u4f18\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5728\u5e73\u5766\u5ea6\u548c\u6e05\u6670\u5ea6\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u4e3a\u4e86\u9a8c\u8bc1SEMA\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5224\u522b\u3001\u751f\u6210\u548c\u56de\u5f52\u4efb\u52a1\u7684\u6bd4\u8f83\u5b9e\u9a8c\uff0c\u5305\u62ec\u56fe\u50cf\u5206\u7c7b\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u5bf9\u8c61\u68c0\u6d4b\u548c\u5206\u5272\u3001\u56fe\u50cf\u751f\u6210\u3001\u89c6\u9891\u9884\u6d4b\u3001\u5c5e\u6027\u56de\u5f52\u548c\u8bed\u8a00\u9020\u578b\u3002\u6d41\u884c\u4f18\u5316\u5668\u548c\u7f51\u7edc\u7684\u7efc\u5408\u7ed3\u679c\u8868\u660e\uff0cSEMA \u901a\u8fc7\u63d0\u9ad8\u6027\u80fd\u548c\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\uff0c\u6210\u4e3a DNN \u8bad\u7ec3\u7684\u514d\u8d39\u5348\u9910\u3002|[2402.09240v1](http://arxiv.org/pdf/2402.09240v1)|null|\n", "2402.09225": "|**2024-02-14**|**Is my Data in your AI Model? Membership Inference Test with Application to Face Images**|\u6211\u7684\u6570\u636e\u5728\u4f60\u4eec\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u4e2d\u5417\uff1f\u5e94\u7528\u4e8e\u4eba\u8138\u56fe\u50cf\u7684\u96b6\u5c5e\u63a8\u7406\u6d4b\u8bd5|Daniel DeAlcala, Aythami Morales, Gonzalo Mancera, Julian Fierrez, Ruben Tolosana, Javier Ortega-Garcia|This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs). The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are achieved using our proposed MINT approach, suggesting that it is possible to recognize if an AI model has been trained with specific data.|\u672c\u6587\u4ecb\u7ecd\u4e86\u6210\u5458\u63a8\u7406\u6d4b\u8bd5\uff08MINT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u51ed\u7ecf\u9a8c\u8bc4\u4f30\u5728\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u662f\u5426\u4f7f\u7528\u4e86\u7279\u5b9a\u6570\u636e\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684 MINT \u67b6\u6784\uff0c\u65e8\u5728\u5b66\u4e60\u5f53\u5ba1\u8ba1\u6a21\u578b\u66b4\u9732\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7684\u6570\u636e\u65f6\u51fa\u73b0\u7684\u4e0d\u540c\u6fc0\u6d3b\u6a21\u5f0f\u3002\u7b2c\u4e00\u4e2a\u67b6\u6784\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u7f51\u7edc\uff0c\u7b2c\u4e8c\u4e2a\u67b6\u6784\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN)\u3002\u6240\u63d0\u51fa\u7684 MINT \u67b6\u6784\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8003\u8651\u4e86\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u3002\u5b9e\u9a8c\u4f7f\u7528\u516d\u4e2a\u516c\u5f00\u6570\u636e\u5e93\u8fdb\u884c\uff0c\u603b\u5171\u5305\u542b\u8d85\u8fc7 2200 \u4e07\u5f20\u4eba\u8138\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u6839\u636e\u8981\u6d4b\u8bd5\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u53ef\u7528\u4e0a\u4e0b\u6587\uff0c\u4f1a\u8003\u8651\u4e0d\u540c\u7684\u5b9e\u9a8c\u573a\u666f\u3002\u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684 MINT \u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe 90% \u7684\u51c6\u786e\u7387\uff0c\u8fd9\u8868\u660e\u53ef\u4ee5\u8bc6\u522b AI \u6a21\u578b\u662f\u5426\u5df2\u7ecf\u4f7f\u7528\u7279\u5b9a\u6570\u636e\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002|[2402.09225v1](http://arxiv.org/pdf/2402.09225v1)|null|\n", "2402.09204": "|**2024-02-14**|**Domain-adaptive and Subgroup-specific Cascaded Temperature Regression for Out-of-distribution Calibration**|\u7528\u4e8e\u5206\u5e03\u5916\u6821\u51c6\u7684\u57df\u81ea\u9002\u5e94\u548c\u5b50\u7ec4\u7279\u5b9a\u7ea7\u8054\u6e29\u5ea6\u56de\u5f52|Jiexin Wang, Jiahao Chen, Bing Su|Although deep neural networks yield high classification accuracy given sufficient training data, their predictions are typically overconfident or under-confident, i.e., the prediction confidences cannot truly reflect the accuracy. Post-hoc calibration tackles this problem by calibrating the prediction confidences without re-training the classification model. However, current approaches assume congruence between test and validation data distributions, limiting their applicability to out-of-distribution scenarios. To this end, we propose a novel meta-set-based cascaded temperature regression method for post-hoc calibration. Our method tailors fine-grained scaling functions to distinct test sets by simulating various domain shifts through data augmentation on the validation set. We partition each meta-set into subgroups based on predicted category and confidence level, capturing diverse uncertainties. A regression network is then trained to derive category-specific and confidence-level-specific scaling, achieving calibration across meta-sets. Extensive experimental results on MNIST, CIFAR-10, and TinyImageNet demonstrate the effectiveness of the proposed method.|\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u7ed9\u5b9a\u8db3\u591f\u7684\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u4ea7\u751f\u5f88\u9ad8\u7684\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4f46\u5b83\u4eec\u7684\u9884\u6d4b\u901a\u5e38\u8fc7\u4e8e\u81ea\u4fe1\u6216\u4fe1\u5fc3\u4e0d\u8db3\uff0c\u5373\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e0d\u80fd\u771f\u6b63\u53cd\u6620\u51c6\u786e\u6027\u3002\u4e8b\u540e\u6821\u51c6\u901a\u8fc7\u6821\u51c6\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5206\u7c7b\u6a21\u578b\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u5047\u8bbe\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u662f\u4e00\u81f4\u7684\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u5206\u5e03\u5916\u573a\u666f\u7684\u9002\u7528\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5143\u96c6\u7684\u7ea7\u8054\u6e29\u5ea6\u56de\u5f52\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e8b\u540e\u6821\u51c6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6570\u636e\u589e\u5f3a\u6765\u6a21\u62df\u5404\u79cd\u57df\u8f6c\u6362\uff0c\u4ece\u800c\u9488\u5bf9\u4e0d\u540c\u7684\u6d4b\u8bd5\u96c6\u5b9a\u5236\u7ec6\u7c92\u5ea6\u7684\u7f29\u653e\u51fd\u6570\u3002\u6211\u4eec\u6839\u636e\u9884\u6d4b\u7c7b\u522b\u548c\u7f6e\u4fe1\u6c34\u5e73\u5c06\u6bcf\u4e2a\u5143\u96c6\u5212\u5206\u4e3a\u5b50\u7ec4\uff0c\u6355\u83b7\u4e0d\u540c\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u7136\u540e\u8bad\u7ec3\u56de\u5f52\u7f51\u7edc\u6765\u5bfc\u51fa\u7279\u5b9a\u4e8e\u7c7b\u522b\u548c\u7279\u5b9a\u4e8e\u7f6e\u4fe1\u6c34\u5e73\u7684\u7f29\u653e\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u5143\u96c6\u7684\u6821\u51c6\u3002 MNIST\u3001CIFAR-10 \u548c TinyImageNet \u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2402.09204v1](http://arxiv.org/pdf/2402.09204v1)|null|\n", "2402.09156": "|**2024-02-14**|**Crop and Couple: cardiac image segmentation using interlinked specialist networks**|Crop and Couple\uff1a\u4f7f\u7528\u4e92\u8fde\u7684\u4e13\u5bb6\u7f51\u7edc\u8fdb\u884c\u5fc3\u810f\u56fe\u50cf\u5206\u5272|Abbas Khan, Muhammad Asad, Martin Benning, Caroline Roney, Gregory Slabaugh|Diagnosis of cardiovascular disease using automated methods often relies on the critical task of cardiac image segmentation. We propose a novel strategy that performs segmentation using specialist networks that focus on a single anatomy (left ventricle, right ventricle, or myocardium). Given an input long-axis cardiac MR image, our method performs a ternary segmentation in the first stage to identify these anatomical regions, followed by cropping the original image to focus subsequent processing on the anatomical regions. The specialist networks are coupled through an attention mechanism that performs cross-attention to interlink features from different anatomies, serving as a soft relative shape prior. Central to our approach is an additive attention block (E-2A block), which is used throughout our architecture thanks to its efficiency.|\u4f7f\u7528\u81ea\u52a8\u5316\u65b9\u6cd5\u8bca\u65ad\u5fc3\u8840\u7ba1\u75be\u75c5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5fc3\u810f\u56fe\u50cf\u5206\u5272\u7684\u5173\u952e\u4efb\u52a1\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b56\u7565\uff0c\u4f7f\u7528\u4e13\u6ce8\u4e8e\u5355\u4e00\u89e3\u5256\u7ed3\u6784\uff08\u5de6\u5fc3\u5ba4\u3001\u53f3\u5fc3\u5ba4\u6216\u5fc3\u808c\uff09\u7684\u4e13\u4e1a\u7f51\u7edc\u8fdb\u884c\u5206\u5272\u3002\u7ed9\u5b9a\u8f93\u5165\u7684\u957f\u8f74\u5fc3\u810f MR \u56fe\u50cf\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7b2c\u4e00\u9636\u6bb5\u6267\u884c\u4e09\u5143\u5206\u5272\u4ee5\u8bc6\u522b\u8fd9\u4e9b\u89e3\u5256\u533a\u57df\uff0c\u7136\u540e\u88c1\u526a\u539f\u59cb\u56fe\u50cf\u4ee5\u5c06\u540e\u7eed\u5904\u7406\u96c6\u4e2d\u5728\u89e3\u5256\u533a\u57df\u3002\u4e13\u5bb6\u7f51\u7edc\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8026\u5408\uff0c\u8be5\u673a\u5236\u5bf9\u6765\u81ea\u4e0d\u540c\u89e3\u5256\u7ed3\u6784\u7684\u4e92\u8fde\u7279\u5f81\u6267\u884c\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u5145\u5f53\u8f6f\u76f8\u5bf9\u5f62\u72b6\u5148\u9a8c\u3002\u6211\u4eec\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u9644\u52a0\u6ce8\u610f\u529b\u5757\uff08E-2A \u5757\uff09\uff0c\u7531\u4e8e\u5176\u6548\u7387\uff0c\u5b83\u5728\u6211\u4eec\u7684\u6574\u4e2a\u67b6\u6784\u4e2d\u4f7f\u7528\u3002|[2402.09156v1](http://arxiv.org/pdf/2402.09156v1)|null|\n", "2402.09066": "|**2024-02-14**|**Solid Waste Detection in Remote Sensing Images: A Survey**|\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u56fa\u4f53\u5e9f\u7269\u68c0\u6d4b\uff1a\u8c03\u67e5|Piero Fraternali, Luca Morandini, Sergio Luis Herrera Gonz\u00e1lez|The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locations for new landfills. This review aims to provide a detailed illustration of the most relevant proposals for the detection and monitoring of solid waste sites by describing and comparing the approaches, the implemented techniques, and the employed data. Furthermore, since the data sources are of the utmost importance for developing an effective solid waste detection model, a comprehensive overview of the satellites and publicly available data sets is presented. Finally, this paper identifies the open issues in the state-of-the-art and discusses the relevant research directions for reducing the costs and improving the effectiveness of novel solid waste detection methods.|\u975e\u6cd5\u56fa\u4f53\u5e9f\u7269\u5904\u7f6e\u573a\u7684\u68c0\u6d4b\u548c\u7279\u5f81\u5206\u6790\u5bf9\u4e8e\u73af\u5883\u4fdd\u62a4\uff0c\u7279\u522b\u662f\u51cf\u8f7b\u6c61\u67d3\u548c\u5065\u5eb7\u5371\u5bb3\u81f3\u5173\u91cd\u8981\u3002\u7ba1\u7406\u4e0d\u5f53\u7684\u5783\u573e\u586b\u57cb\u573a\u4f1a\u901a\u8fc7\u96e8\u6c34\u6e17\u900f\u6c61\u67d3\u571f\u58e4\u548c\u5730\u4e0b\u6c34\uff0c\u5bf9\u52a8\u7269\u548c\u4eba\u7c7b\u6784\u6210\u5a01\u80c1\u3002\u4f20\u7edf\u7684\u5783\u573e\u586b\u57cb\u573a\u8bc6\u522b\u65b9\u6cd5\uff08\u4f8b\u5982\u73b0\u573a\u68c0\u67e5\uff09\u65e2\u8017\u65f6\u53c8\u6602\u8d35\u3002\u9065\u611f\u662f\u8bc6\u522b\u548c\u76d1\u6d4b\u56fa\u4f53\u5e9f\u7269\u5904\u7f6e\u573a\u7684\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5b9e\u73b0\u5e7f\u6cdb\u7684\u8986\u76d6\u8303\u56f4\u548c\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u8fdb\u884c\u91cd\u590d\u91c7\u96c6\u3002\u5730\u7403\u89c2\u6d4b\uff08EO\uff09\u536b\u661f\u914d\u5907\u4e86\u4e00\u7cfb\u5217\u4f20\u611f\u5668\u548c\u6210\u50cf\u529f\u80fd\uff0c\u51e0\u5341\u5e74\u6765\u4e00\u76f4\u5728\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u3002\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u5229\u7528\u9065\u611f\u56fe\u50cf\u6267\u884c\u4e00\u7cfb\u5217\u4efb\u52a1\u7684\u4e13\u95e8\u6280\u672f\uff0c\u4f8b\u5982\u5e9f\u7269\u573a\u68c0\u6d4b\u3001\u503e\u5012\u573a\u76d1\u6d4b\u4ee5\u53ca\u8bc4\u4f30\u65b0\u5783\u573e\u586b\u57cb\u573a\u7684\u5408\u9002\u4f4d\u7f6e\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u901a\u8fc7\u63cf\u8ff0\u548c\u6bd4\u8f83\u65b9\u6cd5\u3001\u5b9e\u65bd\u7684\u6280\u672f\u548c\u4f7f\u7528\u7684\u6570\u636e\uff0c\u8be6\u7ec6\u8bf4\u660e\u56fa\u4f53\u5e9f\u7269\u573a\u5730\u68c0\u6d4b\u548c\u76d1\u6d4b\u7684\u6700\u76f8\u5173\u5efa\u8bae\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u6570\u636e\u6e90\u5bf9\u4e8e\u5f00\u53d1\u6709\u6548\u7684\u56fa\u4f53\u5e9f\u7269\u68c0\u6d4b\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u5bf9\u536b\u661f\u548c\u516c\u5f00\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5168\u9762\u6982\u8ff0\u3002\u6700\u540e\uff0c\u672c\u6587\u6307\u51fa\u4e86\u6700\u65b0\u6280\u672f\u4e2d\u7684\u672a\u89e3\u51b3\u95ee\u9898\uff0c\u5e76\u8ba8\u8bba\u4e86\u964d\u4f4e\u6210\u672c\u548c\u63d0\u9ad8\u65b0\u578b\u56fa\u4f53\u5e9f\u7269\u68c0\u6d4b\u65b9\u6cd5\u6709\u6548\u6027\u7684\u76f8\u5173\u7814\u7a76\u65b9\u5411\u3002|[2402.09066v1](http://arxiv.org/pdf/2402.09066v1)|null|\n", "2402.08960": "|**2024-02-14**|**Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision**|\u5177\u6709\u4e0d\u914d\u5bf9\u63a9\u7801\u6587\u672c\u76d1\u7763\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272|Zhaoqing Wang, Xiaobo Xia, Ziye Chen, Xiao He, Yandong Guo, Mingming Gong, Tongliang Liu|Contemporary cutting-edge open-vocabulary segmentation approaches commonly rely on image-mask-text triplets, yet this restricted annotation is labour-intensive and encounters scalability hurdles in complex real-world scenarios. Although some methods are proposed to reduce the annotation cost with only text supervision, the incompleteness of supervision severely limits the versatility and performance. In this paper, we liberate the strict correspondence between masks and texts by using independent image-mask and image-text pairs, which can be easily collected respectively. With this unpaired mask-text supervision, we propose a new weakly-supervised open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident pairs of mask predictions and entities in text descriptions. Using the independent image-mask and image-text pairs, we predict a set of binary masks and associate them with entities by resorting to the CLIP embedding space. However, the inherent noise in the correspondence between masks and entities poses a significant challenge when obtaining reliable pairs. In light of this, we advocate using the large vision-language model (LVLM) to refine text descriptions and devise a multi-scale ensemble to stablise the matching between masks and entities. Compared to text-only weakly-supervised methods, our Uni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K datasets, and even surpasses fully-supervised methods on the challenging PASCAL Context-459 dataset.|\u5f53\u4ee3\u5c16\u7aef\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u56fe\u50cf-\u63a9\u6a21-\u6587\u672c\u4e09\u5143\u7ec4\uff0c\u4f46\u8fd9\u79cd\u53d7\u9650\u6ce8\u91ca\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\uff0c\u5e76\u4e14\u5728\u590d\u6742\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u9047\u5230\u53ef\u6269\u5c55\u6027\u969c\u788d\u3002\u5c3d\u7ba1\u63d0\u51fa\u4e86\u4e00\u4e9b\u4ec5\u901a\u8fc7\u6587\u672c\u76d1\u7763\u6765\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u4f46\u76d1\u7763\u7684\u4e0d\u5b8c\u6574\u6027\u4e25\u91cd\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u72ec\u7acb\u7684\u56fe\u50cf-\u63a9\u6a21\u548c\u56fe\u50cf-\u6587\u672c\u5bf9\u6765\u89e3\u653e\u63a9\u6a21\u548c\u6587\u672c\u4e4b\u95f4\u7684\u4e25\u683c\u5bf9\u5e94\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5bf9\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u5206\u522b\u6536\u96c6\u3002\u901a\u8fc7\u8fd9\u79cd\u4e0d\u6210\u5bf9\u7684\u63a9\u7801\u6587\u672c\u76d1\u7763\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f31\u76d1\u7763\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u6846\u67b6\uff08Uni-OVSeg\uff09\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6587\u672c\u63cf\u8ff0\u4e2d\u7684\u63a9\u7801\u9884\u6d4b\u548c\u5b9e\u4f53\u7684\u7f6e\u4fe1\u5bf9\u3002\u4f7f\u7528\u72ec\u7acb\u7684\u56fe\u50cf-\u63a9\u7801\u548c\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u6211\u4eec\u9884\u6d4b\u4e00\u7ec4\u4e8c\u8fdb\u5236\u63a9\u7801\uff0c\u5e76\u901a\u8fc7 CLIP \u5d4c\u5165\u7a7a\u95f4\u5c06\u5b83\u4eec\u4e0e\u5b9e\u4f53\u76f8\u5173\u8054\u3002\u7136\u800c\uff0c\u63a9\u6a21\u548c\u5b9e\u4f53\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u4e2d\u7684\u56fa\u6709\u566a\u58f0\u5728\u83b7\u5f97\u53ef\u9760\u5bf9\u65f6\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u6709\u9274\u4e8e\u6b64\uff0c\u6211\u4eec\u4e3b\u5f20\u4f7f\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u6765\u7ec6\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u8bbe\u8ba1\u591a\u5c3a\u5ea6\u96c6\u6210\u6765\u7a33\u5b9a\u63a9\u6a21\u548c\u5b9e\u4f53\u4e4b\u95f4\u7684\u5339\u914d\u3002\u4e0e\u7eaf\u6587\u672c\u5f31\u76d1\u7763\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684 Uni-OVSeg \u5728 ADE20K \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 15.5% mIoU \u7684\u5927\u5e45\u6539\u8fdb\uff0c\u751a\u81f3\u5728\u5177\u6709\u6311\u6218\u6027\u7684 PASCAL Context-459 \u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5b8c\u5168\u76d1\u7763\u65b9\u6cd5\u3002|[2402.08960v1](http://arxiv.org/pdf/2402.08960v1)|**[link](https://github.com/derrickwang005/uni-ovseg.pytorch)**|\n", "2402.08910": "|**2024-02-14**|**Learning-based Bone Quality Classification Method for Spinal Metastasis**|\u57fa\u4e8e\u5b66\u4e60\u7684\u810a\u67f1\u8f6c\u79fb\u9aa8\u8d28\u91cf\u5206\u7c7b\u65b9\u6cd5|Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao|Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions. In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process. The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by $+12.54\\%$, $+7.23\\%$ and $+29.06\\%$ for blastic, mixed and lytic lesions, respectively, meanwhile $+12.33\\%$, $+23.21\\%$ and $+34.25\\%$ at vertebrae level.|\u810a\u67f1\u8f6c\u79fb\u662f\u9aa8\u8f6c\u79fb\u4e2d\u6700\u5e38\u89c1\u7684\u75be\u75c5\uff0c\u53ef\u80fd\u5bfc\u81f4\u75bc\u75db\u3001\u4e0d\u7a33\u5b9a\u548c\u795e\u7ecf\u635f\u4f24\u3002\u65e9\u671f\u53d1\u73b0\u810a\u67f1\u8f6c\u79fb\u7624\u5bf9\u4e8e\u51c6\u786e\u5206\u671f\u548c\u6700\u4f73\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u901a\u5e38\u901a\u8fc7\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf (CT) \u626b\u63cf\u6765\u4fc3\u8fdb\u8bca\u65ad\uff0c\u8fd9\u9700\u8981\u8bad\u7ec3\u6709\u7d20\u7684\u653e\u5c04\u79d1\u533b\u751f\u4ed8\u51fa\u5de8\u5927\u7684\u52aa\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u4e8eCT\u56fe\u50cf\u7684\u810a\u67f1\u8f6c\u79fb\u7624\u81ea\u52a8\u9aa8\u8d28\u91cf\u5206\u7c7b\u65b9\u6cd5\u3002\u6211\u4eec\u540c\u65f6\u8003\u8651\u540e\u5916\u4fa7\u810a\u67f1\u53d7\u7d2f\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u6280\u672f\u6765\u63d0\u9ad8\u6027\u80fd\u3002 MTL \u5145\u5f53\u5f52\u7eb3\u504f\u5dee\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u901a\u8fc7\u5728\u76f8\u5173\u4efb\u52a1\u4e4b\u95f4\u5171\u4eab\u8868\u793a\u6765\u5e2e\u52a9\u6a21\u578b\u66f4\u597d\u5730\u6982\u62ec\u6bcf\u4e2a\u4efb\u52a1\u3002\u57fa\u4e8e\u6df7\u5408\u7c7b\u578b\u53ef\u4ee5\u88ab\u89c6\u4e3a\u7206\u70b8\u6027\u548c\u6eb6\u89e3\u6027\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u6211\u4eec\u5c06\u9aa8\u8d28\u91cf\u5206\u7c7b\u4efb\u52a1\u5efa\u6a21\u4e3a\u4e24\u4e2a\u4e8c\u5143\u5206\u7c7b\u5b50\u4efb\u52a1\uff0c\u5373\u662f\u5426\u7206\u70b8\u6027\u548c\u662f\u5426\u6eb6\u89e3\u6027\uff0c\u5e76\u5229\u7528\u591a\u5c42\u611f\u77e5\u5668\u6765\u7ec4\u5408\u4ed6\u4eec\u7684\u9884\u6d4b\u3002\u4e3a\u4e86\u4f7f\u6a21\u578b\u66f4\u52a0\u9c81\u68d2\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u597d\uff0c\u91c7\u7528\u81ea\u5b9a\u8fdb\u5ea6\u5b66\u4e60\uff0c\u9010\u6b65\u5c06\u6837\u672c\u4ece\u7b80\u5355\u5230\u590d\u6742\u7eb3\u5165\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u4e13\u6709\u7684\u810a\u67f1\u8f6c\u79fb CT \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5728\u5207\u7247\u7ea7\u522b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u4e8e\u6025\u53d8\u6027\u3001\u6df7\u5408\u6027\u548c\u6eb6\u89e3\u6027\u75c5\u53d8\u7684\u7075\u654f\u5ea6\u5206\u522b\u663e\u7740\u4f18\u4e8e 121 \u5c42 DenseNet \u5206\u7c7b\u5668 $+12.54\\%$\u3001$+7.23\\%$ \u548c $+29.06\\%$\uff0c\u540c\u65f6 $+ 12.33\\%$\u3001$+23.21\\%$ \u548c $+34.25\\%$ \u5728\u690e\u9aa8\u6c34\u5e73\u3002|[2402.08910v1](http://arxiv.org/pdf/2402.08910v1)|null|\n", "2402.08892": "|**2024-02-14**|**Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation**|\u8fed\u4ee3\u5207\u7247\u4f20\u64ad\u7684\u690e\u4f53\u5f31\u76d1\u7763\u5206\u5272|Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao|Vertebral body (VB) segmentation is an important preliminary step towards medical visual diagnosis for spinal diseases. However, most previous works require pixel/voxel-wise strong supervisions, which is expensive, tedious and time-consuming for experts to annotate. In this paper, we propose a Weakly supervised Iterative Spinal Segmentation (WISS) method leveraging only four corner landmark weak labels on a single sagittal slice to achieve automatic volumetric segmentation from CT images for VBs. WISS first segments VBs on an annotated sagittal slice in an iterative self-training manner. This self-training method alternates between training and refining labels in the training set. Then WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method to obtain volumetric segmentations. We evaluate the performance of WISS on a private spinal metastases CT dataset and the public lumbar CT dataset. On the first dataset, WISS achieves distinct improvements with regard to two different backbones. For the second dataset, WISS achieves dice coefficients of $91.7\\%$ and $83.7\\%$ for mid-sagittal slices and 3D CT volumes, respectively, saving a lot of labeling costs and only sacrificing a little segmentation performance.|\u690e\u4f53\uff08VB\uff09\u5206\u5272\u662f\u810a\u67f1\u75be\u75c5\u533b\u5b66\u89c6\u89c9\u8bca\u65ad\u7684\u91cd\u8981\u521d\u6b65\u6b65\u9aa4\u3002\u7136\u800c\uff0c\u4e4b\u524d\u7684\u5927\u591a\u6570\u5de5\u4f5c\u90fd\u9700\u8981\u50cf\u7d20/\u4f53\u7d20\u65b9\u9762\u7684\u5f3a\u76d1\u7763\uff0c\u8fd9\u5bf9\u4e8e\u4e13\u5bb6\u6765\u8bf4\u662f\u6602\u8d35\u3001\u7e41\u7410\u4e14\u8017\u65f6\u7684\u6ce8\u91ca\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u8fed\u4ee3\u810a\u67f1\u5206\u5272\uff08WISS\uff09\u65b9\u6cd5\uff0c\u4ec5\u5229\u7528\u5355\u4e2a\u77e2\u72b6\u5207\u7247\u4e0a\u7684\u56db\u4e2a\u89d2\u6807\u5fd7\u5f31\u6807\u7b7e\u6765\u5b9e\u73b0 VB CT \u56fe\u50cf\u7684\u81ea\u52a8\u4f53\u79ef\u5206\u5272\u3002 WISS \u9996\u5148\u4ee5\u8fed\u4ee3\u81ea\u8bad\u7ec3\u65b9\u5f0f\u5728\u5e26\u6ce8\u91ca\u7684\u77e2\u72b6\u5207\u7247\u4e0a\u5206\u5272 VB\u3002\u8fd9\u79cd\u81ea\u6211\u8bad\u7ec3\u65b9\u6cd5\u5728\u8bad\u7ec3\u548c\u7ec6\u5316\u8bad\u7ec3\u96c6\u4e2d\u7684\u6807\u7b7e\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\u3002\u7136\u540eWISS\u7ee7\u7eed\u4f7f\u7528\u5207\u7247\u4f20\u64ad\u65b9\u6cd5\u5bf9\u6574\u4e2aVB\u8fdb\u884c\u5207\u7247\u5206\u5272\u4ee5\u83b7\u5f97\u4f53\u79ef\u5206\u5272\u3002\u6211\u4eec\u8bc4\u4f30\u4e86 WISS \u5728\u79c1\u4eba\u810a\u67f1\u8f6c\u79fb\u7624 CT \u6570\u636e\u96c6\u548c\u516c\u5171\u8170\u690e CT \u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002\u5728\u7b2c\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cWISS \u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u4e3b\u5e72\u7f51\u65b9\u9762\u53d6\u5f97\u4e86\u660e\u663e\u7684\u6539\u8fdb\u3002\u5bf9\u4e8e\u7b2c\u4e8c\u4e2a\u6570\u636e\u96c6\uff0cWISS \u7684\u4e2d\u77e2\u72b6\u5207\u7247\u548c 3D CT \u4f53\u79ef\u7684\u9ab0\u5b50\u7cfb\u6570\u5206\u522b\u4e3a $91.7\\%$ \u548c $83.7\\%$\uff0c\u8282\u7701\u4e86\u5927\u91cf\u6807\u8bb0\u6210\u672c\uff0c\u5e76\u4e14\u53ea\u727a\u7272\u4e86\u4e00\u70b9\u5206\u5272\u6027\u80fd\u3002|[2402.08892v1](http://arxiv.org/pdf/2402.08892v1)|null|\n", "2402.08882": "|**2024-02-14**|**Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation**|\u7528\u4e8e\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u6df1\u5ea6\u5b66\u4e60\u5149\u6d41\u7684\u79fb\u52a8\u5bf9\u8c61\u5efa\u8bae|Ge Shi, Zhili Yang|Dynamic scene understanding is one of the most conspicuous field of interest among computer vision community. In order to enhance dynamic scene understanding, pixel-wise segmentation with neural networks is widely accepted. The latest researches on pixel-wise segmentation combined semantic and motion information and produced good performance. In this work, we propose a state of art architecture of neural networks to accurately and efficiently get the moving object proposals (MOP). We first train an unsupervised convolutional neural network (UnFlow) to generate optical flow estimation. Then we render the output of optical flow net to a fully convolutional SegNet model. The main contribution of our work is (1) Fine-tuning the pretrained optical flow model on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural networks with Encoder-Decoder architecture to segment objects. We developed the codes with TensorFlow, and executed the training and evaluation processes on an AWS EC2 instance.|\u52a8\u6001\u573a\u666f\u7406\u89e3\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u754c\u6700\u5f15\u4eba\u6ce8\u76ee\u7684\u9886\u57df\u4e4b\u4e00\u3002\u4e3a\u4e86\u589e\u5f3a\u52a8\u6001\u573a\u666f\u7406\u89e3\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u50cf\u7d20\u7ea7\u5206\u5272\u88ab\u5e7f\u6cdb\u63a5\u53d7\u3002\u50cf\u7d20\u7ea7\u5206\u5272\u7684\u6700\u65b0\u7814\u7a76\u7ed3\u5408\u4e86\u8bed\u4e49\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u5e76\u4ea7\u751f\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4ee5\u51c6\u786e\u6709\u6548\u5730\u83b7\u5f97\u79fb\u52a8\u5bf9\u8c61\u5efa\u8bae\uff08MOP\uff09\u3002\u6211\u4eec\u9996\u5148\u8bad\u7ec3\u65e0\u76d1\u7763\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08UnFlow\uff09\u6765\u751f\u6210\u5149\u6d41\u4f30\u8ba1\u3002\u7136\u540e\u6211\u4eec\u5c06\u5149\u6d41\u7f51\u7edc\u7684\u8f93\u51fa\u6e32\u67d3\u4e3a\u5168\u5377\u79ef SegNet \u6a21\u578b\u3002\u6211\u4eec\u5de5\u4f5c\u7684\u4e3b\u8981\u8d21\u732e\u662f\uff081\uff09\u5728\u5168\u65b0\u7684 DAVIS \u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u5149\u6d41\u6a21\u578b\uff1b (2) \u5229\u7528\u5177\u6709\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684\u5168\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u5206\u5272\u5bf9\u8c61\u3002\u6211\u4eec\u4f7f\u7528 TensorFlow \u5f00\u53d1\u4ee3\u7801\uff0c\u5e76\u5728 AWS EC2 \u5b9e\u4f8b\u4e0a\u6267\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u8fc7\u7a0b\u3002|[2402.08882v1](http://arxiv.org/pdf/2402.08882v1)|null|\n", "2402.08875": "|**2024-02-14**|**TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition**|TikTokActions\uff1a\u7528\u4e8e\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u7684 TikTok \u884d\u751f\u89c6\u9891\u6570\u636e\u96c6|Yang Qian, Yinan Sun, Ali Kargarandehkordi, Onur Cezmi Mutlu, Saimourya Surabhi, Pingyi Chen, Zain Jabbar, Dennis Paul Wall, Peter Washington|The increasing variety and quantity of tagged multimedia content on platforms such as TikTok provides an opportunity to advance computer vision modeling. We have curated a distinctive dataset of 283,582 unique video clips categorized under 386 hashtags relating to modern human actions. We release this dataset as a valuable resource for building domain-specific foundation models for human movement modeling tasks such as action recognition. To validate this dataset, which we name TikTokActions, we perform two sets of experiments. First, we pretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone on TikTokActions subset, and then fine-tune and evaluate on popular datasets such as UCF101 and the HMDB51. We find that the performance of the model pre-trained using our Tik-Tok dataset is comparable to models trained on larger action recognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, our investigation into the relationship between pre-training dataset size and fine-tuning performance reveals that beyond a certain threshold, the incremental benefit of larger training sets diminishes. This work introduces a useful TikTok video dataset that is available for public use and provides insights into the marginal benefit of increasing pre-training dataset sizes for video-based foundation models.|TikTok \u7b49\u5e73\u53f0\u4e0a\u6807\u8bb0\u591a\u5a92\u4f53\u5185\u5bb9\u7684\u79cd\u7c7b\u548c\u6570\u91cf\u4e0d\u65ad\u589e\u52a0\uff0c\u4e3a\u63a8\u8fdb\u8ba1\u7b97\u673a\u89c6\u89c9\u5efa\u6a21\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002\u6211\u4eec\u6574\u7406\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 283,582 \u4e2a\u72ec\u7279\u7684\u89c6\u9891\u526a\u8f91\uff0c\u8fd9\u4e9b\u89c6\u9891\u526a\u8f91\u88ab\u5206\u7c7b\u4e3a\u4e0e\u73b0\u4ee3\u4eba\u7c7b\u884c\u4e3a\u76f8\u5173\u7684 386 \u4e2a\u4e3b\u9898\u6807\u7b7e\u3002\u6211\u4eec\u53d1\u5e03\u6b64\u6570\u636e\u96c6\u4f5c\u4e3a\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u7528\u4e8e\u4e3a\u4eba\u4f53\u8fd0\u52a8\u5efa\u6a21\u4efb\u52a1\uff08\u4f8b\u5982\u52a8\u4f5c\u8bc6\u522b\uff09\u6784\u5efa\u7279\u5b9a\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u3002\u4e3a\u4e86\u9a8c\u8bc1\u8fd9\u4e2a\u6570\u636e\u96c6\uff08\u6211\u4eec\u5c06\u5176\u547d\u540d\u4e3a TikTokActions\uff09\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u4e24\u7ec4\u5b9e\u9a8c\u3002\u9996\u5148\uff0c\u6211\u4eec\u5728 TikTokActions \u5b50\u96c6\u4e0a\u4f7f\u7528\u57fa\u4e8e ViT \u7684\u9aa8\u5e72\u7f51\u5bf9\u6700\u5148\u8fdb\u7684 VideoMAEv2 \u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5bf9 UCF101 \u548c HMDB51 \u7b49\u6d41\u884c\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u548c\u8bc4\u4f30\u3002\u6211\u4eec\u53d1\u73b0\u4f7f\u7528 Tik-Tok \u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u5728\u66f4\u5927\u7684\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\uff08UCF101 \u4e0a\u4e3a 95.3%\uff0cHMDB51 \u4e0a\u4e3a 53.24%\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5bf9\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u548c\u5fae\u8c03\u6027\u80fd\u4e4b\u95f4\u5173\u7cfb\u7684\u8c03\u67e5\u8868\u660e\uff0c\u8d85\u8fc7\u67d0\u4e2a\u9608\u503c\uff0c\u8f83\u5927\u8bad\u7ec3\u96c6\u7684\u589e\u91cf\u6536\u76ca\u5c31\u4f1a\u51cf\u5f31\u3002\u8fd9\u9879\u5de5\u4f5c\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u6709\u7528\u7684 TikTok \u89c6\u9891\u6570\u636e\u96c6\uff0c\u53ef\u4f9b\u516c\u4f17\u4f7f\u7528\uff0c\u5e76\u6df1\u5165\u4e86\u89e3\u589e\u52a0\u57fa\u4e8e\u89c6\u9891\u7684\u57fa\u7840\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u7684\u8fb9\u9645\u6548\u76ca\u3002|[2402.08875v1](http://arxiv.org/pdf/2402.08875v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2402.08931": "|**2024-02-14**|**Depth-aware Volume Attention for Texture-less Stereo Matching**|\u7528\u4e8e\u65e0\u7eb9\u7406\u7acb\u4f53\u5339\u914d\u7684\u6df1\u5ea6\u611f\u77e5\u4f53\u79ef\u6ce8\u610f|Tong Zhao, Mingyu Ding, Wei Zhan, Masayoshi Tomizuka, Yintao Wei|Stereo matching plays a crucial role in 3D perception and scenario understanding. Despite the proliferation of promising methods, addressing texture-less and texture-repetitive conditions remains challenging due to the insufficient availability of rich geometric and semantic information. In this paper, we propose a lightweight volume refinement scheme to tackle the texture deterioration in practical outdoor scenarios. Specifically, we introduce a depth volume supervised by the ground-truth depth map, capturing the relative hierarchy of image texture. Subsequently, the disparity discrepancy volume undergoes hierarchical filtering through the incorporation of depth-aware hierarchy attention and target-aware disparity attention modules. Local fine structure and context are emphasized to mitigate ambiguity and redundancy during volume aggregation. Furthermore, we propose a more rigorous evaluation metric that considers depth-wise relative error, providing comprehensive evaluations for universal stereo matching and depth estimation models. We extensively validate the superiority of our proposed methods on public datasets. Results demonstrate that our model achieves state-of-the-art performance, particularly excelling in scenarios with texture-less images. The code is available at https://github.com/ztsrxh/DVANet.|\u7acb\u4f53\u5339\u914d\u5728 3D \u611f\u77e5\u548c\u573a\u666f\u7406\u89e3\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5c3d\u7ba1\u6709\u524d\u666f\u7684\u65b9\u6cd5\u4e0d\u65ad\u6d8c\u73b0\uff0c\u4f46\u7531\u4e8e\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u53ef\u7528\u6027\u4e0d\u8db3\uff0c\u89e3\u51b3\u65e0\u7eb9\u7406\u548c\u7eb9\u7406\u91cd\u590d\u7684\u6761\u4ef6\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f53\u79ef\u7ec6\u5316\u65b9\u6848\u6765\u89e3\u51b3\u5b9e\u9645\u6237\u5916\u573a\u666f\u4e2d\u7684\u7eb9\u7406\u6076\u5316\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u7531\u5730\u9762\u771f\u5b9e\u6df1\u5ea6\u56fe\u76d1\u7763\u7684\u6df1\u5ea6\u4f53\u79ef\uff0c\u6355\u83b7\u56fe\u50cf\u7eb9\u7406\u7684\u76f8\u5bf9\u5c42\u6b21\u7ed3\u6784\u3002\u968f\u540e\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u611f\u77e5\u5c42\u6b21\u6ce8\u610f\u529b\u548c\u76ee\u6807\u611f\u77e5\u89c6\u5dee\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5bf9\u89c6\u5dee\u5dee\u5f02\u91cf\u8fdb\u884c\u5c42\u6b21\u8fc7\u6ee4\u3002\u5f3a\u8c03\u5c40\u90e8\u7cbe\u7ec6\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\uff0c\u4ee5\u51cf\u5c11\u5377\u805a\u5408\u671f\u95f4\u7684\u6b67\u4e49\u548c\u5197\u4f59\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u8003\u8651\u6df1\u5ea6\u65b9\u9762\u7684\u76f8\u5bf9\u8bef\u5dee\uff0c\u4e3a\u901a\u7528\u7acb\u4f53\u5339\u914d\u548c\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u63d0\u4f9b\u7efc\u5408\u8bc4\u4f30\u3002\u6211\u4eec\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5e7f\u6cdb\u9a8c\u8bc1\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u65e0\u7eb9\u7406\u56fe\u50cf\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u8be5\u4ee3\u7801\u53ef\u4ece https://github.com/ztsrxh/DVANet \u83b7\u53d6\u3002|[2402.08931v1](http://arxiv.org/pdf/2402.08931v1)|**[link](https://github.com/ztsrxh/DVANet)**|\n"}, "LLM": {}, "Transformer": {"2402.09164": "|**2024-02-14**|**Less is More: Fewer Interpretable Region via Submodular Subset Selection**|\u5c11\u5373\u662f\u591a\uff1a\u901a\u8fc7\u5b50\u6a21\u5b50\u96c6\u9009\u62e9\u51cf\u5c11\u53ef\u89e3\u91ca\u533a\u57df|Ruoyu Chen, Hua Zhang, Siyuan Liang, Jingzhi Li, Xiaochun Cao|Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate fine-grained interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, effectiveness, consistency, and collaboration scores, to assess the importance of various subsets. Moreover, our theoretical analysis substantiates that the proposed function is in fact submodular. Extensive experiments show that the proposed method outperforms SOTA methods on two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset (CUB-200-2011). For correctly predicted samples, the proposed method improves the Deletion and Insertion scores with an average of 4.9% and 2.5% gain relative to HSIC-Attribution. For incorrectly predicted samples, our method achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in the average highest confidence and Insertion score respectively. The code is released at https://github.com/RuoyuChen10/SMDL-Attribution.|\u56fe\u50cf\u5f52\u56e0\u7b97\u6cd5\u65e8\u5728\u8bc6\u522b\u4e0e\u6a21\u578b\u51b3\u7b56\u9ad8\u5ea6\u76f8\u5173\u7684\u91cd\u8981\u533a\u57df\u3002\u5c3d\u7ba1\u73b0\u6709\u7684\u5f52\u56e0\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u6709\u6548\u5730\u5206\u914d\u76ee\u6807\u5143\u7d20\u7684\u91cd\u8981\u6027\uff0c\u4f46\u4ecd\u7136\u9762\u4e34\u4ee5\u4e0b\u6311\u6218\uff1a1\uff09\u73b0\u6709\u7684\u5f52\u56e0\u65b9\u6cd5\u751f\u6210\u4e0d\u51c6\u786e\u7684\u5c0f\u533a\u57df\uff0c\u4ece\u800c\u8bef\u5bfc\u4e86\u6b63\u786e\u5f52\u56e0\u7684\u65b9\u5411\uff1b2\uff09\u6a21\u578b\u65e0\u6cd5\u5bf9\u9519\u8bef\u7684\u6837\u672c\u4ea7\u751f\u826f\u597d\u7684\u5f52\u56e0\u7ed3\u679c\u9884\u6d4b\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u672c\u6587\u5c06\u4e0a\u8ff0\u56fe\u50cf\u5f52\u56e0\u95ee\u9898\u91cd\u65b0\u5efa\u6a21\u4e3a\u5b50\u6a21\u5b50\u96c6\u9009\u62e9\u95ee\u9898\uff0c\u65e8\u5728\u4f7f\u7528\u66f4\u5c11\u7684\u533a\u57df\u6765\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u5bf9\u5c40\u90e8\u533a\u57df\u7f3a\u4e4f\u5173\u6ce8\u7684\u95ee\u9898\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b50\u6a21\u51fd\u6570\u6765\u53d1\u73b0\u66f4\u51c6\u786e\u7684\u7ec6\u7c92\u5ea6\u89e3\u91ca\u533a\u57df\u3002\u4e3a\u4e86\u589e\u5f3a\u6240\u6709\u6837\u672c\u7684\u5f52\u56e0\u6548\u679c\uff0c\u6211\u4eec\u8fd8\u5bf9\u5b50\u533a\u57df\u7684\u9009\u62e9\u65bd\u52a0\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u7ea6\u675f\uff0c\u5373\u7f6e\u4fe1\u5ea6\u3001\u6709\u6548\u6027\u3001\u4e00\u81f4\u6027\u548c\u534f\u4f5c\u5206\u6570\uff0c\u4ee5\u8bc4\u4f30\u5404\u4e2a\u5b50\u96c6\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7406\u8bba\u5206\u6790\u8bc1\u5b9e\u6240\u63d0\u51fa\u7684\u51fd\u6570\u5b9e\u9645\u4e0a\u662f\u5b50\u6a21\u7684\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u4eba\u8138\u6570\u636e\u96c6\uff08Celeb-A \u548c VGG-Face2\uff09\u548c\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\uff08CUB-200-2011\uff09\u4e0a\u4f18\u4e8e SOTA \u65b9\u6cd5\u3002\u5bf9\u4e8e\u6b63\u786e\u9884\u6d4b\u7684\u6837\u672c\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e HSIC-Attribution \u63d0\u9ad8\u4e86\u5220\u9664\u548c\u63d2\u5165\u5206\u6570\uff0c\u5e73\u5747\u63d0\u9ad8\u4e86 4.9% \u548c 2.5%\u3002\u5bf9\u4e8e\u9519\u8bef\u9884\u6d4b\u7684\u6837\u672c\uff0c\u4e0e HSIC-Attribution \u7b97\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5e73\u5747\u6700\u9ad8\u7f6e\u4fe1\u5ea6\u548c\u63d2\u5165\u5206\u6570\u65b9\u9762\u5206\u522b\u5b9e\u73b0\u4e86 81.0% \u548c 18.4% \u7684\u589e\u76ca\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/RuoyuChen10/SMDL-Attribution\u3002|[2402.09164v1](http://arxiv.org/pdf/2402.09164v1)|null|\n", "2402.09016": "|**2024-02-14**|**Pyramid Attention Network for Medical Image Registration**|\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u91d1\u5b57\u5854\u6ce8\u610f\u529b\u7f51\u7edc|Zhuoyuan Wang, Haiqiao Wang, Yi Wang|The advent of deep-learning-based registration networks has addressed the time-consuming challenge in traditional iterative methods.However, the potential of current registration networks for comprehensively capturing spatial relationships has not been fully explored, leading to inadequate performance in large-deformation image registration.The pure convolutional neural networks (CNNs) neglect feature enhancement, while current Transformer-based networks are susceptible to information redundancy.To alleviate these issues, we propose a pyramid attention network (PAN) for deformable medical image registration.Specifically, the proposed PAN incorporates a dual-stream pyramid encoder with channel-wise attention to boost the feature representation.Moreover, a multi-head local attention Transformer is introduced as decoder to analyze motion patterns and generate deformation fields.Extensive experiments on two public brain magnetic resonance imaging (MRI) datasets and one abdominal MRI dataset demonstrate that our method achieves favorable registration performance, while outperforming several CNN-based and Transformer-based registration networks.Our code is publicly available at https://github.com/JuliusWang-7/PAN.|\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u914d\u51c6\u7f51\u7edc\u7684\u51fa\u73b0\u89e3\u51b3\u4e86\u4f20\u7edf\u8fed\u4ee3\u65b9\u6cd5\u4e2d\u8017\u65f6\u7684\u6311\u6218\u3002\u7136\u800c\uff0c\u5f53\u524d\u914d\u51c6\u7f51\u7edc\u5168\u9762\u6355\u83b7\u7a7a\u95f4\u5173\u7cfb\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u6316\u6398\uff0c\u5bfc\u81f4\u5728\u5927\u53d8\u5f62\u56fe\u50cf\u4e2d\u8868\u73b0\u4e0d\u8db3\u7eaf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5ffd\u7565\u7279\u5f81\u589e\u5f3a\uff0c\u800c\u5f53\u524d\u57fa\u4e8e Transformer \u7684\u7f51\u7edc\u5bb9\u6613\u53d7\u5230\u4fe1\u606f\u5197\u4f59\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53ef\u53d8\u5f62\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u91d1\u5b57\u5854\u6ce8\u610f\u7f51\u7edc\uff08PAN\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u63d0\u51fa\u4e86PAN \u7ed3\u5408\u4e86\u5177\u6709\u901a\u9053\u6ce8\u610f\u529b\u7684\u53cc\u6d41\u91d1\u5b57\u5854\u7f16\u7801\u5668\u6765\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002\u6b64\u5916\uff0c\u5f15\u5165\u591a\u5934\u5c40\u90e8\u6ce8\u610f\u529b Transformer \u4f5c\u4e3a\u89e3\u7801\u5668\u6765\u5206\u6790\u8fd0\u52a8\u6a21\u5f0f\u5e76\u751f\u6210\u53d8\u5f62\u573a\u3002\u5728\u4e24\u4e2a\u516c\u5171\u8111\u78c1\u5171\u632f\u6210\u50cf\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c(MRI) \u6570\u636e\u96c6\u548c\u4e00\u4e2a\u8179\u90e8 MRI \u6570\u636e\u96c6\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u914d\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u4f18\u4e8e\u51e0\u4e2a\u57fa\u4e8e CNN \u548c\u57fa\u4e8e Transformer \u7684\u914d\u51c6\u7f51\u7edc\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/JuliusWang-7/PAN \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2402.09016v1](http://arxiv.org/pdf/2402.09016v1)|null|\n", "2402.08994": "|**2024-02-14**|**CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding**|CLIP-MUSED\uff1aCLIP\u5f15\u5bfc\u7684\u591a\u4e3b\u9898\u89c6\u89c9\u795e\u7ecf\u4fe1\u606f\u8bed\u4e49\u89e3\u7801|Qiongyi Zhou, Changde Du, Shengpei Wang, Huiguang He|The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method consists of a Transformer-based feature extractor to effectively model global neural representations. It also incorporates learnable subject-specific tokens that facilitates the aggregation of multi-subject data without a linear increase of parameters. Additionally, we employ representational similarity analysis (RSA) to guide token representation learning based on the topological relationship of visual stimuli in the representation space of CLIP, enabling full characterization of the relationship between neural responses of different subjects under different stimuli. Finally, token representations are used for multi-subject semantic decoding. Our proposed method outperforms single-subject decoding methods and achieves state-of-the-art performance among the existing multi-subject methods on two fMRI datasets. Visualization results provide insights into the effectiveness of our proposed method. Code is available at https://github.com/CLIP-MUSED/CLIP-MUSED.|\u7531\u4e8e\u4e2a\u4f53\u5dee\u5f02\uff0c\u89e3\u7801\u89c6\u89c9\u795e\u7ecf\u4fe1\u606f\u7684\u7814\u7a76\u9762\u4e34\u7740\u5c06\u5355\u53d7\u8bd5\u8005\u89e3\u7801\u6a21\u578b\u63a8\u5e7f\u5230\u591a\u4e2a\u53d7\u8bd5\u8005\u7684\u6311\u6218\u3002\u6b64\u5916\uff0c\u6765\u81ea\u5355\u4e2a\u4e3b\u9898\u7684\u6570\u636e\u7684\u6709\u9650\u53ef\u7528\u6027\u4f1a\u5bf9\u6a21\u578b\u6027\u80fd\u4ea7\u751f\u9650\u5236\u6027\u5f71\u54cd\u3002\u5c3d\u7ba1\u73b0\u6709\u7684\u591a\u5bf9\u8c61\u89e3\u7801\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u7740\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u5305\u62ec\u96be\u4ee5\u63d0\u53d6\u5168\u5c40\u795e\u7ecf\u53cd\u5e94\u7279\u5f81\u3001\u6a21\u578b\u53c2\u6570\u968f\u53d7\u8bd5\u8005\u6570\u91cf\u7ebf\u6027\u7f29\u653e\u4ee5\u53ca\u5bf9\u4e0d\u540c\u795e\u7ecf\u53cd\u5e94\u4e4b\u95f4\u5173\u7cfb\u7684\u523b\u753b\u4e0d\u591f\u5145\u5206\u3002\u4f1a\u53d7\u5230\u5404\u79cd\u523a\u6fc0\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd CLIP \u5f15\u5bfc\u7684\u591a\u4e3b\u4f53\u89c6\u89c9\u795e\u7ecf\u4fe1\u606f\u8bed\u4e49\u89e3\u7801\uff08CLIP-MUSED\uff09\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7531\u57fa\u4e8e Transformer \u7684\u7279\u5f81\u63d0\u53d6\u5668\u7ec4\u6210\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5bf9\u5168\u5c40\u795e\u7ecf\u8868\u793a\u8fdb\u884c\u5efa\u6a21\u3002\u5b83\u8fd8\u5305\u542b\u53ef\u5b66\u4e60\u7684\u7279\u5b9a\u4e8e\u4e3b\u9898\u7684\u6807\u8bb0\uff0c\u6709\u52a9\u4e8e\u591a\u4e3b\u9898\u6570\u636e\u7684\u805a\u5408\uff0c\u800c\u65e0\u9700\u7ebf\u6027\u589e\u52a0\u53c2\u6570\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528\u8868\u5f81\u76f8\u4f3c\u6027\u5206\u6790\uff08RSA\uff09\u6765\u6307\u5bfc\u57fa\u4e8eCLIP\u8868\u5f81\u7a7a\u95f4\u4e2d\u89c6\u89c9\u523a\u6fc0\u7684\u62d3\u6251\u5173\u7cfb\u7684\u6807\u8bb0\u8868\u5f81\u5b66\u4e60\uff0c\u4ece\u800c\u80fd\u591f\u5145\u5206\u8868\u5f81\u4e0d\u540c\u53d7\u8bd5\u8005\u5728\u4e0d\u540c\u523a\u6fc0\u4e0b\u7684\u795e\u7ecf\u53cd\u5e94\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6700\u540e\uff0c\u4ee4\u724c\u8868\u793a\u7528\u4e8e\u591a\u4e3b\u9898\u8bed\u4e49\u89e3\u7801\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u5355\u53d7\u8bd5\u8005\u89e3\u7801\u65b9\u6cd5\uff0c\u5e76\u5728\u4e24\u4e2a fMRI \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u73b0\u6709\u591a\u53d7\u8bd5\u8005\u65b9\u6cd5\u4e2d\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u53ef\u89c6\u5316\u7ed3\u679c\u8ba9\u6211\u4eec\u6df1\u5165\u4e86\u89e3\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/CLIP-MUSED/CLIP-MUSED \u83b7\u53d6\u3002|[2402.08994v1](http://arxiv.org/pdf/2402.08994v1)|null|\n", "2402.08936": "|**2024-02-14**|**Predictive Temporal Attention on Event-based Video Stream for Energy-efficient Situation Awareness**|\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u9891\u6d41\u7684\u9884\u6d4b\u65f6\u95f4\u6ce8\u610f\u529b\u4ee5\u5b9e\u73b0\u8282\u80fd\u6001\u52bf\u611f\u77e5|Yiming Bu, Jiayang Liu, Qinru Qiu|The Dynamic Vision Sensor (DVS) is an innovative technology that efficiently captures and encodes visual information in an event-driven manner. By combining it with event-driven neuromorphic processing, the sparsity in DVS camera output can result in high energy efficiency. However, similar to many embedded systems, the off-chip communication between the camera and processor presents a bottleneck in terms of power consumption. Inspired by the predictive coding model and expectation suppression phenomenon found in human brain, we propose a temporal attention mechanism to throttle the camera output and pay attention to it only when the visual events cannot be well predicted. The predictive attention not only reduces power consumption in the sensor-processor interface but also effectively decreases the computational workload by filtering out noisy events. We demonstrate that the predictive attention can reduce 46.7% of data communication between the camera and the processor and reduce 43.8% computation activities in the processor.|\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668 (DVS) \u662f\u4e00\u9879\u521b\u65b0\u6280\u672f\uff0c\u80fd\u591f\u4ee5\u4e8b\u4ef6\u9a71\u52a8\u7684\u65b9\u5f0f\u6709\u6548\u6355\u83b7\u548c\u7f16\u7801\u89c6\u89c9\u4fe1\u606f\u3002\u901a\u8fc7\u5c06\u5176\u4e0e\u4e8b\u4ef6\u9a71\u52a8\u7684\u795e\u7ecf\u62df\u6001\u5904\u7406\u76f8\u7ed3\u5408\uff0cDVS \u6444\u50cf\u673a\u8f93\u51fa\u7684\u7a00\u758f\u6027\u53ef\u4ee5\u5e26\u6765\u9ad8\u80fd\u6548\u3002\u7136\u800c\uff0c\u4e0e\u8bb8\u591a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7c7b\u4f3c\uff0c\u76f8\u673a\u548c\u5904\u7406\u5668\u4e4b\u95f4\u7684\u7247\u5916\u901a\u4fe1\u5728\u529f\u8017\u65b9\u9762\u5b58\u5728\u74f6\u9888\u3002\u53d7\u4eba\u8111\u4e2d\u53d1\u73b0\u7684\u9884\u6d4b\u7f16\u7801\u6a21\u578b\u548c\u671f\u671b\u6291\u5236\u73b0\u8c61\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u6ce8\u610f\u529b\u673a\u5236\u6765\u9650\u5236\u76f8\u673a\u8f93\u51fa\uff0c\u5e76\u4ec5\u5728\u65e0\u6cd5\u5f88\u597d\u9884\u6d4b\u89c6\u89c9\u4e8b\u4ef6\u65f6\u624d\u5173\u6ce8\u5b83\u3002\u9884\u6d4b\u6ce8\u610f\u529b\u4e0d\u4ec5\u964d\u4f4e\u4e86\u4f20\u611f\u5668-\u5904\u7406\u5668\u63a5\u53e3\u7684\u529f\u8017\uff0c\u800c\u4e14\u8fd8\u901a\u8fc7\u6ee4\u9664\u566a\u58f0\u4e8b\u4ef6\u6709\u6548\u5730\u51cf\u5c11\u4e86\u8ba1\u7b97\u5de5\u4f5c\u91cf\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u9884\u6d4b\u6ce8\u610f\u529b\u53ef\u4ee5\u51cf\u5c11\u76f8\u673a\u548c\u5904\u7406\u5668\u4e4b\u95f4 46.7% \u7684\u6570\u636e\u901a\u4fe1\uff0c\u5e76\u51cf\u5c11\u5904\u7406\u5668\u4e2d 43.8% \u7684\u8ba1\u7b97\u6d3b\u52a8\u3002|[2402.08936v1](http://arxiv.org/pdf/2402.08936v1)|null|\n"}, "3D/CG": {"2402.09341": "|**2024-02-14**|**Registration of Longitudinal Spine CTs for Monitoring Lesion Growth**|\u7528\u4e8e\u76d1\u6d4b\u75c5\u53d8\u751f\u957f\u7684\u7eb5\u5411\u810a\u67f1 CT \u914d\u51c6|Malika Sanhinova, Nazim Haouchine, Steve D. Pieper, William M. Wells III, Tracy A. Balboni, Alexander Spektor, Mai Anh Huynh, Jeffrey P. Guenette, Bryan Czajkowski, Sarah Caplan, et.al.|Accurate and reliable registration of longitudinal spine images is essential for assessment of disease progression and surgical outcome. Implementing a fully automatic and robust registration is crucial for clinical use, however, it is challenging due to substantial change in shape and appearance due to lesions. In this paper we present a novel method to automatically align longitudinal spine CTs and accurately assess lesion progression. Our method follows a two-step pipeline where vertebrae are first automatically localized, labeled and 3D surfaces are generated using a deep learning model, then longitudinally aligned using a Gaussian mixture model surface registration. We tested our approach on 37 vertebrae, from 5 patients, with baseline CTs and 3, 6, and 12 months follow-ups leading to 111 registrations. Our experiment showed accurate registration with an average Hausdorff distance of 0.65 mm and average Dice score of 0.92.|\u51c6\u786e\u53ef\u9760\u7684\u7eb5\u5411\u810a\u67f1\u56fe\u50cf\u914d\u51c6\u5bf9\u4e8e\u8bc4\u4f30\u75be\u75c5\u8fdb\u5c55\u548c\u624b\u672f\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002\u5b9e\u73b0\u5168\u81ea\u52a8\u548c\u7a33\u5065\u7684\u914d\u51c6\u5bf9\u4e8e\u4e34\u5e8a\u4f7f\u7528\u81f3\u5173\u91cd\u8981\uff0c\u7136\u800c\uff0c\u7531\u4e8e\u75c5\u53d8\u5bfc\u81f4\u5f62\u72b6\u548c\u5916\u89c2\u53d1\u751f\u5de8\u5927\u53d8\u5316\uff0c\u8fd9\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5bf9\u9f50\u7eb5\u5411\u810a\u67f1 CT \u5e76\u51c6\u786e\u8bc4\u4f30\u75c5\u53d8\u8fdb\u5c55\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9075\u5faa\u4e24\u6b65\u6d41\u7a0b\uff0c\u9996\u5148\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u81ea\u52a8\u5b9a\u4f4d\u3001\u6807\u8bb0\u690e\u9aa8\u5e76\u751f\u6210 3D \u8868\u9762\uff0c\u7136\u540e\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u8868\u9762\u914d\u51c6\u8fdb\u884c\u7eb5\u5411\u5bf9\u9f50\u3002\u6211\u4eec\u5728 5 \u540d\u60a3\u8005\u7684 37 \u5757\u690e\u9aa8\u4e0a\u6d4b\u8bd5\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u57fa\u7ebf CT \u626b\u63cf\u548c 3\u30016 \u548c 12 \u4e2a\u6708\u7684\u968f\u8bbf\uff0c\u6700\u7ec8\u83b7\u5f97\u4e86 111 \u4f8b\u6ce8\u518c\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u663e\u793a\u914d\u51c6\u51c6\u786e\uff0c\u5e73\u5747 Hausdorff \u8ddd\u79bb\u4e3a 0.65 \u6beb\u7c73\uff0c\u5e73\u5747 Dice \u5f97\u5206\u4e3a 0.92\u3002|[2402.09341v1](http://arxiv.org/pdf/2402.09341v1)|null|\n", "2402.08876": "|**2024-02-14**|**DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling**|DUDF\uff1a\u5177\u6709\u53cc\u66f2\u6807\u5ea6\u7684\u53ef\u5fae\u5206\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a|Miguel Fainstein, Viviana Siless, Emmanuel Iarussi|In recent years, there has been a growing interest in training Neural Networks to approximate Unsigned Distance Fields (UDFs) for representing open surfaces in the context of 3D reconstruction. However, UDFs are non-differentiable at the zero level set which leads to significant errors in distances and gradients, generally resulting in fragmented and discontinuous surfaces. In this paper, we propose to learn a hyperbolic scaling of the unsigned distance field, which defines a new Eikonal problem with distinct boundary conditions. This allows our formulation to integrate seamlessly with state-of-the-art continuously differentiable implicit neural representation networks, largely applied in the literature to represent signed distance fields. Our approach not only addresses the challenge of open surface representation but also demonstrates significant improvement in reconstruction quality and training performance. Moreover, the unlocked field's differentiability allows the accurate computation of essential topological properties such as normal directions and curvatures, pervasive in downstream tasks such as rendering. Through extensive experiments, we validate our approach across various data sets and against competitive baselines. The results demonstrate enhanced accuracy and up to an order of magnitude increase in speed compared to previous methods.|\u8fd1\u5e74\u6765\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u8fd1\u4f3c\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a (UDF)\uff0c\u4ee5\u5728 3D \u91cd\u5efa\u7684\u80cc\u666f\u4e0b\u8868\u793a\u5f00\u653e\u8868\u9762\u3002\u7136\u800c\uff0cUDF \u5728\u96f6\u6c34\u5e73\u96c6\u4e0a\u662f\u4e0d\u53ef\u5fae\u7684\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8ddd\u79bb\u548c\u68af\u5ea6\u51fa\u73b0\u663e\u7740\u8bef\u5dee\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u8868\u9762\u788e\u7247\u5316\u548c\u4e0d\u8fde\u7eed\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u5b66\u4e60\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\u7684\u53cc\u66f2\u7f29\u653e\uff0c\u5b83\u5b9a\u4e49\u4e86\u5177\u6709\u4e0d\u540c\u8fb9\u754c\u6761\u4ef6\u7684\u65b0 Eikonal \u95ee\u9898\u3002\u8fd9\u4f7f\u5f97\u6211\u4eec\u7684\u516c\u5f0f\u80fd\u591f\u4e0e\u6700\u5148\u8fdb\u7684\u8fde\u7eed\u53ef\u5fae\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7f51\u7edc\u65e0\u7f1d\u96c6\u6210\uff0c\u8be5\u7f51\u7edc\u5728\u6587\u732e\u4e2d\u4e3b\u8981\u7528\u4e8e\u8868\u793a\u5e26\u7b26\u53f7\u7684\u8ddd\u79bb\u573a\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5f00\u653e\u8868\u9762\u8868\u793a\u7684\u6311\u6218\uff0c\u800c\u4e14\u8fd8\u5c55\u793a\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u8bad\u7ec3\u6027\u80fd\u7684\u663e\u7740\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u672a\u9501\u5b9a\u573a\u7684\u53ef\u5fae\u6027\u5141\u8bb8\u7cbe\u786e\u8ba1\u7b97\u57fa\u672c\u62d3\u6251\u5c5e\u6027\uff0c\u4f8b\u5982\u6cd5\u7ebf\u65b9\u5411\u548c\u66f2\u7387\uff0c\u666e\u904d\u5b58\u5728\u4e8e\u6e32\u67d3\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5e76\u5bf9\u7167\u7ade\u4e89\u57fa\u7ebf\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7cbe\u5ea6\u63d0\u9ad8\u4e86\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002|[2402.08876v1](http://arxiv.org/pdf/2402.08876v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {}, "\u5176\u4ed6": {"2402.09237": "|**2024-02-14**|**Weatherproofing Retrieval for Localization with Generative AI and Geometric Consistency**|\u5229\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u8fdb\u884c\u672c\u5730\u5316\u9632\u98ce\u96e8\u68c0\u7d22|Yannis Kalantidis, Mert B\u00fclent Sar\u0131y\u0131ld\u0131z, Rafael S. Rezende, Philippe Weinzaepfel, Diane Larlus, Gabriela Csurka|State-of-the-art visual localization approaches generally rely on a first image retrieval step whose role is crucial. Yet, retrieval often struggles when facing varying conditions, due to e.g. weather or time of day, with dramatic consequences on the visual localization accuracy. In this paper, we improve this retrieval step and tailor it to the final localization task. Among the several changes we advocate for, we propose to synthesize variants of the training set images, obtained from generative text-to-image models, in order to automatically expand the training set towards a number of nameable variations that particularly hurt visual localization. After expanding the training set, we propose a training approach that leverages the specificities and the underlying geometry of this mix of real and synthetic images. We experimentally show that those changes translate into large improvements for the most challenging visual localization datasets. Project page: https://europe.naverlabs.com/ret4loc|\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7b2c\u4e00\u4e2a\u56fe\u50cf\u68c0\u7d22\u6b65\u9aa4\uff0c\u5176\u4f5c\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728\u9762\u5bf9\u4e0d\u540c\u7684\u6761\u4ef6\u65f6\uff0c\u68c0\u7d22\u5e38\u5e38\u4f1a\u9047\u5230\u56f0\u96be\uff0c\u4f8b\u5982\uff0c\u5929\u6c14\u6216\u4e00\u5929\u4e2d\u7684\u65f6\u95f4\uff0c\u5bf9\u89c6\u89c9\u5b9a\u4f4d\u7cbe\u5ea6\u4ea7\u751f\u5de8\u5927\u5f71\u54cd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6539\u8fdb\u4e86\u8fd9\u4e2a\u68c0\u7d22\u6b65\u9aa4\uff0c\u5e76\u5c06\u5176\u8c03\u6574\u4e3a\u6700\u7ec8\u7684\u672c\u5730\u5316\u4efb\u52a1\u3002\u5728\u6211\u4eec\u63d0\u5021\u7684\u51e0\u9879\u6539\u53d8\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u5408\u6210\u4ece\u751f\u6210\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u83b7\u5f97\u7684\u8bad\u7ec3\u96c6\u56fe\u50cf\u7684\u53d8\u4f53\uff0c\u4ee5\u4fbf\u81ea\u52a8\u5c06\u8bad\u7ec3\u96c6\u6269\u5c55\u5230\u8bb8\u591a\u7279\u522b\u635f\u5bb3\u89c6\u89c9\u5b9a\u4f4d\u7684\u53ef\u547d\u540d\u53d8\u4f53\u3002\u6269\u5c55\u8bad\u7ec3\u96c6\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u771f\u5b9e\u56fe\u50cf\u548c\u5408\u6210\u56fe\u50cf\u6df7\u5408\u7684\u7279\u6b8a\u6027\u548c\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u53d8\u5316\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6700\u5177\u6311\u6218\u6027\u7684\u89c6\u89c9\u5b9a\u4f4d\u6570\u636e\u96c6\u7684\u5de8\u5927\u6539\u8fdb\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://europe.naverlabs.com/ret4loc|[2402.09237v1](http://arxiv.org/pdf/2402.09237v1)|null|\n", "2402.09189": "|**2024-02-14**|**Traj-LIO: A Resilient Multi-LiDAR Multi-IMU State Estimator Through Sparse Gaussian Process**|Traj-LIO\uff1a\u901a\u8fc7\u7a00\u758f\u9ad8\u65af\u8fc7\u7a0b\u7684\u5f39\u6027\u591a LiDAR \u591a IMU \u72b6\u6001\u4f30\u8ba1\u5668|Xin Zheng, Jianke Zhu|Nowadays, sensor suits have been equipped with redundant LiDARs and IMUs to mitigate the risks associated with sensor failure. It is challenging for the previous discrete-time and IMU-driven kinematic systems to incorporate multiple asynchronized sensors, which are susceptible to abnormal IMU data. To address these limitations, we introduce a multi-LiDAR multi-IMU state estimator by taking advantage of Gaussian Process (GP) that predicts a non-parametric continuous-time trajectory to capture sensors' spatial-temporal movement with limited control states. Since the kinematic model driven by three types of linear time-invariant stochastic differential equations are independent of external sensor measurements, our proposed approach is capable of handling different sensor configurations and resilient to sensor failures. Moreover, we replace the conventional $\\mathrm{SE}(3)$ state representation with the combination of $\\mathrm{SO}(3)$ and vector space, which enables GP-based LiDAR-inertial system to fulfill the real-time requirement. Extensive experiments on the public datasets demonstrate the versatility and resilience of our proposed multi-LiDAR multi-IMU state estimator. To contribute to the community, we will make our source code publicly available.|\u5982\u4eca\uff0c\u4f20\u611f\u5668\u5957\u4ef6\u5df2\u914d\u5907\u5197\u4f59 LiDAR \u548c IMU\uff0c\u4ee5\u51cf\u8f7b\u4e0e\u4f20\u611f\u5668\u6545\u969c\u76f8\u5173\u7684\u98ce\u9669\u3002\u5bf9\u4e8e\u4ee5\u524d\u7684\u79bb\u6563\u65f6\u95f4\u548c IMU \u9a71\u52a8\u7684\u8fd0\u52a8\u7cfb\u7edf\u6765\u8bf4\uff0c\u6574\u5408\u591a\u4e2a\u5f02\u6b65\u4f20\u611f\u5668\u662f\u4e00\u9879\u6311\u6218\uff0c\u8fd9\u4e9b\u4f20\u611f\u5668\u5bb9\u6613\u53d7\u5230\u5f02\u5e38 IMU \u6570\u636e\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u591a LiDAR \u591a IMU \u72b6\u6001\u4f30\u8ba1\u5668\uff0c\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b (GP) \u9884\u6d4b\u975e\u53c2\u6570\u8fde\u7eed\u65f6\u95f4\u8f68\u8ff9\uff0c\u4ee5\u6355\u83b7\u5177\u6709\u6709\u9650\u63a7\u5236\u72b6\u6001\u7684\u4f20\u611f\u5668\u7684\u65f6\u7a7a\u8fd0\u52a8\u3002\u7531\u4e8e\u7531\u4e09\u79cd\u7c7b\u578b\u7684\u7ebf\u6027\u65f6\u4e0d\u53d8\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u9a71\u52a8\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\u72ec\u7acb\u4e8e\u5916\u90e8\u4f20\u611f\u5668\u6d4b\u91cf\uff0c\u56e0\u6b64\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4e0d\u540c\u7684\u4f20\u611f\u5668\u914d\u7f6e\u5e76\u5bf9\u4f20\u611f\u5668\u6545\u969c\u5177\u6709\u5f39\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u4f20\u7edf\u7684$\\mathrm{SE}(3)$\u72b6\u6001\u8868\u793a\u66ff\u6362\u4e3a$\\mathrm{SO}(3)$\u548c\u5411\u91cf\u7a7a\u95f4\u7684\u7ec4\u5408\uff0c\u8fd9\u4f7f\u5f97\u57fa\u4e8eGP\u7684LiDAR\u60ef\u6027\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u5b9e\u6570\u65f6\u95f4\u8981\u6c42\u3002\u5bf9\u516c\u5171\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u591a LiDAR \u591a IMU \u72b6\u6001\u4f30\u8ba1\u5668\u7684\u591a\u529f\u80fd\u6027\u548c\u5f39\u6027\u3002\u4e3a\u4e86\u4e3a\u793e\u533a\u505a\u51fa\u8d21\u732e\uff0c\u6211\u4eec\u5c06\u516c\u5f00\u6e90\u4ee3\u7801\u3002|[2402.09189v1](http://arxiv.org/pdf/2402.09189v1)|null|\n", "2402.09178": "|**2024-02-14**|**Generalized Portrait Quality Assessment**|\u5e7f\u4e49\u8096\u50cf\u8d28\u91cf\u8bc4\u4f30|Nicolas Chahine, Sira Ferradans, Javier Vazquez-Corral, Jean Ponce|Automated and robust portrait quality assessment (PQA) is of paramount importance in high-impact applications such as smartphone photography. This paper presents FHIQA, a learning-based approach to PQA that introduces a simple but effective quality score rescaling method based on image semantics, to enhance the precision of fine-grained image quality metrics while ensuring robust generalization to various scene settings beyond the training dataset. The proposed approach is validated by extensive experiments on the PIQ23 benchmark and comparisons with the current state of the art. The source code of FHIQA will be made publicly available on the PIQ23 GitHub repository at https://github.com/DXOMARK-Research/PIQ2023.|\u81ea\u52a8\u5316\u4e14\u5f3a\u5927\u7684\u8096\u50cf\u8d28\u91cf\u8bc4\u4f30 (PQA) \u5728\u667a\u80fd\u624b\u673a\u6444\u5f71\u7b49\u9ad8\u5f71\u54cd\u529b\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86 FHIQA\uff0c\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684 PQA \u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u8bed\u4e49\u7684\u7b80\u5355\u4f46\u6709\u6548\u7684\u8d28\u91cf\u5206\u6570\u91cd\u65b0\u7f29\u653e\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u7ec6\u7c92\u5ea6\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u786e\u4fdd\u5bf9\u8bad\u7ec3\u6570\u636e\u96c6\u4e4b\u5916\u7684\u5404\u79cd\u573a\u666f\u8bbe\u7f6e\u7684\u9c81\u68d2\u6cdb\u5316\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7 PIQ23 \u57fa\u51c6\u7684\u5927\u91cf\u5b9e\u9a8c\u4ee5\u53ca\u4e0e\u5f53\u524d\u6280\u672f\u6c34\u5e73\u7684\u6bd4\u8f83\u5f97\u5230\u9a8c\u8bc1\u3002 FHIQA \u7684\u6e90\u4ee3\u7801\u5c06\u5728 PIQ23 GitHub \u5b58\u50a8\u5e93\uff08https://github.com/DXOMARK-Research/PIQ2023\uff09\u4e0a\u516c\u5f00\u63d0\u4f9b\u3002|[2402.09178v1](http://arxiv.org/pdf/2402.09178v1)|**[link](https://github.com/dxomark-research/piq2023)**|\n"}}