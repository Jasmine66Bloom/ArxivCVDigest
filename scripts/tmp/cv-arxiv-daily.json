{"\u751f\u6210\u6a21\u578b": {"2402.16506": "|**2024-02-26**|**Stochastic Conditional Diffusion Models for Semantic Image Synthesis**|\u7528\u4e8e\u8bed\u4e49\u56fe\u50cf\u5408\u6210\u7684\u968f\u673a\u6761\u4ef6\u6269\u6563\u6a21\u578b|Juyeon Ko, Inho Kong, Hyunwoo J. Kim|Semantic image synthesis (SIS) is a task to generate realistic images corresponding to semantic maps (labels). It can be applied to diverse real-world practices such as photo editing or content creation. However, in real-world applications, SIS often encounters noisy user inputs. To address this, we propose Stochastic Conditional Diffusion Model (SCDM), which is a robust conditional diffusion model that features novel forward and generation processes tailored for SIS with noisy labels. It enhances robustness by stochastically perturbing the semantic label maps through Label Diffusion, which diffuses the labels with discrete diffusion. Through the diffusion of labels, the noisy and clean semantic maps become similar as the timestep increases, eventually becoming identical at $t=T$. This facilitates the generation of an image close to a clean image, enabling robust generation. Furthermore, we propose a class-wise noise schedule to differentially diffuse the labels depending on the class. We demonstrate that the proposed method generates high-quality samples through extensive experiments and analyses on benchmark datasets, including a novel experimental setup simulating human errors during real-world applications.|\u8bed\u4e49\u56fe\u50cf\u5408\u6210\uff08SIS\uff09\u662f\u751f\u6210\u4e0e\u8bed\u4e49\u56fe\uff08\u6807\u7b7e\uff09\u76f8\u5bf9\u5e94\u7684\u771f\u5b9e\u56fe\u50cf\u7684\u4efb\u52a1\u3002\u5b83\u53ef\u4ee5\u5e94\u7528\u4e8e\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u7684\u5b9e\u8df5\uff0c\u4f8b\u5982\u7167\u7247\u7f16\u8f91\u6216\u5185\u5bb9\u521b\u5efa\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cSIS \u7ecf\u5e38\u9047\u5230\u5608\u6742\u7684\u7528\u6237\u8f93\u5165\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u968f\u673a\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08SCDM\uff09\uff0c\u5b83\u662f\u4e00\u79cd\u7a33\u5065\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u5177\u6709\u4e3a\u5e26\u6709\u566a\u58f0\u6807\u7b7e\u7684 SIS \u91cf\u8eab\u5b9a\u5236\u7684\u65b0\u9896\u7684\u524d\u5411\u548c\u751f\u6210\u8fc7\u7a0b\u3002\u5b83\u901a\u8fc7\u6807\u7b7e\u6269\u6563\u968f\u673a\u6270\u52a8\u8bed\u4e49\u6807\u7b7e\u56fe\u6765\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u6807\u7b7e\u6269\u6563\u901a\u8fc7\u79bb\u6563\u6269\u6563\u6765\u6269\u6563\u6807\u7b7e\u3002\u901a\u8fc7\u6807\u7b7e\u7684\u6269\u6563\uff0c\u968f\u7740\u65f6\u95f4\u6b65\u957f\u7684\u589e\u52a0\uff0c\u566a\u58f0\u548c\u5e72\u51c0\u7684\u8bed\u4e49\u56fe\u53d8\u5f97\u76f8\u4f3c\uff0c\u6700\u7ec8\u5728 $t=T$ \u65f6\u53d8\u5f97\u76f8\u540c\u3002\u8fd9\u6709\u5229\u4e8e\u751f\u6210\u63a5\u8fd1\u5e72\u51c0\u56fe\u50cf\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5065\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6309\u7c7b\u522b\u7684\u566a\u58f0\u8ba1\u5212\uff0c\u4ee5\u6839\u636e\u7c7b\u522b\u5dee\u5f02\u5730\u6269\u6563\u6807\u7b7e\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5bf9\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u5206\u6790\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6837\u672c\uff0c\u5305\u62ec\u6a21\u62df\u73b0\u5b9e\u5e94\u7528\u4e2d\u4eba\u4e3a\u9519\u8bef\u7684\u65b0\u9896\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u3002|[2402.16506v1](http://arxiv.org/pdf/2402.16506v1)|null|\n", "2402.16421": "|**2024-02-26**|**Outline-Guided Object Inpainting with Diffusion Models**|\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8f6e\u5ed3\u5f15\u5bfc\u7684\u5bf9\u8c61\u4fee\u590d|Markus Pobitzer, Filip Janicki, Mattia Rigotti, Cristiano Malossi|Instance segmentation datasets play a crucial role in training accurate and robust computer vision models. However, obtaining accurate mask annotations to produce high-quality segmentation datasets is a costly and labor-intensive process. In this work, we show how this issue can be mitigated by starting with small annotated instance segmentation datasets and augmenting them to effectively obtain a sizeable annotated dataset. We achieve that by creating variations of the available annotated object instances in a way that preserves the provided mask annotations, thereby resulting in new image-mask pairs to be added to the set of annotated images. Specifically, we generate new images using a diffusion-based inpainting model to fill out the masked area with a desired object class by guiding the diffusion through the object outline. We show that the object outline provides a simple, but also reliable and convenient training-free guidance signal for the underlying inpainting model that is often sufficient to fill out the mask with an object of the correct class without further text guidance and preserve the correspondence between generated images and the mask annotations with high precision. Our experimental results reveal that our method successfully generates realistic variations of object instances, preserving their shape characteristics while introducing diversity within the augmented area. We also show that the proposed method can naturally be combined with text guidance and other image augmentation techniques.|\u5b9e\u4f8b\u5206\u5272\u6570\u636e\u96c6\u5728\u8bad\u7ec3\u51c6\u786e\u4e14\u7a33\u5065\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u83b7\u5f97\u51c6\u786e\u7684\u63a9\u6a21\u6ce8\u91ca\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5206\u5272\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u6210\u672c\u9ad8\u6602\u4e14\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u8fc7\u7a0b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u4ece\u5c0f\u578b\u5e26\u6ce8\u91ca\u7684\u5b9e\u4f8b\u5206\u5272\u6570\u636e\u96c6\u5f00\u59cb\u5e76\u5bf9\u5176\u8fdb\u884c\u6269\u5145\u4ee5\u6709\u6548\u5730\u83b7\u5f97\u76f8\u5f53\u5927\u7684\u5e26\u6ce8\u91ca\u6570\u636e\u96c6\u6765\u7f13\u89e3\u6b64\u95ee\u9898\u3002\u6211\u4eec\u901a\u8fc7\u4ee5\u4fdd\u7559\u63d0\u4f9b\u7684\u63a9\u6a21\u6ce8\u91ca\u7684\u65b9\u5f0f\u521b\u5efa\u53ef\u7528\u5e26\u6ce8\u91ca\u7684\u5bf9\u8c61\u5b9e\u4f8b\u7684\u53d8\u4f53\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u4ece\u800c\u5bfc\u81f4\u65b0\u7684\u56fe\u50cf\u63a9\u6a21\u5bf9\u6dfb\u52a0\u5230\u5e26\u6ce8\u91ca\u7684\u56fe\u50cf\u96c6\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u4fee\u590d\u6a21\u578b\u751f\u6210\u65b0\u56fe\u50cf\uff0c\u901a\u8fc7\u5f15\u5bfc\u6269\u6563\u7a7f\u8fc7\u5bf9\u8c61\u8f6e\u5ed3\uff0c\u7528\u6240\u9700\u7684\u5bf9\u8c61\u7c7b\u586b\u5145\u906e\u7f69\u533a\u57df\u3002\u6211\u4eec\u8868\u660e\uff0c\u5bf9\u8c61\u8f6e\u5ed3\u4e3a\u5e95\u5c42\u4fee\u590d\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u53ef\u9760\u4e14\u65b9\u4fbf\u7684\u514d\u8bad\u7ec3\u6307\u5bfc\u4fe1\u53f7\uff0c\u8be5\u4fe1\u53f7\u901a\u5e38\u8db3\u4ee5\u7528\u6b63\u786e\u7c7b\u522b\u7684\u5bf9\u8c61\u586b\u5145\u63a9\u6a21\uff0c\u800c\u65e0\u9700\u8fdb\u4e00\u6b65\u7684\u6587\u672c\u6307\u5bfc\uff0c\u5e76\u4fdd\u7559\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u751f\u6210\u7684\u56fe\u50cf\u548c\u63a9\u6a21\u6ce8\u91ca\u5177\u6709\u9ad8\u7cbe\u5ea6\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u5bf9\u8c61\u5b9e\u4f8b\u7684\u771f\u5b9e\u53d8\u5316\uff0c\u4fdd\u7559\u4e86\u5b83\u4eec\u7684\u5f62\u72b6\u7279\u5f81\uff0c\u540c\u65f6\u5728\u589e\u5f3a\u533a\u57df\u5185\u5f15\u5165\u4e86\u591a\u6837\u6027\u3002\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u81ea\u7136\u5730\u4e0e\u6587\u672c\u5f15\u5bfc\u548c\u5176\u4ed6\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u76f8\u7ed3\u5408\u3002|[2402.16421v1](http://arxiv.org/pdf/2402.16421v1)|null|\n", "2402.16392": "|**2024-02-26**|**Placing Objects in Context via Inpainting for Out-of-distribution Segmentation**|\u901a\u8fc7\u4fee\u590d\u5c06\u5bf9\u8c61\u653e\u7f6e\u5728\u4e0a\u4e0b\u6587\u4e2d\u4ee5\u8fdb\u884c\u5206\u5e03\u5916\u5206\u5272|Pau de Jorge, Riccardo Volpi, Puneet K. Dokania, Philip H. S. Torr, Gregory Rogez|When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training. Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities. However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous. Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts. In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via diffusion models. POC can be used to easily extend any dataset with an arbitrary number of objects. In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly fine-tuning methods in several standardized benchmarks. POC is also effective to learn new classes. For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline. This corroborates the low sim-to-real gap of models trained on POC-generated images.|\u5f53\u5c06\u8bed\u4e49\u5206\u5272\u6a21\u578b\u90e8\u7f72\u5230\u73b0\u5b9e\u4e16\u754c\u65f6\uff0c\u5b83\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u9047\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u770b\u4e0d\u89c1\u7684\u8bed\u4e49\u7c7b\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u5b89\u5168\u90e8\u7f72\u6b64\u7c7b\u7cfb\u7edf\uff0c\u51c6\u786e\u8bc4\u4f30\u548c\u63d0\u9ad8\u5176\u5f02\u5e38\u5206\u5272\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u83b7\u53d6\u548c\u6807\u8bb0\u8bed\u4e49\u5206\u5272\u6570\u636e\u7684\u6210\u672c\u5f88\u9ad8\uff0c\u800c\u4e14\u610f\u5916\u60c5\u51b5\u662f\u957f\u5c3e\u7684\u4e14\u5177\u6709\u6f5c\u5728\u5371\u9669\u3002\u4e8b\u5b9e\u4e0a\uff0c\u73b0\u6709\u7684\u5f02\u5e38\u5206\u5272\u6570\u636e\u96c6\u6355\u83b7\u7684\u5f02\u5e38\u6570\u91cf\u6709\u9650\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6027\u6216\u5177\u6709\u5f88\u5f3a\u7684\u9886\u57df\u53d8\u5316\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5c06\u5bf9\u8c61\u653e\u5165\u4e0a\u4e0b\u6587\uff08POC\uff09\u7ba1\u9053\uff0c\u4ee5\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5c06\u4efb\u4f55\u5bf9\u8c61\u5b9e\u9645\u6dfb\u52a0\u5230\u4efb\u4f55\u56fe\u50cf\u4e2d\u3002 POC \u53ef\u7528\u4e8e\u8f7b\u677e\u6269\u5c55\u5177\u6709\u4efb\u610f\u6570\u91cf\u5bf9\u8c61\u7684\u4efb\u4f55\u6570\u636e\u96c6\u3002\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u57fa\u4e8e POC \u751f\u6210\u7684\u6570\u636e\u63d0\u51fa\u4e86\u4e0d\u540c\u7684\u5f02\u5e38\u5206\u5272\u6570\u636e\u96c6\uff0c\u5e76\u8868\u660e POC \u53ef\u4ee5\u63d0\u9ad8\u6700\u8fd1\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u5fae\u8c03\u65b9\u6cd5\u5728\u51e0\u4e2a\u6807\u51c6\u5316\u57fa\u51c6\u4e2d\u7684\u6027\u80fd\u3002 POC\u5bf9\u4e8e\u5b66\u4e60\u65b0\u8bfe\u7a0b\u4e5f\u5f88\u6709\u6548\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u4f7f\u7528\u5b83\u901a\u8fc7\u6dfb\u52a0 Pascal \u7c7b\u7684\u5b50\u96c6\u6765\u7f16\u8f91 Cityscapes \u6837\u672c\uff0c\u5e76\u8868\u660e\u5728\u6b64\u7c7b\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u4e0e Pascal \u8bad\u7ec3\u7684\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002\u8fd9\u8bc1\u5b9e\u4e86\u5728 POC \u751f\u6210\u7684\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u6a21\u62df\u4e0e\u771f\u5b9e\u5dee\u8ddd\u8f83\u5c0f\u3002|[2402.16392v1](http://arxiv.org/pdf/2402.16392v1)|null|\n", "2402.16369": "|**2024-02-26**|**Generative AI in Vision: A Survey on Models, Metrics and Applications**|\u89c6\u89c9\u4e2d\u7684\u751f\u6210\u4eba\u5de5\u667a\u80fd\uff1a\u6a21\u578b\u3001\u6307\u6807\u548c\u5e94\u7528\u7684\u8c03\u67e5|Gaurav Raut, Apoorv Singh|Generative AI models have revolutionized various fields by enabling the creation of realistic and diverse data samples. Among these models, diffusion models have emerged as a powerful approach for generating high-quality images, text, and audio. This survey paper provides a comprehensive overview of generative AI diffusion and legacy models, focusing on their underlying techniques, applications across different domains, and their challenges. We delve into the theoretical foundations of diffusion models, including concepts such as denoising diffusion probabilistic models (DDPM) and score-based generative modeling. Furthermore, we explore the diverse applications of these models in text-to-image, image inpainting, and image super-resolution, along with others, showcasing their potential in creative tasks and data augmentation. By synthesizing existing research and highlighting critical advancements in this field, this survey aims to provide researchers and practitioners with a comprehensive understanding of generative AI diffusion and legacy models and inspire future innovations in this exciting area of artificial intelligence.|\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u901a\u8fc7\u521b\u5efa\u771f\u5b9e\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\u6837\u672c\uff0c\u7ed9\u5404\u4e2a\u9886\u57df\u5e26\u6765\u4e86\u9769\u547d\u6027\u7684\u53d8\u5316\u3002\u5728\u8fd9\u4e9b\u6a21\u578b\u4e2d\uff0c\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3001\u6587\u672c\u548c\u97f3\u9891\u7684\u5f3a\u5927\u65b9\u6cd5\u3002\u672c\u8c03\u67e5\u8bba\u6587\u5168\u9762\u6982\u8ff0\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4f20\u64ad\u548c\u9057\u7559\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5e95\u5c42\u6280\u672f\u3001\u8de8\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u53ca\u5176\u6311\u6218\u3002\u6211\u4eec\u6df1\u5165\u7814\u7a76\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5305\u62ec\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u548c\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\u7b49\u6982\u5ff5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u7d22\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u4fee\u590d\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7b49\u65b9\u9762\u7684\u591a\u79cd\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u521b\u9020\u6027\u4efb\u52a1\u548c\u6570\u636e\u589e\u5f3a\u65b9\u9762\u7684\u6f5c\u529b\u3002\u901a\u8fc7\u7efc\u5408\u73b0\u6709\u7814\u7a76\u5e76\u5f3a\u8c03\u8be5\u9886\u57df\u7684\u5173\u952e\u8fdb\u5c55\uff0c\u672c\u6b21\u8c03\u67e5\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u5bf9\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4f20\u64ad\u548c\u9057\u7559\u6a21\u578b\u7684\u5168\u9762\u4e86\u89e3\uff0c\u5e76\u6fc0\u53d1\u8fd9\u4e00\u4ee4\u4eba\u5174\u594b\u7684\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u672a\u6765\u521b\u65b0\u3002|[2402.16369v1](http://arxiv.org/pdf/2402.16369v1)|null|\n"}, "\u591a\u6a21\u6001": {"2402.16318": "|**2024-02-26**|**Gradient-Guided Modality Decoupling for Missing-Modality Robustness**|\u7528\u4e8e\u7f3a\u5931\u6a21\u6001\u9c81\u68d2\u6027\u7684\u68af\u5ea6\u5f15\u5bfc\u6a21\u6001\u89e3\u8026|Hao Wang, Shengda Luo, Guosheng Hu, Jianguo Zhang|Multimodal learning with incomplete input data (missing modality) is practical and challenging. In this work, we conduct an in-depth analysis of this challenge and find that modality dominance has a significant negative impact on the model training, greatly degrading the missing modality performance. Motivated by Grad-CAM, we introduce a novel indicator, gradients, to monitor and reduce modality dominance which widely exists in the missing-modality scenario. In aid of this indicator, we present a novel Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on dominating modalities. Specifically, GMD removes the conflicted gradient components from different modalities to achieve this decoupling, significantly improving the performance. In addition, to flexibly handle modal-incomplete data, we design a parameter-efficient Dynamic Sharing (DS) framework which can adaptively switch on/off the network parameters based on whether one modality is available. We conduct extensive experiments on three popular multimodal benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and CMU-MOSEI for sentiment analysis. The results show that our method can significantly outperform the competitors, showing the effectiveness of the proposed solutions. Our code is released here: https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling.|\u8f93\u5165\u6570\u636e\u4e0d\u5b8c\u6574\uff08\u6a21\u6001\u7f3a\u5931\uff09\u7684\u591a\u6a21\u6001\u5b66\u4e60\u65e2\u5b9e\u7528\u53c8\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5bf9\u8fd9\u4e00\u6311\u6218\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u53d1\u73b0\u6a21\u6001\u4f18\u52bf\u5bf9\u6a21\u578b\u8bad\u7ec3\u6709\u663e\u7740\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u7f3a\u5931\u7684\u6a21\u6001\u6027\u80fd\u3002\u5728 Grad-CAM \u7684\u63a8\u52a8\u4e0b\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6807\u2014\u2014\u68af\u5ea6\uff0c\u6765\u76d1\u63a7\u548c\u51cf\u5c11\u5e7f\u6cdb\u5b58\u5728\u4e8e\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e2d\u7684\u6a21\u6001\u4e3b\u5bfc\u3002\u4e3a\u4e86\u5e2e\u52a9\u8fd9\u4e2a\u6307\u6807\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u68af\u5ea6\u5f15\u5bfc\u6a21\u6001\u89e3\u8026\uff08GMD\uff09\u65b9\u6cd5\u6765\u89e3\u8026\u5bf9\u4e3b\u5bfc\u6a21\u6001\u7684\u4f9d\u8d56\u3002\u5177\u4f53\u6765\u8bf4\uff0cGMD \u6d88\u9664\u4e86\u4e0d\u540c\u6a21\u6001\u4e2d\u51b2\u7a81\u7684\u68af\u5ea6\u5206\u91cf\u4ee5\u5b9e\u73b0\u8fd9\u79cd\u89e3\u8026\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u7075\u6d3b\u5904\u7406\u6a21\u6001\u4e0d\u5b8c\u6574\u6570\u636e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u52a8\u6001\u5171\u4eab\uff08DS\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u6839\u636e\u4e00\u79cd\u6a21\u6001\u662f\u5426\u53ef\u7528\u81ea\u9002\u5e94\u5730\u6253\u5f00/\u5173\u95ed\u7f51\u7edc\u53c2\u6570\u3002\u6211\u4eec\u5bf9\u4e09\u4e2a\u6d41\u884c\u7684\u591a\u6a21\u6001\u57fa\u51c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5305\u62ec\u7528\u4e8e\u533b\u5b66\u5206\u5272\u7684 BraTS 2018\u3001\u7528\u4e8e\u60c5\u611f\u5206\u6790\u7684 CMU-MOSI \u548c CMU-MOSEI\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u663e\u7740\u4f18\u4e8e\u7ade\u4e89\u5bf9\u624b\uff0c\u663e\u793a\u4e86\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53d1\u5e03\u5728\u8fd9\u91cc\uff1ahttps://github.com/HaoWang420/Gradient-guided-Modality-Decoupling\u3002|[2402.16318v1](http://arxiv.org/pdf/2402.16318v1)|null|\n", "2402.16267": "|**2024-02-26**|**Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space**|CLIP \u5d4c\u5165\u7a7a\u95f4\u4e2d\u5177\u6709\u8bed\u8a00\u9a71\u52a8\u635f\u5931\u7684\u7ea2\u5916\u548c\u53ef\u89c1\u56fe\u50cf\u878d\u5408|Yuhao Wang, Lingjuan Miao, Zhiqiang Zhou, Lei Zhang, Yajun Qiao|Infrared-visible image fusion (IVIF) has attracted much attention owing to the highly-complementary properties of the two image modalities. Due to the lack of ground-truth fused images, the fusion output of current deep-learning based methods heavily depends on the loss functions defined mathematically. As it is hard to well mathematically define the fused image without ground truth, the performance of existing fusion methods is limited. In this paper, we first propose to use natural language to express the objective of IVIF, which can avoid the explicit mathematical modeling of fusion output in current losses, and make full use of the advantage of language expression to improve the fusion performance. For this purpose, we present a comprehensive language-expressed fusion objective, and encode relevant texts into the multi-modal embedding space using CLIP. A language-driven fusion model is then constructed in the embedding space, by establishing the relationship among the embedded vectors to represent the fusion objective and input image modalities. Finally, a language-driven loss is derived to make the actual IVIF aligned with the embedded language-driven fusion model via supervised training. Experiments show that our method can obtain much better fusion results than existing techniques.|\u7ea2\u5916-\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\uff08IVIF\uff09\u7531\u4e8e\u4e24\u79cd\u56fe\u50cf\u6a21\u5f0f\u7684\u9ad8\u5ea6\u4e92\u8865\u7279\u6027\u800c\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u7684\u878d\u5408\u56fe\u50cf\uff0c\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u7684\u878d\u5408\u8f93\u51fa\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6570\u5b66\u5b9a\u4e49\u7684\u635f\u5931\u51fd\u6570\u3002\u7531\u4e8e\u5728\u6ca1\u6709\u5730\u9762\u5b9e\u51b5\u7684\u60c5\u51b5\u4e0b\u5f88\u96be\u5728\u6570\u5b66\u4e0a\u5f88\u597d\u5730\u5b9a\u4e49\u878d\u5408\u56fe\u50cf\uff0c\u56e0\u6b64\u73b0\u6709\u878d\u5408\u65b9\u6cd5\u7684\u6027\u80fd\u53d7\u5230\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u6765\u8868\u8fbeIVIF\u7684\u76ee\u6807\uff0c\u8fd9\u6837\u53ef\u4ee5\u907f\u514d\u5f53\u524d\u635f\u5931\u4e2d\u878d\u5408\u8f93\u51fa\u7684\u663e\u5f0f\u6570\u5b66\u5efa\u6a21\uff0c\u5e76\u5145\u5206\u5229\u7528\u8bed\u8a00\u8868\u8fbe\u7684\u4f18\u52bf\u6765\u63d0\u9ad8\u878d\u5408\u6027\u80fd\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8bed\u8a00\u8868\u8fbe\u878d\u5408\u76ee\u6807\uff0c\u5e76\u4f7f\u7528 CLIP \u5c06\u76f8\u5173\u6587\u672c\u7f16\u7801\u5230\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5efa\u7acb\u5d4c\u5165\u5411\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u6765\u8868\u793a\u878d\u5408\u76ee\u6807\u548c\u8f93\u5165\u56fe\u50cf\u6a21\u6001\uff0c\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6784\u5efa\u8bed\u8a00\u9a71\u52a8\u7684\u878d\u5408\u6a21\u578b\u3002\u6700\u540e\uff0c\u5bfc\u51fa\u8bed\u8a00\u9a71\u52a8\u7684\u635f\u5931\uff0c\u901a\u8fc7\u76d1\u7763\u8bad\u7ec3\u4f7f\u5b9e\u9645\u7684 IVIF \u4e0e\u5d4c\u5165\u5f0f\u8bed\u8a00\u9a71\u52a8\u7684\u878d\u5408\u6a21\u578b\u4fdd\u6301\u4e00\u81f4\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u83b7\u5f97\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u597d\u7684\u878d\u5408\u7ed3\u679c\u3002|[2402.16267v1](http://arxiv.org/pdf/2402.16267v1)|null|\n"}, "Nerf": {"2402.16407": "|**2024-02-26**|**CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency**|CMC\uff1a\u901a\u8fc7\u8de8\u89c6\u56fe\u591a\u5e73\u9762\u4e00\u81f4\u6027\u8fdb\u884c\u5c11\u6837\u672c\u65b0\u9896\u89c6\u56fe\u5408\u6210|Hanxin Zhu, Tianyu He, Zhibo Chen|Neural Radiance Field (NeRF) has shown impressive results in novel view synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR), thanks to its ability to represent scenes continuously. However, when just a few input view images are available, NeRF tends to overfit the given views and thus make the estimated depths of pixels share almost the same value. Unlike previous methods that conduct regularization by introducing complex priors or additional supervisions, we propose a simple yet effective method that explicitly builds depth-aware consistency across input views to tackle this challenge. Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we are able to strengthen the interactions between views and therefore alleviate the overfitting problem. To achieve this, we build the neural networks on layered representations (\\textit{i.e.}, multiplane images), and the sampling point can thus be resampled on multiple discrete planes. Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same. Although simple, extensive experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods.|\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7531\u4e8e\u5176\u8fde\u7eed\u8868\u793a\u573a\u666f\u7684\u80fd\u529b\uff0c\u5728\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u548c\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4e2d\u3002\u7136\u800c\uff0c\u5f53\u53ea\u6709\u51e0\u4e2a\u8f93\u5165\u89c6\u56fe\u56fe\u50cf\u53ef\u7528\u65f6\uff0cNeRF \u5f80\u5f80\u4f1a\u8fc7\u5ea6\u62df\u5408\u7ed9\u5b9a\u7684\u89c6\u56fe\uff0c\u4ece\u800c\u4f7f\u50cf\u7d20\u7684\u4f30\u8ba1\u6df1\u5ea6\u5171\u4eab\u51e0\u4e4e\u76f8\u540c\u7684\u503c\u3002\u4e0e\u4e4b\u524d\u901a\u8fc7\u5f15\u5165\u590d\u6742\u5148\u9a8c\u6216\u989d\u5916\u76d1\u7763\u6765\u8fdb\u884c\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u660e\u786e\u5730\u5728\u8f93\u5165\u89c6\u56fe\u4e4b\u95f4\u6784\u5efa\u6df1\u5ea6\u611f\u77e5\u7684\u4e00\u81f4\u6027\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002\u6211\u4eec\u7684\u4e3b\u8981\u89c1\u89e3\u662f\uff0c\u901a\u8fc7\u5f3a\u5236\u5728\u4e0d\u540c\u7684\u8f93\u5165\u89c6\u56fe\u4e2d\u91cd\u590d\u91c7\u6837\u76f8\u540c\u7684\u7a7a\u95f4\u70b9\uff0c\u6211\u4eec\u80fd\u591f\u52a0\u5f3a\u89c6\u56fe\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u7f13\u89e3\u8fc7\u5ea6\u62df\u5408\u95ee\u9898\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u5728\u5206\u5c42\u8868\u793a\uff08\\textit{\u5373\u591a\u5e73\u9762\u56fe\u50cf\uff09\u4e0a\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728\u591a\u4e2a\u79bb\u6563\u5e73\u9762\u4e0a\u5bf9\u91c7\u6837\u70b9\u8fdb\u884c\u91cd\u65b0\u91c7\u6837\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u89c4\u8303\u770b\u4e0d\u89c1\u7684\u76ee\u6807\u89c6\u56fe\uff0c\u6211\u4eec\u5c06\u4e0d\u540c\u8f93\u5165\u89c6\u56fe\u7684\u6e32\u67d3\u989c\u8272\u548c\u6df1\u5ea6\u9650\u5236\u4e3a\u76f8\u540c\u3002\u867d\u7136\u7b80\u5355\uff0c\u4f46\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5b9e\u73b0\u66f4\u597d\u7684\u5408\u6210\u8d28\u91cf\u3002|[2402.16407v1](http://arxiv.org/pdf/2402.16407v1)|null|\n", "2402.16366": "|**2024-02-26**|**SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field**|SPC-NeRF\uff1a\u57fa\u4e8e\u4f53\u7d20\u7684\u8f90\u5c04\u573a\u7684\u7a7a\u95f4\u9884\u6d4b\u538b\u7f29|Zetian Song, Wenhong Duan, Yuhuai Zhang, Shiqi Wang, Siwei Ma, Wen Gao|Representing the Neural Radiance Field (NeRF) with the explicit voxel grid (EVG) is a promising direction for improving NeRFs. However, the EVG representation is not efficient for storage and transmission because of the terrific memory cost. Current methods for compressing EVG mainly inherit the methods designed for neural network compression, such as pruning and quantization, which do not take full advantage of the spatial correlation of voxels. Inspired by prosperous digital image compression techniques, this paper proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG compression. The proposed framework can remove spatial redundancy efficiently for better compression performance.Moreover, we model the bitrate and design a novel form of the loss function, where we can jointly optimize compression ratio and distortion to achieve higher coding efficiency. Extensive experiments demonstrate that our method can achieve 32% bit saving compared to the state-of-the-art method VQRF on multiple representative test datasets, with comparable training time.|\u7528\u663e\u5f0f\u4f53\u7d20\u7f51\u683c\uff08EVG\uff09\u8868\u793a\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u662f\u6539\u8fdb NeRF \u7684\u4e00\u4e2a\u6709\u524d\u9014\u7684\u65b9\u5411\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5b58\u50a8\u6210\u672c\u8fc7\u9ad8\uff0cEVG \u8868\u793a\u7684\u5b58\u50a8\u548c\u4f20\u8f93\u6548\u7387\u4e0d\u9ad8\u3002\u76ee\u524d\u7684EVG\u538b\u7f29\u65b9\u6cd5\u4e3b\u8981\u7ee7\u627f\u4e86\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u7684\u526a\u679d\u3001\u91cf\u5316\u7b49\u65b9\u6cd5\uff0c\u6ca1\u6709\u5145\u5206\u5229\u7528\u4f53\u7d20\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u3002\u53d7\u7e41\u8363\u7684\u6570\u5b57\u56fe\u50cf\u538b\u7f29\u6280\u672f\u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51fa\u4e86 SPC-NeRF\uff0c\u4e00\u79cd\u5728 EVG \u538b\u7f29\u4e2d\u5e94\u7528\u7a7a\u95f4\u9884\u6d4b\u7f16\u7801\u7684\u65b0\u9896\u6846\u67b6\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u6d88\u9664\u7a7a\u95f4\u5197\u4f59\uff0c\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u538b\u7f29\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5bf9\u6bd4\u7279\u7387\u8fdb\u884c\u5efa\u6a21\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u5f62\u5f0f\u7684\u635f\u5931\u51fd\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u8054\u5408\u4f18\u5316\u538b\u7f29\u6bd4\u548c\u5931\u771f\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u7f16\u7801\u6548\u7387\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5 VQRF \u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u4ee3\u8868\u6027\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u53ef\u4ee5\u8282\u7701 32% \u7684\u6bd4\u7279\uff0c\u5e76\u4e14\u8bad\u7ec3\u65f6\u95f4\u76f8\u5f53\u3002|[2402.16366v1](http://arxiv.org/pdf/2402.16366v1)|null|\n"}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.16291": "|**2024-02-26**|**mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection**|mAPm\uff1a\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u91d1\u5b57\u5854\u6a21\u5757\uff0c\u7528\u4e8e\u589e\u5f3a RLD \u68c0\u6d4b\u4e2d\u7684\u5c3a\u5ea6\u53d8\u5316|Yunusa Haruna, Shiyin Qin, Abdulrahman Hamman Adama Chukkol, Isah Bello, Adamu Lawan|Detecting objects across various scales remains a significant challenge in computer vision, particularly in tasks such as Rice Leaf Disease (RLD) detection, where objects exhibit considerable scale variations. Traditional object detection methods often struggle to address these variations, resulting in missed detections or reduced accuracy. In this study, we propose the multi-scale Attention Pyramid module (mAPm), a novel approach that integrates dilated convolutions into the Feature Pyramid Network (FPN) to enhance multi-scale information ex-traction. Additionally, we incorporate a global Multi-Head Self-Attention (MHSA) mechanism and a deconvolutional layer to refine the up-sampling process. We evaluate mAPm on YOLOv7 using the MRLD and COCO datasets. Compared to vanilla FPN, BiFPN, NAS-FPN, PANET, and ACFPN, mAPm achieved a significant improvement in Average Precision (AP), with a +2.61% increase on the MRLD dataset compared to the baseline FPN method in YOLOv7. This demonstrates its effectiveness in handling scale variations. Furthermore, the versatility of mAPm allows its integration into various FPN-based object detection models, showcasing its potential to advance object detection techniques.|\u68c0\u6d4b\u4e0d\u540c\u5c3a\u5ea6\u7684\u7269\u4f53\u4ecd\u7136\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7a3b\u53f6\u75c5 (RLD) \u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\uff0c\u5176\u4e2d\u7269\u4f53\u8868\u73b0\u51fa\u76f8\u5f53\u5927\u7684\u5c3a\u5ea6\u53d8\u5316\u3002\u4f20\u7edf\u7684\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u53d8\u5316\uff0c\u4ece\u800c\u5bfc\u81f4\u68c0\u6d4b\u5931\u8d25\u6216\u51c6\u786e\u6027\u964d\u4f4e\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u91d1\u5b57\u5854\u6a21\u5757\uff08mAPm\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5c06\u6269\u5f20\u5377\u79ef\u96c6\u6210\u5230\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff08FPN\uff09\u4e2d\u4ee5\u589e\u5f3a\u591a\u5c3a\u5ea6\u4fe1\u606f\u63d0\u53d6\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u91c7\u7528\u4e86\u5168\u5c40\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08MHSA\uff09\u673a\u5236\u548c\u53cd\u5377\u79ef\u5c42\u6765\u6539\u8fdb\u4e0a\u91c7\u6837\u8fc7\u7a0b\u3002\u6211\u4eec\u4f7f\u7528 MRLD \u548c COCO \u6570\u636e\u96c6\u5728 YOLOv7 \u4e0a\u8bc4\u4f30 mAPm\u3002\u4e0e\u666e\u901a FPN\u3001BiFPN\u3001NAS-FPN\u3001PANET \u548c ACFPN \u76f8\u6bd4\uff0cmAPm \u5728\u5e73\u5747\u7cbe\u5ea6 (AP) \u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u63d0\u9ad8\uff0c\u4e0e YOLOv7 \u4e2d\u7684\u57fa\u7ebf FPN \u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728 MRLD \u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e86 +2.61%\u3002\u8fd9\u8bc1\u660e\u4e86\u5b83\u5728\u5904\u7406\u89c4\u6a21\u53d8\u5316\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0cmAPm \u7684\u591a\u529f\u80fd\u6027\u5141\u8bb8\u5176\u96c6\u6210\u5230\u5404\u79cd\u57fa\u4e8e FPN \u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4e2d\uff0c\u5c55\u793a\u4e86\u5176\u63a8\u8fdb\u76ee\u6807\u68c0\u6d4b\u6280\u672f\u7684\u6f5c\u529b\u3002|[2402.16291v1](http://arxiv.org/pdf/2402.16291v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.16486": "|**2024-02-26**|**Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification**|\u667a\u80fd\u5df2\u77e5\u548c\u65b0\u578b\u98de\u673a\u8bc6\u522b\u2014\u2014\u6218\u6597\u8bc6\u522b\u4ece\u5206\u7c7b\u5230\u76f8\u4f3c\u5b66\u4e60\u7684\u8f6c\u53d8|Ahmad Saeed, Haasha Bin Atif, Usman Habib, Mohsin Bilal|Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification. This research addresses this problem with a novel, scalable, and AI-driven solution. The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types. Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes. Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft. It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification. To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully supervised manner. Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936). The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality. The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition.|\u4f4e\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u7cbe\u786e\u98de\u673a\u8bc6\u522b\u662f\u822a\u7a7a\u9886\u57df\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u4f46\u53c8\u81f3\u5173\u91cd\u8981\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u6218\u6597\u8bc6\u522b\u3002\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53ef\u6269\u5c55\u7684\u3001\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\u3002\u9065\u611f\u56fe\u50cf\u4e2d\u6218\u6597\u8bc6\u522b\u7684\u4e3b\u8981\u969c\u788d\u662f\u9664\u4e86\u5df2\u77e5\u7c7b\u578b\u4e4b\u5916\u8fd8\u51c6\u786e\u8bc6\u522b\u65b0\u578b/\u672a\u77e5\u7c7b\u578b\u7684\u98de\u673a\u3002\u4f20\u7edf\u65b9\u6cd5\u3001\u4eba\u7c7b\u4e13\u5bb6\u9a71\u52a8\u7684\u6218\u6597\u8bc6\u522b\u548c\u56fe\u50cf\u5206\u7c7b\u5728\u8bc6\u522b\u65b0\u7c7b\u522b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u76f8\u4f3c\u6027\u5b66\u4e60\u6765\u8fa8\u522b\u5404\u79cd\u519b\u7528\u548c\u6c11\u7528\u98de\u673a\u7684\u7279\u5f81\u3002\u5b83\u53ef\u4ee5\u8bc6\u522b\u5df2\u77e5\u548c\u65b0\u9896\u7684\u98de\u673a\u7c7b\u578b\uff0c\u5229\u7528\u5ea6\u91cf\u5b66\u4e60\u8fdb\u884c\u8bc6\u522b\uff0c\u5e76\u5229\u7528\u76d1\u7763\u5f0f\u5c0f\u6837\u672c\u5b66\u4e60\u8fdb\u884c\u98de\u673a\u7c7b\u578b\u5206\u7c7b\u3002\u4e3a\u4e86\u5e94\u5bf9\u6709\u9650\u7684\u4f4e\u5206\u8fa8\u7387\u9065\u611f\u6570\u636e\u7684\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u4ee5\u5b8c\u5168\u76d1\u7763\u7684\u65b9\u5f0f\u8bad\u7ec3\u901a\u7528\u5d4c\u5165\u5668\u6765\u9002\u5e94\u519b\u7528\u98de\u673a\u8bc6\u522b\u7684\u591a\u6837\u5316\u548c\u591a\u529f\u80fd\u8fc7\u7a0b\u3002\u4e0e\u65e9\u671f\u98de\u673a\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\u7684\u6bd4\u8f83\u5206\u6790\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u4e8e\u98de\u673a\u56fe\u50cf\u5206\u7c7b\uff08F1 \u5206\u6570\u98de\u673a\u7c7b\u578b\u4e3a 0.861\uff09\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5728\u91cf\u5316\u65b0\u9896\u7c7b\u578b\u7684\u8bc6\u522b\u65b9\u9762\u5904\u4e8e\u9886\u5148\u5730\u4f4d\uff08F1 \u5206\u6570\u4e8c\u5206\u533a\u4e3a 0.936\uff09\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u9065\u611f\u6570\u636e\u7684\u56fa\u6709\u6311\u6218\uff0c\u4ece\u800c\u4e3a\u6570\u636e\u96c6\u8d28\u91cf\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002\u8be5\u7814\u7a76\u4e3a\u9886\u57df\u4e13\u5bb6\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u5e76\u5c55\u793a\u4e86\u533a\u5206\u5404\u79cd\u98de\u673a\u7c7b\u578b\u7684\u72ec\u7279\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u4e3a\u5b9e\u65f6\u98de\u673a\u8bc6\u522b\u63d0\u4f9b\u66f4\u5f3a\u5927\u3001\u66f4\u9002\u5e94\u9886\u57df\u7684\u6f5c\u529b\u3002|[2402.16486v1](http://arxiv.org/pdf/2402.16486v1)|null|\n", "2402.16479": "|**2024-02-26**|**Edge Detectors Can Make Deep Convolutional Neural Networks More Robust**|\u8fb9\u7f18\u68c0\u6d4b\u5668\u53ef\u4ee5\u4f7f\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u66f4\u52a0\u9c81\u68d2|Jin Ding, Jie-Chao Zhao, Yong-Zhi Sun, Ping Tan, Jia-Wei Wang, Ji-En Ma, You-Tong Fang|Deep convolutional neural networks (DCNN for short) are vulnerable to examples with small perturbations. Improving DCNN's robustness is of great significance to the safety-critical applications, such as autonomous driving and industry automation. Inspired by the principal way that human eyes recognize objects, i.e., largely relying on the shape features, this paper first employs the edge detectors as layer kernels and designs a binary edge feature branch (BEFB for short) to learn the binary edge features, which can be easily integrated into any popular backbone. The four edge detectors can learn the horizontal, vertical, positive diagonal, and negative diagonal edge features, respectively, and the branch is stacked by multiple Sobel layers (using edge detectors as kernels) and one threshold layer. The binary edge features learned by the branch, concatenated with the texture features learned by the backbone, are fed into the fully connected layers for classification. We integrate the proposed branch into VGG16 and ResNet34, respectively, and conduct experiments on multiple datasets. Experimental results demonstrate the BEFB is lightweight and has no side effects on training. And the accuracy of the BEFB integrated models is better than the original ones on all datasets when facing FGSM, PGD, and C\\&W attacks. Besides, BEFB integrated models equipped with the robustness enhancing techniques can achieve better classification accuracy compared to the original models. The work in this paper for the first time shows it is feasible to enhance the robustness of DCNNs through combining both shape-like features and texture features.|\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08\u7b80\u79f0 DCNN\uff09\u5bb9\u6613\u53d7\u5230\u5c0f\u6270\u52a8\u6837\u672c\u7684\u5f71\u54cd\u3002\u63d0\u9ad8DCNN\u7684\u9c81\u68d2\u6027\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u3001\u5de5\u4e1a\u81ea\u52a8\u5316\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u53d7\u4eba\u773c\u8bc6\u522b\u7269\u4f53\u4e3b\u8981\u4f9d\u9760\u5f62\u72b6\u7279\u5f81\u7684\u65b9\u5f0f\u7684\u542f\u53d1\uff0c\u672c\u6587\u9996\u5148\u91c7\u7528\u8fb9\u7f18\u68c0\u6d4b\u5668\u4f5c\u4e3a\u5c42\u6838\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u4e8c\u503c\u8fb9\u7f18\u7279\u5f81\u5206\u652f\uff08\u7b80\u79f0BEFB\uff09\u6765\u5b66\u4e60\u4e8c\u503c\u8fb9\u7f18\u7279\u5f81\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u4efb\u4f55\u6d41\u884c\u7684\u9aa8\u5e72\u7f51\u4e2d\u3002\u56db\u4e2a\u8fb9\u7f18\u68c0\u6d4b\u5668\u53ef\u4ee5\u5206\u522b\u5b66\u4e60\u6c34\u5e73\u3001\u5782\u76f4\u3001\u6b63\u5bf9\u89d2\u7ebf\u548c\u8d1f\u5bf9\u89d2\u7ebf\u8fb9\u7f18\u7279\u5f81\uff0c\u5e76\u4e14\u5206\u652f\u7531\u591a\u4e2aSobel\u5c42\uff08\u4f7f\u7528\u8fb9\u7f18\u68c0\u6d4b\u5668\u4f5c\u4e3a\u5185\u6838\uff09\u548c\u4e00\u4e2a\u9608\u503c\u5c42\u5806\u53e0\u3002\u5206\u652f\u5b66\u4e60\u5230\u7684\u4e8c\u8fdb\u5236\u8fb9\u7f18\u7279\u5f81\u4e0e\u4e3b\u5e72\u5b66\u4e60\u5230\u7684\u7eb9\u7406\u7279\u5f81\u76f8\u8fde\u63a5\uff0c\u88ab\u8f93\u5165\u5230\u5168\u8fde\u63a5\u5c42\u8fdb\u884c\u5206\u7c7b\u3002\u6211\u4eec\u5c06\u6240\u63d0\u51fa\u7684\u5206\u652f\u5206\u522b\u96c6\u6210\u5230 VGG16 \u548c ResNet34 \u4e2d\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eBEFB\u662f\u8f7b\u91cf\u7ea7\u7684\u5e76\u4e14\u5bf9\u8bad\u7ec3\u6ca1\u6709\u526f\u4f5c\u7528\u3002\u5e76\u4e14\u5728\u9762\u5bf9 FGSM\u3001PGD \u548c C\\&W \u653b\u51fb\u65f6\uff0cBEFB \u96c6\u6210\u6a21\u578b\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u6027\u90fd\u4f18\u4e8e\u539f\u59cb\u6a21\u578b\u3002\u6b64\u5916\uff0c\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u6bd4\uff0c\u914d\u5907\u9c81\u68d2\u6027\u589e\u5f3a\u6280\u672f\u7684BEFB\u96c6\u6210\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u5206\u7c7b\u7cbe\u5ea6\u3002\u672c\u6587\u7684\u5de5\u4f5c\u9996\u6b21\u8868\u660e\u901a\u8fc7\u7ed3\u5408\u5f62\u72b6\u7279\u5f81\u548c\u7eb9\u7406\u7279\u5f81\u6765\u589e\u5f3a DCNN \u7684\u9c81\u68d2\u6027\u662f\u53ef\u884c\u7684\u3002|[2402.16479v1](http://arxiv.org/pdf/2402.16479v1)|null|\n", "2402.16370": "|**2024-02-26**|**DEYO: DETR with YOLO for End-to-End Object Detection**|DEYO\uff1aDETR \u4e0e YOLO \u4e00\u8d77\u7528\u4e8e\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b|Haodong Ouyang|The training paradigm of DETRs is heavily contingent upon pre-training their backbone on the ImageNet dataset. However, the limited supervisory signals provided by the image classification task and one-to-one matching strategy result in an inadequately pre-trained neck for DETRs. Additionally, the instability of matching in the early stages of training engenders inconsistencies in the optimization objectives of DETRs. To address these issues, we have devised an innovative training methodology termed step-by-step training. Specifically, in the first stage of training, we employ a classic detector, pre-trained with a one-to-many matching strategy, to initialize the backbone and neck of the end-to-end detector. In the second stage of training, we froze the backbone and neck of the end-to-end detector, necessitating the training of the decoder from scratch. Through the application of step-by-step training, we have introduced the first real-time end-to-end object detection model that utilizes a purely convolutional structure encoder, DETR with YOLO (DEYO). Without reliance on any supplementary training data, DEYO surpasses all existing real-time object detectors in both speed and accuracy. Moreover, the comprehensive DEYO series can complete its second-phase training on the COCO dataset using a single 8GB RTX 4060 GPU, significantly reducing the training expenditure. Source code and pre-trained models are available at https://github.com/ouyanghaodong/DEYO.|DETR \u7684\u8bad\u7ec3\u8303\u5f0f\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u5728 ImageNet \u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u5176\u9aa8\u5e72\u7f51\u3002\u7136\u800c\uff0c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u548c\u4e00\u5bf9\u4e00\u5339\u914d\u7b56\u7565\u63d0\u4f9b\u7684\u76d1\u7763\u4fe1\u53f7\u6709\u9650\uff0c\u5bfc\u81f4 DETR \u7684\u9888\u90e8\u9884\u8bad\u7ec3\u4e0d\u5145\u5206\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u521d\u671f\u5339\u914d\u7684\u4e0d\u7a33\u5b9a\u6027\u5bfc\u81f4DETR\u7684\u4f18\u5316\u76ee\u6807\u4e0d\u4e00\u81f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u57f9\u8bad\u65b9\u6cd5\uff0c\u79f0\u4e3a\u9010\u6b65\u57f9\u8bad\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u8bad\u7ec3\u7684\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u91c7\u7528\u7ecf\u5178\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u4e00\u5bf9\u591a\u5339\u914d\u7b56\u7565\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u6765\u521d\u59cb\u5316\u7aef\u5230\u7aef\u68c0\u6d4b\u5668\u7684\u4e3b\u5e72\u548c\u9888\u90e8\u3002\u5728\u8bad\u7ec3\u7684\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u51bb\u7ed3\u4e86\u7aef\u5230\u7aef\u68c0\u6d4b\u5668\u7684\u4e3b\u5e72\u548c\u9888\u90e8\uff0c\u9700\u8981\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u89e3\u7801\u5668\u3002\u901a\u8fc7\u9010\u6b65\u8bad\u7ec3\u7684\u5e94\u7528\uff0c\u6211\u4eec\u63a8\u51fa\u4e86\u7b2c\u4e00\u4e2a\u5229\u7528\u7eaf\u5377\u79ef\u7ed3\u6784\u7f16\u7801\u5668\u7684\u5b9e\u65f6\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0cDETR with YOLO (DEYO)\u3002\u5728\u4e0d\u4f9d\u8d56\u4efb\u4f55\u8865\u5145\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0cDEYO \u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u90fd\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u7684\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\u5668\u3002\u6b64\u5916\uff0c\u7efc\u5408DEYO\u7cfb\u5217\u53ef\u4ee5\u4f7f\u7528\u5355\u4e2a8GB RTX 4060 GPU\u5b8c\u6210\u5728COCO\u6570\u636e\u96c6\u4e0a\u7684\u7b2c\u4e8c\u9636\u6bb5\u8bad\u7ec3\uff0c\u5927\u5927\u51cf\u5c11\u4e86\u8bad\u7ec3\u652f\u51fa\u3002\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728 https://github.com/ouyanghaodong/DEYO \u83b7\u53d6\u3002|[2402.16370v1](http://arxiv.org/pdf/2402.16370v1)|null|\n", "2402.16368": "|**2024-02-26**|**SPINEPS -- Automatic Whole Spine Segmentation of T2-weighted MR images using a Two-Phase Approach to Multi-class Semantic and Instance Segmentation**|SPINEPS\u2014\u2014\u4f7f\u7528\u591a\u7c7b\u8bed\u4e49\u548c\u5b9e\u4f8b\u5206\u5272\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u5bf9 T2 \u52a0\u6743 MR \u56fe\u50cf\u8fdb\u884c\u81ea\u52a8\u5168\u810a\u67f1\u5206\u5272|Hendrik M\u00f6ller, Robert Graf, Joachim Schmitt, Benjamin Keinert, Matan Atad, Anjany Sekuboyina, Felix Streckenbach, Hanna Sch\u00f6n, Florian Kofler, Thomas Kroencke, et.al.|Purpose. To present SPINEPS, an open-source deep learning approach for semantic and instance segmentation of 14 spinal structures (ten vertebra substructures, intervertebral discs, spinal cord, spinal canal, and sacrum) in whole body T2w MRI.   Methods. During this HIPPA-compliant, retrospective study, we utilized the public SPIDER dataset (218 subjects, 63% female) and a subset of the German National Cohort (1423 subjects, mean age 53, 49% female) for training and evaluation. We combined CT and T2w segmentations to train models that segment 14 spinal structures in T2w sagittal scans both semantically and instance-wise. Performance evaluation metrics included Dice similarity coefficient, average symmetrical surface distance, panoptic quality, segmentation quality, and recognition quality. Statistical significance was assessed using the Wilcoxon signed-rank test. An in-house dataset was used to qualitatively evaluate out-of-distribution samples.   Results. On the public dataset, our approach outperformed the baseline (instance-wise vertebra dice score 0.929 vs. 0.907, p-value<0.001). Training on auto-generated annotations and evaluating on manually corrected test data from the GNC yielded global dice scores of 0.900 for vertebrae, 0.960 for intervertebral discs, and 0.947 for the spinal canal. Incorporating the SPIDER dataset during training increased these scores to 0.920, 0.967, 0.958, respectively.   Conclusions. The proposed segmentation approach offers robust segmentation of 14 spinal structures in T2w sagittal images, including the spinal cord, spinal canal, intervertebral discs, endplate, sacrum, and vertebrae. The approach yields both a semantic and instance mask as output, thus being easy to utilize. This marks the first publicly available algorithm for whole spine segmentation in sagittal T2w MR imaging.|\u76ee\u7684\u3002\u4ecb\u7ecd SPINEPS\uff0c\u4e00\u79cd\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5168\u8eab T2w MRI \u4e2d 14 \u4e2a\u810a\u67f1\u7ed3\u6784\uff08\u5341\u4e2a\u690e\u9aa8\u5b50\u7ed3\u6784\u3001\u690e\u95f4\u76d8\u3001\u810a\u9ad3\u3001\u690e\u7ba1\u548c\u9ab6\u9aa8\uff09\u7684\u8bed\u4e49\u548c\u5b9e\u4f8b\u5206\u5272\u3002\u65b9\u6cd5\u3002\u5728\u8fd9\u9879\u7b26\u5408 HIPPA \u6807\u51c6\u7684\u56de\u987e\u6027\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u516c\u5171 SPIDER \u6570\u636e\u96c6\uff08218 \u540d\u53d7\u8bd5\u8005\uff0c63% \u5973\u6027\uff09\u548c\u5fb7\u56fd\u56fd\u5bb6\u961f\u5217\u7684\u5b50\u96c6\uff081423 \u540d\u53d7\u8bd5\u8005\uff0c\u5e73\u5747\u5e74\u9f84 53 \u5c81\uff0c49% \u5973\u6027\uff09\u8fdb\u884c\u57f9\u8bad\u548c\u8bc4\u4f30\u3002\u6211\u4eec\u7ed3\u5408 CT \u548c T2w \u5206\u5272\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u5728 T2w \u77e2\u72b6\u626b\u63cf\u4e2d\u4ece\u8bed\u4e49\u548c\u5b9e\u4f8b\u89d2\u5ea6\u5206\u5272 14 \u4e2a\u810a\u67f1\u7ed3\u6784\u3002\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u5305\u62ecDice\u76f8\u4f3c\u7cfb\u6570\u3001\u5e73\u5747\u5bf9\u79f0\u8868\u9762\u8ddd\u79bb\u3001\u5168\u666f\u8d28\u91cf\u3001\u5206\u5272\u8d28\u91cf\u548c\u8bc6\u522b\u8d28\u91cf\u3002\u4f7f\u7528 Wilcoxon \u7b26\u53f7\u79e9\u68c0\u9a8c\u8bc4\u4f30\u7edf\u8ba1\u663e\u7740\u6027\u3002\u4f7f\u7528\u5185\u90e8\u6570\u636e\u96c6\u6765\u5b9a\u6027\u8bc4\u4f30\u5206\u5e03\u5916\u6837\u672c\u3002\u7ed3\u679c\u3002\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\uff08\u5b9e\u4f8b\u65b9\u9762\u7684\u690e\u9aa8\u9ab0\u5b50\u5f97\u5206 0.929 \u4e0e 0.907\uff0cp \u503c<0.001\uff09\u3002\u5bf9\u81ea\u52a8\u751f\u6210\u7684\u6ce8\u91ca\u8fdb\u884c\u8bad\u7ec3\u5e76\u5bf9\u6765\u81ea GNC \u7684\u624b\u52a8\u6821\u6b63\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff0c\u5f97\u51fa\u690e\u9aa8\u7684\u5168\u5c40 dice \u5206\u6570\u4e3a 0.900\uff0c\u690e\u95f4\u76d8\u7684\u5168\u5c40\u9ab0\u5b50\u5206\u6570\u4e3a 0.960\uff0c\u690e\u7ba1\u7684\u5168\u5c40\u9ab0\u5b50\u5206\u6570\u4e3a 0.947\u3002\u5728\u8bad\u7ec3\u671f\u95f4\u5408\u5e76 SPIDER \u6570\u636e\u96c6\u5c06\u8fd9\u4e9b\u5206\u6570\u5206\u522b\u63d0\u9ad8\u5230 0.920\u30010.967\u30010.958\u3002\u7ed3\u8bba\u3002\u6240\u63d0\u51fa\u7684\u5206\u5272\u65b9\u6cd5\u53ef\u4ee5\u5bf9 T2w \u77e2\u72b6\u56fe\u50cf\u4e2d\u7684 14 \u4e2a\u810a\u67f1\u7ed3\u6784\u8fdb\u884c\u7a33\u5065\u5206\u5272\uff0c\u5305\u62ec\u810a\u9ad3\u3001\u690e\u7ba1\u3001\u690e\u95f4\u76d8\u3001\u7ec8\u677f\u3001\u9ab6\u9aa8\u548c\u690e\u9aa8\u3002\u8be5\u65b9\u6cd5\u4ea7\u751f\u8bed\u4e49\u548c\u5b9e\u4f8b\u63a9\u7801\u4f5c\u4e3a\u8f93\u51fa\uff0c\u56e0\u6b64\u6613\u4e8e\u4f7f\u7528\u3002\u8fd9\u6807\u5fd7\u7740\u77e2\u72b6 T2w MR \u6210\u50cf\u4e2d\u7b2c\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6574\u4e2a\u810a\u67f1\u5206\u5272\u7b97\u6cd5\u3002|[2402.16368v1](http://arxiv.org/pdf/2402.16368v1)|null|\n", "2402.16356": "|**2024-02-26**|**What Text Design Characterizes Book Genres?**|\u4e66\u7c4d\u7c7b\u578b\u7684\u6587\u672c\u8bbe\u8ba1\u6709\u4f55\u7279\u70b9\uff1f|Daichi Haraguchi, Brian Kenji Iwana, Seiichi Uchida|This study analyzes the relationship between non-verbal information (e.g., genres) and text design (e.g., font style, character color, etc.) through the classification of book genres using text design on book covers. Text images have both semantic information about the word itself and other information (non-semantic information or visual design), such as font style, character color, etc. When we read a word printed on some materials, we receive impressions or other information from both the word itself and the visual design. Basically, we can understand verbal information only from semantic information, i.e., the words themselves; however, we can consider that text design is helpful for understanding other additional information (i.e., non-verbal information), such as impressions, genre, etc. To investigate the effect of text design, we analyze text design using words printed on book covers and their genres in two scenarios. First, we attempted to understand the importance of visual design for determining the genre (i.e., non-verbal information) of books by analyzing the differences in the relationship between semantic information/visual design and genres. In the experiment, we found that semantic information is sufficient to determine the genre; however, text design is helpful in adding more discriminative features for book genres. Second, we investigated the effect of each text design on book genres. As a result, we found that each text design characterizes some book genres. For example, font style is useful to add more discriminative features for genres of ``Mystery, Thriller \\& Suspense'' and ``Christian books \\& Bibles.''|\u672c\u7814\u7a76\u901a\u8fc7\u4e66\u7c4d\u5c01\u9762\u4e0a\u7684\u6587\u672c\u8bbe\u8ba1\u5bf9\u4e66\u7c4d\u7c7b\u578b\u8fdb\u884c\u5206\u7c7b\uff0c\u5206\u6790\u975e\u8bed\u8a00\u4fe1\u606f\uff08\u4f8b\u5982\u7c7b\u578b\uff09\u4e0e\u6587\u672c\u8bbe\u8ba1\uff08\u4f8b\u5982\u5b57\u4f53\u6837\u5f0f\u3001\u5b57\u7b26\u989c\u8272\u7b49\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6587\u672c\u56fe\u50cf\u65e2\u5177\u6709\u5355\u8bcd\u672c\u8eab\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4e5f\u5177\u6709\u5176\u4ed6\u4fe1\u606f\uff08\u975e\u8bed\u4e49\u4fe1\u606f\u6216\u89c6\u89c9\u8bbe\u8ba1\uff09\uff0c\u4f8b\u5982\u5b57\u4f53\u6837\u5f0f\u3001\u5b57\u7b26\u989c\u8272\u7b49\u3002\u5f53\u6211\u4eec\u9605\u8bfb\u67d0\u4e9b\u6750\u6599\u4e0a\u6253\u5370\u7684\u5355\u8bcd\u65f6\uff0c\u6211\u4eec\u4f1a\u6536\u5230\u5370\u8c61\u6216\u5176\u4ed6\u4fe1\u606f\u65e0\u8bba\u662f\u5355\u8bcd\u672c\u8eab\u8fd8\u662f\u89c6\u89c9\u8bbe\u8ba1\u3002\u57fa\u672c\u4e0a\uff0c\u6211\u4eec\u53ea\u80fd\u4ece\u8bed\u4e49\u4fe1\u606f\uff08\u5373\u5355\u8bcd\u672c\u8eab\uff09\u6765\u7406\u89e3\u8a00\u8bed\u4fe1\u606f\uff1b\u7136\u800c\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba4\u4e3a\u6587\u672c\u8bbe\u8ba1\u6709\u52a9\u4e8e\u7406\u89e3\u5176\u4ed6\u9644\u52a0\u4fe1\u606f\uff08\u5373\u975e\u8bed\u8a00\u4fe1\u606f\uff09\uff0c\u4f8b\u5982\u5370\u8c61\u3001\u7c7b\u578b\u7b49\u3002\u4e3a\u4e86\u7814\u7a76\u6587\u672c\u8bbe\u8ba1\u7684\u6548\u679c\uff0c\u6211\u4eec\u4f7f\u7528\u4e66\u7c4d\u5c01\u9762\u4e0a\u5370\u5237\u7684\u6587\u5b57\u6765\u5206\u6790\u6587\u672c\u8bbe\u8ba1\u4ee5\u53ca\u4ed6\u4eec\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u6d41\u6d3e\u3002\u9996\u5148\uff0c\u6211\u4eec\u8bd5\u56fe\u901a\u8fc7\u5206\u6790\u8bed\u4e49\u4fe1\u606f/\u89c6\u89c9\u8bbe\u8ba1\u4e0e\u4f53\u88c1\u4e4b\u95f4\u5173\u7cfb\u7684\u5dee\u5f02\u6765\u7406\u89e3\u89c6\u89c9\u8bbe\u8ba1\u5bf9\u4e8e\u786e\u5b9a\u4e66\u7c4d\u4f53\u88c1\uff08\u5373\u975e\u8bed\u8a00\u4fe1\u606f\uff09\u7684\u91cd\u8981\u6027\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\u8bed\u4e49\u4fe1\u606f\u8db3\u4ee5\u5224\u65ad\u6d41\u6d3e\uff1b\u7136\u800c\uff0c\u6587\u672c\u8bbe\u8ba1\u6709\u52a9\u4e8e\u4e3a\u4e66\u7c4d\u7c7b\u578b\u6dfb\u52a0\u66f4\u591a\u533a\u5206\u7279\u5f81\u3002\u5176\u6b21\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u6bcf\u79cd\u6587\u672c\u8bbe\u8ba1\u5bf9\u4e66\u7c4d\u7c7b\u578b\u7684\u5f71\u54cd\u3002\u7ed3\u679c\uff0c\u6211\u4eec\u53d1\u73b0\u6bcf\u79cd\u6587\u672c\u8bbe\u8ba1\u90fd\u4f53\u73b0\u4e86\u67d0\u4e9b\u4e66\u7c4d\u7c7b\u578b\u7684\u7279\u5f81\u3002\u4f8b\u5982\uff0c\u5b57\u4f53\u6837\u5f0f\u6709\u52a9\u4e8e\u4e3a\u201c\u795e\u79d8\u3001\u60ca\u609a\\&\u60ac\u7591\u201d\u548c\u201c\u57fa\u7763\u6559\u4e66\u7c4d\\&\u5723\u7ecf\u201d\u7c7b\u578b\u6dfb\u52a0\u66f4\u591a\u533a\u5206\u529f\u80fd\u3002|[2402.16356v1](http://arxiv.org/pdf/2402.16356v1)|null|\n", "2402.16338": "|**2024-02-26**|**BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM**|BLO-SAM\uff1a\u57fa\u4e8e\u53cc\u5c42\u4f18\u5316\u7684 SAM \u9632\u8fc7\u62df\u5408\u5fae\u8c03|Li Zhang, Youwei Liang, Pengtao Xie|The Segment Anything Model (SAM), a foundation model pretrained on millions of images and segmentation masks, has significantly advanced semantic segmentation, a fundamental task in computer vision. Despite its strengths, SAM encounters two major challenges. Firstly, it struggles with segmenting specific objects autonomously, as it relies on users to manually input prompts like points or bounding boxes to identify targeted objects. Secondly, SAM faces challenges in excelling at specific downstream tasks, like medical imaging, due to a disparity between the distribution of its pretraining data, which predominantly consists of general-domain images, and the data used in downstream tasks. Current solutions to these problems, which involve finetuning SAM, often lead to overfitting, a notable issue in scenarios with very limited data, like in medical imaging. To overcome these limitations, we introduce BLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approach allows for automatic image segmentation without the need for manual prompts, by optimizing a learnable prompt embedding. Furthermore, it significantly reduces the risk of overfitting by training the model's weight parameters and the prompt embedding on two separate subsets of the training dataset, each at a different level of optimization. We apply BLO-SAM to diverse semantic segmentation tasks in general and medical domains. The results demonstrate BLO-SAM's superior performance over various state-of-the-art image semantic segmentation methods.|Segment Anything Model (SAM) \u662f\u4e00\u79cd\u5728\u6570\u767e\u4e07\u5f20\u56fe\u50cf\u548c\u5206\u5272\u63a9\u6a21\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5177\u6709\u663e\u7740\u5148\u8fdb\u7684\u8bed\u4e49\u5206\u5272\uff08\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u9879\u57fa\u672c\u4efb\u52a1\uff09\u3002\u5c3d\u7ba1\u6709\u5176\u4f18\u52bf\uff0cSAM \u4ecd\u9762\u4e34\u4e24\u5927\u6311\u6218\u3002\u9996\u5148\uff0c\u5b83\u5f88\u96be\u81ea\u4e3b\u5206\u5272\u7279\u5b9a\u5bf9\u8c61\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u7528\u6237\u624b\u52a8\u8f93\u5165\u70b9\u6216\u8fb9\u754c\u6846\u7b49\u63d0\u793a\u6765\u8bc6\u522b\u76ee\u6807\u5bf9\u8c61\u3002\u5176\u6b21\uff0c\u7531\u4e8e\u9884\u8bad\u7ec3\u6570\u636e\uff08\u4e3b\u8981\u7531\u901a\u7528\u9886\u57df\u56fe\u50cf\u7ec4\u6210\uff09\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f7f\u7528\u7684\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0cSAM \u5728\u64c5\u957f\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\uff08\u4f8b\u5982\u533b\u5b66\u6210\u50cf\uff09\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u76ee\u524d\u8fd9\u4e9b\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u6d89\u53ca\u5bf9 SAM \u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u62df\u5408\uff0c\u8fd9\u5728\u6570\u636e\u975e\u5e38\u6709\u9650\u7684\u573a\u666f\uff08\u4f8b\u5982\u533b\u5b66\u6210\u50cf\uff09\u4e2d\u662f\u4e00\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 BLO-SAM\uff0c\u5b83\u57fa\u4e8e\u53cc\u5c42\u4f18\u5316 (BLO) \u5bf9 SAM \u8fdb\u884c\u5fae\u8c03\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u5d4c\u5165\uff0c\u5141\u8bb8\u81ea\u52a8\u56fe\u50cf\u5206\u5272\uff0c\u65e0\u9700\u624b\u52a8\u63d0\u793a\u3002\u6b64\u5916\uff0c\u5b83\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u7684\u6743\u91cd\u53c2\u6570\u548c\u63d0\u793a\u5d4c\u5165\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u4e24\u4e2a\u72ec\u7acb\u5b50\u96c6\uff08\u6bcf\u4e2a\u5b50\u96c6\u5904\u4e8e\u4e0d\u540c\u7684\u4f18\u5316\u7ea7\u522b\uff09\u6765\u663e\u7740\u964d\u4f4e\u8fc7\u5ea6\u62df\u5408\u7684\u98ce\u9669\u3002\u6211\u4eec\u5c06 BLO-SAM \u5e94\u7528\u4e8e\u4e00\u822c\u548c\u533b\u5b66\u9886\u57df\u7684\u5404\u79cd\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u3002\u7ed3\u679c\u8868\u660e\uff0cBLO-SAM \u6bd4\u5404\u79cd\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002|[2402.16338v1](http://arxiv.org/pdf/2402.16338v1)|null|\n", "2402.16315": "|**2024-02-26**|**Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models**|\u66f4\u7cbe\u7ec6\uff1a\u7814\u7a76\u548c\u589e\u5f3a\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u6982\u5ff5\u8bc6\u522b|Jeonghwan Kim, Heng Ji|Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the LLMs. In an effort to further the community's endeavor in this direction, we propose a multiple granularity attribute-centric evaluation benchmark, Finer, which aims to establish a ground to evaluate LVLMs' fine-grained visual comprehension ability and provide significantly improved explainability.|\u6307\u4ee4\u8c03\u6574\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u7684\u6700\u65b0\u8fdb\u5c55\u4f7f\u6a21\u578b\u80fd\u591f\u8f7b\u677e\u751f\u6210\u57fa\u4e8e\u56fe\u50cf\u7684\u9ad8\u7ea7\u89e3\u91ca\u3002\u867d\u7136\u8fd9\u79cd\u80fd\u529b\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5f52\u529f\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u5305\u542b\u7684\u4e30\u5bcc\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u4f46\u6211\u4eec\u7684\u5de5\u4f5c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u516d\u79cd\u4e0d\u540c\u57fa\u51c6\u8bbe\u7f6e\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\uff08FGVC\uff09\u65b9\u9762\u7684\u7f3a\u70b9\u3002\u6700\u8fd1\u6700\u5148\u8fdb\u7684 LVLM\uff0c\u5982 LLaVa-1.5\u3001InstructBLIP \u548c GPT-4V\uff0c\u4e0d\u4ec5\u5728\u5206\u7c7b\u6027\u80fd\u65b9\u9762\u4e25\u91cd\u6076\u5316\uff0c\u4f8b\u5982 LLaVA-1.5 \u7684\u65af\u5766\u798f\u72d7\u7684 EM \u5e73\u5747\u4e0b\u964d 65.58\uff0c\u800c\u4e14\u8fd8\u96be\u4ee5\u5c3d\u7ba1\u5b83\u4eec\u80fd\u591f\u751f\u6210\u6574\u4f53\u56fe\u50cf\u7ea7\u63cf\u8ff0\uff0c\u4f46\u4ecd\u53ef\u4ee5\u6839\u636e\u8f93\u5165\u56fe\u50cf\u4e2d\u51fa\u73b0\u7684\u6982\u5ff5\u751f\u6210\u5177\u6709\u8be6\u7ec6\u5c5e\u6027\u7684\u51c6\u786e\u89e3\u91ca\u3002\u6df1\u5165\u5206\u6790\u8868\u660e\uff0c\u6307\u4ee4\u8c03\u6574\u7684 LVLM \u8868\u73b0\u51fa\u6a21\u6001\u5dee\u8ddd\uff0c\u5f53\u7ed9\u5b9a\u5bf9\u5e94\u4e8e\u540c\u4e00\u6982\u5ff5\u7684\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\u65f6\uff0c\u4f1a\u663e\u793a\u51fa\u5dee\u5f02\uff0c\u4ece\u800c\u963b\u6b62\u56fe\u50cf\u6a21\u6001\u5229\u7528 LLM \u5185\u4e30\u5bcc\u7684\u53c2\u6570\u77e5\u8bc6\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63a8\u52a8\u793e\u533a\u671d\u8fd9\u4e2a\u65b9\u5411\u52aa\u529b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u591a\u7c92\u5ea6\u5c5e\u6027\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u57fa\u51c6 Finer\uff0c\u65e8\u5728\u4e3a\u8bc4\u4f30 LVLM \u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u663e\u7740\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002|[2402.16315v1](http://arxiv.org/pdf/2402.16315v1)|null|\n", "2402.16298": "|**2024-02-26**|**MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer**|MV-Swin-T\uff1a\u4f7f\u7528\u591a\u89c6\u56fe Swin Transformer \u8fdb\u884c\u4e73\u623f X \u5149\u68c0\u67e5\u5206\u7c7b|Sushmita Sarker, Prithul Sarker, George Bebis, Alireza Tavakkoli|Traditional deep learning approaches for breast cancer classification has predominantly concentrated on single-view analysis. In clinical practice, however, radiologists concurrently examine all views within a mammography exam, leveraging the inherent correlations in these views to effectively detect tumors. Acknowledging the significance of multi-view analysis, some studies have introduced methods that independently process mammogram views, either through distinct convolutional branches or simple fusion strategies, inadvertently leading to a loss of crucial inter-view correlations. In this paper, we propose an innovative multi-view network exclusively based on transformers to address challenges in mammographic image classification. Our approach introduces a novel shifted window-based dynamic attention block, facilitating the effective integration of multi-view information and promoting the coherent transfer of this information between views at the spatial feature map level. Furthermore, we conduct a comprehensive comparative analysis of the performance and effectiveness of transformer-based models under diverse settings, employing the CBIS-DDSM and Vin-Dr Mammo datasets. Our code is publicly available at https://github.com/prithuls/MV-Swin-T|\u4f20\u7edf\u7684\u4e73\u817a\u764c\u5206\u7c7b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u89c6\u56fe\u5206\u6790\u3002\u7136\u800c\uff0c\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u653e\u5c04\u79d1\u533b\u751f\u540c\u65f6\u68c0\u67e5\u4e73\u623f X \u5149\u68c0\u67e5\u4e2d\u7684\u6240\u6709\u89c6\u56fe\uff0c\u5229\u7528\u8fd9\u4e9b\u89c6\u56fe\u4e2d\u7684\u56fa\u6709\u76f8\u5173\u6027\u6765\u6709\u6548\u68c0\u6d4b\u80bf\u7624\u3002\u8ba4\u8bc6\u5230\u591a\u89c6\u56fe\u5206\u6790\u7684\u91cd\u8981\u6027\uff0c\u4e00\u4e9b\u7814\u7a76\u5f15\u5165\u4e86\u72ec\u7acb\u5904\u7406\u4e73\u623fX\u5149\u7167\u7247\u89c6\u56fe\u7684\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u901a\u8fc7\u4e0d\u540c\u7684\u5377\u79ef\u5206\u652f\u8fd8\u662f\u7b80\u5355\u7684\u878d\u5408\u7b56\u7565\uff0c\u65e0\u610f\u4e2d\u5bfc\u81f4\u5173\u952e\u7684\u89c6\u56fe\u95f4\u76f8\u5173\u6027\u7684\u4e22\u5931\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u521b\u65b0\u591a\u89c6\u56fe\u7f51\u7edc\uff0c\u4ee5\u89e3\u51b3\u4e73\u817aX\u7ebf\u6444\u5f71\u56fe\u50cf\u5206\u7c7b\u7684\u6311\u6218\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u79fb\u4f4d\u7a97\u53e3\u7684\u52a8\u6001\u6ce8\u610f\u5757\uff0c\u4fc3\u8fdb\u591a\u89c6\u56fe\u4fe1\u606f\u7684\u6709\u6548\u96c6\u6210\uff0c\u5e76\u4fc3\u8fdb\u8be5\u4fe1\u606f\u5728\u7a7a\u95f4\u7279\u5f81\u56fe\u7ea7\u522b\u7684\u89c6\u56fe\u4e4b\u95f4\u7684\u8fde\u8d2f\u4f20\u8f93\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528 CBIS-DDSM \u548c Vin-Dr Mammo \u6570\u636e\u96c6\uff0c\u5bf9\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u7684\u6027\u80fd\u548c\u6709\u6548\u6027\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6bd4\u8f83\u5206\u6790\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/prithuls/MV-Swin-T \u4e0a\u516c\u5f00\u83b7\u53d6|[2402.16298v1](http://arxiv.org/pdf/2402.16298v1)|null|\n", "2402.16280": "|**2024-02-26**|**Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation**|\u7528\u4e8e\u9ad8\u6548\u6ce8\u91ca\u7684\u6838\u5b9e\u4f8b\u5206\u5272\u7684\u5c11\u6837\u672c\u5b66\u4e60|Yu Ming, Zihao Wu, Jie Yang, Danyi Li, Yuan Gao, Changxin Gao, Gui-Song Xia, Yuanqing Li, Li Liang, Jin-Gang Yu|Nucleus instance segmentation from histopathology images suffers from the extremely laborious and expert-dependent annotation of nucleus instances. As a promising solution to this task, annotation-efficient deep learning paradigms have recently attracted much research interest, such as weakly-/semi-supervised learning, generative adversarial learning, etc. In this paper, we propose to formulate annotation-efficient nucleus instance segmentation from the perspective of few-shot learning (FSL). Our work was motivated by that, with the prosperity of computational pathology, an increasing number of fully-annotated datasets are publicly accessible, and we hope to leverage these external datasets to assist nucleus instance segmentation on the target dataset which only has very limited annotation. To achieve this goal, we adopt the meta-learning based FSL paradigm, which however has to be tailored in two substantial aspects before adapting to our task. First, since the novel classes may be inconsistent with those of the external dataset, we extend the basic definition of few-shot instance segmentation (FSIS) to generalized few-shot instance segmentation (GFSIS). Second, to cope with the intrinsic challenges of nucleus segmentation, including touching between adjacent cells, cellular heterogeneity, etc., we further introduce a structural guidance mechanism into the GFSIS network, finally leading to a unified Structurally-Guided Generalized Few-Shot Instance Segmentation (SGFSIS) framework. Extensive experiments on a couple of publicly accessible datasets demonstrate that, SGFSIS can outperform other annotation-efficient learning baselines, including semi-supervised learning, simple transfer learning, etc., with comparable performance to fully supervised learning with less than 5% annotations.|\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u5206\u5272\u7ec6\u80de\u6838\u5b9e\u4f8b\u9700\u8981\u5bf9\u7ec6\u80de\u6838\u5b9e\u4f8b\u8fdb\u884c\u6781\u5176\u8d39\u529b\u4e14\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u7684\u6ce8\u91ca\u3002\u4f5c\u4e3a\u8be5\u4efb\u52a1\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6ce8\u91ca\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\u6700\u8fd1\u5f15\u8d77\u4e86\u5f88\u591a\u7814\u7a76\u5174\u8da3\uff0c\u4f8b\u5982\u5f31/\u534a\u76d1\u7763\u5b66\u4e60\u3001\u751f\u6210\u5bf9\u6297\u5b66\u4e60\u7b49\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u5236\u5b9a\u6ce8\u91ca\u9ad8\u6548\u7684\u6838\u5fc3\u5b9e\u4f8b\u4ece\u5c11\u6837\u672c\u5b66\u4e60\uff08FSL\uff09\u7684\u89d2\u5ea6\u8fdb\u884c\u5206\u5272\u3002\u6211\u4eec\u5de5\u4f5c\u7684\u52a8\u673a\u662f\uff0c\u968f\u7740\u8ba1\u7b97\u75c5\u7406\u5b66\u7684\u7e41\u8363\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u5b8c\u5168\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u516c\u5f00\u8bbf\u95ee\uff0c\u6211\u4eec\u5e0c\u671b\u5229\u7528\u8fd9\u4e9b\u5916\u90e8\u6570\u636e\u96c6\u6765\u534f\u52a9\u5bf9\u53ea\u6709\u975e\u5e38\u6709\u9650\u7684\u6ce8\u91ca\u7684\u76ee\u6807\u6570\u636e\u96c6\u8fdb\u884c\u6838\u5b9e\u4f8b\u5206\u5272\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u5143\u5b66\u4e60\u7684 FSL \u8303\u5f0f\uff0c\u4f46\u662f\u5728\u9002\u5e94\u6211\u4eec\u7684\u4efb\u52a1\u4e4b\u524d\uff0c\u5fc5\u987b\u5728\u4e24\u4e2a\u5b9e\u8d28\u6027\u65b9\u9762\u8fdb\u884c\u5b9a\u5236\u3002\u9996\u5148\uff0c\u7531\u4e8e\u65b0\u7684\u7c7b\u53ef\u80fd\u4e0e\u5916\u90e8\u6570\u636e\u96c6\u7684\u7c7b\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u5c11\u6837\u672c\u5b9e\u4f8b\u5206\u5272\uff08FSIS\uff09\u7684\u57fa\u672c\u5b9a\u4e49\u6269\u5c55\u5230\u5e7f\u4e49\u5c11\u6837\u672c\u5b9e\u4f8b\u5206\u5272\uff08GFSIS\uff09\u3002\u5176\u6b21\uff0c\u4e3a\u4e86\u5e94\u5bf9\u7ec6\u80de\u6838\u5206\u5272\u7684\u5185\u5728\u6311\u6218\uff0c\u5305\u62ec\u76f8\u90bb\u7ec6\u80de\u4e4b\u95f4\u7684\u63a5\u89e6\u3001\u7ec6\u80de\u5f02\u8d28\u6027\u7b49\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5728 GFSIS \u7f51\u7edc\u4e2d\u5f15\u5165\u7ed3\u6784\u6307\u5bfc\u673a\u5236\uff0c\u6700\u7ec8\u5f62\u6210\u7edf\u4e00\u7684\u7ed3\u6784\u5f15\u5bfc\u5e7f\u4e49\u5c11\u6837\u672c\u5b9e\u4f8b\u5206\u5272\uff08SGFSIS\uff09\u6846\u67b6\u3002\u5bf9\u51e0\u4e2a\u53ef\u516c\u5f00\u8bbf\u95ee\u7684\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSGFSIS \u53ef\u4ee5\u8d85\u8d8a\u5176\u4ed6\u6ce8\u91ca\u9ad8\u6548\u7684\u5b66\u4e60\u57fa\u7ebf\uff0c\u5305\u62ec\u534a\u76d1\u7763\u5b66\u4e60\u3001\u7b80\u5355\u8fc1\u79fb\u5b66\u4e60\u7b49\uff0c\u5176\u6027\u80fd\u4e0e\u6ce8\u91ca\u7387\u4f4e\u4e8e 5% \u7684\u5b8c\u5168\u76d1\u7763\u5b66\u4e60\u76f8\u5f53\u3002|[2402.16280v1](http://arxiv.org/pdf/2402.16280v1)|null|\n", "2402.16249": "|**2024-02-26**|**SeqTrack3D: Exploring Sequence Information for Robust 3D Point Cloud Tracking**|SeqTrack3D\uff1a\u63a2\u7d22\u7a33\u5065 3D \u70b9\u4e91\u8ddf\u8e2a\u7684\u5e8f\u5217\u4fe1\u606f|Yu Lin, Zhiheng Li, Yubo Cui, Zheng Fang|3D single object tracking (SOT) is an important and challenging task for the autonomous driving and mobile robotics. Most existing methods perform tracking between two consecutive frames while ignoring the motion patterns of the target over a series of frames, which would cause performance degradation in the scenes with sparse points. To break through this limitation, we introduce Sequence-to-Sequence tracking paradigm and a tracker named SeqTrack3D to capture target motion across continuous frames. Unlike previous methods that primarily adopted three strategies: matching two consecutive point clouds, predicting relative motion, or utilizing sequential point clouds to address feature degradation, our SeqTrack3D combines both historical point clouds and bounding box sequences. This novel method ensures robust tracking by leveraging location priors from historical boxes, even in scenes with sparse points. Extensive experiments conducted on large-scale datasets show that SeqTrack3D achieves new state-of-the-art performances, improving by 6.00% on NuScenes and 14.13% on Waymo dataset. The code will be made public at https://github.com/aron-lin/seqtrack3d.|3D \u5355\u76ee\u6807\u8ddf\u8e2a (SOT) \u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u79fb\u52a8\u673a\u5668\u4eba\u6765\u8bf4\u662f\u4e00\u9879\u91cd\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u5728\u4e24\u4e2a\u8fde\u7eed\u5e27\u4e4b\u95f4\u6267\u884c\u8ddf\u8e2a\uff0c\u800c\u5ffd\u7565\u76ee\u6807\u5728\u4e00\u7cfb\u5217\u5e27\u4e0a\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u7a00\u758f\u70b9\u573a\u666f\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u3002\u4e3a\u4e86\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5e8f\u5217\u5230\u5e8f\u5217\u8ddf\u8e2a\u8303\u4f8b\u548c\u540d\u4e3a SeqTrack3D \u7684\u8ddf\u8e2a\u5668\u6765\u6355\u83b7\u8fde\u7eed\u5e27\u4e2d\u7684\u76ee\u6807\u8fd0\u52a8\u3002\u4e0e\u4e4b\u524d\u4e3b\u8981\u91c7\u7528\u4e09\u79cd\u7b56\u7565\u7684\u65b9\u6cd5\u4e0d\u540c\uff1a\u5339\u914d\u4e24\u4e2a\u8fde\u7eed\u70b9\u4e91\u3001\u9884\u6d4b\u76f8\u5bf9\u8fd0\u52a8\u6216\u5229\u7528\u987a\u5e8f\u70b9\u4e91\u6765\u89e3\u51b3\u7279\u5f81\u9000\u5316\u95ee\u9898\uff0c\u6211\u4eec\u7684 SeqTrack3D \u7ed3\u5408\u4e86\u5386\u53f2\u70b9\u4e91\u548c\u8fb9\u754c\u6846\u5e8f\u5217\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5386\u53f2\u6846\u4e2d\u7684\u4f4d\u7f6e\u5148\u9a8c\u6765\u786e\u4fdd\u7a33\u5065\u7684\u8ddf\u8e2a\uff0c\u5373\u4f7f\u5728\u5177\u6709\u7a00\u758f\u70b9\u7684\u573a\u666f\u4e2d\u4e5f\u662f\u5982\u6b64\u3002\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSeqTrack3D \u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728 NuScenes \u4e0a\u63d0\u9ad8\u4e86 6.00%\uff0c\u5728 Waymo \u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e86 14.13%\u3002\u8be5\u4ee3\u7801\u5c06\u5728 https://github.com/aron-lin/seqtrack3d \u4e0a\u516c\u5f00\u3002|[2402.16249v1](http://arxiv.org/pdf/2402.16249v1)|null|\n", "2402.16246": "|**2024-02-26**|**Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices**|\u57fa\u4e8e\u79fb\u52a8\u8bbe\u5907\u4e0a\u65e0\u4eba\u673a\u4ea4\u901a\u89c6\u9891\u7684\u5b9e\u65f6\u8f66\u8f86\u68c0\u6d4b\u548c\u57ce\u5e02\u4ea4\u901a\u884c\u4e3a\u5206\u6790|Yuan Zhu, Yanqiang Wang, Yadong An, Hong Yang, Yiming Pan|This paper focuses on a real-time vehicle detection and urban traffic behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video. By using UAV to collect traffic data and combining the YOLOv8 model and SORT tracking algorithm, the object detection and tracking functions are implemented on the iOS mobile platform. For the problem of traffic data acquisition and analysis, the dynamic computing method is used to process the performance in real time and calculate the micro and macro traffic parameters of the vehicles, and real-time traffic behavior analysis is conducted and visualized. The experiment results reveals that the vehicle object detection can reach 98.27% precision rate and 87.93% recall rate, and the real-time processing capacity is stable at 30 frames per seconds. This work integrates drone technology, iOS development, and deep learning techniques to integrate traffic video acquisition, object detection, object tracking, and traffic behavior analysis functions on mobile devices. It provides new possibilities for lightweight traffic information collection and data analysis, and offers innovative solutions to improve the efficiency of analyzing road traffic conditions and addressing transportation issues for transportation authorities.|\u672c\u6587\u91cd\u70b9\u7814\u7a76\u57fa\u4e8e\u65e0\u4eba\u673a\u4ea4\u901a\u89c6\u9891\u7684\u5b9e\u65f6\u8f66\u8f86\u68c0\u6d4b\u548c\u57ce\u5e02\u4ea4\u901a\u884c\u4e3a\u5206\u6790\u7cfb\u7edf\u3002\u5229\u7528\u65e0\u4eba\u673a\u91c7\u96c6\u4ea4\u901a\u6570\u636e\uff0c\u7ed3\u5408YOLOv8\u6a21\u578b\u548cSORT\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u5728iOS\u79fb\u52a8\u5e73\u53f0\u4e0a\u5b9e\u73b0\u7269\u4f53\u68c0\u6d4b\u548c\u8ddf\u8e2a\u529f\u80fd\u3002\u9488\u5bf9\u4ea4\u901a\u6570\u636e\u91c7\u96c6\u548c\u5206\u6790\u95ee\u9898\uff0c\u91c7\u7528\u52a8\u6001\u8ba1\u7b97\u65b9\u6cd5\u5bf9\u6027\u80fd\u8fdb\u884c\u5b9e\u65f6\u5904\u7406\uff0c\u8ba1\u7b97\u51fa\u8f66\u8f86\u7684\u5fae\u89c2\u548c\u5b8f\u89c2\u4ea4\u901a\u53c2\u6570\uff0c\u8fdb\u884c\u5b9e\u65f6\u4ea4\u901a\u884c\u4e3a\u5206\u6790\u5e76\u53ef\u89c6\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8f66\u8f86\u76ee\u6807\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523098.27%\uff0c\u53ec\u56de\u7387\u8fbe\u523087.93%\uff0c\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u7a33\u5b9a\u572830\u5e27/\u79d2\u3002\u8fd9\u9879\u5de5\u4f5c\u96c6\u6210\u4e86\u65e0\u4eba\u673a\u6280\u672f\u3001iOS\u5f00\u53d1\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u96c6\u6210\u4ea4\u901a\u89c6\u9891\u91c7\u96c6\u3001\u7269\u4f53\u68c0\u6d4b\u3001\u7269\u4f53\u8ddf\u8e2a\u548c\u4ea4\u901a\u884c\u4e3a\u5206\u6790\u529f\u80fd\u3002\u5b83\u4e3a\u8f7b\u91cf\u7ea7\u4ea4\u901a\u4fe1\u606f\u91c7\u96c6\u548c\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u4ea4\u901a\u4e3b\u7ba1\u90e8\u95e8\u63d0\u9ad8\u5206\u6790\u9053\u8def\u4ea4\u901a\u72b6\u51b5\u548c\u89e3\u51b3\u4ea4\u901a\u95ee\u9898\u7684\u6548\u7387\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002|[2402.16246v1](http://arxiv.org/pdf/2402.16246v1)|null|\n", "2402.16242": "|**2024-02-26**|**HSONet:A Siamese foreground association-driven hard case sample optimization network for high-resolution remote sensing image change detection**|HSONet\uff1a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u7684\u8fde\u4f53\u524d\u666f\u5173\u8054\u9a71\u52a8\u7684\u786c\u4f8b\u6837\u672c\u4f18\u5316\u7f51\u7edc|Chao Tao, Dongsheng Kuang, Zhenyang Huang, Chengli Peng, Haifeng Li|In the later training stages, further improvement of the models ability to determine changes relies on how well the change detection (CD) model learns hard cases; however, there are two additional challenges to learning hard case samples: (1) change labels are limited and tend to pointer only to foreground targets, yet hard case samples are prevalent in the background, which leads to optimizing the loss function focusing on the foreground targets and ignoring the background hard cases, which we call imbalance. (2) Complex situations, such as light shadows, target occlusion, and seasonal changes, induce hard case samples, and in the absence of both supervisory and scene information, it is difficult for the model to learn hard case samples directly to accurately obtain the feature representations of the change information, which we call missingness. We propose a Siamese foreground association-driven hard case sample optimization network (HSONet). To deal with this imbalance, we propose an equilibrium optimization loss function to regulate the optimization focus of the foreground and background, determine the hard case samples through the distribution of the loss values, and introduce dynamic weights in the loss term to gradually shift the optimization focus of the loss from the foreground to the background hard cases as the training progresses. To address this missingness, we understand hard case samples with the help of the scene context, propose the scene-foreground association module, use potential remote sensing spatial scene information to model the association between the target of interest in the foreground and the related context to obtain scene embedding, and apply this information to the feature reinforcement of hard cases. Experiments on four public datasets show that HSONet outperforms current state-of-the-art CD methods, particularly in detecting hard case samples.|\u5728\u540e\u671f\u7684\u8bad\u7ec3\u9636\u6bb5\uff0c\u6a21\u578b\u5224\u65ad\u53d8\u5316\u80fd\u529b\u7684\u8fdb\u4e00\u6b65\u63d0\u9ad8\u53d6\u51b3\u4e8e\u53d8\u5316\u68c0\u6d4b\uff08CD\uff09\u6a21\u578b\u5bf9\u56f0\u96be\u6848\u4f8b\u7684\u5b66\u4e60\u7a0b\u5ea6\uff1b\u7136\u800c\uff0c\u5b66\u4e60\u56f0\u96be\u6848\u4f8b\u6837\u672c\u8fd8\u5b58\u5728\u4e24\u4e2a\u989d\u5916\u7684\u6311\u6218\uff1a\uff081\uff09\u53d8\u5316\u6807\u7b7e\u662f\u6709\u9650\u7684\uff0c\u5e76\u4e14\u5f80\u5f80\u4ec5\u6307\u5411\u524d\u666f\u76ee\u6807\uff0c\u800c\u56f0\u96be\u6848\u4f8b\u6837\u672c\u5728\u80cc\u666f\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u8fd9\u5bfc\u81f4\u4f18\u5316\u4e13\u6ce8\u4e8e\u524d\u666f\u7684\u635f\u5931\u51fd\u6570\u76ee\u6807\u800c\u5ffd\u89c6\u80cc\u666f\u7591\u96be\u6848\u4f8b\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u4e0d\u5e73\u8861\u3002 (2) \u5149\u5f71\u3001\u76ee\u6807\u906e\u6321\u3001\u5b63\u8282\u53d8\u5316\u7b49\u590d\u6742\u60c5\u51b5\u4f1a\u8bf1\u53d1\u56f0\u96be\u6837\u672c\uff0c\u5728\u7f3a\u4e4f\u76d1\u7763\u4fe1\u606f\u548c\u573a\u666f\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u5f88\u96be\u76f4\u63a5\u5b66\u4e60\u56f0\u96be\u6837\u672c\u6765\u51c6\u786e\u83b7\u5f97\u53d8\u5316\u4fe1\u606f\u7684\u7279\u5f81\u8868\u793a\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u7f3a\u5931\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u66b9\u7f57\u524d\u666f\u5173\u8054\u9a71\u52a8\u7684\u786c\u6848\u4f8b\u6837\u672c\u4f18\u5316\u7f51\u7edc\uff08HSONet\uff09\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5e73\u8861\u4f18\u5316\u635f\u5931\u51fd\u6570\u6765\u8c03\u8282\u524d\u666f\u548c\u80cc\u666f\u7684\u4f18\u5316\u7126\u70b9\uff0c\u901a\u8fc7\u635f\u5931\u503c\u7684\u5206\u5e03\u6765\u786e\u5b9a\u56f0\u96be\u60c5\u51b5\u6837\u672c\uff0c\u5e76\u5728\u635f\u5931\u9879\u4e2d\u5f15\u5165\u52a8\u6001\u6743\u91cd\u6765\u9010\u6b65\u8f6c\u79fb\u4f18\u5316\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\uff0c\u635f\u5931\u7684\u7126\u70b9\u4ece\u524d\u53f0\u5230\u540e\u53f0\u56f0\u96be\u60c5\u51b5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u7f3a\u5931\uff0c\u6211\u4eec\u501f\u52a9\u573a\u666f\u4e0a\u4e0b\u6587\u6765\u7406\u89e3\u56f0\u96be\u6837\u672c\uff0c\u63d0\u51fa\u573a\u666f-\u524d\u666f\u5173\u8054\u6a21\u5757\uff0c\u5229\u7528\u6f5c\u5728\u7684\u9065\u611f\u7a7a\u95f4\u573a\u666f\u4fe1\u606f\u5bf9\u524d\u666f\u4e2d\u611f\u5174\u8da3\u7684\u76ee\u6807\u4e0e\u76f8\u5173\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u5173\u8054\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u83b7\u5f97\u573a\u666f\u5d4c\u5165\uff0c\u5e76\u5c06\u8be5\u4fe1\u606f\u5e94\u7528\u4e8e\u786c\u6848\u4f8b\u7684\u7279\u5f81\u5f3a\u5316\u3002\u5bf9\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHSONet \u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684 CD \u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u56f0\u96be\u6837\u672c\u65b9\u9762\u3002|[2402.16242v1](http://arxiv.org/pdf/2402.16242v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "LLM": {}, "Transformer": {}, "3D/CG": {}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.16424": "|**2024-02-26**|**COMAE: COMprehensive Attribute Exploration for Zero-shot Hashing**|COMAE\uff1a\u96f6\u6837\u672c\u54c8\u5e0c\u7684\u7efc\u5408\u5c5e\u6027\u63a2\u7d22|Yihang Zhou, Qingqing Long, Yuchen Yan, Xiao Luo, Zeyu Dong, Xuezhi Wang, Zhen Meng, Pengfei Wang, Yuanchun Zhou|Zero-shot hashing (ZSH) has shown excellent success owing to its efficiency and generalization in large-scale retrieval scenarios. While considerable success has been achieved, there still exist urgent limitations. Existing works ignore the locality relationships of representations and attributes, which have effective transferability between seeable classes and unseeable classes. Also, the continuous-value attributes are not fully harnessed. In response, we conduct a COMprehensive Attribute Exploration for ZSH, named COMAE, which depicts the relationships from seen classes to unseen ones through three meticulously designed explorations, i.e., point-wise, pair-wise and class-wise consistency constraints. By regressing attributes from the proposed attribute prototype network, COMAE learns the local features that are relevant to the visual attributes. Then COMAE utilizes contrastive learning to comprehensively depict the context of attributes, rather than instance-independent optimization. Finally, the class-wise constraint is designed to cohesively learn the hash code, image representation, and visual attributes more effectively. Experimental results on the popular ZSH datasets demonstrate that COMAE outperforms state-of-the-art hashing techniques, especially in scenarios with a larger number of unseen label classes.|\u96f6\u6837\u672c\u54c8\u5e0c\uff08ZSH\uff09\u7531\u4e8e\u5176\u5728\u5927\u89c4\u6a21\u68c0\u7d22\u573a\u666f\u4e2d\u7684\u6548\u7387\u548c\u6cdb\u5316\u6027\u800c\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u6210\u529f\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u6210\u529f\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u7d27\u8feb\u7684\u5c40\u9650\u6027\u3002\u73b0\u6709\u7684\u5de5\u4f5c\u5ffd\u7565\u4e86\u8868\u793a\u548c\u5c5e\u6027\u7684\u5c40\u90e8\u6027\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5c40\u90e8\u6027\u5173\u7cfb\u5728\u53ef\u89c1\u7c7b\u548c\u4e0d\u53ef\u89c1\u7c7b\u4e4b\u95f4\u5177\u6709\u6709\u6548\u7684\u53ef\u8f6c\u79fb\u6027\u3002\u6b64\u5916\uff0c\u8fde\u7eed\u503c\u5c5e\u6027\u6ca1\u6709\u5f97\u5230\u5145\u5206\u5229\u7528\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5bf9 ZSH \u8fdb\u884c\u4e86\u7efc\u5408\u5c5e\u6027\u63a2\u7d22\uff0c\u540d\u4e3a COMAE\uff0c\u5b83\u901a\u8fc7\u4e09\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63a2\u7d22\uff08\u5373\u9010\u70b9\u4e00\u81f4\u6027\u7ea6\u675f\u3001\u9010\u5bf9\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u7c7b\u4e00\u81f4\u6027\u7ea6\u675f\uff09\u6765\u63cf\u8ff0\u4ece\u53ef\u89c1\u7c7b\u5230\u672a\u89c1\u7c7b\u7684\u5173\u7cfb\u3002\u901a\u8fc7\u4ece\u6240\u63d0\u51fa\u7684\u5c5e\u6027\u539f\u578b\u7f51\u7edc\u56de\u5f52\u5c5e\u6027\uff0cCOMAE \u5b66\u4e60\u4e0e\u89c6\u89c9\u5c5e\u6027\u76f8\u5173\u7684\u5c40\u90e8\u7279\u5f81\u3002\u7136\u540e COMAE \u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u5168\u9762\u63cf\u8ff0\u5c5e\u6027\u7684\u4e0a\u4e0b\u6587\uff0c\u800c\u4e0d\u662f\u4e0e\u5b9e\u4f8b\u65e0\u5173\u7684\u4f18\u5316\u3002\u6700\u540e\uff0c\u7c7b\u7ea6\u675f\u88ab\u8bbe\u8ba1\u4e3a\u66f4\u6709\u6548\u5730\u7ed3\u5408\u5b66\u4e60\u54c8\u5e0c\u7801\u3001\u56fe\u50cf\u8868\u793a\u548c\u89c6\u89c9\u5c5e\u6027\u3002\u6d41\u884c\u7684 ZSH \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCOMAE \u7684\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u54c8\u5e0c\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u5927\u91cf\u672a\u89c1\u6807\u7b7e\u7c7b\u7684\u573a\u666f\u4e2d\u3002|[2402.16424v1](http://arxiv.org/pdf/2402.16424v1)|null|\n"}, "\u5176\u4ed6": {"2402.16473": "|**2024-02-26**|**DCVSMNet: Double Cost Volume Stereo Matching Network**|DCVSMNet\uff1a\u53cc\u6210\u672c\u4f53\u79ef\u7acb\u4f53\u5339\u914d\u7f51\u7edc|Mahmoud Tahmasebi, Saif Huq, Kevin Meehan, Marion McAfee|We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a novel architecture characterised by by two small upper (group-wise) and lower (norm correlation) cost volumes. Each cost volume is processed separately, and a coupling module is proposed to fuse the geometry information extracted from the upper and lower cost volumes. DCVSMNet is a fast stereo matching network with a 67 ms inference time and strong generalization ability which can produce competitive results compared to state-of-the-art methods. The results on several bench mark datasets show that DCVSMNet achieves better accuracy than methods such as CGI-Stereo and BGNet at the cost of greater inference time.|\u6211\u4eec\u5f15\u5165\u4e86\u53cc\u6210\u672c\u4f53\u79ef\u7acb\u4f53\u5339\u914d\u7f51\u7edc\uff08DCVSMNet\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u5176\u7279\u5f81\u662f\u4e24\u4e2a\u5c0f\u7684\u4e0a\u90e8\uff08\u5206\u7ec4\uff09\u548c\u4e0b\u90e8\uff08\u89c4\u8303\u76f8\u5173\uff09\u6210\u672c\u4f53\u79ef\u3002\u6bcf\u4e2a\u6210\u672c\u5377\u88ab\u5355\u72ec\u5904\u7406\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u4e2a\u8026\u5408\u6a21\u5757\u6765\u878d\u5408\u4ece\u4e0a\u90e8\u548c\u4e0b\u90e8\u6210\u672c\u5377\u4e2d\u63d0\u53d6\u7684\u51e0\u4f55\u4fe1\u606f\u3002 DCVSMNet \u662f\u4e00\u79cd\u5feb\u901f\u7acb\u4f53\u5339\u914d\u7f51\u7edc\uff0c\u5177\u6709 67 ms \u7684\u63a8\u7406\u65f6\u95f4\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53ef\u4ee5\u4ea7\u751f\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u7ed3\u679c\u8868\u660e\uff0cDCVSMNet \u6bd4 CGI-Stereo \u548c BGNet \u7b49\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u7cbe\u5ea6\uff0c\u4f46\u4ee3\u4ef7\u662f\u66f4\u957f\u7684\u63a8\u7406\u65f6\u95f4\u3002|[2402.16473v1](http://arxiv.org/pdf/2402.16473v1)|null|\n", "2402.16442": "|**2024-02-26**|**On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions**|\u57fa\u4e8e\u6210\u5bf9\u5b50\u6a21\u51fd\u6570\u7684\u5206\u5e03\u5f0f\u5927\u4e8e\u5185\u5b58\u5b50\u96c6\u9009\u62e9|Maximilian B\u00f6ther, Abraham Sebastian, Pranjal Awasthi, Ana Klimovic, Srikumar Ramalingam|Many learning problems hinge on the fundamental problem of subset selection, i.e., identifying a subset of important and representative points. For example, selecting the most significant samples in ML training cannot only reduce training costs but also enhance model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, partition-based distributed greedy algorithm to identify the remaining subset. We show that these algorithms find high quality subsets on CIFAR-100 and ImageNet with marginal or no loss in quality compared to centralized methods, and scale to a dataset with 13 billion points.|\u8bb8\u591a\u5b66\u4e60\u95ee\u9898\u90fd\u53d6\u51b3\u4e8e\u5b50\u96c6\u9009\u62e9\u7684\u57fa\u672c\u95ee\u9898\uff0c\u5373\u8bc6\u522b\u91cd\u8981\u4e14\u6709\u4ee3\u8868\u6027\u7684\u70b9\u7684\u5b50\u96c6\u3002\u4f8b\u5982\uff0c\u5728\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u4e2d\u9009\u62e9\u6700\u91cd\u8981\u7684\u6837\u672c\u4e0d\u4ec5\u53ef\u4ee5\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\uff0c\u8fd8\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u8d28\u91cf\u3002\u5b50\u6a21\u6027\u662f\u51f8\u6027\u7684\u79bb\u6563\u6a21\u62df\uff0c\u901a\u5e38\u7528\u4e8e\u89e3\u51b3\u5b50\u96c6\u9009\u62e9\u95ee\u9898\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4f18\u5316\u5b50\u6a21\u51fd\u6570\u7684\u7b97\u6cd5\u662f\u987a\u5e8f\u7684\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u65b9\u6cd5\u9700\u8981\u81f3\u5c11\u4e00\u53f0\u4e2d\u592e\u673a\u6765\u62df\u5408\u76ee\u6807\u5b50\u96c6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u5177\u6709\u53ef\u8bc1\u660e\u7684\u8fd1\u4f3c\u4fdd\u8bc1\u7684\u65b0\u578b\u5206\u5e03\u5f0f\u8fb9\u754c\u7b97\u6cd5\uff0c\u653e\u5bbd\u4e86\u5bf9\u76ee\u6807\u5b50\u96c6\u62e5\u6709\u4e2d\u592e\u673a\u5668\u7684\u8981\u6c42\u3002\u8be5\u7b97\u6cd5\u8fed\u4ee3\u5730\u9650\u5236\u6700\u5c0f\u548c\u6700\u5927\u6548\u7528\u503c\uff0c\u4ee5\u9009\u62e9\u9ad8\u8d28\u91cf\u70b9\u5e76\u4e22\u5f03\u4e0d\u91cd\u8981\u7684\u70b9\u3002\u5f53\u8fb9\u754c\u6ca1\u6709\u627e\u5230\u5b8c\u6574\u7684\u5b50\u96c6\u65f6\uff0c\u6211\u4eec\u4f7f\u7528\u591a\u8f6e\u3001\u57fa\u4e8e\u5206\u533a\u7684\u5206\u5e03\u5f0f\u8d2a\u5fc3\u7b97\u6cd5\u6765\u8bc6\u522b\u5269\u4f59\u7684\u5b50\u96c6\u3002\u6211\u4eec\u8868\u660e\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u80fd\u591f\u5728 CIFAR-100 \u548c ImageNet \u4e0a\u627e\u5230\u9ad8\u8d28\u91cf\u7684\u5b50\u96c6\uff0c\u4e0e\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8d28\u91cf\u51e0\u4e4e\u6ca1\u6709\u635f\u5931\u6216\u6ca1\u6709\u635f\u5931\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u5305\u542b 130 \u4ebf\u4e2a\u70b9\u7684\u6570\u636e\u96c6\u3002|[2402.16442v1](http://arxiv.org/pdf/2402.16442v1)|null|\n", "2402.16399": "|**2024-02-26**|**Analysis of Embeddings Learned by End-to-End Machine Learning Eye Movement-driven Biometrics Pipeline**|\u901a\u8fc7\u7aef\u5230\u7aef\u673a\u5668\u5b66\u4e60\u773c\u52a8\u9a71\u52a8\u7684\u751f\u7269\u8bc6\u522b\u7ba1\u9053\u5b66\u4e60\u7684\u5d4c\u5165\u5206\u6790|Mehedi Hasan Raju, Lee Friedman, Dillon J Lohr, Oleg V Komogortsev|This paper expands on the foundational concept of temporal persistence in biometric systems, specifically focusing on the domain of eye movement biometrics facilitated by machine learning. Unlike previous studies that primarily focused on developing biometric authentication systems, our research delves into the embeddings learned by these systems, particularly examining their temporal persistence, reliability, and biometric efficacy in response to varying input data. Utilizing two publicly available eye-movement datasets, we employed the state-of-the-art Eye Know You Too machine learning pipeline for our analysis. We aim to validate whether the machine learning-derived embeddings in eye movement biometrics mirror the temporal persistence observed in traditional biometrics. Our methodology involved conducting extensive experiments to assess how different lengths and qualities of input data influence the performance of eye movement biometrics more specifically how it impacts the learned embeddings. We also explored the reliability and consistency of the embeddings under varying data conditions. Three key metrics (kendall's coefficient of concordance, intercorrelations, and equal error rate) were employed to quantitatively evaluate our findings. The results reveal while data length significantly impacts the stability of the learned embeddings, however, the intercorrelations among embeddings show minimal effect.|\u672c\u6587\u6269\u5c55\u4e86\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u4e2d\u65f6\u95f4\u6301\u4e45\u6027\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u7279\u522b\u5173\u6ce8\u673a\u5668\u5b66\u4e60\u4fc3\u8fdb\u7684\u773c\u52a8\u751f\u7269\u8bc6\u522b\u9886\u57df\u3002\u4e0e\u4e4b\u524d\u4e3b\u8981\u4e13\u6ce8\u4e8e\u5f00\u53d1\u751f\u7269\u8bc6\u522b\u8ba4\u8bc1\u7cfb\u7edf\u7684\u7814\u7a76\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u7814\u7a76\u6df1\u5165\u7814\u7a76\u4e86\u8fd9\u4e9b\u7cfb\u7edf\u5b66\u4e60\u5230\u7684\u5d4c\u5165\uff0c\u7279\u522b\u662f\u68c0\u67e5\u5b83\u4eec\u54cd\u5e94\u4e0d\u540c\u8f93\u5165\u6570\u636e\u7684\u65f6\u95f4\u6301\u4e45\u6027\u3001\u53ef\u9760\u6027\u548c\u751f\u7269\u8bc6\u522b\u529f\u6548\u3002\u5229\u7528\u4e24\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u773c\u52a8\u6570\u636e\u96c6\uff0c\u6211\u4eec\u91c7\u7528\u6700\u5148\u8fdb\u7684 Eye Know You Too \u673a\u5668\u5b66\u4e60\u7ba1\u9053\u8fdb\u884c\u5206\u6790\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u9a8c\u8bc1\u773c\u52a8\u751f\u7269\u8bc6\u522b\u4e2d\u673a\u5668\u5b66\u4e60\u884d\u751f\u7684\u5d4c\u5165\u662f\u5426\u53cd\u6620\u4e86\u4f20\u7edf\u751f\u7269\u8bc6\u522b\u4e2d\u89c2\u5bdf\u5230\u7684\u65f6\u95f4\u6301\u4e45\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d89\u53ca\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u8bc4\u4f30\u8f93\u5165\u6570\u636e\u7684\u4e0d\u540c\u957f\u5ea6\u548c\u8d28\u91cf\u5982\u4f55\u5f71\u54cd\u773c\u52a8\u751f\u7269\u8bc6\u522b\u6280\u672f\u7684\u6027\u80fd\uff0c\u66f4\u5177\u4f53\u5730\u8bf4\u662f\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u7684\u5d4c\u5165\u3002\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86\u4e0d\u540c\u6570\u636e\u6761\u4ef6\u4e0b\u5d4c\u5165\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002\u91c7\u7528\u4e09\u4e2a\u5173\u952e\u6307\u6807\uff08\u80af\u5fb7\u5c14\u4e00\u81f4\u6027\u7cfb\u6570\u3001\u4e92\u76f8\u5173\u6027\u548c\u7b49\u9519\u8bef\u7387\uff09\u6765\u5b9a\u91cf\u8bc4\u4f30\u6211\u4eec\u7684\u53d1\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u6570\u636e\u957f\u5ea6\u663e\u7740\u5f71\u54cd\u5b66\u4e60\u5d4c\u5165\u7684\u7a33\u5b9a\u6027\uff0c\u4f46\u662f\u5d4c\u5165\u4e4b\u95f4\u7684\u76f8\u4e92\u76f8\u5173\u6027\u5f71\u54cd\u5f88\u5c0f\u3002|[2402.16399v1](http://arxiv.org/pdf/2402.16399v1)|null|\n", "2402.16350": "|**2024-02-26**|**Impression-CLIP: Contrastive Shape-Impression Embedding for Fonts**|Impression-CLIP\uff1a\u5b57\u4f53\u7684\u5bf9\u6bd4\u5f62\u72b6\u5370\u8c61\u5d4c\u5165|Yugo Kubota, Daichi Haraguchi, Seiichi Uchida|Fonts convey different impressions to readers. These impressions often come from the font shapes. However, the correlation between fonts and their impression is weak and unstable because impressions are subjective. To capture such weak and unstable cross-modal correlation between font shapes and their impressions, we propose Impression-CLIP, which is a novel machine-learning model based on CLIP (Contrastive Language-Image Pre-training). By using the CLIP-based model, font image features and their impression features are pulled closer, and font image features and unrelated impression features are pushed apart. This procedure realizes co-embedding between font image and their impressions. In our experiment, we perform cross-modal retrieval between fonts and impressions through co-embedding. The results indicate that Impression-CLIP achieves better retrieval accuracy than the state-of-the-art method. Additionally, our model shows the robustness to noise and missing tags.|\u5b57\u4f53\u5411\u8bfb\u8005\u4f20\u8fbe\u4e0d\u540c\u7684\u5370\u8c61\u3002\u8fd9\u4e9b\u5370\u8c61\u901a\u5e38\u6765\u81ea\u5b57\u4f53\u5f62\u72b6\u3002\u7136\u800c\uff0c\u5b57\u4f53\u4e0e\u5176\u5370\u8c61\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u5f88\u5f31\u4e14\u4e0d\u7a33\u5b9a\uff0c\u56e0\u4e3a\u5370\u8c61\u662f\u4e3b\u89c2\u7684\u3002\u4e3a\u4e86\u6355\u6349\u5b57\u4f53\u5f62\u72b6\u4e0e\u5176\u5370\u8c61\u4e4b\u95f4\u8fd9\u79cd\u5f31\u4e14\u4e0d\u7a33\u5b9a\u7684\u8de8\u6a21\u6001\u76f8\u5173\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Impression-CLIP\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e CLIP\uff08\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\uff09\u7684\u65b0\u578b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8eCLIP\u7684\u6a21\u578b\uff0c\u5b57\u4f53\u56fe\u50cf\u7279\u5f81\u548c\u5b83\u4eec\u7684\u5370\u8c61\u7279\u5f81\u88ab\u62c9\u8fd1\uff0c\u5e76\u4e14\u5b57\u4f53\u56fe\u50cf\u7279\u5f81\u548c\u4e0d\u76f8\u5173\u7684\u5370\u8c61\u7279\u5f81\u88ab\u63a8\u5f00\u3002\u8be5\u8fc7\u7a0b\u5b9e\u73b0\u4e86\u5b57\u4f53\u56fe\u50cf\u4e0e\u5176\u5370\u8c61\u4e4b\u95f4\u7684\u5171\u540c\u5d4c\u5165\u3002\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5171\u540c\u5d4c\u5165\u5728\u5b57\u4f53\u548c\u5370\u8c61\u4e4b\u95f4\u6267\u884c\u8de8\u6a21\u5f0f\u68c0\u7d22\u3002\u7ed3\u679c\u8868\u660e\uff0cImpression-CLIP \u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u68c0\u7d22\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u663e\u793a\u4e86\u5bf9\u566a\u58f0\u548c\u7f3a\u5931\u6807\u7b7e\u7684\u9c81\u68d2\u6027\u3002|[2402.16350v1](http://arxiv.org/pdf/2402.16350v1)|null|\n"}}