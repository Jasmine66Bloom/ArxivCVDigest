{"\u751f\u6210\u6a21\u578b": {"2404.04243": "|**2024-04-05**|**Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models**|\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u7684\u8eab\u4efd\u89e3\u8026|Sangwon Jang, Jaehyeong Jo, Kimin Lee, Sung Ju Hwang|Text-to-image diffusion models have shown remarkable success in generating a personalized subject based on a few reference images. However, current methods struggle with handling multiple subjects simultaneously, often resulting in mixed identities with combined attributes from different subjects. In this work, we present MuDI, a novel framework that enables multi-subject personalization by effectively decoupling identities from multiple subjects. Our main idea is to utilize segmented subjects generated by the Segment Anything Model for both training and inference, as a form of data augmentation for training and initialization for the generation process. Our experiments demonstrate that MuDI can produce high-quality personalized images without identity mixing, even for highly similar subjects as shown in Figure 1. In human evaluation, MuDI shows twice as many successes for personalizing multiple subjects without identity mixing over existing baselines and is preferred over 70% compared to the strongest baseline. More results are available at https://mudi-t2i.github.io/.||[2404.04243v1](http://arxiv.org/pdf/2404.04243v1)|null|\n", "2404.04202": "|**2024-04-05**|**Deep-learning Segmentation of Small Volumes in CT images for Radiotherapy Treatment Planning**|\u7528\u4e8e\u653e\u5c04\u6cbb\u7597\u8ba1\u5212\u7684 CT \u56fe\u50cf\u4e2d\u5c0f\u4f53\u79ef\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u5272|Jianxin Zhou, Kadishe Fejza, Massimiliano Salvatori, Daniele Della Latta, Gregory M. Hermann, Angela Di Fulvio|Our understanding of organs at risk is progressing to include physical small tissues such as coronary arteries and the radiosensitivities of many small organs and tissues are high. Therefore, the accurate segmentation of small volumes in external radiotherapy is crucial to protect them from over-irradiation. Moreover, with the development of the particle therapy and on-board imaging, the treatment becomes more accurate and precise. The purpose of this work is to optimize organ segmentation algorithms for small organs. We used 50 three-dimensional (3-D) computed tomography (CT) head and neck images from StructSeg2019 challenge to develop a general-purpose V-Net model to segment 20 organs in the head and neck region. We applied specific strategies to improve the segmentation accuracy of the small volumes in this anatomical region, i.e., the lens of the eye. Then, we used 17 additional head images from OSF healthcare to validate the robustness of the V Net model optimized for small-volume segmentation. With the study of the StructSeg2019 images, we found that the optimization of the image normalization range and classification threshold yielded a segmentation improvement of the lens of the eye of approximately 50%, compared to the use of the V-Net not optimized for small volumes. We used the optimized model to segment 17 images acquired using heterogeneous protocols. We obtained comparable Dice coefficient values for the clinical and StructSeg2019 images (0.61 plus/minus 0.07 and 0.58 plus/minus 0.10 for the left and right lens of the eye, respectively)||[2404.04202v1](http://arxiv.org/pdf/2404.04202v1)|null|\n", "2404.04095": "|**2024-04-05**|**Dynamic Prompt Optimizing for Text-to-Image Generation**|\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u52a8\u6001\u63d0\u793a\u4f18\u5316|Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, Qing Yang|Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the \\textbf{P}rompt \\textbf{A}uto-\\textbf{E}diting (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The reward function during training encourages the model to consider aesthetic score, semantic consistency, and user preferences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at https://github.com/Mowenyii/PAE.||[2404.04095v1](http://arxiv.org/pdf/2404.04095v1)|null|\n", "2404.04057": "|**2024-04-05**|**Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation**|\u5206\u6570\u6052\u7b49\u84b8\u998f\uff1a\u7528\u4e8e\u4e00\u6b65\u751f\u6210\u7684\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u6307\u6570\u5feb\u901f\u84b8\u998f|Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, Hai Huang|We introduce Score identity Distillation (SiD), an innovative data-free method that distills the generative capabilities of pretrained diffusion models into a single-step generator. SiD not only facilitates an exponentially fast reduction in Fr\\'echet inception distance (FID) during distillation but also approaches or even exceeds the FID performance of the original teacher diffusion models. By reformulating forward diffusion processes as semi-implicit distributions, we leverage three score-related identities to create an innovative loss mechanism. This mechanism achieves rapid FID reduction by training the generator using its own synthesized images, eliminating the need for real data or reverse-diffusion-based generation, all accomplished within significantly shortened generation time. Upon evaluation across four benchmark datasets, the SiD algorithm demonstrates high iteration efficiency during distillation and surpasses competing distillation approaches, whether they are one-step or few-step, data-free, or dependent on training data, in terms of generation quality. This achievement not only redefines the benchmarks for efficiency and effectiveness in diffusion distillation but also in the broader field of diffusion-based generation. Our PyTorch implementation will be publicly accessible on GitHub.||[2404.04057v1](http://arxiv.org/pdf/2404.04057v1)|null|\n", "2404.04037": "|**2024-04-05**|**InstructHumans: Editing Animated 3D Human Textures with Instructions**|InstructHumans\uff1a\u4f7f\u7528\u8bf4\u660e\u7f16\u8f91\u52a8\u753b 3D \u4eba\u4f53\u7eb9\u7406|Jiayin Zhu, Linlin Yang, Angela Yao|We present InstructHumans, a novel framework for instruction-driven 3D human texture editing. Existing text-based editing methods use Score Distillation Sampling (SDS) to distill guidance from generative models. This work shows that naively using such scores is harmful to editing as they destroy consistency with the source avatar. Instead, we propose an alternate SDS for Editing (SDS-E) that selectively incorporates subterms of SDS across diffusion timesteps. We further enhance SDS-E with spatial smoothness regularization and gradient-based viewpoint sampling to achieve high-quality edits with sharp and high-fidelity detailing. InstructHumans significantly outperforms existing 3D editing methods, consistent with the initial avatar while faithful to the textual instructions. Project page: https://jyzhu.top/instruct-humans .||[2404.04037v1](http://arxiv.org/pdf/2404.04037v1)|null|\n", "2404.03998": "|**2024-04-05**|**Physics-Inspired Synthesized Underwater Image Dataset**|\u53d7\u7269\u7406\u542f\u53d1\u7684\u5408\u6210\u6c34\u4e0b\u56fe\u50cf\u6570\u636e\u96c6|Reina Kaneko, Hiroshi Higashi, Yuichi Tanaka|This paper introduces the physics-inspired synthesized underwater image dataset (PHISWID), a dataset tailored for enhancing underwater image processing through physics-inspired image synthesis. Deep learning approaches to underwater image enhancement typically demand extensive datasets, yet acquiring paired clean and degraded underwater ones poses significant challenges. While several underwater image datasets have been proposed using physics-based synthesis, a publicly accessible collection has been lacking. Additionally, most underwater image synthesis approaches do not intend to reproduce atmospheric scenes, resulting in incomplete enhancement. PHISWID addresses this gap by offering a set of paired ground-truth (atmospheric) and synthetically degraded underwater images, showcasing not only color degradation but also the often-neglected effects of marine snow, a composite of organic matter and sand particles that considerably impairs underwater image clarity. The dataset applies these degradations to atmospheric RGB-D images, enhancing the dataset's realism and applicability. PHISWID is particularly valuable for training deep neural networks in a supervised learning setting and for objectively assessing image quality in benchmark analyses. Our results reveal that even a basic U-Net architecture, when trained with PHISWID, substantially outperforms existing methods in underwater image enhancement. We intend to release PHISWID publicly, contributing a significant resource to the advancement of underwater imaging technology.||[2404.03998v1](http://arxiv.org/pdf/2404.03998v1)|null|\n", "2404.03913": "|**2024-04-05**|**Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models**|Concept Weaver\uff1a\u5728\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u5b9e\u73b0\u591a\u6982\u5ff5\u878d\u5408|Gihyun Kwon, Simon Jenni, Dingzeyu Li, Joon-Young Lee, Jong Chul Ye, Fabian Caba Heilbron|While there has been significant progress in customizing text-to-image generation models, generating images that combine multiple personalized concepts remains challenging. In this work, we introduce Concept Weaver, a method for composing customized text-to-image diffusion models at inference time. Specifically, the method breaks the process into two steps: creating a template image aligned with the semantics of input prompts, and then personalizing the template using a concept fusion strategy. The fusion strategy incorporates the appearance of the target concepts into the template image while retaining its structural details. The results indicate that our method can generate multiple custom concepts with higher identity fidelity compared to alternative approaches. Furthermore, the method is shown to seamlessly handle more than two concepts and closely follow the semantic meaning of the input prompt without blending appearances across different subjects.||[2404.03913v1](http://arxiv.org/pdf/2404.03913v1)|null|\n"}, "\u591a\u6a21\u6001": {"2404.04256": "|**2024-04-05**|**Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation**|Sigma\uff1a\u7528\u4e8e\u591a\u6a21\u6001\u8bed\u4e49\u5206\u5272\u7684\u66b9\u7f57\u66fc\u5df4\u7f51\u7edc|Zifu Wan, Yuhao Wang, Silong Yong, Pingping Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie|Multi-modal semantic segmentation significantly enhances AI agents' perception and scene understanding, especially under adverse conditions like low-light or overexposed environments. Leveraging additional modalities (X-modality) like thermal and depth alongside traditional RGB provides complementary information, enabling more robust and reliable segmentation. In this work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic segmentation, utilizing the Selective Structured State Space Model, Mamba. Unlike conventional methods that rely on CNNs, with their limited local receptive fields, or Vision Transformers (ViTs), which offer global receptive fields at the cost of quadratic complexity, our model achieves global receptive fields coverage with linear complexity. By employing a Siamese encoder and innovating a Mamba fusion mechanism, we effectively select essential information from different modalities. A decoder is then developed to enhance the channel-wise modeling ability of the model. Our method, Sigma, is rigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks, demonstrating its superiority and marking the first successful application of State Space Models (SSMs) in multi-modal perception tasks. Code is available at https://github.com/zifuwan/Sigma.||[2404.04256v1](http://arxiv.org/pdf/2404.04256v1)|null|\n", "2404.04026": "|**2024-04-05**|**MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes**|MM-Gaussian\uff1a\u57fa\u4e8e 3D \u9ad8\u65af\u7684\u591a\u6a21\u6001\u878d\u5408\uff0c\u7528\u4e8e\u65e0\u754c\u573a\u666f\u4e2d\u7684\u5b9a\u4f4d\u548c\u91cd\u5efa|Chenyang Wu, Yifan Duan, Xinran Zhang, Yu Sheng, Jianmin Ji, Yanyong Zhang|Localization and mapping are critical tasks for various applications such as autonomous vehicles and robotics. The challenges posed by outdoor environments present particular complexities due to their unbounded characteristics. In this work, we present MM-Gaussian, a LiDAR-camera multi-modal fusion system for localization and mapping in unbounded scenes. Our approach is inspired by the recently developed 3D Gaussians, which demonstrate remarkable capabilities in achieving high rendering quality and fast rendering speed. Specifically, our system fully utilizes the geometric structure information provided by solid-state LiDAR to address the problem of inaccurate depth encountered when relying solely on visual solutions in unbounded, outdoor scenarios. Additionally, we utilize 3D Gaussian point clouds, with the assistance of pixel-level gradient descent, to fully exploit the color information in photos, thereby achieving realistic rendering effects. To further bolster the robustness of our system, we designed a relocalization module, which assists in returning to the correct trajectory in the event of a localization failure. Experiments conducted in multiple scenarios demonstrate the effectiveness of our method.||[2404.04026v1](http://arxiv.org/pdf/2404.04026v1)|null|\n", "2404.03892": "|**2024-04-05**|**Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**|\u589e\u5f3a\u4e73\u623f X \u5149\u68c0\u67e5\u4e2d\u7684\u4e73\u817a\u764c\u8bca\u65ad\uff1a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u7684\u8bc4\u4f30\u548c\u96c6\u6210|Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir|The study introduces an integrated framework combining Convolutional Neural Networks (CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced diagnosis of breast cancer using the CBIS-DDSM dataset. Utilizing a fine-tuned ResNet50 architecture, our investigation not only provides effective differentiation of mammographic images into benign and malignant categories but also addresses the opaque \"black-box\" nature of deep learning models by employing XAI methodologies, namely Grad-CAM, LIME, and SHAP, to interpret CNN decision-making processes for healthcare professionals. Our methodology encompasses an elaborate data preprocessing pipeline and advanced data augmentation techniques to counteract dataset limitations, and transfer learning using pre-trained networks, such as VGG-16, DenseNet and ResNet was employed. A focal point of our study is the evaluation of XAI's effectiveness in interpreting model predictions, highlighted by utilising the Hausdorff measure to assess the alignment between AI-generated explanations and expert annotations quantitatively. This approach plays a critical role for XAI in promoting trustworthiness and ethical fairness in AI-assisted diagnostics. The findings from our research illustrate the effective collaboration between CNNs and XAI in advancing diagnostic methods for breast cancer, thereby facilitating a more seamless integration of advanced AI technologies within clinical settings. By enhancing the interpretability of AI-driven decisions, this work lays the groundwork for improved collaboration between AI systems and medical practitioners, ultimately enriching patient care. Furthermore, the implications of our research extend well beyond the current methodologies, advocating for subsequent inquiries into the integration of multimodal data and the refinement of AI explanations to satisfy the needs of clinical practice.||[2404.03892v1](http://arxiv.org/pdf/2404.03892v1)|null|\n", "2404.03854": "|**2024-04-05**|**Mitigating Heterogeneity in Federated Multimodal Learning with Biomedical Vision-Language Pre-training**|\u901a\u8fc7\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u51cf\u8f7b\u8054\u5408\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u5f02\u8d28\u6027|Zitao Shuai, Liyue Shen|Vision-language pre-training (VLP) has arised as an efficient scheme for multimodal representation learning, but it requires large-scale multimodal data for pre-training, making it an obstacle especially for biomedical applications. To overcome the data limitation, federated learning (FL) can be a promising strategy to scale up the dataset for biomedical VLP while protecting data privacy. However, client data are often heterogeneous in real-world scenarios, and we observe that local training on heterogeneous client data would distort the multimodal representation learning and lead to biased cross-modal alignment. To address this challenge, we propose Federated distributional Robust Guidance-Based (FedRGB) learning framework for federated VLP with robustness to data heterogeneity. Specifically, we utilize a guidance-based local training scheme to reduce feature distortions, and employ a distribution-based min-max optimization to learn unbiased cross-modal alignment. The experiments on real-world datasets show our method successfully promotes efficient federated multimodal learning for biomedical VLP with data heterogeneity.||[2404.03854v1](http://arxiv.org/pdf/2404.03854v1)|null|\n"}, "Nerf": {"2404.04211": "|**2024-04-05**|**Robust Gaussian Splatting**|\u9c81\u68d2\u9ad8\u65af\u6cfc\u6e85|Fran\u00e7ois Darmon, Lorenzo Porzi, Samuel Rota-Bul\u00f2, Peter Kontschieder|In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant benchmark datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines.||[2404.04211v1](http://arxiv.org/pdf/2404.04211v1)|null|\n"}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2404.04245": "|**2024-04-05**|**Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism**|\u8bc4\u4f30\u5bf9\u6297\u9c81\u68d2\u6027\uff1aFGSM\u3001Carlini-Wagner \u653b\u51fb\u7684\u6bd4\u8f83\u4ee5\u53ca\u84b8\u998f\u4f5c\u4e3a\u9632\u5fa1\u673a\u5236\u7684\u4f5c\u7528|Trilokesh Ranjan Sarkar, Nilanjan Das, Pralay Sankar Maitra, Bijoy Some, Ritwik Saha, Orijita Adhikary, Bishal Bose, Jaydip Sen|This technical report delves into an in-depth exploration of adversarial attacks specifically targeted at Deep Neural Networks (DNNs) utilized for image classification. The study also investigates defense mechanisms aimed at bolstering the robustness of machine learning models. The research focuses on comprehending the ramifications of two prominent attack methodologies: the Fast Gradient Sign Method (FGSM) and the Carlini-Wagner (CW) approach. These attacks are examined concerning three pre-trained image classifiers: Resnext50_32x4d, DenseNet-201, and VGG-19, utilizing the Tiny-ImageNet dataset. Furthermore, the study proposes the robustness of defensive distillation as a defense mechanism to counter FGSM and CW attacks. This defense mechanism is evaluated using the CIFAR-10 dataset, where CNN models, specifically resnet101 and Resnext50_32x4d, serve as the teacher and student models, respectively. The proposed defensive distillation model exhibits effectiveness in thwarting attacks such as FGSM. However, it is noted to remain susceptible to more sophisticated techniques like the CW attack. The document presents a meticulous validation of the proposed scheme. It provides detailed and comprehensive results, elucidating the efficacy and limitations of the defense mechanisms employed. Through rigorous experimentation and analysis, the study offers insights into the dynamics of adversarial attacks on DNNs, as well as the effectiveness of defensive strategies in mitigating their impact.||[2404.04245v1](http://arxiv.org/pdf/2404.04245v1)|null|\n", "2404.03936": "|**2024-04-05**|**Deep Learning for Satellite Image Time Series Analysis: A Review**|\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\uff1a\u7efc\u8ff0|Lynn Miller, Charlotte Pelletier, Geoffrey I. Webb|Earth observation (EO) satellite missions have been providing detailed images about the state of the Earth and its land cover for over 50 years. Long term missions, such as NASA's Landsat, Terra, and Aqua satellites, and more recently, the ESA's Sentinel missions, record images of the entire world every few days. Although single images provide point-in-time data, repeated images of the same area, or satellite image time series (SITS) provide information about the changing state of vegetation and land use. These SITS are useful for modeling dynamic processes and seasonal changes such as plant phenology. They have potential benefits for many aspects of land and natural resource management, including applications in agricultural, forest, water, and disaster management, urban planning, and mining. However, the resulting satellite image time series (SITS) are complex, incorporating information from the temporal, spatial, and spectral dimensions. Therefore, deep learning methods are often deployed as they can analyze these complex relationships. This review presents a summary of the state-of-the-art methods of modelling environmental, agricultural, and other Earth observation variables from SITS data using deep learning methods. We aim to provide a resource for remote sensing experts interested in using deep learning techniques to enhance Earth observation models with temporal information.||[2404.03936v1](http://arxiv.org/pdf/2404.03936v1)|null|\n", "2404.03898": "|**2024-04-05**|**VoltaVision: A Transfer Learning model for electronic component classification**|VoltaVision\uff1a\u7535\u5b50\u5143\u4ef6\u5206\u7c7b\u7684\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b|Anas Mohammad Ishfaqul Muktadir Osmani, Taimur Rahman, Salekul Islam|In this paper, we analyze the effectiveness of transfer learning on classifying electronic components. Transfer learning reuses pre-trained models to save time and resources in building a robust classifier rather than learning from scratch. Our work introduces a lightweight CNN, coined as VoltaVision, and compares its performance against more complex models. We test the hypothesis that transferring knowledge from a similar task to our target domain yields better results than state-of-the-art models trained on general datasets. Our dataset and code for this work are available at https://github.com/AnasIshfaque/VoltaVision.||[2404.03898v1](http://arxiv.org/pdf/2404.03898v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2404.04254": "|**2024-04-05**|**Watermark-based Detection and Attribution of AI-Generated Content**|\u57fa\u4e8e\u6c34\u5370\u7684 AI \u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u548c\u5f52\u56e0|Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Neil Zhenqiang Gong|Several companies--such as Google, Microsoft, and OpenAI--have deployed techniques to watermark AI-generated content to enable proactive detection. However, existing literature mainly focuses on user-agnostic detection. Attribution aims to further trace back the user of a generative-AI service who generated a given content detected as AI-generated. Despite its growing importance, attribution is largely unexplored. In this work, we aim to bridge this gap by providing the first systematic study on watermark-based, user-aware detection and attribution of AI-generated content. Specifically, we theoretically study the detection and attribution performance via rigorous probabilistic analysis. Moreover, we develop an efficient algorithm to select watermarks for the users to enhance attribution performance. Both our theoretical and empirical results show that watermark-based detection and attribution inherit the accuracy and (non-)robustness properties of the watermarking method.||[2404.04254v1](http://arxiv.org/pdf/2404.04254v1)|null|\n", "2404.04231": "|**2024-04-05**|**Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation**|\u7528\u4e8e\u6587\u672c\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u56fe\u50cf-\u6587\u672c\u8054\u5408\u5206\u89e3|Ji-Jia Wu, Andy Chia-Hao Chang, Chieh-Yu Chuang, Chun-Pei Chen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Yung-Yu Chuang, Yen-Yu Lin|This paper addresses text-supervised semantic segmentation, aiming to learn a model capable of segmenting arbitrary visual concepts within images by using only image-text pairs without dense annotations. Existing methods have demonstrated that contrastive learning on image-text pairs effectively aligns visual segments with the meanings of texts. We notice that there is a discrepancy between text alignment and semantic segmentation: A text often consists of multiple semantic concepts, whereas semantic segmentation strives to create semantically homogeneous segments. To address this issue, we propose a novel framework, Image-Text Co-Decomposition (CoDe), where the paired image and text are jointly decomposed into a set of image regions and a set of word segments, respectively, and contrastive learning is developed to enforce region-word alignment. To work with a vision-language model, we present a prompt learning mechanism that derives an extra representation to highlight an image segment or a word segment of interest, with which more effective features can be extracted from that segment. Comprehensive experimental results demonstrate that our method performs favorably against existing text-supervised semantic segmentation methods on six benchmark datasets.||[2404.04231v1](http://arxiv.org/pdf/2404.04231v1)|null|\n", "2404.04179": "|**2024-04-05**|**SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in Transmission and Distribution Towers**|SCAResNet\uff1a\u9488\u5bf9\u8f93\u7535\u548c\u914d\u7535\u5854\u4e2d\u5fae\u5c0f\u7269\u4f53\u68c0\u6d4b\u800c\u4f18\u5316\u7684 ResNet \u53d8\u4f53|Weile Li, Muqing Shi, Zhonghua Hong|Traditional deep learning-based object detection networks often resize images during the data preprocessing stage to achieve a uniform size and scale in the feature map. Resizing is done to facilitate model propagation and fully connected classification. However, resizing inevitably leads to object deformation and loss of valuable information in the images. This drawback becomes particularly pronounced for tiny objects like distribution towers with linear shapes and few pixels. To address this issue, we propose abandoning the resizing operation. Instead, we introduce Positional-Encoding Multi-head Criss-Cross Attention. This allows the model to capture contextual information and learn from multiple representation subspaces, effectively enriching the semantics of distribution towers. Additionally, we enhance Spatial Pyramid Pooling by reshaping three pooled feature maps into a new unified one while also reducing the computational burden. This approach allows images of different sizes and scales to generate feature maps with uniform dimensions and can be employed in feature map propagation. Our SCAResNet incorporates these aforementioned improvements into the backbone network ResNet. We evaluated our SCAResNet using the Electric Transmission and Distribution Infrastructure Imagery dataset from Duke University. Without any additional tricks, we employed various object detection models with Gaussian Receptive Field based Label Assignment as the baseline. When incorporating the SCAResNet into the baseline model, we achieved a 2.1% improvement in mAPs. This demonstrates the advantages of our SCAResNet in detecting transmission and distribution towers and its value in tiny object detection. The source code is available at https://github.com/LisavilaLee/SCAResNet_mmdet.||[2404.04179v1](http://arxiv.org/pdf/2404.04179v1)|null|\n", "2404.04159": "|**2024-04-05**|**Noisy Label Processing for Classification: A Survey**|\u7528\u4e8e\u5206\u7c7b\u7684\u566a\u58f0\u6807\u7b7e\u5904\u7406\uff1a\u8c03\u67e5|Mengting Li, Chuang Zhu|In recent years, deep neural networks (DNNs) have gained remarkable achievement in computer vision tasks, and the success of DNNs often depends greatly on the richness of data. However, the acquisition process of data and high-quality ground truth requires a lot of manpower and money. In the long, tedious process of data annotation, annotators are prone to make mistakes, resulting in incorrect labels of images, i.e., noisy labels. The emergence of noisy labels is inevitable. Moreover, since research shows that DNNs can easily fit noisy labels, the existence of noisy labels will cause significant damage to the model training process. Therefore, it is crucial to combat noisy labels for computer vision tasks, especially for classification tasks. In this survey, we first comprehensively review the evolution of different deep learning approaches for noisy label combating in the image classification task. In addition, we also review different noise patterns that have been proposed to design robust algorithms. Furthermore, we explore the inner pattern of real-world label noise and propose an algorithm to generate a synthetic label noise pattern guided by real-world data. We test the algorithm on the well-known real-world dataset CIFAR-10N to form a new real-world data-guided synthetic benchmark and evaluate some typical noise-robust methods on the benchmark.||[2404.04159v1](http://arxiv.org/pdf/2404.04159v1)|null|\n", "2404.04155": "|**2024-04-05**|**MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector**|MarsSeg\uff1a\u5177\u6709\u591a\u7ea7\u63d0\u53d6\u5668\u548c\u8fde\u63a5\u5668\u7684\u706b\u661f\u8868\u9762\u8bed\u4e49\u5206\u5272|Junbo Li, Keyan Chen, Gengju Tian, Lu Li, Zhenwei Shi|The segmentation and interpretation of the Martian surface play a pivotal role in Mars exploration, providing essential data for the trajectory planning and obstacle avoidance of rovers. However, the complex topography, similar surface features, and the lack of extensive annotated data pose significant challenges to the high-precision semantic segmentation of the Martian surface. To address these challenges, we propose a novel encoder-decoder based Mars segmentation network, termed MarsSeg. Specifically, we employ an encoder-decoder structure with a minimized number of down-sampling layers to preserve local details. To facilitate a high-level semantic understanding across the shadow multi-level feature maps, we introduce a feature enhancement connection layer situated between the encoder and decoder. This layer incorporates Mini Atrous Spatial Pyramid Pooling (Mini-ASPP), Polarized Self-Attention (PSA), and Strip Pyramid Pooling Module (SPPM). The Mini-ASPP and PSA are specifically designed for shadow feature enhancement, thereby enabling the expression of local details and small objects. Conversely, the SPPM is employed for deep feature enhancement, facilitating the extraction of high-level semantic category-related information. Experimental results derived from the Mars-Seg and AI4Mars datasets substantiate that the proposed MarsSeg outperforms other state-of-the-art methods in segmentation performance, validating the efficacy of each proposed component.||[2404.04155v1](http://arxiv.org/pdf/2404.04155v1)|null|\n", "2404.04140": "|**2024-04-05**|**Improving Detection in Aerial Images by Capturing Inter-Object Relationships**|\u901a\u8fc7\u6355\u83b7\u5bf9\u8c61\u95f4\u7684\u5173\u7cfb\u6765\u6539\u8fdb\u822a\u7a7a\u56fe\u50cf\u7684\u68c0\u6d4b|Botao Ren, Botian Xu, Yifan Pu, Jingyi Wang, Zhidong Deng|In many image domains, the spatial distribution of objects in a scene exhibits meaningful patterns governed by their semantic relationships. In most modern detection pipelines, however, the detection proposals are processed independently, overlooking the underlying relationships between objects. In this work, we introduce a transformer-based approach to capture these inter-object relationships to refine classification and regression outcomes for detected objects. Building on two-stage detectors, we tokenize the region of interest (RoI) proposals to be processed by a transformer encoder. Specific spatial and geometric relations are incorporated into the attention weights and adaptively modulated and regularized. Experimental results demonstrate that the proposed method achieves consistent performance improvement on three benchmarks including DOTA-v1.0, DOTA-v1.5, and HRSC 2016, especially ranking first on both DOTA-v1.5 and HRSC 2016. Specifically, our new method has an increase of 1.59 mAP on DOTA-v1.0, 4.88 mAP on DOTA-v1.5, and 2.1 mAP on HRSC 2016, respectively, compared to the baselines.||[2404.04140v1](http://arxiv.org/pdf/2404.04140v1)|null|\n", "2404.04072": "|**2024-04-05**|**Label Propagation for Zero-shot Classification with Vision-Language Models**|\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\u7684\u6807\u7b7e\u4f20\u64ad|Vladan Stojni\u0107, Yannis Kalantidis, Giorgos Tolias|Vision-Language Models (VLMs) have demonstrated impressive performance on zero-shot classification, i.e. classification when provided merely with a list of class names. In this paper, we tackle the case of zero-shot classification in the presence of unlabeled data. We leverage the graph structure of the unlabeled data and introduce ZLaP, a method based on label propagation (LP) that utilizes geodesic distances for classification. We tailor LP to graphs containing both text and image features and further propose an efficient method for performing inductive inference based on a dual solution and a sparsification step. We perform extensive experiments to evaluate the effectiveness of our method on 14 common datasets and show that ZLaP outperforms the latest related works. Code: https://github.com/vladan-stojnic/ZLaP||[2404.04072v1](http://arxiv.org/pdf/2404.04072v1)|null|\n", "2404.04050": "|**2024-04-05**|**No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation**|\u6ca1\u6709\u65f6\u95f4\u8bad\u7ec3\uff1a\u652f\u6301\u975e\u53c2\u6570\u7f51\u7edc\u8fdb\u884c\u5c11\u955c\u5934 3D \u573a\u666f\u5206\u5272|Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Han Xiao, Chaoyou Fu, Hao Dong, Peng Gao|To reduce the reliance on large-scale datasets, recent works in 3D segmentation resort to few-shot learning. Current 3D few-shot segmentation methods first pre-train models on 'seen' classes, and then evaluate their generalization performance on 'unseen' classes. However, the prior pre-training stage not only introduces excessive time overhead but also incurs a significant domain gap on 'unseen' classes. To tackle these issues, we propose a Non-parametric Network for few-shot 3D Segmentation, Seg-NN, and its Parametric variant, Seg-PN. Without training, Seg-NN extracts dense representations by hand-crafted filters and achieves comparable performance to existing parametric models. Due to the elimination of pre-training, Seg-NN can alleviate the domain gap issue and save a substantial amount of time. Based on Seg-NN, Seg-PN only requires training a lightweight QUEry-Support Transferring (QUEST) module, which enhances the interaction between the support set and query set. Experiments suggest that Seg-PN outperforms previous state-of-the-art method by +4.19% and +7.71% mIoU on S3DIS and ScanNet datasets respectively, while reducing training time by -90%, indicating its effectiveness and efficiency.||[2404.04050v1](http://arxiv.org/pdf/2404.04050v1)|null|\n", "2404.03991": "|**2024-04-05**|**Towards Efficient and Accurate CT Segmentation via Edge-Preserving Probabilistic Downsampling**|\u901a\u8fc7\u8fb9\u7f18\u4fdd\u7559\u6982\u7387\u4e0b\u91c7\u6837\u5b9e\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u7684 CT \u5206\u5272|Shahzad Ali, Yu Rim Lee, Soo Young Park, Won Young Tak, Soon Ki Jung|Downsampling images and labels, often necessitated by limited resources or to expedite network training, leads to the loss of small objects and thin boundaries. This undermines the segmentation network's capacity to interpret images accurately and predict detailed labels, resulting in diminished performance compared to processing at original resolutions. This situation exemplifies the trade-off between efficiency and accuracy, with higher downsampling factors further impairing segmentation outcomes. Preserving information during downsampling is especially critical for medical image segmentation tasks. To tackle this challenge, we introduce a novel method named Edge-preserving Probabilistic Downsampling (EPD). It utilizes class uncertainty within a local window to produce soft labels, with the window size dictating the downsampling factor. This enables a network to produce quality predictions at low resolutions. Beyond preserving edge details more effectively than conventional nearest-neighbor downsampling, employing a similar algorithm for images, it surpasses bilinear interpolation in image downsampling, enhancing overall performance. Our method significantly improved Intersection over Union (IoU) to 2.85%, 8.65%, and 11.89% when downsampling data to 1/2, 1/4, and 1/8, respectively, compared to conventional interpolation methods.||[2404.03991v1](http://arxiv.org/pdf/2404.03991v1)|null|\n", "2404.03924": "|**2024-04-05**|**Learning Correlation Structures for Vision Transformers**|\u5b66\u4e60\u89c6\u89c9\u53d8\u538b\u5668\u7684\u76f8\u5173\u7ed3\u6784|Manjin Kim, Paul Hongsuck Seo, Cordelia Schmid, Minsu Cho|We introduce a new attention mechanism, dubbed structural self-attention (StructSA), that leverages rich correlation patterns naturally emerging in key-query interactions of attention. StructSA generates attention maps by recognizing space-time structures of key-query correlations via convolution and uses them to dynamically aggregate local contexts of value features. This effectively leverages rich structural patterns in images and videos such as scene layouts, object motion, and inter-object relations. Using StructSA as a main building block, we develop the structural vision transformer (StructViT) and evaluate its effectiveness on both image and video classification tasks, achieving state-of-the-art results on ImageNet-1K, Kinetics-400, Something-Something V1 & V2, Diving-48, and FineGym.||[2404.03924v1](http://arxiv.org/pdf/2404.03924v1)|null|\n", "2404.03883": "|**2024-04-05**|**LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification**|LiDAR \u5f15\u5bfc\u7684\u4ea4\u53c9\u6ce8\u610f\u878d\u5408\u7528\u4e8e\u9ad8\u5149\u8c31\u6ce2\u6bb5\u9009\u62e9\u548c\u56fe\u50cf\u5206\u7c7b|Judy X Yang, Jun Zhou, Jing Wang, Hui Tian, Wee Chung Liew|The fusion of hyperspectral and LiDAR data has been an active research topic. Existing fusion methods have ignored the high-dimensionality and redundancy challenges in hyperspectral images, despite that band selection methods have been intensively studied for hyperspectral image (HSI) processing. This paper addresses this significant gap by introducing a cross-attention mechanism from the transformer architecture for the selection of HSI bands guided by LiDAR data. LiDAR provides high-resolution vertical structural information, which can be useful in distinguishing different types of land cover that may have similar spectral signatures but different structural profiles. In our approach, the LiDAR data are used as the \"query\" to search and identify the \"key\" from the HSI to choose the most pertinent bands for LiDAR. This method ensures that the selected HSI bands drastically reduce redundancy and computational requirements while working optimally with the LiDAR data. Extensive experiments have been undertaken on three paired HSI and LiDAR data sets: Houston 2013, Trento and MUUFL. The results highlight the superiority of the cross-attention mechanism, underlining the enhanced classification accuracy of the identified HSI bands when fused with the LiDAR features. The results also show that the use of fewer bands combined with LiDAR surpasses the performance of state-of-the-art fusion models.||[2404.03883v1](http://arxiv.org/pdf/2404.03883v1)|null|\n", "2404.03876": "|**2024-04-05**|**Increasing Fairness in Classification of Out of Distribution Data for Facial Recognition**|\u63d0\u9ad8\u9762\u90e8\u8bc6\u522b\u7684\u5206\u5e03\u5916\u6570\u636e\u5206\u7c7b\u7684\u516c\u5e73\u6027|Gianluca Barone, Aashrit Cunchala, Rudy Nunez|Standard classification theory assumes that the distribution of images in the test and training sets are identical. Unfortunately, real-life scenarios typically feature unseen data (\"out-of-distribution data\") which is different from data in the training distribution(\"in-distribution\"). This issue is most prevalent in social justice problems where data from under-represented groups may appear in the test data without representing an equal proportion of the training data. This may result in a model returning confidently wrong decisions and predictions. We are interested in the following question: Can the performance of a neural network improve on facial images of out-of-distribution data when it is trained simultaneously on multiple datasets of in-distribution data? We approach this problem by incorporating the Outlier Exposure model and investigate how the model's performance changes when other datasets of facial images were implemented. We observe that the accuracy and other metrics of the model can be increased by applying Outlier Exposure, incorporating a trainable weight parameter to increase the machine's emphasis on outlier images, and by re-weighting the importance of different class labels. We also experimented with whether sorting the images and determining outliers via image features would have more of an effect on the metrics than sorting by average pixel value. Our goal was to make models not only more accurate but also more fair by scanning a more expanded range of images. We also tested the datasets in reverse order to see whether a more fair dataset with balanced features has an effect on the model's accuracy.||[2404.03876v1](http://arxiv.org/pdf/2404.03876v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2404.04104": "|**2024-04-05**|**3D Facial Expressions through Analysis-by-Neural-Synthesis**|\u901a\u8fc7\u795e\u7ecf\u7efc\u5408\u5206\u6790\u7684 3D \u9762\u90e8\u8868\u60c5|George Retsinas, Panagiotis P. Filntisis, Radek Danecek, Victoria F. Abrevaya, Anastasios Roussos, Timo Bolkart, Petros Maragos|While existing methods for 3D face reconstruction from in-the-wild images excel at recovering the overall face shape, they commonly miss subtle, extreme, asymmetric, or rarely observed expressions. We improve upon these methods with SMIRK (Spatial Modeling for Image-based Reconstruction of Kinesics), which faithfully reconstructs expressive 3D faces from images. We identify two key limitations in existing methods: shortcomings in their self-supervised training formulation, and a lack of expression diversity in the training images. For training, most methods employ differentiable rendering to compare a predicted face mesh with the input image, along with a plethora of additional loss functions. This differentiable rendering loss not only has to provide supervision to optimize for 3D face geometry, camera, albedo, and lighting, which is an ill-posed optimization problem, but the domain gap between rendering and input image further hinders the learning process. Instead, SMIRK replaces the differentiable rendering with a neural rendering module that, given the rendered predicted mesh geometry, and sparsely sampled pixels of the input image, generates a face image. As the neural rendering gets color information from sampled image pixels, supervising with neural rendering-based reconstruction loss can focus solely on the geometry. Further, it enables us to generate images of the input identity with varying expressions while training. These are then utilized as input to the reconstruction model and used as supervision with ground truth geometry. This effectively augments the training data and enhances the generalization for diverse expressions. Our qualitative, quantitative and particularly our perceptual evaluations demonstrate that SMIRK achieves the new state-of-the art performance on accurate expression reconstruction. Project webpage: https://georgeretsi.github.io/smirk/.||[2404.04104v1](http://arxiv.org/pdf/2404.04104v1)|null|\n", "2404.03925": "|**2024-04-05**|**LightOctree: Lightweight 3D Spatially-Coherent Indoor Lighting Estimation**|LightOctree\uff1a\u8f7b\u91cf\u7ea7 3D \u7a7a\u95f4\u76f8\u5e72\u5ba4\u5185\u7167\u660e\u4f30\u8ba1|Xuecan Wang, Shibang Xiao, Xiaohui Liang|We present a lightweight solution for estimating spatially-coherent indoor lighting from a single RGB image. Previous methods for estimating illumination using volumetric representations have overlooked the sparse distribution of light sources in space, necessitating substantial memory and computational resources for achieving high-quality results. We introduce a unified, voxel octree-based illumination estimation framework to produce 3D spatially-coherent lighting. Additionally, a differentiable voxel octree cone tracing rendering layer is proposed to eliminate regular volumetric representation throughout the entire process and ensure the retention of features across different frequency domains. This reduction significantly decreases spatial usage and required floating-point operations without substantially compromising precision. Experimental results demonstrate that our approach achieves high-quality coherent estimation with minimal cost compared to previous methods.||[2404.03925v1](http://arxiv.org/pdf/2404.03925v1)|null|\n", "2404.03906": "|**2024-04-05**|**Deep Phase Coded Image Prior**|\u6df1\u76f8\u4f4d\u7f16\u7801\u56fe\u50cf\u5148\u9a8c|Nimrod Shabtay, Eli Schwartz, Raja Giryes|Phase-coded imaging is a computational imaging method designed to tackle tasks such as passive depth estimation and extended depth of field (EDOF) using depth cues inserted during image capture. Most of the current deep learning-based methods for depth estimation or all-in-focus imaging require a training dataset with high-quality depth maps and an optimal focus point at infinity for all-in-focus images. Such datasets are difficult to create, usually synthetic, and require external graphic programs. We propose a new method named \"Deep Phase Coded Image Prior\" (DPCIP) for jointly recovering the depth map and all-in-focus image from a coded-phase image using solely the captured image and the optical information of the imaging system. Our approach does not depend on any specific dataset and surpasses prior supervised techniques utilizing the same imaging system. This improvement is achieved through the utilization of a problem formulation based on implicit neural representation (INR) and deep image prior (DIP). Due to our zero-shot method, we overcome the barrier of acquiring accurate ground-truth data of depth maps and all-in-focus images for each new phase-coded system introduced. This allows focusing mainly on developing the imaging system, and not on ground-truth data collection.||[2404.03906v1](http://arxiv.org/pdf/2404.03906v1)|null|\n"}, "LLM": {"2404.04251": "|**2024-04-05**|**Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)**|\u8c01\u6765\u8bc4\u4f30\u8bc4\u4ef7\uff1f\u4f7f\u7528 T2IScoreScore (TS2) \u5ba2\u89c2\u5730\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u63d0\u793a\u4e00\u81f4\u6027\u6307\u6807\u8fdb\u884c\u8bc4\u5206|Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, William Yang Wang|With advances in the quality of text-to-image (T2I) models has come interest in benchmarking their prompt faithfulness-the semantic coherence of generated images to the prompts they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and vision-language models (VLMs). However, these metrics are not rigorously compared and benchmarked, instead presented against few weak baselines by correlation to human Likert scores over a set of easy-to-discriminate images.   We introduce T2IScoreScore (TS2), a curated set of semantic error graphs containing a prompt and a set increasingly erroneous images. These allow us to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. TS2 will enable the development of better T2I prompt faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.||[2404.04251v1](http://arxiv.org/pdf/2404.04251v1)|null|\n"}, "Transformer": {"2404.04244": "|**2024-04-05**|**DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration**|DiffOp-net\uff1a\u57fa\u4e8e\u5dee\u5206\u7b97\u5b50\u7684\u5168\u5377\u79ef\u7f51\u7edc\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u53ef\u53d8\u5f62\u56fe\u50cf\u914d\u51c6|Jiong Wu|Existing unsupervised deformable image registration methods usually rely on metrics applied to the gradients of predicted displacement or velocity fields as a regularization term to ensure transformation smoothness, which potentially limits registration accuracy. In this study, we propose a novel approach to enhance unsupervised deformable image registration by introducing a new differential operator into the registration framework. This operator, acting on the velocity field and mapping it to a dual space, ensures the smoothness of the velocity field during optimization, facilitating accurate deformable registration. In addition, to tackle the challenge of capturing large deformations inside image pairs, we introduce a Cross-Coordinate Attention module (CCA) and embed it into a proposed Fully Convolutional Networks (FCNs)-based multi-resolution registration architecture. Evaluation experiments are conducted on two magnetic resonance imaging (MRI) datasets. Compared to various state-of-the-art registration approaches, including a traditional algorithm and three representative unsupervised learning-based methods, our method achieves superior accuracies, maintaining desirable diffeomorphic properties, and exhibiting promising registration speed.||[2404.04244v1](http://arxiv.org/pdf/2404.04244v1)|null|\n"}, "3D/CG": {"2404.04242": "|**2024-04-05**|**Physical Property Understanding from Language-Embedded Feature Fields**|\u4ece\u8bed\u8a00\u5d4c\u5165\u7684\u7279\u5f81\u5b57\u6bb5\u7406\u89e3\u7269\u7406\u5c5e\u6027|Albert J. Zhai, Yuan Shen, Emily Y. Chen, Gloria X. Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, Shenlong Wang|Can computers perceive the physical properties of objects solely through vision? Research in cognitive science and vision science has shown that humans excel at identifying materials and estimating their physical properties based purely on visual appearance. In this paper, we present a novel approach for dense prediction of the physical properties of objects using a collection of images. Inspired by how humans reason about physics through vision, we leverage large language models to propose candidate materials for each object. We then construct a language-embedded point cloud and estimate the physical properties of each 3D point using a zero-shot kernel regression approach. Our method is accurate, annotation-free, and applicable to any object in the open world. Experiments demonstrate the effectiveness of the proposed approach in various physical property reasoning tasks, such as estimating the mass of common objects, as well as other properties like friction and hardness.||[2404.04242v1](http://arxiv.org/pdf/2404.04242v1)|null|\n", "2404.03962": "|**2024-04-05**|**RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications**|RaSim\uff1a\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u7684\u8303\u56f4\u611f\u77e5\u9ad8\u4fdd\u771f RGB-D \u6570\u636e\u6a21\u62df\u7ba1\u9053|Xingyu Liu, Chenyangguang Zhang, Gu Wang, Ruida Zhang, Xiangyang Ji|In robotic vision, a de-facto paradigm is to learn in simulated environments and then transfer to real-world applications, which poses an essential challenge in bridging the sim-to-real domain gap. While mainstream works tackle this problem in the RGB domain, we focus on depth data synthesis and develop a range-aware RGB-D data simulation pipeline (RaSim). In particular, high-fidelity depth data is generated by imitating the imaging principle of real-world sensors. A range-aware rendering strategy is further introduced to enrich data diversity. Extensive experiments show that models trained with RaSim can be directly applied to real-world scenarios without any finetuning and excel at downstream RGB-D perception tasks.||[2404.03962v1](http://arxiv.org/pdf/2404.03962v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {}, "\u5176\u4ed6": {"2404.04040": "|**2024-04-05**|**Dynamic Risk Assessment Methodology with an LDM-based System for Parking Scenarios**|\u57fa\u4e8e LDM \u7684\u505c\u8f66\u573a\u666f\u7cfb\u7edf\u7684\u52a8\u6001\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5|Paola Natalia Ca\u00f1as, Mikel Garc\u00eda, Nerea Aranjuelo, Marcos Nieto, Aitor Iglesias, Igor Rodr\u00edguez|This paper describes the methodology for building a dynamic risk assessment for ADAS (Advanced Driving Assistance Systems) algorithms in parking scenarios, fusing exterior and interior perception for a better understanding of the scene and a more comprehensive risk estimation. This includes the definition of a dynamic risk methodology that depends on the situation from inside and outside the vehicle, the creation of a multi-sensor dataset of risk assessment for ADAS benchmarking purposes, and a Local Dynamic Map (LDM) that fuses data from the exterior and interior of the car to build an LDM-based Dynamic Risk Assessment System (DRAS).||[2404.04040v1](http://arxiv.org/pdf/2404.04040v1)|null|\n", "2404.04025": "|**2024-04-05**|**Framework to generate perfusion map from CT and CTA images in patients with acute ischemic stroke: A longitudinal and cross-sectional study**|\u4ece\u6025\u6027\u7f3a\u8840\u6027\u4e2d\u98ce\u60a3\u8005\u7684 CT \u548c CTA \u56fe\u50cf\u751f\u6210\u704c\u6ce8\u56fe\u7684\u6846\u67b6\uff1a\u7eb5\u5411\u548c\u6a2a\u65ad\u9762\u7814\u7a76|Chayanin Tangwiriyasakul, Pedro Borges, Stefano Moriconi, Paul Wright, Yee-Haur Mah, James Teo, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso|Stroke is a leading cause of disability and death. Effective treatment decisions require early and informative vascular imaging. 4D perfusion imaging is ideal but rarely available within the first hour after stroke, whereas plain CT and CTA usually are. Hence, we propose a framework to extract a predicted perfusion map (PPM) derived from CT and CTA images. In all eighteen patients, we found significantly high spatial similarity (with average Spearman's correlation = 0.7893) between our predicted perfusion map (PPM) and the T-max map derived from 4D-CTP. Voxelwise correlations between the PPM and National Institutes of Health Stroke Scale (NIHSS) subscores for L/R hand motor, gaze, and language on a large cohort of 2,110 subjects reliably mapped symptoms to expected infarct locations. Therefore our PPM could serve as an alternative for 4D perfusion imaging, if the latter is unavailable, to investigate blood perfusion in the first hours after hospital admission.||[2404.04025v1](http://arxiv.org/pdf/2404.04025v1)|null|\n", "2404.04007": "|**2024-04-05**|**Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering**|\u795e\u7ecf\u7b26\u53f7\u89c6\u9891\u95ee\u7b54\uff1a\u5b66\u4e60\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u95ee\u7b54\u7684\u7ec4\u5408\u65f6\u7a7a\u63a8\u7406|Lili Liang, Guanglu Sun, Jin Qiu, Lizhong Zhang|Compositional spatio-temporal reasoning poses a significant challenge in the field of video question answering (VideoQA). Existing approaches struggle to establish effective symbolic reasoning structures, which are crucial for answering compositional spatio-temporal questions. To address this challenge, we propose a neural-symbolic framework called Neural-Symbolic VideoQA (NS-VideoQA), specifically designed for real-world VideoQA tasks. The uniqueness and superiority of NS-VideoQA are two-fold: 1) It proposes a Scene Parser Network (SPN) to transform static-dynamic video scenes into Symbolic Representation (SR), structuralizing persons, objects, relations, and action chronologies. 2) A Symbolic Reasoning Machine (SRM) is designed for top-down question decompositions and bottom-up compositional reasonings. Specifically, a polymorphic program executor is constructed for internally consistent reasoning from SR to the final answer. As a result, Our NS-VideoQA not only improves the compositional spatio-temporal reasoning in real-world VideoQA task, but also enables step-by-step error analysis by tracing the intermediate results. Experimental evaluations on the AGQA Decomp benchmark demonstrate the effectiveness of the proposed NS-VideoQA framework. Empirical studies further confirm that NS-VideoQA exhibits internal consistency in answering compositional questions and significantly improves the capability of spatio-temporal and logical inference for VideoQA tasks.||[2404.04007v1](http://arxiv.org/pdf/2404.04007v1)|null|\n", "2404.03999": "|**2024-04-05**|**Finsler-Laplace-Beltrami Operators with Application to Shape Analysis**|Finsler-Laplace-Beltrami \u7b97\u5b50\u5728\u5f62\u72b6\u5206\u6790\u4e2d\u7684\u5e94\u7528|Simon Weber, Thomas Dag\u00e8s, Maolin Gao, Daniel Cremers|The Laplace-Beltrami operator (LBO) emerges from studying manifolds equipped with a Riemannian metric. It is often called the Swiss army knife of geometry processing as it allows to capture intrinsic shape information and gives rise to heat diffusion, geodesic distances, and a multitude of shape descriptors. It also plays a central role in geometric deep learning. In this work, we explore Finsler manifolds as a generalization of Riemannian manifolds. We revisit the Finsler heat equation and derive a Finsler heat kernel and a Finsler-Laplace-Beltrami Operator (FLBO): a novel theoretically justified anisotropic Laplace-Beltrami operator (ALBO). In experimental evaluations we demonstrate that the proposed FLBO is a valuable alternative to the traditional Riemannian-based LBO and ALBOs for spatial filtering and shape correspondence estimation. We hope that the proposed Finsler heat kernel and the FLBO will inspire further exploration of Finsler geometry in the computer vision community.||[2404.03999v1](http://arxiv.org/pdf/2404.03999v1)|null|\n", "2404.03992": "|**2024-04-05**|**Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks**|\u63b7\u9ab0\u5b50\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6df1\u5ea6\u5b66\u4e60\u6027\u80fd\uff1a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u968f\u673a\u6027\u6280\u672f\u7684\u7814\u7a76|Mohammed Ghaith Altarabichi, S\u0142awomir Nowaczyk, Sepideh Pashami, Peyman Sheikholharam Mashhadi, Julia Handl|This paper investigates how various randomization techniques impact Deep Neural Networks (DNNs). Randomization, like weight noise and dropout, aids in reducing overfitting and enhancing generalization, but their interactions are poorly understood. The study categorizes randomness techniques into four types and proposes new methods: adding noise to the loss function and random masking of gradient updates. Using Particle Swarm Optimizer (PSO) for hyperparameter optimization, it explores optimal configurations across MNIST, FASHION-MNIST, CIFAR10, and CIFAR100 datasets. Over 30,000 configurations are evaluated, revealing data augmentation and weight initialization randomness as main performance contributors. Correlation analysis shows different optimizers prefer distinct randomization types. The complete implementation and dataset are available on GitHub.||[2404.03992v1](http://arxiv.org/pdf/2404.03992v1)|null|\n", "2404.03930": "|**2024-04-05**|**Real-GDSR: Real-World Guided DSM Super-Resolution via Edge-Enhancing Residual Network**|Real-GDSR\uff1a\u901a\u8fc7\u8fb9\u7f18\u589e\u5f3a\u6b8b\u5dee\u7f51\u7edc\u5b9e\u73b0\u73b0\u5b9e\u4e16\u754c\u5f15\u5bfc\u7684 DSM \u8d85\u5206\u8fa8\u7387|Daniel Panangian, Ksenia Bittner|A low-resolution digital surface model (DSM) features distinctive attributes impacted by noise, sensor limitations and data acquisition conditions, which failed to be replicated using simple interpolation methods like bicubic. This causes super-resolution models trained on synthetic data does not perform effectively on real ones. Training a model on real low and high resolution DSMs pairs is also a challenge because of the lack of information. On the other hand, the existence of other imaging modalities of the same scene can be used to enrich the information needed for large-scale super-resolution. In this work, we introduce a novel methodology to address the intricacies of real-world DSM super-resolution, named REAL-GDSR, breaking down this ill-posed problem into two steps. The first step involves the utilization of a residual local refinement network. This strategic approach departs from conventional methods that trained to directly predict height values instead of the differences (residuals) and utilize large receptive fields in their networks. The second step introduces a diffusion-based technique that enhances the results on a global scale, with a primary focus on smoothing and edge preservation. Our experiments underscore the effectiveness of the proposed method. We conduct a comprehensive evaluation, comparing it to recent state-of-the-art techniques in the domain of real-world DSM super-resolution (SR). Our approach consistently outperforms these existing methods, as evidenced through qualitative and quantitative assessments.||[2404.03930v1](http://arxiv.org/pdf/2404.03930v1)|null|\n"}}