{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.10229": "|**2024-01-18**|**OMG-Seg: Is One Model Good Enough For All Segmentation?**|OMG-Seg\uff1a\u4e00\u79cd\u6a21\u578b\u8db3\u4ee5\u9002\u7528\u4e8e\u6240\u6709\u7ec6\u5206\u5417\uff1f|Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy|In this work, we address various segmentation tasks, each traditionally tackled by distinct or partially unified models. We propose OMG-Seg, One Model that is Good enough to efficiently and effectively handle all the segmentation tasks, including image semantic, instance, and panoptic segmentation, as well as their video counterparts, open vocabulary settings, prompt-driven, interactive segmentation like SAM, and video object segmentation. To our knowledge, this is the first model to handle all these tasks in one model and achieve satisfactory performance. We show that OMG-Seg, a transformer-based encoder-decoder architecture with task-specific queries and outputs, can support over ten distinct segmentation tasks and yet significantly reduce computational and parameter overhead across various tasks and datasets. We rigorously evaluate the inter-task influences and correlations during co-training. Code and models are available at https://github.com/lxtGH/OMG-Seg.|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u5404\u79cd\u5206\u5272\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u4f20\u7edf\u4e0a\u90fd\u662f\u901a\u8fc7\u4e0d\u540c\u6216\u90e8\u5206\u7edf\u4e00\u7684\u6a21\u578b\u6765\u5904\u7406\u7684\u3002\u6211\u4eec\u63d0\u51fa OMG-Seg\uff0c\u4e00\u79cd\u8db3\u4ee5\u9ad8\u6548\u4e14\u6709\u6548\u5730\u5904\u7406\u6240\u6709\u5206\u5272\u4efb\u52a1\u7684\u6a21\u578b\uff0c\u5305\u62ec\u56fe\u50cf\u8bed\u4e49\u3001\u5b9e\u4f8b\u548c\u5168\u666f\u5206\u5272\uff0c\u4ee5\u53ca\u5b83\u4eec\u7684\u89c6\u9891\u5bf9\u5e94\u9879\u3001\u5f00\u653e\u8bcd\u6c47\u8bbe\u7f6e\u3001\u63d0\u793a\u9a71\u52a8\u3001\u4ea4\u4e92\u5f0f\u5206\u5272\uff0c\u4f8b\u5982SAM \u548c\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\u5904\u7406\u6240\u6709\u8fd9\u4e9b\u4efb\u52a1\u5e76\u53d6\u5f97\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\u7684\u6a21\u578b\u3002\u6211\u4eec\u8bc1\u660e OMG-Seg \u662f\u4e00\u79cd\u57fa\u4e8e Transformer \u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5177\u6709\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u67e5\u8be2\u548c\u8f93\u51fa\uff0c\u53ef\u4ee5\u652f\u6301\u5341\u591a\u4e2a\u4e0d\u540c\u7684\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u4e14\u663e\u7740\u51cf\u5c11\u5404\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u7684\u8ba1\u7b97\u548c\u53c2\u6570\u5f00\u9500\u3002\u6211\u4eec\u5728\u534f\u540c\u8bad\u7ec3\u671f\u95f4\u4e25\u683c\u8bc4\u4f30\u4efb\u52a1\u95f4\u7684\u5f71\u54cd\u548c\u76f8\u5173\u6027\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 https://github.com/lxtGH/OMG-Seg \u83b7\u53d6\u3002|[2401.10229v1](http://arxiv.org/pdf/2401.10229v1)|null|\n", "2401.10228": "|**2024-01-18**|**RAP-SAM: Towards Real-Time All-Purpose Segment Anything**|RAP-SAM\uff1a\u8fc8\u5411\u5b9e\u65f6\u901a\u7528\u5206\u6bb5\u4efb\u4f55\u5185\u5bb9|Shilin Xu, Haobo Yuan, Qingyu Shi, Lu Qi, Jingbo Wang, Yibo Yang, Yining Li, Kai Chen, Yunhai Tong, Bernard Ghanem, et.al.|Advanced by transformer architecture, vision foundation models (VFMs) achieve remarkable progress in performance and generalization ability. Segment Anything Model (SAM) is one remarkable model that can achieve generalized segmentation. However, most VFMs cannot run in realtime, which makes it difficult to transfer them into several products. On the other hand, current real-time segmentation mainly has one purpose, such as semantic segmentation on the driving scene. We argue that diverse outputs are needed for real applications. Thus, this work explores a new real-time segmentation setting, named all-purpose segmentation in real-time, to transfer VFMs in real-time deployment. It contains three different tasks, including interactive segmentation, panoptic segmentation, and video segmentation. We aim to use one model to achieve the above tasks in real-time. We first benchmark several strong baselines. Then, we present Real-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an efficient decoupled decoder to perform prompt-driven decoding. Moreover, we further explore different training strategies and tuning methods to boost co-training performance further. Our code and model are available at https://github.com/xushilin1/RAP-SAM/.|\u901a\u8fc7 Transformer \u67b6\u6784\u7684\u6539\u8fdb\uff0c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u8fdb\u6b65\u3002 Segment Anything Model (SAM) \u662f\u4e00\u79cd\u53ef\u4ee5\u5b9e\u73b0\u5e7f\u4e49\u5206\u5272\u7684\u5353\u8d8a\u6a21\u578b\u3002\u7136\u800c\uff0c\u5927\u591a\u6570 VFM \u65e0\u6cd5\u5b9e\u65f6\u8fd0\u884c\uff0c\u8fd9\u4f7f\u5f97\u5c06\u5b83\u4eec\u8f6c\u79fb\u5230\u591a\u4e2a\u4ea7\u54c1\u4e2d\u53d8\u5f97\u56f0\u96be\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5f53\u524d\u7684\u5b9e\u65f6\u5206\u5272\u4e3b\u8981\u6709\u4e00\u4e2a\u76ee\u7684\uff0c\u4f8b\u5982\u9a7e\u9a76\u573a\u666f\u7684\u8bed\u4e49\u5206\u5272\u3002\u6211\u4eec\u8ba4\u4e3a\u5b9e\u9645\u5e94\u7528\u9700\u8981\u591a\u6837\u5316\u7684\u8f93\u51fa\u3002\u56e0\u6b64\uff0c\u8fd9\u9879\u5de5\u4f5c\u63a2\u7d22\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e\u65f6\u5206\u6bb5\u8bbe\u7f6e\uff0c\u79f0\u4e3a\u5b9e\u65f6\u901a\u7528\u5206\u6bb5\uff0c\u4ee5\u5728\u5b9e\u65f6\u90e8\u7f72\u4e2d\u4f20\u8f93 VFM\u3002\u5b83\u5305\u542b\u4e09\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\uff0c\u5305\u62ec\u4ea4\u4e92\u5f0f\u5206\u5272\u3001\u5168\u666f\u5206\u5272\u548c\u89c6\u9891\u5206\u5272\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u4f7f\u7528\u4e00\u79cd\u6a21\u578b\u6765\u5b9e\u65f6\u5b8c\u6210\u4e0a\u8ff0\u4efb\u52a1\u3002\u6211\u4eec\u9996\u5148\u5bf9\u51e0\u4e2a\u5f3a\u57fa\u7ebf\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5b9e\u65f6\u901a\u7528 SAM (RAP-SAM)\u3002\u5b83\u5305\u542b\u4e00\u4e2a\u9ad8\u6548\u7684\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u9ad8\u6548\u7684\u89e3\u8026\u89e3\u7801\u5668\u6765\u6267\u884c\u63d0\u793a\u9a71\u52a8\u7684\u89e3\u7801\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e0d\u540c\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u8c03\u6574\u65b9\u6cd5\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u534f\u540c\u8bad\u7ec3\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 https://github.com/xushilin1/RAP-SAM/ \u83b7\u53d6\u3002|[2401.10228v1](http://arxiv.org/pdf/2401.10228v1)|null|\n", "2401.10227": "|**2024-01-18**|**A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting**|\u7528\u4e8e\u5168\u666f\u5206\u5272\u548c\u63a9\u6a21\u4fee\u590d\u7684\u7b80\u5355\u6f5c\u5728\u6269\u6563\u65b9\u6cd5|Wouter Van Gansbeke, Bert De Brabandere|Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities. Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation. The experimental validation yields promising results for both panoptic segmentation and mask inpainting. While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties.|\u5168\u666f\u548c\u5b9e\u4f8b\u5206\u5272\u7f51\u7edc\u901a\u5e38\u4f7f\u7528\u4e13\u95e8\u7684\u5bf9\u8c61\u68c0\u6d4b\u6a21\u5757\u3001\u590d\u6742\u7684\u635f\u5931\u51fd\u6570\u548c\u4e34\u65f6\u540e\u5904\u7406\u6b65\u9aa4\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u5904\u7406\u5b9e\u4f8b\u63a9\u6a21\u7684\u6392\u5217\u4e0d\u53d8\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u5728\u7a33\u5b9a\u6269\u6563\u7684\u57fa\u7840\u4e0a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5168\u666f\u5206\u5272\u7684\u6f5c\u5728\u6269\u6563\u65b9\u6cd5\uff0c\u4ece\u800c\u4ea7\u751f\u4e86\u4e00\u4e2a\u5ffd\u7565\u8fd9\u4e9b\u590d\u6742\u6027\u7684\u7b80\u5355\u67b6\u6784\u3002\u6211\u4eec\u7684\u8bad\u7ec3\u8fc7\u7a0b\u5305\u62ec\u4e24\u4e2a\u6b65\u9aa4\uff1a\uff081\uff09\u8bad\u7ec3\u6d45\u5c42\u81ea\u52a8\u7f16\u7801\u5668\u5c06\u5206\u5272\u63a9\u6a21\u6295\u5f71\u5230\u6f5c\u5728\u7a7a\u95f4\uff1b (2) \u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4ee5\u5141\u8bb8\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u56fe\u50cf\u6761\u4ef6\u91c7\u6837\u3002\u751f\u6210\u6a21\u578b\u7684\u4f7f\u7528\u5f00\u542f\u4e86\u5bf9\u63a9\u6a21\u5b8c\u6210\u6216\u4fee\u590d\u7684\u63a2\u7d22\uff0c\u8fd9\u5728\u4ea4\u4e92\u5f0f\u5206\u5272\u4e2d\u5177\u6709\u5e94\u7528\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e3a\u5168\u666f\u5206\u5272\u548c\u63a9\u6a21\u4fee\u590d\u5e26\u6765\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u867d\u7136\u6ca1\u6709\u521b\u9020\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4f46\u6211\u4eec\u7684\u6a21\u578b\u7684\u7b80\u5355\u6027\u3001\u901a\u7528\u6027\u548c\u63a9\u6a21\u5b8c\u6210\u80fd\u529b\u662f\u7406\u60f3\u7684\u7279\u6027\u3002|[2401.10227v1](http://arxiv.org/pdf/2401.10227v1)|null|\n", "2401.10217": "|**2024-01-18**|**Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions**|\u89e3\u91ca\u9690\u5f0f\u795e\u7ecf\u753b\u5e03\uff1a\u901a\u8fc7\u8ffd\u8e2a\u50cf\u7d20\u7684\u8d21\u732e\u5c06\u50cf\u7d20\u4e0e\u795e\u7ecf\u5143\u8fde\u63a5\u8d77\u6765|Namitha Padmanabhan, Matthew Gwilliam, Pulkit Kumar, Shishira R Maiya, Max Ehrlich, Abhinav Shrivastava|The many variations of Implicit Neural Representations (INRs), where a neural network is trained as a continuous representation of a signal, have tremendous practical utility for downstream tasks including novel view synthesis, video compression, and image superresolution. Unfortunately, the inner workings of these networks are seriously under-studied. Our work, eXplaining the Implicit Neural Canvas (XINC), is a unified framework for explaining properties of INRs by examining the strength of each neuron's contribution to each output pixel. We call the aggregate of these contribution maps the Implicit Neural Canvas and we use this concept to demonstrate that the INRs which we study learn to ''see'' the frames they represent in surprising ways. For example, INRs tend to have highly distributed representations. While lacking high-level object semantics, they have a significant bias for color and edges, and are almost entirely space-agnostic. We arrive at our conclusions by examining how objects are represented across time in video INRs, using clustering to visualize similar neurons across layers and architectures, and show that this is dominated by motion. These insights demonstrate the general usefulness of our analysis framework. Our project page is available at https://namithap10.github.io/xinc.|\u9690\u5f0f\u795e\u7ecf\u8868\u793a (INR) \u7684\u8bb8\u591a\u53d8\u4f53\uff08\u5176\u4e2d\u795e\u7ecf\u7f51\u7edc\u88ab\u8bad\u7ec3\u4e3a\u4fe1\u53f7\u7684\u8fde\u7eed\u8868\u793a\uff09\u5bf9\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff08\u5305\u62ec\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u3001\u89c6\u9891\u538b\u7f29\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff09\u5177\u6709\u5de8\u5927\u7684\u5b9e\u7528\u6027\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u8fd9\u4e9b\u7f51\u7edc\u7684\u5185\u90e8\u8fd0\u4f5c\u673a\u5236\u8fd8\u6ca1\u6709\u88ab\u5145\u5206\u7814\u7a76\u3002\u6211\u4eec\u7684\u5de5\u4f5c\uff0ceXplaining the Implicit Neural Canvas (XINC)\uff0c\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u68c0\u67e5\u6bcf\u4e2a\u795e\u7ecf\u5143\u5bf9\u6bcf\u4e2a\u8f93\u51fa\u50cf\u7d20\u7684\u8d21\u732e\u5f3a\u5ea6\u6765\u89e3\u91ca INR \u7684\u5c5e\u6027\u3002\u6211\u4eec\u5c06\u8fd9\u4e9b\u8d21\u732e\u56fe\u7684\u96c6\u5408\u79f0\u4e3a\u9690\u5f0f\u795e\u7ecf\u753b\u5e03\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e2a\u6982\u5ff5\u6765\u8bc1\u660e\u6211\u4eec\u7814\u7a76\u7684 INR \u5b66\u4f1a\u4ee5\u4ee4\u4eba\u60ca\u8bb6\u7684\u65b9\u5f0f\u201c\u770b\u5230\u201d\u5b83\u4eec\u6240\u4ee3\u8868\u7684\u6846\u67b6\u3002\u4f8b\u5982\uff0cINR \u5f80\u5f80\u5177\u6709\u9ad8\u5ea6\u5206\u5e03\u5f0f\u7684\u8868\u793a\u3002\u867d\u7136\u7f3a\u4e4f\u9ad8\u7ea7\u5bf9\u8c61\u8bed\u4e49\uff0c\u4f46\u5b83\u4eec\u5bf9\u989c\u8272\u548c\u8fb9\u7f18\u6709\u5f88\u5927\u7684\u504f\u89c1\uff0c\u5e76\u4e14\u51e0\u4e4e\u5b8c\u5168\u4e0e\u7a7a\u95f4\u65e0\u5173\u3002\u6211\u4eec\u901a\u8fc7\u68c0\u67e5\u5bf9\u8c61\u5728\u89c6\u9891 INR \u4e2d\u5982\u4f55\u968f\u65f6\u95f4\u8868\u793a\u800c\u5f97\u51fa\u7ed3\u8bba\uff0c\u4f7f\u7528\u805a\u7c7b\u6765\u53ef\u89c6\u5316\u8de8\u5c42\u548c\u67b6\u6784\u7684\u76f8\u4f3c\u795e\u7ecf\u5143\uff0c\u5e76\u8868\u660e\u8fd9\u662f\u7531\u8fd0\u52a8\u4e3b\u5bfc\u7684\u3002\u8fd9\u4e9b\u89c1\u89e3\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u5206\u6790\u6846\u67b6\u7684\u666e\u904d\u5b9e\u7528\u6027\u3002\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\u4f4d\u4e8e https://namithap10.github.io/xinc\u3002|[2401.10217v1](http://arxiv.org/pdf/2401.10217v1)|null|\n", "2401.10176": "|**2024-01-18**|**Comprehensive OOD Detection Improvements**|\u5168\u9762\u7684 OOD \u68c0\u6d4b\u6539\u8fdb|Anish Lakkapragada, Amol Khanna, Edward Raff, Nathan Inkawhich|As machine learning becomes increasingly prevalent in impactful decisions, recognizing when inference data is outside the model's expected input distribution is paramount for giving context to predictions. Out-of-distribution (OOD) detection methods have been created for this task. Such methods can be split into representation-based or logit-based methods from whether they respectively utilize the model's embeddings or predictions for OOD detection. In contrast to most papers which solely focus on one such group, we address both. We employ dimensionality reduction on feature embeddings in representation-based methods for both time speedups and improved performance. Additionally, we propose DICE-COL, a modification of the popular logit-based method Directed Sparsification (DICE) that resolves an unnoticed flaw. We demonstrate the effectiveness of our methods on the OpenOODv1.5 benchmark framework, where they significantly improve performance and set state-of-the-art results.|\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u6709\u5f71\u54cd\u529b\u7684\u51b3\u7b56\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u8bc6\u522b\u63a8\u7406\u6570\u636e\u4f55\u65f6\u8d85\u51fa\u6a21\u578b\u7684\u9884\u671f\u8f93\u5165\u5206\u5e03\u5bf9\u4e8e\u4e3a\u9884\u6d4b\u63d0\u4f9b\u4e0a\u4e0b\u6587\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\u4efb\u52a1\u521b\u5efa\u4e86\u5206\u5e03\u5916 (OOD) \u68c0\u6d4b\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u6839\u636e\u5b83\u4eec\u662f\u5426\u5206\u522b\u5229\u7528\u6a21\u578b\u7684\u5d4c\u5165\u6216\u9884\u6d4b\u8fdb\u884c OOD \u68c0\u6d4b\u5206\u4e3a\u57fa\u4e8e\u8868\u793a\u7684\u65b9\u6cd5\u6216\u57fa\u4e8e\u903b\u8f91\u7684\u65b9\u6cd5\u3002\u4e0e\u5927\u591a\u6570\u53ea\u5173\u6ce8\u5176\u4e2d\u4e00\u4e2a\u7fa4\u4f53\u7684\u8bba\u6587\u4e0d\u540c\uff0c\u6211\u4eec\u540c\u65f6\u5173\u6ce8\u8fd9\u4e24\u7c7b\u7fa4\u4f53\u3002\u6211\u4eec\u5728\u57fa\u4e8e\u8868\u793a\u7684\u65b9\u6cd5\u4e2d\u5bf9\u7279\u5f81\u5d4c\u5165\u8fdb\u884c\u964d\u7ef4\uff0c\u4ee5\u52a0\u5feb\u65f6\u95f4\u5e76\u63d0\u9ad8\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DICE-COL\uff0c\u8fd9\u662f\u5bf9\u6d41\u884c\u7684\u57fa\u4e8e logit \u7684\u65b9\u6cd5\u5b9a\u5411\u7a00\u758f\u5316 (DICE) \u7684\u4fee\u6539\uff0c\u89e3\u51b3\u4e86\u4e00\u4e2a\u672a\u88ab\u6ce8\u610f\u5230\u7684\u7f3a\u9677\u3002\u6211\u4eec\u5728 OpenOODv1.5 \u57fa\u51c6\u6846\u67b6\u4e0a\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b83\u4eec\u663e\u7740\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u8bbe\u7f6e\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002|[2401.10176v1](http://arxiv.org/pdf/2401.10176v1)|null|\n", "2401.10129": "|**2024-01-18**|**Few-shot learning for COVID-19 Chest X-Ray Classification with Imbalanced Data: An Inter vs. Intra Domain Study**|\u5177\u6709\u4e0d\u5e73\u8861\u6570\u636e\u7684 COVID-19 \u80f8\u90e8 X \u5c04\u7ebf\u5206\u7c7b\u7684\u5c11\u6837\u672c\u5b66\u4e60\uff1a\u57df\u95f4\u4e0e\u57df\u5185\u7814\u7a76|Alejandro Gal\u00e1n-Cuenca, Antonio Javier Gallego, Marcelo Saval-Calvo, Antonio Pertusa|Medical image datasets are essential for training models used in computer-aided diagnosis, treatment planning, and medical research. However, some challenges are associated with these datasets, including variability in data distribution, data scarcity, and transfer learning issues when using models pre-trained from generic images. This work studies the effect of these challenges at the intra- and inter-domain level in few-shot learning scenarios with severe data imbalance. For this, we propose a methodology based on Siamese neural networks in which a series of techniques are integrated to mitigate the effects of data scarcity and distribution imbalance. Specifically, different initialization and data augmentation methods are analyzed, and four adaptations to Siamese networks of solutions to deal with imbalanced data are introduced, including data balancing and weighted loss, both separately and combined, and with a different balance of pairing ratios. Moreover, we also assess the inference process considering four classifiers, namely Histogram, $k$NN, SVM, and Random Forest. Evaluation is performed on three chest X-ray datasets with annotated cases of both positive and negative COVID-19 diagnoses. The accuracy of each technique proposed for the Siamese architecture is analyzed separately and their results are compared to those obtained using equivalent methods on a state-of-the-art CNN. We conclude that the introduced techniques offer promising improvements over the baseline in almost all cases, and that the selection of the technique may vary depending on the amount of data available and the level of imbalance.|\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u5bf9\u4e8e\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u3001\u6cbb\u7597\u8ba1\u5212\u548c\u533b\u5b66\u7814\u7a76\u4e2d\u4f7f\u7528\u7684\u8bad\u7ec3\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u5b58\u5728\u4e00\u4e9b\u6311\u6218\uff0c\u5305\u62ec\u6570\u636e\u5206\u5e03\u7684\u53ef\u53d8\u6027\u3001\u6570\u636e\u7a00\u7f3a\u6027\u4ee5\u53ca\u4f7f\u7528\u4ece\u901a\u7528\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u65f6\u7684\u8fc1\u79fb\u5b66\u4e60\u95ee\u9898\u3002\u8fd9\u9879\u5de5\u4f5c\u7814\u7a76\u4e86\u8fd9\u4e9b\u6311\u6218\u5728\u6570\u636e\u4e25\u91cd\u4e0d\u5e73\u8861\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e2d\u5728\u57df\u5185\u548c\u57df\u95f4\u5c42\u9762\u7684\u5f71\u54cd\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66b9\u7f57\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u96c6\u6210\u4e86\u4e00\u7cfb\u5217\u6280\u672f\u6765\u51cf\u8f7b\u6570\u636e\u7a00\u7f3a\u548c\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u5f71\u54cd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u7684\u521d\u59cb\u5316\u548c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u4ecb\u7ecd\u4e86\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u7684\u66b9\u7f57\u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u7684\u56db\u79cd\u9002\u5e94\u65b9\u6cd5\uff0c\u5305\u62ec\u5355\u72ec\u548c\u7ec4\u5408\u7684\u6570\u636e\u5e73\u8861\u548c\u52a0\u6743\u635f\u5931\uff0c\u4ee5\u53ca\u4e0d\u540c\u7684\u914d\u5bf9\u6bd4\u7387\u5e73\u8861\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bc4\u4f30\u4e86\u8003\u8651\u56db\u4e2a\u5206\u7c7b\u5668\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5373\u76f4\u65b9\u56fe\u3001$k$NN\u3001SVM \u548c\u968f\u673a\u68ee\u6797\u3002\u5bf9\u4e09\u4e2a\u80f8\u90e8 X \u5149\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5176\u4e2d\u6807\u6ce8\u4e86\u9633\u6027\u548c\u9634\u6027 COVID-19 \u8bca\u65ad\u75c5\u4f8b\u3002\u5206\u522b\u5206\u6790\u4e86\u4e3a Siamese \u67b6\u6784\u63d0\u51fa\u7684\u6bcf\u79cd\u6280\u672f\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5c06\u5176\u7ed3\u679c\u4e0e\u5728\u6700\u5148\u8fdb\u7684 CNN \u4e0a\u4f7f\u7528\u7b49\u6548\u65b9\u6cd5\u83b7\u5f97\u7684\u7ed3\u679c\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u7ed3\u8bba\u662f\uff0c\u6240\u5f15\u5165\u7684\u6280\u672f\u5728\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u6bd4\u57fa\u7ebf\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u6539\u8fdb\uff0c\u5e76\u4e14\u6280\u672f\u7684\u9009\u62e9\u53ef\u80fd\u4f1a\u6839\u636e\u53ef\u7528\u6570\u636e\u91cf\u548c\u4e0d\u5e73\u8861\u7a0b\u5ea6\u800c\u6709\u6240\u4e0d\u540c\u3002|[2401.10129v1](http://arxiv.org/pdf/2401.10129v1)|null|\n", "2401.10113": "|**2024-01-18**|**Exposing Lip-syncing Deepfakes from Mouth Inconsistencies**|\u63ed\u9732\u53e3\u578b\u4e0d\u4e00\u81f4\u7684 Deepfakes|Soumyya Kanti Datta, Shan Jia, Siwei Lyu|A lip-syncing deepfake is a digitally manipulated video in which a person's lip movements are created convincingly using AI models to match altered or entirely new audio. Lip-syncing deepfakes are a dangerous type of deepfakes as the artifacts are limited to the lip region and more difficult to discern. In this paper, we describe a novel approach, LIP-syncing detection based on mouth INConsistency (LIPINC), for lip-syncing deepfake detection by identifying temporal inconsistencies in the mouth region. These inconsistencies are seen in the adjacent frames and throughout the video. Our model can successfully capture these irregularities and outperforms the state-of-the-art methods on several benchmark deepfake datasets.|\u53e3\u578b\u540c\u6b65 Deepfake \u662f\u4e00\u79cd\u7ecf\u8fc7\u6570\u5b57\u5904\u7406\u7684\u89c6\u9891\uff0c\u5176\u4e2d\u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u4ee4\u4eba\u4fe1\u670d\u5730\u521b\u5efa\u4e00\u4e2a\u4eba\u7684\u5634\u5507\u52a8\u4f5c\uff0c\u4ee5\u5339\u914d\u66f4\u6539\u6216\u5168\u65b0\u7684\u97f3\u9891\u3002\u53e3\u578b\u540c\u6b65\u6df1\u5ea6\u4f2a\u9020\u662f\u4e00\u79cd\u5371\u9669\u7684\u6df1\u5ea6\u4f2a\u9020\uff0c\u56e0\u4e3a\u4f2a\u5f71\u4ec5\u9650\u4e8e\u5634\u5507\u533a\u57df\u5e76\u4e14\u66f4\u96be\u4ee5\u8fa8\u522b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5373\u57fa\u4e8e\u5634 INConsistency (LIPINC) \u7684\u5507\u540c\u6b65\u68c0\u6d4b\uff0c\u901a\u8fc7\u8bc6\u522b\u5634\u533a\u57df\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6765\u8fdb\u884c\u5507\u540c\u6b65\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u3002\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u5728\u76f8\u90bb\u5e27\u548c\u6574\u4e2a\u89c6\u9891\u4e2d\u90fd\u53ef\u4ee5\u770b\u5230\u3002\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u6210\u529f\u6355\u83b7\u8fd9\u4e9b\u4e0d\u89c4\u5219\u6027\uff0c\u5e76\u4e14\u5728\u51e0\u4e2a\u57fa\u51c6\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2401.10113v1](http://arxiv.org/pdf/2401.10113v1)|null|\n", "2401.10110": "|**2024-01-18**|**VIPTR: A Vision Permutable Extractor for Fast and Efficient Scene Text Recognition**|VIPTR\uff1a\u7528\u4e8e\u5feb\u901f\u9ad8\u6548\u573a\u666f\u6587\u672c\u8bc6\u522b\u7684\u89c6\u89c9\u53ef\u53d8\u6362\u63d0\u53d6\u5668|Xianfu Cheng, Weixiao Zhou, Xiang Li, Xiaoming Chen, Jian Yang, Tongliang Li, Zhoujun Li|Scene Text Recognition (STR) is a challenging task that involves recognizing text within images of natural scenes. Although current state-of-the-art models for STR exhibit high performance, they typically suffer from low inference efficiency due to their reliance on hybrid architectures comprised of visual encoders and sequence decoders. In this work, we propose the VIsion Permutable extractor for fast and efficient scene Text Recognition (VIPTR), which achieves an impressive balance between high performance and rapid inference speeds in the domain of STR. Specifically, VIPTR leverages a visual-semantic extractor with a pyramid structure, characterized by multiple self-attention layers, while eschewing the traditional sequence decoder. This design choice results in a lightweight and efficient model capable of handling inputs of varying sizes. Extensive experimental results on various standard datasets for both Chinese and English scene text recognition validate the superiority of VIPTR. Notably, the VIPTR-T (Tiny) variant delivers highly competitive accuracy on par with other lightweight models and achieves SOTA inference speeds. Meanwhile, the VIPTR-L (Large) variant attains greater recognition accuracy, while maintaining a low parameter count and favorable inference speed. Our proposed method provides a compelling solution for the STR challenge, which blends high accuracy with efficiency and greatly benefits real-world applications requiring fast and reliable text recognition. The code is publicly available at https://github.com/cxfyxl/VIPTR.|\u573a\u666f\u6587\u672c\u8bc6\u522b (STR) \u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u6d89\u53ca\u8bc6\u522b\u81ea\u7136\u573a\u666f\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u3002\u5c3d\u7ba1\u5f53\u524d\u6700\u5148\u8fdb\u7684 STR \u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u4f9d\u8d56\u4e8e\u7531\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5e8f\u5217\u89e3\u7801\u5668\u7ec4\u6210\u7684\u6df7\u5408\u67b6\u6784\uff0c\u5b83\u4eec\u901a\u5e38\u4f1a\u9047\u5230\u63a8\u7406\u6548\u7387\u8f83\u4f4e\u7684\u95ee\u9898\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u5feb\u901f\u9ad8\u6548\u7684\u573a\u666f\u6587\u672c\u8bc6\u522b\uff08VIPTR\uff09\u7684 VIsion Permutable \u63d0\u53d6\u5668\uff0c\u5b83\u5728 STR \u9886\u57df\u7684\u9ad8\u6027\u80fd\u548c\u5feb\u901f\u63a8\u7406\u901f\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u5e73\u8861\u3002\u5177\u4f53\u6765\u8bf4\uff0cVIPTR \u5229\u7528\u5177\u6709\u91d1\u5b57\u5854\u7ed3\u6784\u7684\u89c6\u89c9\u8bed\u4e49\u63d0\u53d6\u5668\uff0c\u5176\u7279\u5f81\u662f\u591a\u4e2a\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u540c\u65f6\u907f\u5f00\u4e86\u4f20\u7edf\u7684\u5e8f\u5217\u89e3\u7801\u5668\u3002\u8fd9\u79cd\u8bbe\u8ba1\u9009\u62e9\u4ea7\u751f\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u5927\u5c0f\u7684\u8f93\u5165\u3002\u5728\u5404\u79cd\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u4e2d\u82f1\u6587\u573a\u666f\u6587\u672c\u8bc6\u522b\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86 VIPTR \u7684\u4f18\u8d8a\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cVIPTR-T (Tiny) \u53d8\u4f53\u53ef\u63d0\u4f9b\u4e0e\u5176\u4ed6\u8f7b\u91cf\u7ea7\u6a21\u578b\u76f8\u5f53\u7684\u6781\u5177\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0 SOTA \u63a8\u7406\u901f\u5ea6\u3002\u540c\u65f6\uff0cVIPTR-L\uff08\u5927\uff09\u53d8\u4f53\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u53c2\u6570\u6570\u91cf\u548c\u826f\u597d\u7684\u63a8\u7406\u901f\u5ea6\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a STR \u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u5c06\u9ad8\u7cbe\u5ea6\u4e0e\u6548\u7387\u878d\u4e3a\u4e00\u4f53\uff0c\u6781\u5927\u5730\u6709\u5229\u4e8e\u9700\u8981\u5feb\u901f\u53ef\u9760\u7684\u6587\u672c\u8bc6\u522b\u7684\u5b9e\u9645\u5e94\u7528\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/cxfyxl/VIPTR \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2401.10110v1](http://arxiv.org/pdf/2401.10110v1)|null|\n", "2401.10050": "|**2024-01-18**|**ContextMix: A context-aware data augmentation method for industrial visual inspection systems**|ContextMix\uff1a\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6570\u636e\u589e\u5f3a\u65b9\u6cd5|Hyungmin Kim, Donghun Kim, Pyunghwan Ahn, Sungho Suh, Hansang Cho, Junmo Kim|While deep neural networks have achieved remarkable performance, data augmentation has emerged as a crucial strategy to mitigate overfitting and enhance network performance. These techniques hold particular significance in industrial manufacturing contexts. Recently, image mixing-based methods have been introduced, exhibiting improved performance on public benchmark datasets. However, their application to industrial tasks remains challenging. The manufacturing environment generates massive amounts of unlabeled data on a daily basis, with only a few instances of abnormal data occurrences. This leads to severe data imbalance. Thus, creating well-balanced datasets is not straightforward due to the high costs associated with labeling. Nonetheless, this is a crucial step for enhancing productivity. For this reason, we introduce ContextMix, a method tailored for industrial applications and benchmark datasets. ContextMix generates novel data by resizing entire images and integrating them into other images within the batch. This approach enables our method to learn discriminative features based on varying sizes from resized images and train informative secondary features for object recognition using occluded images. With the minimal additional computation cost of image resizing, ContextMix enhances performance compared to existing augmentation techniques. We evaluate its effectiveness across classification, detection, and segmentation tasks using various network architectures on public benchmark datasets. Our proposed method demonstrates improved results across a range of robustness tasks. Its efficacy in real industrial environments is particularly noteworthy, as demonstrated using the passive component dataset.|\u867d\u7136\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4f46\u6570\u636e\u589e\u5f3a\u5df2\u6210\u4e3a\u51cf\u8f7b\u8fc7\u5ea6\u62df\u5408\u548c\u589e\u5f3a\u7f51\u7edc\u6027\u80fd\u7684\u5173\u952e\u7b56\u7565\u3002\u8fd9\u4e9b\u6280\u672f\u5728\u5de5\u4e1a\u5236\u9020\u73af\u5883\u4e2d\u5177\u6709\u7279\u522b\u91cd\u8981\u7684\u610f\u4e49\u3002\u6700\u8fd1\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u56fe\u50cf\u6df7\u5408\u7684\u65b9\u6cd5\uff0c\u5728\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e86\u6539\u8fdb\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u5de5\u4e1a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5236\u9020\u73af\u5883\u6bcf\u5929\u90fd\u4f1a\u4ea7\u751f\u5927\u91cf\u672a\u6807\u8bb0\u7684\u6570\u636e\uff0c\u53ea\u6709\u5c11\u6570\u60c5\u51b5\u4e0b\u51fa\u73b0\u5f02\u5e38\u6570\u636e\u3002\u8fd9\u5bfc\u81f4\u4e86\u4e25\u91cd\u7684\u6570\u636e\u4e0d\u5e73\u8861\u3002\u56e0\u6b64\uff0c\u7531\u4e8e\u4e0e\u6807\u8bb0\u76f8\u5173\u7684\u6210\u672c\u5f88\u9ad8\uff0c\u521b\u5efa\u5e73\u8861\u7684\u6570\u636e\u96c6\u5e76\u4e0d\u7b80\u5355\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u8fd9\u662f\u63d0\u9ad8\u751f\u4ea7\u529b\u7684\u5173\u952e\u4e00\u6b65\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ContextMix\uff0c\u4e00\u79cd\u9488\u5bf9\u5de5\u4e1a\u5e94\u7528\u548c\u57fa\u51c6\u6570\u636e\u96c6\u91cf\u8eab\u5b9a\u5236\u7684\u65b9\u6cd5\u3002 ContextMix \u901a\u8fc7\u8c03\u6574\u6574\u4e2a\u56fe\u50cf\u7684\u5927\u5c0f\u5e76\u5c06\u5176\u96c6\u6210\u5230\u6279\u6b21\u4e2d\u7684\u5176\u4ed6\u56fe\u50cf\u4e2d\u6765\u751f\u6210\u65b0\u9896\u7684\u6570\u636e\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u8c03\u6574\u5927\u5c0f\u7684\u56fe\u50cf\u7684\u4e0d\u540c\u5927\u5c0f\u6765\u5b66\u4e60\u5224\u522b\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u906e\u6321\u56fe\u50cf\u8bad\u7ec3\u4fe1\u606f\u4e30\u5bcc\u7684\u8f85\u52a9\u7279\u5f81\u4ee5\u8fdb\u884c\u5bf9\u8c61\u8bc6\u522b\u3002\u4e0e\u73b0\u6709\u7684\u589e\u5f3a\u6280\u672f\u76f8\u6bd4\uff0cContextMix \u51ed\u501f\u6700\u5c0f\u7684\u56fe\u50cf\u8c03\u6574\u5927\u5c0f\u989d\u5916\u8ba1\u7b97\u6210\u672c\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u6211\u4eec\u5728\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u5404\u79cd\u7f51\u7edc\u67b6\u6784\u8bc4\u4f30\u5176\u5728\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u4e00\u7cfb\u5217\u9c81\u68d2\u6027\u4efb\u52a1\u7684\u6539\u8fdb\u7ed3\u679c\u3002\u6b63\u5982\u4f7f\u7528\u65e0\u6e90\u5143\u4ef6\u6570\u636e\u96c6\u6240\u8bc1\u660e\u7684\u90a3\u6837\uff0c\u5b83\u5728\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u529f\u6548\u5c24\u5176\u503c\u5f97\u6ce8\u610f\u3002|[2401.10050v1](http://arxiv.org/pdf/2401.10050v1)|null|\n", "2401.10044": "|**2024-01-18**|**Deep spatial context: when attention-based models meet spatial regression**|\u6df1\u5c42\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff1a\u5f53\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u9047\u5230\u7a7a\u95f4\u56de\u5f52\u65f6|Paulina Tomaszewska, El\u017cbieta Sienkiewicz, Mai P. Hoang, Przemys\u0142aw Biecek|We propose 'Deep spatial context' (DSCon) method, which serves for investigation of the attention-based vision models using the concept of spatial context. It was inspired by histopathologists, however, the method can be applied to various domains. The DSCon allows for a quantitative measure of the spatial context's role using three Spatial Context Measures: $SCM_{features}$, $SCM_{targets}$, $SCM_{residuals}$ to distinguish whether the spatial context is observable within the features of neighboring regions, their target values (attention scores) or residuals, respectively. It is achieved by integrating spatial regression into the pipeline. The DSCon helps to verify research questions. The experiments reveal that spatial relationships are much bigger in the case of the classification of tumor lesions than normal tissues. Moreover, it turns out that the larger the size of the neighborhood taken into account within spatial regression, the less valuable contextual information is. Furthermore, it is observed that the spatial context measure is the largest when considered within the feature space as opposed to the targets and residuals.|\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u6df1\u5c42\u7a7a\u95f4\u4e0a\u4e0b\u6587\u201d\uff08DSCon\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7528\u4e8e\u4f7f\u7528\u7a7a\u95f4\u4e0a\u4e0b\u6587\u6982\u5ff5\u7814\u7a76\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u89c6\u89c9\u6a21\u578b\u3002\u5b83\u53d7\u5230\u7ec4\u7ec7\u75c5\u7406\u5b66\u5bb6\u7684\u542f\u53d1\uff0c\u4f46\u662f\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5e94\u7528\u4e8e\u5404\u4e2a\u9886\u57df\u3002 DSCon \u5141\u8bb8\u4f7f\u7528\u4e09\u4e2a\u7a7a\u95f4\u4e0a\u4e0b\u6587\u6d4b\u91cf\u6765\u5b9a\u91cf\u6d4b\u91cf\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u4f5c\u7528\uff1a$SCM_{features}$\u3001$SCM_{targets}$\u3001$SCM_{residuals}$\uff0c\u4ee5\u533a\u5206\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5728\u4ee5\u4e0b\u7279\u5f81\u4e2d\u662f\u5426\u53ef\u89c2\u5bdf\u5230\uff1a\u76f8\u90bb\u533a\u57df\uff0c\u5206\u522b\u662f\u5b83\u4eec\u7684\u76ee\u6807\u503c\uff08\u6ce8\u610f\u529b\u5206\u6570\uff09\u6216\u6b8b\u5dee\u3002\u5b83\u662f\u901a\u8fc7\u5c06\u7a7a\u95f4\u56de\u5f52\u96c6\u6210\u5230\u7ba1\u9053\u4e2d\u6765\u5b9e\u73b0\u7684\u3002 DSCon \u6709\u52a9\u4e8e\u9a8c\u8bc1\u7814\u7a76\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u80bf\u7624\u75c5\u53d8\u5206\u7c7b\u65f6\u7684\u7a7a\u95f4\u5173\u7cfb\u6bd4\u6b63\u5e38\u7ec4\u7ec7\u5927\u5f97\u591a\u3002\u6b64\u5916\uff0c\u4e8b\u5b9e\u8bc1\u660e\uff0c\u7a7a\u95f4\u56de\u5f52\u4e2d\u8003\u8651\u7684\u90bb\u57df\u89c4\u6a21\u8d8a\u5927\uff0c\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u4ef7\u503c\u5c31\u8d8a\u4f4e\u3002\u6b64\u5916\uff0c\u636e\u89c2\u5bdf\uff0c\u5f53\u5728\u7279\u5f81\u7a7a\u95f4\u5185\u8003\u8651\u65f6\uff0c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5ea6\u91cf\u76f8\u5bf9\u4e8e\u76ee\u6807\u548c\u6b8b\u5dee\u662f\u6700\u5927\u7684\u3002|[2401.10044v1](http://arxiv.org/pdf/2401.10044v1)|null|\n", "2401.10041": "|**2024-01-18**|**CMFN: Cross-Modal Fusion Network for Irregular Scene Text Recognition**|CMFN\uff1a\u7528\u4e8e\u4e0d\u89c4\u5219\u573a\u666f\u6587\u672c\u8bc6\u522b\u7684\u8de8\u6a21\u6001\u878d\u5408\u7f51\u7edc|Jinzhi Zheng, Ruyi Ji, Libo Zhang, Yanjun Wu, Chen Zhao|Scene text recognition, as a cross-modal task involving vision and text, is an important research topic in computer vision. Most existing methods use language models to extract semantic information for optimizing visual recognition. However, the guidance of visual cues is ignored in the process of semantic mining, which limits the performance of the algorithm in recognizing irregular scene text. To tackle this issue, we propose a novel cross-modal fusion network (CMFN) for irregular scene text recognition, which incorporates visual cues into the semantic mining process. Specifically, CMFN consists of a position self-enhanced encoder, a visual recognition branch and an iterative semantic recognition branch. The position self-enhanced encoder provides character sequence position encoding for both the visual recognition branch and the iterative semantic recognition branch. The visual recognition branch carries out visual recognition based on the visual features extracted by CNN and the position encoding information provided by the position self-enhanced encoder. The iterative semantic recognition branch, which consists of a language recognition module and a cross-modal fusion gate, simulates the way that human recognizes scene text and integrates cross-modal visual cues for text recognition. The experiments demonstrate that the proposed CMFN algorithm achieves comparable performance to state-of-the-art algorithms, indicating its effectiveness.|\u573a\u666f\u6587\u672c\u8bc6\u522b\u4f5c\u4e3a\u6d89\u53ca\u89c6\u89c9\u548c\u6587\u672c\u7684\u8de8\u6a21\u6001\u4efb\u52a1\uff0c\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u7814\u7a76\u8bfe\u9898\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\u4ee5\u4f18\u5316\u89c6\u89c9\u8bc6\u522b\u3002\u7136\u800c\uff0c\u8bed\u4e49\u6316\u6398\u8fc7\u7a0b\u4e2d\u5ffd\u7565\u4e86\u89c6\u89c9\u7ebf\u7d22\u7684\u5f15\u5bfc\uff0c\u9650\u5236\u4e86\u7b97\u6cd5\u8bc6\u522b\u4e0d\u89c4\u5219\u573a\u666f\u6587\u672c\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e0d\u89c4\u5219\u573a\u666f\u6587\u672c\u8bc6\u522b\u7684\u65b0\u578b\u8de8\u6a21\u6001\u878d\u5408\u7f51\u7edc\uff08CMFN\uff09\uff0c\u5b83\u5c06\u89c6\u89c9\u7ebf\u7d22\u7eb3\u5165\u8bed\u4e49\u6316\u6398\u8fc7\u7a0b\u3002\u5177\u4f53\u6765\u8bf4\uff0cCMFN\u7531\u4f4d\u7f6e\u81ea\u589e\u5f3a\u7f16\u7801\u5668\u3001\u89c6\u89c9\u8bc6\u522b\u5206\u652f\u548c\u8fed\u4ee3\u8bed\u4e49\u8bc6\u522b\u5206\u652f\u7ec4\u6210\u3002\u4f4d\u7f6e\u81ea\u589e\u5f3a\u7f16\u7801\u5668\u4e3a\u89c6\u89c9\u8bc6\u522b\u5206\u652f\u548c\u8fed\u4ee3\u8bed\u4e49\u8bc6\u522b\u5206\u652f\u63d0\u4f9b\u5b57\u7b26\u5e8f\u5217\u4f4d\u7f6e\u7f16\u7801\u3002\u89c6\u89c9\u8bc6\u522b\u5206\u652f\u6839\u636eCNN\u63d0\u53d6\u7684\u89c6\u89c9\u7279\u5f81\u548c\u4f4d\u7f6e\u81ea\u589e\u5f3a\u7f16\u7801\u5668\u63d0\u4f9b\u7684\u4f4d\u7f6e\u7f16\u7801\u4fe1\u606f\u8fdb\u884c\u89c6\u89c9\u8bc6\u522b\u3002\u8fed\u4ee3\u8bed\u4e49\u8bc6\u522b\u5206\u652f\u7531\u8bed\u8a00\u8bc6\u522b\u6a21\u5757\u548c\u8de8\u6a21\u6001\u878d\u5408\u95e8\u7ec4\u6210\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u573a\u666f\u6587\u672c\u7684\u65b9\u5f0f\uff0c\u5e76\u96c6\u6210\u8de8\u6a21\u6001\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u6587\u672c\u8bc6\u522b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 CMFN \u7b97\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u7b97\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8868\u660e\u4e86\u5176\u6709\u6548\u6027\u3002|[2401.10041v1](http://arxiv.org/pdf/2401.10041v1)|null|\n", "2401.10039": "|**2024-01-18**|**GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot Egocentric Action Recognition**|GPT4Ego\uff1a\u91ca\u653e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u81ea\u6211\u4e2d\u5fc3\u52a8\u4f5c\u8bc6\u522b|Guangzhao Dai, Xiangbo Shu, Wenhao Wu|Vision-Language Models (VLMs), pre-trained on large-scale datasets, have shown impressive performance in various visual recognition tasks. This advancement paves the way for notable performance in Zero-Shot Egocentric Action Recognition (ZS-EAR). Typically, VLMs handle ZS-EAR as a global video-text matching task, which often leads to suboptimal alignment of vision and linguistic knowledge. We propose a refined approach for ZS-EAR using VLMs, emphasizing fine-grained concept-description alignment that capitalizes on the rich semantic and contextual details in egocentric videos. In this paper, we introduce GPT4Ego, a straightforward yet remarkably potent VLM framework for ZS-EAR, designed to enhance the fine-grained alignment of concept and description between vision and language. Extensive experiments demonstrate GPT4Ego significantly outperforms existing VLMs on three large-scale egocentric video benchmarks, i.e., EPIC-KITCHENS-100 (33.2%, +9.4%), EGTEA (39.6%, +5.5%), and CharadesEgo (31.5%, +2.6%).|\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5404\u79cd\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u8fd9\u4e00\u8fdb\u6b65\u4e3a\u96f6\u6837\u672c\u81ea\u6211\u4e2d\u5fc3\u52a8\u4f5c\u8bc6\u522b\uff08ZS-EAR\uff09\u7684\u663e\u7740\u6027\u80fd\u94fa\u5e73\u4e86\u9053\u8def\u3002\u901a\u5e38\uff0cVLM \u5c06 ZS-EAR \u4f5c\u4e3a\u5168\u5c40\u89c6\u9891\u6587\u672c\u5339\u914d\u4efb\u52a1\u6765\u5904\u7406\uff0c\u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u89c6\u89c9\u548c\u8bed\u8a00\u77e5\u8bc6\u7684\u5bf9\u9f50\u4e0d\u7406\u60f3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528 VLM \u7684 ZS-EAR \u6539\u8fdb\u65b9\u6cd5\uff0c\u5f3a\u8c03\u7ec6\u7c92\u5ea6\u7684\u6982\u5ff5\u63cf\u8ff0\u5bf9\u9f50\uff0c\u5229\u7528\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u7ec6\u8282\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 GPT4Ego\uff0c\u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u4f46\u975e\u5e38\u5f3a\u5927\u7684 ZS-EAR VLM \u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u89c6\u89c9\u548c\u8bed\u8a00\u4e4b\u95f4\u6982\u5ff5\u548c\u63cf\u8ff0\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGPT4Ego \u5728\u4e09\u4e2a\u5927\u89c4\u6a21\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u7740\u4f18\u4e8e\u73b0\u6709 VLM\uff0c\u5373 EPIC-KITCHENS-100\uff0833.2%\uff0c+9.4%\uff09\u3001EGTEA\uff0839.6%\uff0c+5.5%\uff09\u548c CharadesEgo\uff0831.5%\uff0c+\uff09 2.6%\uff09\u3002|[2401.10039v1](http://arxiv.org/pdf/2401.10039v1)|null|\n", "2401.10037": "|**2024-01-18**|**Depth Over RGB: Automatic Evaluation of Open Surgery Skills Using Depth Camera**|Depth Over RGB\uff1a\u4f7f\u7528\u6df1\u5ea6\u76f8\u673a\u81ea\u52a8\u8bc4\u4f30\u5f00\u653e\u624b\u672f\u6280\u80fd|Ido Zuckerman, Nicole Werner, Jonathan Kouchly, Emma Huston, Shannon DiMarco, Paul DiMusto, Shlomi Laufer|Purpose: In this paper, we present a novel approach to the automatic evaluation of open surgery skills using depth cameras. This work is intended to show that depth cameras achieve similar results to RGB cameras, which is the common method in the automatic evaluation of open surgery skills. Moreover, depth cameras offer advantages such as robustness to lighting variations, camera positioning, simplified data compression, and enhanced privacy, making them a promising alternative to RGB cameras.   Methods: Experts and novice surgeons completed two simulators of open suturing. We focused on hand and tool detection, and action segmentation in suturing procedures. YOLOv8 was used for tool detection in RGB and depth videos. Furthermore, UVAST and MSTCN++ were used for action segmentation. Our study includes the collection and annotation of a dataset recorded with Azure Kinect.   Results: We demonstrated that using depth cameras in object detection and action segmentation achieves comparable results to RGB cameras. Furthermore, we analyzed 3D hand path length, revealing significant differences between experts and novice surgeons, emphasizing the potential of depth cameras in capturing surgical skills. We also investigated the influence of camera angles on measurement accuracy, highlighting the advantages of 3D cameras in providing a more accurate representation of hand movements.   Conclusion: Our research contributes to advancing the field of surgical skill assessment by leveraging depth cameras for more reliable and privacy evaluations. The findings suggest that depth cameras can be valuable in assessing surgical skills and provide a foundation for future research in this area.|\u76ee\u7684\uff1a\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6df1\u5ea6\u76f8\u673a\u81ea\u52a8\u8bc4\u4f30\u5f00\u653e\u624b\u672f\u6280\u80fd\u7684\u65b0\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u8868\u660e\u6df1\u5ea6\u76f8\u673a\u53ef\u4ee5\u8fbe\u5230\u4e0e RGB \u76f8\u673a\u76f8\u4f3c\u7684\u7ed3\u679c\uff0c\u8fd9\u662f\u5f00\u653e\u624b\u672f\u6280\u80fd\u81ea\u52a8\u8bc4\u4f30\u4e2d\u7684\u5e38\u7528\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6df1\u5ea6\u76f8\u673a\u5177\u6709\u5bf9\u5149\u7167\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3001\u76f8\u673a\u5b9a\u4f4d\u3001\u7b80\u5316\u7684\u6570\u636e\u538b\u7f29\u548c\u589e\u5f3a\u7684\u9690\u79c1\u6027\u7b49\u4f18\u52bf\uff0c\u4f7f\u5176\u6210\u4e3a RGB \u76f8\u673a\u7684\u6709\u524d\u9014\u7684\u66ff\u4ee3\u54c1\u3002\u65b9\u6cd5\uff1a\u4e13\u5bb6\u548c\u65b0\u624b\u5916\u79d1\u533b\u751f\u5b8c\u6210\u4e86\u4e24\u4e2a\u5f00\u653e\u7f1d\u5408\u6a21\u62df\u5668\u3002\u6211\u4eec\u4e13\u6ce8\u4e8e\u624b\u548c\u5de5\u5177\u68c0\u6d4b\u4ee5\u53ca\u7f1d\u5408\u8fc7\u7a0b\u4e2d\u7684\u52a8\u4f5c\u5206\u5272\u3002 YOLOv8 \u7528\u4e8e RGB \u548c\u6df1\u5ea6\u89c6\u9891\u4e2d\u7684\u5de5\u5177\u68c0\u6d4b\u3002\u6b64\u5916\uff0cUVAST\u548cMSTCN++\u7528\u4e8e\u52a8\u4f5c\u5206\u5272\u3002\u6211\u4eec\u7684\u7814\u7a76\u5305\u62ec\u4f7f\u7528 Azure Kinect \u8bb0\u5f55\u7684\u6570\u636e\u96c6\u7684\u6536\u96c6\u548c\u6ce8\u91ca\u3002\u7ed3\u679c\uff1a\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u5bf9\u8c61\u68c0\u6d4b\u548c\u52a8\u4f5c\u5206\u5272\u4e2d\u4f7f\u7528\u6df1\u5ea6\u76f8\u673a\u53ef\u4ee5\u8fbe\u5230\u4e0e RGB \u76f8\u673a\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5206\u6790\u4e86 3D \u624b\u90e8\u8def\u5f84\u957f\u5ea6\uff0c\u63ed\u793a\u4e86\u4e13\u5bb6\u548c\u65b0\u624b\u5916\u79d1\u533b\u751f\u4e4b\u95f4\u7684\u663e\u7740\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u6df1\u5ea6\u76f8\u673a\u5728\u6355\u6349\u624b\u672f\u6280\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u6444\u50cf\u5934\u89d2\u5ea6\u5bf9\u6d4b\u91cf\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86 3D \u6444\u50cf\u5934\u5728\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u624b\u90e8\u8fd0\u52a8\u8868\u793a\u65b9\u9762\u7684\u4f18\u52bf\u3002\u7ed3\u8bba\uff1a\u6211\u4eec\u7684\u7814\u7a76\u901a\u8fc7\u5229\u7528\u6df1\u5ea6\u6444\u50cf\u5934\u8fdb\u884c\u66f4\u53ef\u9760\u548c\u9690\u79c1\u7684\u8bc4\u4f30\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u624b\u672f\u6280\u80fd\u8bc4\u4f30\u9886\u57df\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6df1\u5ea6\u76f8\u673a\u5728\u8bc4\u4f30\u624b\u672f\u6280\u80fd\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002|[2401.10037v1](http://arxiv.org/pdf/2401.10037v1)|null|\n", "2401.10017": "|**2024-01-18**|**Text Region Multiple Information Perception Network for Scene Text Detection**|\u7528\u4e8e\u573a\u666f\u6587\u672c\u68c0\u6d4b\u7684\u6587\u672c\u533a\u57df\u591a\u4fe1\u606f\u611f\u77e5\u7f51\u7edc|Jinzhi Zheng, Libo Zhang, Yanjun Wu, Chen Zhao|Segmentation-based scene text detection algorithms can handle arbitrary shape scene texts and have strong robustness and adaptability, so it has attracted wide attention. Existing segmentation-based scene text detection algorithms usually only segment the pixels in the center region of the text, while ignoring other information of the text region, such as edge information, distance information, etc., thus limiting the detection accuracy of the algorithm for scene text. This paper proposes a plug-and-play module called the Region Multiple Information Perception Module (RMIPM) to enhance the detection performance of segmentation-based algorithms. Specifically, we design an improved module that can perceive various types of information about scene text regions, such as text foreground classification maps, distance maps, direction maps, etc. Experiments on MSRA-TD500 and TotalText datasets show that our method achieves comparable performance with current state-of-the-art algorithms.|\u57fa\u4e8e\u5206\u5272\u7684\u573a\u666f\u6587\u672c\u68c0\u6d4b\u7b97\u6cd5\u53ef\u4ee5\u5904\u7406\u4efb\u610f\u5f62\u72b6\u7684\u573a\u666f\u6587\u672c\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u56e0\u6b64\u53d7\u5230\u4e86\u5e7f\u6cdb\u7684\u5173\u6ce8\u3002\u73b0\u6709\u57fa\u4e8e\u5206\u5272\u7684\u573a\u666f\u6587\u672c\u68c0\u6d4b\u7b97\u6cd5\u901a\u5e38\u53ea\u5bf9\u6587\u672c\u4e2d\u5fc3\u533a\u57df\u7684\u50cf\u7d20\u8fdb\u884c\u5206\u5272\uff0c\u800c\u5ffd\u7565\u4e86\u6587\u672c\u533a\u57df\u7684\u5176\u4ed6\u4fe1\u606f\uff0c\u5982\u8fb9\u7f18\u4fe1\u606f\u3001\u8ddd\u79bb\u4fe1\u606f\u7b49\uff0c\u4ece\u800c\u9650\u5236\u4e86\u7b97\u6cd5\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002\u573a\u666f\u6587\u672c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u533a\u57df\u591a\u4fe1\u606f\u611f\u77e5\u6a21\u5757\uff08RMIPM\uff09\u7684\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u4ee5\u589e\u5f3a\u57fa\u4e8e\u5206\u5272\u7684\u7b97\u6cd5\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u611f\u77e5\u573a\u666f\u6587\u672c\u533a\u57df\u7684\u5404\u79cd\u7c7b\u578b\u7684\u4fe1\u606f\uff0c\u4f8b\u5982\u6587\u672c\u524d\u666f\u5206\u7c7b\u56fe\u3001\u8ddd\u79bb\u56fe\u3001\u65b9\u5411\u56fe\u7b49\u3002\u5728MSRA-TD500\u548cTotalText\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002|[2401.10017v1](http://arxiv.org/pdf/2401.10017v1)|null|\n", "2401.09997": "|**2024-01-18**|**BPDO:Boundary Points Dynamic Optimization for Arbitrary Shape Scene Text Detection**|BPDO\uff1a\u4efb\u610f\u5f62\u72b6\u573a\u666f\u6587\u672c\u68c0\u6d4b\u7684\u8fb9\u754c\u70b9\u52a8\u6001\u4f18\u5316|Jinzhi Zheng, Libo Zhang, Yanjun Wu, Chen Zhao|Arbitrary shape scene text detection is of great importance in scene understanding tasks. Due to the complexity and diversity of text in natural scenes, existing scene text algorithms have limited accuracy for detecting arbitrary shape text. In this paper, we propose a novel arbitrary shape scene text detector through boundary points dynamic optimization(BPDO). The proposed model is designed with a text aware module (TAM) and a boundary point dynamic optimization module (DOM). Specifically, the model designs a text aware module based on segmentation to obtain boundary points describing the central region of the text by extracting a priori information about the text region. Then, based on the idea of deformable attention, it proposes a dynamic optimization model for boundary points, which gradually optimizes the exact position of the boundary points based on the information of the adjacent region of each boundary point. Experiments on CTW-1500, Total-Text, and MSRA-TD500 datasets show that the model proposed in this paper achieves a performance that is better than or comparable to the state-of-the-art algorithm, proving the effectiveness of the model.|\u4efb\u610f\u5f62\u72b6\u573a\u666f\u6587\u672c\u68c0\u6d4b\u5728\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u975e\u5e38\u91cd\u8981\u3002\u7531\u4e8e\u81ea\u7136\u573a\u666f\u4e2d\u6587\u672c\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff0c\u73b0\u6709\u7684\u573a\u666f\u6587\u672c\u7b97\u6cd5\u5bf9\u4e8e\u68c0\u6d4b\u4efb\u610f\u5f62\u72b6\u6587\u672c\u7684\u7cbe\u5ea6\u6709\u9650\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u8fb9\u754c\u70b9\u52a8\u6001\u4f18\u5316\uff08BPDO\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u610f\u5f62\u72b6\u573a\u666f\u6587\u672c\u68c0\u6d4b\u5668\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u8bbe\u8ba1\u6709\u6587\u672c\u611f\u77e5\u6a21\u5757\uff08TAM\uff09\u548c\u8fb9\u754c\u70b9\u52a8\u6001\u4f18\u5316\u6a21\u5757\uff08DOM\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8be5\u6a21\u578b\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5206\u5272\u7684\u6587\u672c\u611f\u77e5\u6a21\u5757\uff0c\u901a\u8fc7\u63d0\u53d6\u6587\u672c\u533a\u57df\u7684\u5148\u9a8c\u4fe1\u606f\u6765\u83b7\u5f97\u63cf\u8ff0\u6587\u672c\u4e2d\u5fc3\u533a\u57df\u7684\u8fb9\u754c\u70b9\u3002\u7136\u540e\uff0c\u57fa\u4e8e\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u7684\u601d\u60f3\uff0c\u63d0\u51fa\u4e86\u8fb9\u754c\u70b9\u52a8\u6001\u4f18\u5316\u6a21\u578b\uff0c\u6839\u636e\u6bcf\u4e2a\u8fb9\u754c\u70b9\u76f8\u90bb\u533a\u57df\u7684\u4fe1\u606f\u9010\u6b65\u4f18\u5316\u8fb9\u754c\u70b9\u7684\u51c6\u786e\u4f4d\u7f6e\u3002\u5728CTW-1500\u3001Total-Text\u548cMSRA-TD500\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\u53d6\u5f97\u4e86\u4f18\u4e8e\u6216\u76f8\u5f53\u4e8estate-of-the-art\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002|[2401.09997v1](http://arxiv.org/pdf/2401.09997v1)|null|\n", "2401.09988": "|**2024-01-18**|**Developing an AI-based Integrated System for Bee Health Evaluation**|\u5f00\u53d1\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u871c\u8702\u5065\u5eb7\u8bc4\u4f30\u7efc\u5408\u7cfb\u7edf|Andrew Liang|Honey bees pollinate about one-third of the world's food supply, but bee colonies have alarmingly declined by nearly 40% over the past decade due to several factors, including pesticides and pests. Traditional methods for monitoring beehives, such as human inspection, are subjective, disruptive, and time-consuming. To overcome these limitations, artificial intelligence has been used to assess beehive health. However, previous studies have lacked an end-to-end solution and primarily relied on data from a single source, either bee images or sounds. This study introduces a comprehensive system consisting of bee object detection and health evaluation. Additionally, it utilized a combination of visual and audio signals to analyze bee behaviors. An Attention-based Multimodal Neural Network (AMNN) was developed to adaptively focus on key features from each type of signal for accurate bee health assessment. The AMNN achieved an overall accuracy of 92.61%, surpassing eight existing single-signal Convolutional Neural Networks and Recurrent Neural Networks. It outperformed the best image-based model by 32.51% and the top sound-based model by 13.98% while maintaining efficient processing times. Furthermore, it improved prediction robustness, attaining an F1-score higher than 90% across all four evaluated health conditions. The study also shows that audio signals are more reliable than images for assessing bee health. By seamlessly integrating AMNN with image and sound data in a comprehensive bee health monitoring system, this approach provides a more efficient and non-invasive solution for the early detection of bee diseases and the preservation of bee colonies.|\u871c\u8702\u4e3a\u4e16\u754c\u4e0a\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u98df\u7269\u6388\u7c89\uff0c\u4f46\u7531\u4e8e\u6740\u866b\u5242\u548c\u5bb3\u866b\u7b49\u591a\u79cd\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u8702\u7fa4\u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\u60ca\u4eba\u5730\u51cf\u5c11\u4e86\u8fd1 40%\u3002\u76d1\u6d4b\u8702\u7bb1\u7684\u4f20\u7edf\u65b9\u6cd5\uff08\u4f8b\u5982\u4eba\u5de5\u68c0\u67e5\uff09\u662f\u4e3b\u89c2\u7684\u3001\u7834\u574f\u6027\u7684\u4e14\u8017\u65f6\u7684\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u4eba\u5de5\u667a\u80fd\u5df2\u88ab\u7528\u6765\u8bc4\u4f30\u8702\u7bb1\u7684\u5065\u5eb7\u72b6\u51b5\u3002\u7136\u800c\uff0c\u4e4b\u524d\u7684\u7814\u7a76\u7f3a\u4e4f\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e14\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5355\u4e00\u6765\u6e90\u7684\u6570\u636e\uff0c\u8981\u4e48\u662f\u871c\u8702\u56fe\u50cf\uff0c\u8981\u4e48\u662f\u58f0\u97f3\u3002\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7531\u871c\u8702\u76ee\u6807\u68c0\u6d4b\u548c\u5065\u5eb7\u8bc4\u4f30\u7ec4\u6210\u7684\u7efc\u5408\u7cfb\u7edf\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u7ed3\u5408\u89c6\u89c9\u548c\u97f3\u9891\u4fe1\u53f7\u6765\u5206\u6790\u871c\u8702\u7684\u884c\u4e3a\u3002\u5f00\u53d1\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u6a21\u6001\u795e\u7ecf\u7f51\u7edc\uff08AMNN\uff09\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u5173\u6ce8\u6bcf\u79cd\u7c7b\u578b\u4fe1\u53f7\u7684\u5173\u952e\u7279\u5f81\uff0c\u4ee5\u8fdb\u884c\u51c6\u786e\u7684\u871c\u8702\u5065\u5eb7\u8bc4\u4f30\u3002 AMNN \u7684\u603b\u4f53\u51c6\u786e\u7387\u8fbe\u5230 92.61%\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u7684 8 \u4e2a\u5355\u4fe1\u53f7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3002\u5b83\u6bd4\u6700\u597d\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u6a21\u578b\u9ad8\u51fa 32.51%\uff0c\u6bd4\u6700\u597d\u7684\u57fa\u4e8e\u58f0\u97f3\u7684\u6a21\u578b\u9ad8\u51fa 13.98%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7684\u5904\u7406\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u7a33\u5065\u6027\uff0c\u5728\u6240\u6709\u56db\u79cd\u8bc4\u4f30\u7684\u5065\u5eb7\u72b6\u51b5\u4e0b\u83b7\u5f97\u4e86\u9ad8\u4e8e 90% \u7684 F1 \u5206\u6570\u3002\u8be5\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u5728\u8bc4\u4f30\u871c\u8702\u5065\u5eb7\u72b6\u51b5\u65b9\u9762\uff0c\u97f3\u9891\u4fe1\u53f7\u6bd4\u56fe\u50cf\u66f4\u53ef\u9760\u3002\u901a\u8fc7\u5c06 AMNN \u4e0e\u56fe\u50cf\u548c\u58f0\u97f3\u6570\u636e\u65e0\u7f1d\u96c6\u6210\u5728\u5168\u9762\u7684\u871c\u8702\u5065\u5eb7\u76d1\u6d4b\u7cfb\u7edf\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e3a\u871c\u8702\u75be\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u548c\u8702\u7fa4\u7684\u4fdd\u5b58\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u975e\u4fb5\u5165\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002|[2401.09988v1](http://arxiv.org/pdf/2401.09988v1)|null|\n", "2401.09980": "|**2024-01-18**|**Ventricular Segmentation: A Brief Comparison of U-Net Derivatives**|\u5fc3\u5ba4\u5206\u5272\uff1aU-Net \u5bfc\u6570\u7684\u7b80\u8981\u6bd4\u8f83|Ketan Suhaas Saichandran|Medical imaging refers to the technologies and methods utilized to view the human body and its inside, in order to diagnose, monitor, or even treat medical disorders. This paper aims to explore the application of deep learning techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and treatment of medical disorders related to the heart. The focus centers on implementing various architectures that are derivatives of U-Net, to effectively isolate specific parts of the heart for comprehensive anatomical and functional analysis. Through a combination of images, graphs, and quantitative metrics, the efficacy of the models and their predictions are showcased. Additionally, this paper addresses encountered challenges and outline strategies for future improvements. This abstract provides a concise overview of the efforts in utilizing deep learning for cardiac image segmentation, emphasizing both the accomplishments and areas for further refinement.|\u533b\u5b66\u6210\u50cf\u662f\u6307\u7528\u4e8e\u89c2\u5bdf\u4eba\u4f53\u53ca\u5176\u5185\u90e8\u4ee5\u8bca\u65ad\u3001\u76d1\u6d4b\u751a\u81f3\u6cbb\u7597\u533b\u5b66\u75be\u75c5\u7684\u6280\u672f\u548c\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728\u5fc3\u810f\u77ed\u8f74MRI\uff08\u78c1\u5171\u632f\u6210\u50cf\uff09\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u589e\u5f3a\u4e0e\u5fc3\u810f\u76f8\u5173\u7684\u533b\u5b66\u75be\u75c5\u7684\u8bca\u65ad\u3001\u76d1\u6d4b\u548c\u6cbb\u7597\u3002\u91cd\u70b9\u662f\u5b9e\u73b0 U-Net \u884d\u751f\u7684\u5404\u79cd\u67b6\u6784\uff0c\u4ee5\u6709\u6548\u9694\u79bb\u5fc3\u810f\u7684\u7279\u5b9a\u90e8\u5206\u4ee5\u8fdb\u884c\u5168\u9762\u7684\u89e3\u5256\u548c\u529f\u80fd\u5206\u6790\u3002\u901a\u8fc7\u56fe\u50cf\u3001\u56fe\u8868\u548c\u5b9a\u91cf\u6307\u6807\u7684\u7ed3\u5408\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u53ca\u5176\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u89e3\u51b3\u4e86\u9047\u5230\u7684\u6311\u6218\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u6539\u8fdb\u7684\u7b56\u7565\u3002\u672c\u6458\u8981\u7b80\u8981\u6982\u8ff0\u4e86\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u5fc3\u810f\u56fe\u50cf\u5206\u5272\u7684\u5de5\u4f5c\uff0c\u5f3a\u8c03\u4e86\u6240\u53d6\u5f97\u7684\u6210\u5c31\u548c\u9700\u8981\u8fdb\u4e00\u6b65\u5b8c\u5584\u7684\u9886\u57df\u3002|[2401.09980v1](http://arxiv.org/pdf/2401.09980v1)|null|\n", "2401.09962": "|**2024-01-18**|**CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects**|CustomVideo\uff1a\u81ea\u5b9a\u4e49\u591a\u4e2a\u4e3b\u9898\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210|Zhao Wang, Aoxue Li, Enze Xie, Lingting Zhu, Yong Guo, Qi Dou, Zhenguo Li|Customized text-to-video generation aims to generate high-quality videos guided by text prompts and subject references. Current approaches designed for single subjects suffer from tackling multiple subjects, which is a more challenging and practical scenario. In this work, we aim to promote multi-subject guided text-to-video customization. We propose CustomVideo, a novel framework that can generate identity-preserving videos with the guidance of multiple subjects. To be specific, firstly, we encourage the co-occurrence of multiple subjects via composing them in a single image. Further, upon a basic text-to-video diffusion model, we design a simple yet effective attention control strategy to disentangle different subjects in the latent space of diffusion model. Moreover, to help the model focus on the specific object area, we segment the object from given reference images and provide a corresponding object mask for attention learning. Also, we collect a multi-subject text-to-video generation dataset as a comprehensive benchmark, with 69 individual subjects and 57 meaningful pairs. Extensive qualitative, quantitative, and user study results demonstrate the superiority of our method, compared with the previous state-of-the-art approaches.|\u5b9a\u5236\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u65e8\u5728\u6839\u636e\u6587\u672c\u63d0\u793a\u548c\u4e3b\u9898\u53c2\u8003\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u3002\u5f53\u524d\u4e3a\u5355\u4e2a\u4e3b\u9898\u8bbe\u8ba1\u7684\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u591a\u4e2a\u4e3b\u9898\uff0c\u8fd9\u662f\u4e00\u4e2a\u66f4\u5177\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u7684\u573a\u666f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u4fc3\u8fdb\u591a\u4e3b\u9898\u5f15\u5bfc\u7684\u6587\u672c\u5230\u89c6\u9891\u7684\u5b9a\u5236\u3002\u6211\u4eec\u63d0\u51fa\u4e86 CustomVideo\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u591a\u4e2a\u4e3b\u9898\u7684\u6307\u5bfc\u4e0b\u751f\u6210\u4fdd\u7559\u8eab\u4efd\u7684\u89c6\u9891\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u9996\u5148\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u591a\u4e2a\u4e3b\u9898\u7ec4\u5408\u5728\u4e00\u5f20\u56fe\u50cf\u4e2d\u6765\u9f13\u52b1\u5b83\u4eec\u540c\u65f6\u51fa\u73b0\u3002\u6b64\u5916\uff0c\u5728\u57fa\u672c\u7684\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u6ce8\u610f\u529b\u63a7\u5236\u7b56\u7565\uff0c\u4ee5\u5728\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u89e3\u5f00\u4e0d\u540c\u7684\u4e3b\u9898\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5e2e\u52a9\u6a21\u578b\u805a\u7126\u4e8e\u7279\u5b9a\u7684\u7269\u4f53\u533a\u57df\uff0c\u6211\u4eec\u4ece\u7ed9\u5b9a\u7684\u53c2\u8003\u56fe\u50cf\u4e2d\u5206\u5272\u7269\u4f53\uff0c\u5e76\u4e3a\u6ce8\u610f\u529b\u5b66\u4e60\u63d0\u4f9b\u76f8\u5e94\u7684\u7269\u4f53\u63a9\u6a21\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u6536\u96c6\u4e86\u4e00\u4e2a\u591a\u4e3b\u9898\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6570\u636e\u96c6\u4f5c\u4e3a\u7efc\u5408\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u542b 69 \u4e2a\u5355\u72ec\u4e3b\u9898\u548c 57 \u4e2a\u6709\u610f\u4e49\u7684\u5bf9\u3002\u4e0e\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e7f\u6cdb\u7684\u5b9a\u6027\u3001\u5b9a\u91cf\u548c\u7528\u6237\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002|[2401.09962v1](http://arxiv.org/pdf/2401.09962v1)|null|\n", "2401.09942": "|**2024-01-18**|**Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking**|\u7528\u4e8e\u8fd0\u52a8\u89c6\u89c9\u8ddf\u8e2a\u7684\u8054\u5408\u91cd\u65b0\u8bc6\u522b\u3001\u56e2\u961f\u5f52\u5c5e\u548c\u89d2\u8272\u5206\u7c7b\u7684\u591a\u4efb\u52a1\u5b66\u4e60|Amir M. Mansourian, Vladimir Somers, Christophe De Vleeschouwer, Shohreh Kasaei|Effective tracking and re-identification of players is essential for analyzing soccer videos. But, it is a challenging task due to the non-linear motion of players, the similarity in appearance of players from the same team, and frequent occlusions. Therefore, the ability to extract meaningful embeddings to represent players is crucial in developing an effective tracking and re-identification system. In this paper, a multi-purpose part-based person representation method, called PRTreID, is proposed that performs three tasks of role classification, team affiliation, and re-identification, simultaneously. In contrast to available literature, a single network is trained with multi-task supervision to solve all three tasks, jointly. The proposed joint method is computationally efficient due to the shared backbone. Also, the multi-task learning leads to richer and more discriminative representations, as demonstrated by both quantitative and qualitative results. To demonstrate the effectiveness of PRTreID, it is integrated with a state-of-the-art tracking method, using a part-based post-processing module to handle long-term tracking. The proposed tracking method outperforms all existing tracking methods on the challenging SoccerNet tracking dataset.|\u6709\u6548\u8ddf\u8e2a\u548c\u91cd\u65b0\u8bc6\u522b\u7403\u5458\u5bf9\u4e8e\u5206\u6790\u8db3\u7403\u89c6\u9891\u81f3\u5173\u91cd\u8981\u3002\u4f46\u662f\uff0c\u7531\u4e8e\u7403\u5458\u7684\u975e\u7ebf\u6027\u8fd0\u52a8\u3001\u6765\u81ea\u540c\u4e00\u7403\u961f\u7684\u7403\u5458\u7684\u5916\u89c2\u76f8\u4f3c\u4ee5\u53ca\u9891\u7e41\u7684\u906e\u6321\uff0c\u8fd9\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u63d0\u53d6\u6709\u610f\u4e49\u7684\u5d4c\u5165\u6765\u4ee3\u8868\u73a9\u5bb6\u7684\u80fd\u529b\u5bf9\u4e8e\u5f00\u53d1\u6709\u6548\u7684\u8ddf\u8e2a\u548c\u91cd\u65b0\u8bc6\u522b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7528\u9014\u7684\u57fa\u4e8e\u90e8\u4f4d\u7684\u4eba\u7269\u8868\u793a\u65b9\u6cd5\uff0c\u79f0\u4e3a PRTreID\uff0c\u8be5\u65b9\u6cd5\u540c\u65f6\u6267\u884c\u89d2\u8272\u5206\u7c7b\u3001\u56e2\u961f\u5f52\u5c5e\u548c\u91cd\u65b0\u8bc6\u522b\u4e09\u4e2a\u4efb\u52a1\u3002\u4e0e\u73b0\u6709\u6587\u732e\u76f8\u6bd4\uff0c\u5355\u4e2a\u7f51\u7edc\u7ecf\u8fc7\u591a\u4efb\u52a1\u76d1\u7763\u8bad\u7ec3\u4ee5\u5171\u540c\u89e3\u51b3\u6240\u6709\u4e09\u4e2a\u4efb\u52a1\u3002\u7531\u4e8e\u5171\u4eab\u4e3b\u5e72\uff0c\u6240\u63d0\u51fa\u7684\u8054\u5408\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u662f\u9ad8\u6548\u7684\u3002\u6b64\u5916\uff0c\u6b63\u5982\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u6240\u8bc1\u660e\u7684\u90a3\u6837\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u4f1a\u5e26\u6765\u66f4\u4e30\u5bcc\u3001\u66f4\u5177\u8fa8\u522b\u529b\u7684\u8868\u793a\u3002\u4e3a\u4e86\u8bc1\u660e PRTreID \u7684\u6709\u6548\u6027\uff0c\u5b83\u4e0e\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u65b9\u6cd5\u96c6\u6210\uff0c\u4f7f\u7528\u57fa\u4e8e\u90e8\u4ef6\u7684\u540e\u5904\u7406\u6a21\u5757\u6765\u5904\u7406\u957f\u671f\u8ddf\u8e2a\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684 SoccerNet \u8ddf\u8e2a\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u8ddf\u8e2a\u65b9\u6cd5\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684\u8ddf\u8e2a\u65b9\u6cd5\u3002|[2401.09942v1](http://arxiv.org/pdf/2401.09942v1)|null|\n", "2401.09939": "|**2024-01-18**|**ICGNet: A Unified Approach for Instance-Centric Grasping**|ICGNet\uff1a\u4ee5\u5b9e\u4f8b\u4e3a\u4e2d\u5fc3\u7684\u6293\u53d6\u7684\u7edf\u4e00\u65b9\u6cd5|Ren\u00e9 Zurbr\u00fcgg, Yifan Liu, Francis Engelmann, Suryansh Kumar, Marco Hutter, Vaishakh Patil, Fisher Yu|Accurate grasping is the key to several robotic tasks including assembly and household robotics. Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding: First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps. These grasps need to be compliant with the local object geometry. Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in the scene. Finally, the robot must compute a collision-free grasp trajectory while taking into account the geometry of the target object. Most grasp detection algorithms directly predict grasp poses in a monolithic fashion, which does not capture the composability of the environment. In this paper, we introduce an end-to-end architecture for object-centric grasping. The method uses pointcloud data from a single arbitrary viewing direction as an input and generates an instance-centric representation for each partially observed object in the scene. This representation is further used for object reconstruction and grasp detection in cluttered table-top scenes. We show the effectiveness of the proposed method by extensively evaluating it against state-of-the-art methods on synthetic datasets, indicating superior performance for grasping and reconstruction. Additionally, we demonstrate real-world applicability by decluttering scenes with varying numbers of objects.|\u51c6\u786e\u7684\u6293\u53d6\u662f\u5305\u62ec\u88c5\u914d\u548c\u5bb6\u7528\u673a\u5668\u4eba\u5728\u5185\u7684\u591a\u9879\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5173\u952e\u3002\u5728\u6742\u4e71\u7684\u73af\u5883\u4e2d\u6210\u529f\u6293\u53d6\u9700\u8981\u591a\u5c42\u6b21\u7684\u573a\u666f\u7406\u89e3\uff1a\u9996\u5148\uff0c\u673a\u5668\u4eba\u9700\u8981\u5206\u6790\u5355\u4e2a\u7269\u4f53\u7684\u51e0\u4f55\u5c5e\u6027\u4ee5\u627e\u5230\u53ef\u884c\u7684\u6293\u53d6\u3002\u8fd9\u4e9b\u6293\u53d6\u9700\u8981\u7b26\u5408\u5c40\u90e8\u5bf9\u8c61\u7684\u51e0\u4f55\u5f62\u72b6\u3002\u5176\u6b21\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u63d0\u51fa\u7684\u6293\u53d6\uff0c\u673a\u5668\u4eba\u9700\u8981\u63a8\u7406\u4e0e\u573a\u666f\u4e2d\u5176\u4ed6\u5bf9\u8c61\u7684\u4ea4\u4e92\u3002\u6700\u540e\uff0c\u673a\u5668\u4eba\u5fc5\u987b\u8ba1\u7b97\u65e0\u78b0\u649e\u6293\u53d6\u8f68\u8ff9\uff0c\u540c\u65f6\u8003\u8651\u76ee\u6807\u7269\u4f53\u7684\u51e0\u4f55\u5f62\u72b6\u3002\u5927\u591a\u6570\u6293\u53d6\u68c0\u6d4b\u7b97\u6cd5\u76f4\u63a5\u4ee5\u6574\u4f53\u65b9\u5f0f\u9884\u6d4b\u6293\u53d6\u59ff\u52bf\uff0c\u8fd9\u4e0d\u4f1a\u6355\u83b7\u73af\u5883\u7684\u53ef\u7ec4\u5408\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6293\u53d6\u7684\u7aef\u5230\u7aef\u67b6\u6784\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u6765\u81ea\u5355\u4e2a\u4efb\u610f\u89c2\u5bdf\u65b9\u5411\u7684\u70b9\u4e91\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u4e3a\u573a\u666f\u4e2d\u6bcf\u4e2a\u90e8\u5206\u89c2\u5bdf\u7684\u5bf9\u8c61\u751f\u6210\u4ee5\u5b9e\u4f8b\u4e3a\u4e2d\u5fc3\u7684\u8868\u793a\u3002\u8fd9\u79cd\u8868\u793a\u8fdb\u4e00\u6b65\u7528\u4e8e\u6742\u4e71\u684c\u9762\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\u91cd\u5efa\u548c\u6293\u53d6\u68c0\u6d4b\u3002\u6211\u4eec\u901a\u8fc7\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\u6765\u5c55\u793a\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5176\u5728\u6293\u53d6\u548c\u91cd\u5efa\u65b9\u9762\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u6574\u7406\u5177\u6709\u4e0d\u540c\u6570\u91cf\u5bf9\u8c61\u7684\u573a\u666f\u6765\u5c55\u793a\u73b0\u5b9e\u4e16\u754c\u7684\u9002\u7528\u6027\u3002|[2401.09939v1](http://arxiv.org/pdf/2401.09939v1)|null|\n", "2401.09923": "|**2024-01-18**|**MAMBA: Multi-level Aggregation via Memory Bank for Video Object Detection**|MAMBA\uff1a\u901a\u8fc7\u5185\u5b58\u5e93\u8fdb\u884c\u591a\u7ea7\u805a\u5408\uff0c\u7528\u4e8e\u89c6\u9891\u5bf9\u8c61\u68c0\u6d4b|Guanxiong Sun, Yang Hua, Guosheng Hu, Neil Robertson|State-of-the-art video object detection methods maintain a memory structure, either a sliding window or a memory queue, to enhance the current frame using attention mechanisms. However, we argue that these memory structures are not efficient or sufficient because of two implied operations: (1) concatenating all features in memory for enhancement, leading to a heavy computational cost; (2) frame-wise memory updating, preventing the memory from capturing more temporal information. In this paper, we propose a multi-level aggregation architecture via memory bank called MAMBA. Specifically, our memory bank employs two novel operations to eliminate the disadvantages of existing methods: (1) light-weight key-set construction which can significantly reduce the computational cost; (2) fine-grained feature-wise updating strategy which enables our method to utilize knowledge from the whole video. To better enhance features from complementary levels, i.e., feature maps and proposals, we further propose a generalized enhancement operation (GEO) to aggregate multi-level features in a unified manner. We conduct extensive evaluations on the challenging ImageNetVID dataset. Compared with existing state-of-the-art methods, our method achieves superior performance in terms of both speed and accuracy. More remarkably, MAMBA achieves mAP of 83.7/84.6% at 12.6/9.1 FPS with ResNet-101. Code is available at https://github.com/guanxiongsun/video_feature_enhancement.|\u6700\u5148\u8fdb\u7684\u89c6\u9891\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\u7ef4\u62a4\u5185\u5b58\u7ed3\u6784\uff08\u6ed1\u52a8\u7a97\u53e3\u6216\u5185\u5b58\u961f\u5217\uff09\uff0c\u4ee5\u4f7f\u7528\u6ce8\u610f\u673a\u5236\u589e\u5f3a\u5f53\u524d\u5e27\u3002\u7136\u800c\uff0c\u6211\u4eec\u8ba4\u4e3a\u8fd9\u4e9b\u5185\u5b58\u7ed3\u6784\u5e76\u4e0d\u9ad8\u6548\u6216\u4e0d\u591f\u5145\u5206\uff0c\u56e0\u4e3a\u6709\u4e24\u4e2a\u9690\u542b\u7684\u64cd\u4f5c\uff1a\uff081\uff09\u8fde\u63a5\u5185\u5b58\u4e2d\u7684\u6240\u6709\u7279\u5f81\u8fdb\u884c\u589e\u5f3a\uff0c\u5bfc\u81f4\u5927\u91cf\u7684\u8ba1\u7b97\u6210\u672c\uff1b (2)\u9010\u5e27\u8bb0\u5fc6\u66f4\u65b0\uff0c\u9632\u6b62\u8bb0\u5fc6\u6355\u83b7\u66f4\u591a\u7684\u65f6\u95f4\u4fe1\u606f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5185\u5b58\u5e93\u7684\u591a\u7ea7\u805a\u5408\u67b6\u6784\uff0c\u79f0\u4e3a MAMBA\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u5b58\u50a8\u5e93\u91c7\u7528\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u64cd\u4f5c\u6765\u6d88\u9664\u73b0\u6709\u65b9\u6cd5\u7684\u7f3a\u70b9\uff1a\uff081\uff09\u8f7b\u91cf\u7ea7\u5bc6\u94a5\u96c6\u6784\u9020\uff0c\u53ef\u4ee5\u663e\u7740\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1b \uff082\uff09\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81\u66f4\u65b0\u7b56\u7565\uff0c\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5229\u7528\u6574\u4e2a\u89c6\u9891\u4e2d\u7684\u77e5\u8bc6\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u589e\u5f3a\u4e92\u8865\u7ea7\u522b\u7684\u7279\u5f81\uff0c\u5373\u7279\u5f81\u56fe\u548c\u63d0\u6848\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u589e\u5f3a\u64cd\u4f5c\uff08GEO\uff09\u4ee5\u7edf\u4e00\u7684\u65b9\u5f0f\u805a\u5408\u591a\u7ea7\u7279\u5f81\u3002\u6211\u4eec\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684 ImageNetVID \u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\u3002\u4e0e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002\u66f4\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cMAMBA \u4f7f\u7528 ResNet-101 \u5728 12.6/9.1 FPS \u4e0b\u5b9e\u73b0\u4e86 83.7/84.6% \u7684 mAP\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/guanxiongsun/video_feature_enhancement \u83b7\u53d6\u3002|[2401.09923v1](http://arxiv.org/pdf/2401.09923v1)|null|\n", "2401.09921": "|**2024-01-18**|**BlenDA: Domain Adaptive Object Detection through diffusion-based blending**|BlenDA\uff1a\u901a\u8fc7\u57fa\u4e8e\u6269\u6563\u7684\u6df7\u5408\u8fdb\u884c\u57df\u81ea\u9002\u5e94\u5bf9\u8c61\u68c0\u6d4b|Tzuhsuan Huang, Chen-Che Huang, Chung-Hao Ku, Jun-Cheng Chen|Unsupervised domain adaptation (UDA) aims to transfer a model learned using labeled data from the source domain to unlabeled data in the target domain. To address the large domain gap issue between the source and target domains, we propose a novel regularization method for domain adaptive object detection, BlenDA, by generating the pseudo samples of the intermediate domains and their corresponding soft domain labels for adaptation training. The intermediate samples are generated by dynamically blending the source images with their corresponding translated images using an off-the-shelf pre-trained text-to-image diffusion model which takes the text label of the target domain as input and has demonstrated superior image-to-image translation quality. Based on experimental results from two adaptation benchmarks, our proposed approach can significantly enhance the performance of the state-of-the-art domain adaptive object detector, Adversarial Query Transformer (AQT). Particularly, in the Cityscapes to Foggy Cityscapes adaptation, we achieve an impressive 53.4% mAP on the Foggy Cityscapes dataset, surpassing the previous state-of-the-art by 1.5%. It is worth noting that our proposed method is also applicable to various paradigms of domain adaptive object detection. The code is available at:https://github.com/aiiu-lab/BlenDA|\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u65e8\u5728\u5c06\u4f7f\u7528\u6e90\u57df\u4e2d\u7684\u6807\u8bb0\u6570\u636e\u5b66\u4e60\u7684\u6a21\u578b\u8f6c\u79fb\u5230\u76ee\u6807\u57df\u4e2d\u7684\u672a\u6807\u8bb0\u6570\u636e\u3002\u4e3a\u4e86\u89e3\u51b3\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u5927\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57df\u81ea\u9002\u5e94\u5bf9\u8c61\u68c0\u6d4b\u6b63\u5219\u5316\u65b9\u6cd5\uff0cBlenDA\uff0c\u901a\u8fc7\u751f\u6210\u4e2d\u95f4\u57df\u7684\u4f2a\u6837\u672c\u53ca\u5176\u76f8\u5e94\u7684\u8f6f\u57df\u6807\u7b7e\u8fdb\u884c\u81ea\u9002\u5e94\u8bad\u7ec3\u3002\u4e2d\u95f4\u6837\u672c\u662f\u901a\u8fc7\u4f7f\u7528\u73b0\u6210\u7684\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u52a8\u6001\u6df7\u5408\u6e90\u56fe\u50cf\u4e0e\u5176\u76f8\u5e94\u7684\u7ffb\u8bd1\u56fe\u50cf\u6765\u751f\u6210\u7684\uff0c\u8be5\u6a21\u578b\u4ee5\u76ee\u6807\u57df\u7684\u6587\u672c\u6807\u7b7e\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u56fe\u50cf-\u5230\u56fe\u50cf\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002\u57fa\u4e8e\u4e24\u4e2a\u81ea\u9002\u5e94\u57fa\u51c6\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u6700\u5148\u8fdb\u7684\u57df\u81ea\u9002\u5e94\u5bf9\u8c61\u68c0\u6d4b\u5668\u2014\u2014\u5bf9\u6297\u67e5\u8be2\u53d8\u6362\u5668\uff08AQT\uff09\u7684\u6027\u80fd\u3002\u7279\u522b\u662f\uff0c\u5728 Cityscapes \u5230 Foggy Cityscapes \u7684\u9002\u5e94\u4e2d\uff0c\u6211\u4eec\u5728 Foggy Cityscapes \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684 53.4% mAP\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u6c34\u5e73 1.5%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4e5f\u9002\u7528\u4e8e\u57df\u81ea\u9002\u5e94\u5bf9\u8c61\u68c0\u6d4b\u7684\u5404\u79cd\u8303\u4f8b\u3002\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/aiiu-lab/BlenDA|[2401.09921v1](http://arxiv.org/pdf/2401.09921v1)|null|\n", "2401.09900": "|**2024-01-18**|**XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection**|\u7528\u4e8e\u89c6\u89c9\u8d28\u91cf\u68c0\u6d4b\u7684 XAI \u589e\u5f3a\u8bed\u4e49\u5206\u5272\u6a21\u578b|Tobias Clement, Truong Thanh Hung Nguyen, Mohamed Abdelaal, Hung Cao|Visual quality inspection systems, crucial in sectors like manufacturing and logistics, employ computer vision and machine learning for precise, rapid defect detection. However, their unexplained nature can hinder trust, error identification, and system improvement. This paper presents a framework to bolster visual quality inspection by using CAM-based explanations to refine semantic segmentation models. Our approach consists of 1) Model Training, 2) XAI-based Model Explanation, 3) XAI Evaluation, and 4) Annotation Augmentation for Model Enhancement, informed by explanations and expert insights. Evaluations show XAI-enhanced models surpass original DeepLabv3-ResNet101 models, especially in intricate object segmentation.|\u89c6\u89c9\u8d28\u91cf\u68c0\u6d4b\u7cfb\u7edf\u5728\u5236\u9020\u548c\u7269\u6d41\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u5b83\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u6765\u8fdb\u884c\u7cbe\u786e\u3001\u5feb\u901f\u7684\u7f3a\u9677\u68c0\u6d4b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u65e0\u6cd5\u89e3\u91ca\u7684\u6027\u8d28\u53ef\u80fd\u4f1a\u963b\u788d\u4fe1\u4efb\u3001\u9519\u8bef\u8bc6\u522b\u548c\u7cfb\u7edf\u6539\u8fdb\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e CAM \u7684\u89e3\u91ca\u6765\u5b8c\u5584\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u4ece\u800c\u589e\u5f3a\u89c6\u89c9\u8d28\u91cf\u68c0\u67e5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec 1) \u6a21\u578b\u8bad\u7ec3\u30012) \u57fa\u4e8e XAI \u7684\u6a21\u578b\u89e3\u91ca\u30013) XAI \u8bc4\u4f30\u548c 4) \u901a\u8fc7\u89e3\u91ca\u548c\u4e13\u5bb6\u89c1\u89e3\u8fdb\u884c\u6ce8\u91ca\u589e\u5f3a\u4ee5\u589e\u5f3a\u6a21\u578b\u3002\u8bc4\u4f30\u663e\u793a XAI \u589e\u5f3a\u6a21\u578b\u8d85\u8d8a\u4e86\u539f\u59cb DeepLabv3-ResNet101 \u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u5bf9\u8c61\u5206\u5272\u65b9\u9762\u3002|[2401.09900v1](http://arxiv.org/pdf/2401.09900v1)|null|\n", "2401.09895": "|**2024-01-18**|**Skeleton-Guided Instance Separation for Fine-Grained Segmentation in Microscopy**|\u7528\u4e8e\u663e\u5fae\u955c\u4e2d\u7ec6\u7c92\u5ea6\u5206\u5272\u7684\u9aa8\u67b6\u5f15\u5bfc\u5b9e\u4f8b\u5206\u79bb|Jun Wang, Chengfeng Zhou, Zhaoyan Ming, Lina Wei, Xudong Jiang, Dahong Qian|One of the fundamental challenges in microscopy (MS) image analysis is instance segmentation (IS), particularly when segmenting cluster regions where multiple objects of varying sizes and shapes may be connected or even overlapped in arbitrary orientations. Existing IS methods usually fail in handling such scenarios, as they rely on coarse instance representations such as keypoints and horizontal bounding boxes (h-bboxes). In this paper, we propose a novel one-stage framework named A2B-IS to address this challenge and enhance the accuracy of IS in MS images. Our approach represents each instance with a pixel-level mask map and a rotated bounding box (r-bbox). Unlike two-stage methods that use box proposals for segmentations, our method decouples mask and box predictions, enabling simultaneous processing to streamline the model pipeline. Additionally, we introduce a Gaussian skeleton map to aid the IS task in two key ways: (1) It guides anchor placement, reducing computational costs while improving the model's capacity to learn RoI-aware features by filtering out noise from background regions. (2) It ensures accurate isolation of densely packed instances by rectifying erroneous box predictions near instance boundaries. To further enhance the performance, we integrate two modules into the framework: (1) An Atrous Attention Block (A2B) designed to extract high-resolution feature maps with fine-grained multiscale information, and (2) A Semi-Supervised Learning (SSL) strategy that leverages both labeled and unlabeled images for model training. Our method has been thoroughly validated on two large-scale MS datasets, demonstrating its superiority over most state-of-the-art approaches.|\u663e\u5fae\u955c (MS) \u56fe\u50cf\u5206\u6790\u7684\u57fa\u672c\u6311\u6218\u4e4b\u4e00\u662f\u5b9e\u4f8b\u5206\u5272 (IS)\uff0c\u7279\u522b\u662f\u5728\u5206\u5272\u7c07\u533a\u57df\u65f6\uff0c\u5176\u4e2d\u591a\u4e2a\u4e0d\u540c\u5927\u5c0f\u548c\u5f62\u72b6\u7684\u5bf9\u8c61\u53ef\u80fd\u4ee5\u4efb\u610f\u65b9\u5411\u8fde\u63a5\u751a\u81f3\u91cd\u53e0\u3002\u73b0\u6709\u7684 IS \u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u5904\u7406\u6b64\u7c7b\u573a\u666f\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u7c97\u7565\u7684\u5b9e\u4f8b\u8868\u793a\uff0c\u4f8b\u5982\u5173\u952e\u70b9\u548c\u6c34\u5e73\u8fb9\u754c\u6846 (h-bboxes)\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a A2B-IS \u7684\u65b0\u578b\u5355\u9636\u6bb5\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u5e76\u63d0\u9ad8 MS \u56fe\u50cf\u4e2d IS \u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7528\u50cf\u7d20\u7ea7\u63a9\u6a21\u56fe\u548c\u65cb\u8f6c\u8fb9\u754c\u6846\uff08r-bbox\uff09\u8868\u793a\u6bcf\u4e2a\u5b9e\u4f8b\u3002\u4e0e\u4f7f\u7528\u6846\u5efa\u8bae\u8fdb\u884c\u5206\u5272\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u63a9\u6a21\u548c\u6846\u9884\u6d4b\u89e3\u8026\uff0c\u4ece\u800c\u80fd\u591f\u540c\u65f6\u5904\u7406\u4ee5\u7b80\u5316\u6a21\u578b\u6d41\u7a0b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u9ad8\u65af\u9aa8\u67b6\u56fe\uff0c\u4ee5\u4e24\u79cd\u5173\u952e\u65b9\u5f0f\u5e2e\u52a9 IS \u4efb\u52a1\uff1a\uff081\uff09\u5b83\u6307\u5bfc\u951a\u70b9\u653e\u7f6e\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u901a\u8fc7\u6ee4\u9664\u80cc\u666f\u533a\u57df\u7684\u566a\u58f0\u6765\u63d0\u9ad8\u6a21\u578b\u5b66\u4e60 RoI \u611f\u77e5\u7279\u5f81\u7684\u80fd\u529b\u3002 (2)\u5b83\u901a\u8fc7\u7ea0\u6b63\u5b9e\u4f8b\u8fb9\u754c\u9644\u8fd1\u7684\u9519\u8bef\u6846\u9884\u6d4b\u6765\u786e\u4fdd\u5bc6\u96c6\u5b9e\u4f8b\u7684\u51c6\u786e\u9694\u79bb\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\uff0c\u6211\u4eec\u5c06\u4e24\u4e2a\u6a21\u5757\u96c6\u6210\u5230\u6846\u67b6\u4e2d\uff1a\uff081\uff09Atrous Attention Block\uff08A2B\uff09\uff0c\u65e8\u5728\u63d0\u53d6\u5177\u6709\u7ec6\u7c92\u5ea6\u591a\u5c3a\u5ea6\u4fe1\u606f\u7684\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\uff0c\u4ee5\u53ca\uff082\uff09\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09 \uff09\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u56fe\u50cf\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u7684\u7b56\u7565\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5df2\u7ecf\u5728\u4e24\u4e2a\u5927\u578b MS \u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u5f7b\u5e95\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u76f8\u5bf9\u4e8e\u5927\u591a\u6570\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002|[2401.09895v1](http://arxiv.org/pdf/2401.09895v1)|null|\n", "2401.09883": "|**2024-01-18**|**Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation**|\u7528\u4e8e\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u95ee\u7b54\u8de8\u8bed\u8a00\u56fe\u50cf\u5339\u914d|Songhe Deng, Wei Zhuo, Jinheng Xie, Linlin Shen|Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model's ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets. Code is available at: https://github.com/CVI-SZU/QA-CLIMS|\u7c7b\u6fc0\u6d3b\u56fe (CAM) \u5df2\u6210\u4e3a\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272 (WSSS) \u7684\u6d41\u884c\u5de5\u5177\uff0c\u5141\u8bb8\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u6765\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u533a\u57df\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684 CAM \u65b9\u6cd5\u5b58\u5728\u76ee\u6807\u5bf9\u8c61\u533a\u57df\u6fc0\u6d3b\u4e0d\u8db3\u548c\u80cc\u666f\u533a\u57df\u9519\u8bef\u6fc0\u6d3b\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u8be6\u7ec6\u7684\u76d1\u7763\u4f1a\u963b\u788d\u6a21\u578b\u7406\u89e3\u6574\u4e2a\u56fe\u50cf\u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 WSSS \u95ee\u7b54\u8de8\u8bed\u8a00\u56fe\u50cf\u5339\u914d\u6846\u67b6\uff08QA-CLIMS\uff09\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u6700\u5927\u5316\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u7406\u89e3\u5e76\u6307\u5bfc\u6fc0\u6d3b\u56fe\u7684\u751f\u6210\u3002\u9996\u5148\uff0c\u901a\u8fc7\u95ee\u7b54\u63d0\u793a\u5de5\u7a0b\uff08QAPE\uff09\u5411 VQA\uff08\u89c6\u89c9\u95ee\u7b54\uff09\u6a21\u578b\u63d0\u51fa\u4e00\u7cfb\u5217\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u4ee5\u751f\u6210\u9002\u5e94\u67e5\u8be2\u56fe\u50cf\u7684\u524d\u666f\u76ee\u6807\u5bf9\u8c61\u548c\u80cc\u666f\u7684\u8bed\u6599\u5e93\u3002\u7136\u540e\uff0c\u6211\u4eec\u5728\u533a\u57df\u56fe\u50cf\u6587\u672c\u5bf9\u6bd4\uff08RITC\uff09\u7f51\u7edc\u4e2d\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5c06\u83b7\u5f97\u7684\u524d\u666f\u548c\u80cc\u666f\u533a\u57df\u4e0e\u751f\u6210\u7684\u8bed\u6599\u5e93\u8fdb\u884c\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u5f00\u653e\u8bcd\u6c47\u8868\u4e2d\u4e30\u5bcc\u7684\u6587\u672c\u4fe1\u606f\u4f5c\u4e3a\u989d\u5916\u7684\u76d1\u7763\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u5177\u6709\u66f4\u5b8c\u6574\u7684\u5bf9\u8c61\u533a\u57df\u7684\u9ad8\u8d28\u91cf CAM\uff0c\u5e76\u51cf\u5c11\u80cc\u666f\u533a\u57df\u7684\u9519\u8bef\u6fc0\u6d3b\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5206\u6790\u6765\u9a8c\u8bc1\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5e76\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u5728 PASCAL VOC 2012 \u548c MS COCO \u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/CVI-SZU/QA-CLIMS|[2401.09883v1](http://arxiv.org/pdf/2401.09883v1)|null|\n", "2401.09866": "|**2024-01-18**|**Boosting Few-Shot Segmentation via Instance-Aware Data Augmentation and Local Consensus Guided Cross Attention**|\u901a\u8fc7\u5b9e\u4f8b\u611f\u77e5\u6570\u636e\u589e\u5f3a\u548c\u5c40\u90e8\u5171\u8bc6\u5f15\u5bfc\u4ea4\u53c9\u6ce8\u610f\u529b\u6765\u4fc3\u8fdb\u5c11\u6837\u672c\u5206\u5272|Li Guo, Haoming Liu, Yuxuan Xia, Chengyu Zhang, Xiaochen Lu|Few-shot segmentation aims to train a segmentation model that can fast adapt to a novel task for which only a few annotated images are provided. Most recent models have adopted a prototype-based paradigm for few-shot inference. These approaches may have limited generalization capacity beyond the standard 1- or 5-shot settings. In this paper, we closely examine and reevaluate the fine-tuning based learning scheme that fine-tunes the classification layer of a deep segmentation network pre-trained on diverse base classes. To improve the generalizability of the classification layer optimized with sparsely annotated samples, we introduce an instance-aware data augmentation (IDA) strategy that augments the support images based on the relative sizes of the target objects. The proposed IDA effectively increases the support set's diversity and promotes the distribution consistency between support and query images. On the other hand, the large visual difference between query and support images may hinder knowledge transfer and cripple the segmentation performance. To cope with this challenge, we introduce the local consensus guided cross attention (LCCA) to align the query feature with support features based on their dense correlation, further improving the model's generalizability to the query image. The significant performance improvements on the standard few-shot segmentation benchmarks PASCAL-$5^i$ and COCO-$20^i$ verify the efficacy of our proposed method.|\u5c11\u955c\u5934\u5206\u5272\u65e8\u5728\u8bad\u7ec3\u4e00\u79cd\u5206\u5272\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u9002\u5e94\u4ec5\u63d0\u4f9b\u5c11\u91cf\u5e26\u6ce8\u91ca\u56fe\u50cf\u7684\u65b0\u4efb\u52a1\u3002\u6700\u8fd1\u7684\u6a21\u578b\u90fd\u91c7\u7528\u4e86\u57fa\u4e8e\u539f\u578b\u7684\u8303\u5f0f\u6765\u8fdb\u884c\u5c11\u6837\u672c\u63a8\u7406\u3002\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u6709\u9650\uff0c\u8d85\u51fa\u6807\u51c6\u7684 1 \u6b21\u6216 5 \u6b21\u8bbe\u7f6e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ed4\u7ec6\u68c0\u67e5\u548c\u91cd\u65b0\u8bc4\u4f30\u57fa\u4e8e\u5fae\u8c03\u7684\u5b66\u4e60\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5bf9\u5728\u4e0d\u540c\u57fa\u7c7b\u4e0a\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5206\u5272\u7f51\u7edc\u7684\u5206\u7c7b\u5c42\u8fdb\u884c\u5fae\u8c03\u3002\u4e3a\u4e86\u63d0\u9ad8\u4f7f\u7528\u7a00\u758f\u6ce8\u91ca\u6837\u672c\u4f18\u5316\u7684\u5206\u7c7b\u5c42\u7684\u901a\u7528\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5b9e\u4f8b\u611f\u77e5\u6570\u636e\u589e\u5f3a\uff08IDA\uff09\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u6839\u636e\u76ee\u6807\u5bf9\u8c61\u7684\u76f8\u5bf9\u5927\u5c0f\u6765\u589e\u5f3a\u652f\u6301\u56fe\u50cf\u3002\u6240\u63d0\u51fa\u7684IDA\u6709\u6548\u5730\u589e\u52a0\u4e86\u652f\u6301\u96c6\u7684\u591a\u6837\u6027\u5e76\u4fc3\u8fdb\u4e86\u652f\u6301\u56fe\u50cf\u548c\u67e5\u8be2\u56fe\u50cf\u4e4b\u95f4\u7684\u5206\u5e03\u4e00\u81f4\u6027\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u67e5\u8be2\u56fe\u50cf\u548c\u652f\u6301\u56fe\u50cf\u4e4b\u95f4\u5de8\u5927\u7684\u89c6\u89c9\u5dee\u5f02\u53ef\u80fd\u4f1a\u963b\u788d\u77e5\u8bc6\u8f6c\u79fb\u5e76\u524a\u5f31\u5206\u5272\u6027\u80fd\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5c40\u90e8\u5171\u8bc6\u5f15\u5bfc\u4ea4\u53c9\u6ce8\u610f\uff08LCCA\uff09\uff0c\u6839\u636e\u67e5\u8be2\u7279\u5f81\u4e0e\u652f\u6301\u7279\u5f81\u7684\u5bc6\u96c6\u76f8\u5173\u6027\u5c06\u5176\u5bf9\u9f50\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u67e5\u8be2\u56fe\u50cf\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6807\u51c6\u5c11\u6837\u672c\u5206\u5272\u57fa\u51c6 PASCAL-$5^i$ \u548c COCO-$20^i$ \u7684\u663e\u7740\u6027\u80fd\u6539\u8fdb\u9a8c\u8bc1\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2401.09866v1](http://arxiv.org/pdf/2401.09866v1)|null|\n", "2401.09865": "|**2024-01-18**|**Improving fine-grained understanding in image-text pre-training**|\u63d0\u9ad8\u56fe\u50cf\u6587\u672c\u9884\u8bad\u7ec3\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3|Ioana Bica, Anastasija Ili\u0107, Matthias Bauer, Goker Erdogan, Matko Bo\u0161njak, Christos Kaplanis, Alexey A. Gritsenko, Matthias Minderer, Charles Blundell, Razvan Pascanu, et.al.|We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thoroughly evaluate our proposed method and show improved performance over competing approaches both on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level tasks relying on fine-grained information, e.g. retrieval, object detection, and segmentation. Moreover, SPARC improves model faithfulness and captioning in foundational vision-language models.|\u6211\u4eec\u5f15\u5165\u4e86 SPARse \u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u5bf9\u9f50 (SPARC)\uff0c\u8fd9\u662f\u4e00\u79cd\u4ece\u56fe\u50cf\u6587\u672c\u5bf9\u4e2d\u9884\u8bad\u7ec3\u66f4\u7ec6\u7c92\u5ea6\u7684\u591a\u6a21\u6001\u8868\u793a\u7684\u7b80\u5355\u65b9\u6cd5\u3002\u9274\u4e8e\u591a\u4e2a\u56fe\u50cf\u5757\u901a\u5e38\u5bf9\u5e94\u4e8e\u5355\u4e2a\u5355\u8bcd\uff0c\u6211\u4eec\u5efa\u8bae\u4e3a\u6807\u9898\u4e2d\u7684\u6bcf\u4e2a\u6807\u8bb0\u5b66\u4e60\u4e00\u7ec4\u56fe\u50cf\u5757\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u5728\u56fe\u50cf\u8865\u4e01\u548c\u8bed\u8a00\u6807\u8bb0\u4e4b\u95f4\u4f7f\u7528\u7a00\u758f\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6807\u8bb0\u8ba1\u7b97\u8bed\u8a00\u5206\u7ec4\u7684\u89c6\u89c9\u5d4c\u5165\u4f5c\u4e3a\u8865\u4e01\u7684\u52a0\u6743\u5e73\u5747\u503c\u3002\u7136\u540e\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u5e8f\u5217\u635f\u5931\u6765\u5bf9\u6bd4\u6807\u8bb0\u548c\u8bed\u8a00\u5206\u7ec4\u7684\u89c6\u89c9\u5d4c\u5165\uff0c\u8be5\u635f\u5931\u4ec5\u53d6\u51b3\u4e8e\u5355\u4e2a\u6837\u672c\uff0c\u4e0d\u9700\u8981\u5176\u4ed6\u6279\u6b21\u6837\u672c\u4f5c\u4e3a\u8d1f\u6837\u672c\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u4ee5\u8ba1\u7b97\u6210\u672c\u4f4e\u5ec9\u7684\u65b9\u5f0f\u5b66\u4e60\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\u3002 SPARC \u5c06\u8fd9\u79cd\u7ec6\u7c92\u5ea6\u635f\u5931\u4e0e\u5168\u5c40\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\u4e4b\u95f4\u7684\u5bf9\u6bd4\u635f\u5931\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b66\u4e60\u540c\u65f6\u7f16\u7801\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u7684\u8868\u793a\u3002\u6211\u4eec\u5f7b\u5e95\u8bc4\u4f30\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u4f9d\u8d56\u4e8e\u7c97\u7c92\u5ea6\u4fe1\u606f\u7684\u56fe\u50cf\u7ea7\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e86\u6bd4\u7ade\u4e89\u65b9\u6cd5\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u4f8b\u5982\u5206\u7c7b\uff0c\u4ee5\u53ca\u4f9d\u8d56\u4e8e\u7ec6\u7c92\u5ea6\u4fe1\u606f\u7684\u533a\u57df\u7ea7\u4efb\u52a1\uff0c\u4f8b\u5982\u68c0\u7d22\u3001\u5bf9\u8c61\u68c0\u6d4b\u548c\u5206\u5272\u3002\u6b64\u5916\uff0cSPARC \u8fd8\u63d0\u9ad8\u4e86\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6a21\u578b\u53ef\u4fe1\u5ea6\u548c\u5b57\u5e55\u8bf4\u660e\u3002|[2401.09865v1](http://arxiv.org/pdf/2401.09865v1)|null|\n", "2401.09852": "|**2024-01-18**|**Enhancing the Fairness and Performance of Edge Cameras with Explainable AI**|\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\u589e\u5f3a\u8fb9\u7f18\u6444\u50cf\u5934\u7684\u516c\u5e73\u6027\u548c\u6027\u80fd|Truong Thanh Hung Nguyen, Vo Thanh Khang Nguyen, Quoc Hung Cao, Van Binh Truong, Quoc Khanh Nguyen, Hung Cao|The rising use of Artificial Intelligence (AI) in human detection on Edge camera systems has led to accurate but complex models, challenging to interpret and debug. Our research presents a diagnostic method using Explainable AI (XAI) for model debugging, with expert-driven problem identification and solution creation. Validated on the Bytetrack model in a real-world office Edge network, we found the training dataset as the main bias source and suggested model augmentation as a solution. Our approach helps identify model biases, essential for achieving fair and trustworthy models.|\u4eba\u5de5\u667a\u80fd (AI) \u5728\u8fb9\u7f18\u6444\u50cf\u5934\u7cfb\u7edf\u4e0a\u7684\u4eba\u4f53\u68c0\u6d4b\u4e2d\u7684\u4f7f\u7528\u4e0d\u65ad\u589e\u52a0\uff0c\u5bfc\u81f4\u6a21\u578b\u51c6\u786e\u4f46\u590d\u6742\uff0c\u96be\u4ee5\u89e3\u91ca\u548c\u8c03\u8bd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd (XAI) \u8fdb\u884c\u6a21\u578b\u8c03\u8bd5\u7684\u8bca\u65ad\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e13\u5bb6\u9a71\u52a8\u7684\u95ee\u9898\u8bc6\u522b\u548c\u89e3\u51b3\u65b9\u6848\u521b\u5efa\u3002\u5728\u73b0\u5b9e\u529e\u516c\u5ba4\u8fb9\u7f18\u7f51\u7edc\u4e2d\u7684 Bytetrack \u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u540e\uff0c\u6211\u4eec\u53d1\u73b0\u8bad\u7ec3\u6570\u636e\u96c6\u662f\u4e3b\u8981\u504f\u5dee\u6e90\uff0c\u5e76\u5efa\u8bae\u5c06\u6a21\u578b\u589e\u5f3a\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u8bc6\u522b\u6a21\u578b\u504f\u5dee\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u73b0\u516c\u5e73\u4e14\u503c\u5f97\u4fe1\u8d56\u7684\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002|[2401.09852v1](http://arxiv.org/pdf/2401.09852v1)|null|\n", "2401.09833": "|**2024-01-18**|**Slicer Networks**|\u5207\u7247\u5668\u7f51\u7edc|Hang Zhang, Xiang Chen, Rongguang Wang, Renjiu Hu, Dongdong Liu, Gaolei Li|In medical imaging, scans often reveal objects with varied contrasts but consistent internal intensities or textures. This characteristic enables the use of low-frequency approximations for tasks such as segmentation and deformation field estimation. Yet, integrating this concept into neural network architectures for medical image analysis remains underexplored. In this paper, we propose the Slicer Network, a novel architecture designed to leverage these traits. Comprising an encoder utilizing models like vision transformers for feature extraction and a slicer employing a learnable bilateral grid, the Slicer Network strategically refines and upsamples feature maps via a splatting-blurring-slicing process. This introduces an edge-preserving low-frequency approximation for the network outcome, effectively enlarging the effective receptive field. The enhancement not only reduces computational complexity but also boosts overall performance. Experiments across different medical imaging applications, including unsupervised and keypoints-based image registration and lesion segmentation, have verified the Slicer Network's improved accuracy and efficiency.|\u5728\u533b\u5b66\u6210\u50cf\u4e2d\uff0c\u626b\u63cf\u901a\u5e38\u4f1a\u663e\u793a\u5177\u6709\u4e0d\u540c\u5bf9\u6bd4\u5ea6\u4f46\u5185\u90e8\u5f3a\u5ea6\u6216\u7eb9\u7406\u4e00\u81f4\u7684\u7269\u4f53\u3002\u8fd9\u4e00\u7279\u6027\u4f7f\u5f97\u4f4e\u9891\u8fd1\u4f3c\u80fd\u591f\u7528\u4e8e\u5206\u5272\u548c\u53d8\u5f62\u573a\u4f30\u8ba1\u7b49\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5c06\u8fd9\u4e00\u6982\u5ff5\u96c6\u6210\u5230\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u4ecd\u7136\u6ca1\u6709\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5207\u7247\u5668\u7f51\u7edc\uff0c\u8fd9\u662f\u4e00\u79cd\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\u7684\u65b0\u9896\u67b6\u6784\u3002\u5207\u7247\u5668\u7f51\u7edc\u7531\u4e00\u4e2a\u5229\u7528\u89c6\u89c9\u53d8\u6362\u5668\u7b49\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u7684\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u5229\u7528\u53ef\u5b66\u4e60\u53cc\u8fb9\u7f51\u683c\u7684\u5207\u7247\u5668\u7ec4\u6210\uff0c\u901a\u8fc7\u6cfc\u6e85-\u6a21\u7cca-\u5207\u7247\u8fc7\u7a0b\u6218\u7565\u6027\u5730\u7ec6\u5316\u548c\u4e0a\u91c7\u6837\u7279\u5f81\u56fe\u3002\u8fd9\u4e3a\u7f51\u7edc\u7ed3\u679c\u5f15\u5165\u4e86\u4fdd\u7559\u8fb9\u7f18\u7684\u4f4e\u9891\u8fd1\u4f3c\uff0c\u6709\u6548\u5730\u6269\u5927\u4e86\u6709\u6548\u611f\u53d7\u91ce\u3002\u8fd9\u4e00\u589e\u5f3a\u4e0d\u4ec5\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u8fd8\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\u3002\u4e0d\u540c\u533b\u5b66\u6210\u50cf\u5e94\u7528\u7684\u5b9e\u9a8c\uff0c\u5305\u62ec\u65e0\u76d1\u7763\u548c\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u56fe\u50cf\u914d\u51c6\u548c\u75c5\u53d8\u5206\u5272\uff0c\u5df2\u7ecf\u9a8c\u8bc1\u4e86\u5207\u7247\u5668\u7f51\u7edc\u63d0\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002|[2401.09833v1](http://arxiv.org/pdf/2401.09833v1)|null|\n", "2401.09828": "|**2024-01-18**|**Enhanced Automated Quality Assessment Network for Interactive Building Segmentation in High-Resolution Remote Sensing Imagery**|\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u4e2d\u4ea4\u4e92\u5f0f\u5efa\u7b51\u5206\u5272\u7684\u589e\u5f3a\u578b\u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u7f51\u7edc|Zhili Zhang, Xiangyun Hu, Jiabo Xu|In this research, we introduce the enhanced automated quality assessment network (IBS-AQSNet), an innovative solution for assessing the quality of interactive building segmentation within high-resolution remote sensing imagery. This is a new challenge in segmentation quality assessment, and our proposed IBS-AQSNet allievate this by identifying missed and mistaken segment areas. First of all, to acquire robust image features, our method combines a robust, pre-trained backbone with a lightweight counterpart for comprehensive feature extraction from imagery and segmentation results. These features are then fused through a simple combination of concatenation, convolution layers, and residual connections. Additionally, ISR-AQSNet incorporates a multi-scale differential quality assessment decoder, proficient in pinpointing areas where segmentation result is either missed or mistaken. Experiments on a newly-built EVLab-BGZ dataset, which includes over 39,198 buildings, demonstrate the superiority of the proposed method in automating segmentation quality assessment, thereby setting a new benchmark in the field.|\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u589e\u5f3a\u578b\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u7f51\u7edc\uff08IBS-AQSNet\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u4e2d\u4ea4\u4e92\u5f0f\u5efa\u7b51\u5206\u5272\u8d28\u91cf\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u662f\u5206\u5272\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u4e00\u4e2a\u65b0\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u7684 IBS-AQSNet \u901a\u8fc7\u8bc6\u522b\u9057\u6f0f\u548c\u9519\u8bef\u7684\u5206\u5272\u533a\u57df\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002\u9996\u5148\uff0c\u4e3a\u4e86\u83b7\u5f97\u9c81\u68d2\u7684\u56fe\u50cf\u7279\u5f81\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u9c81\u68d2\u7684\u3001\u9884\u5148\u8bad\u7ec3\u7684\u4e3b\u5e72\u4e0e\u8f7b\u91cf\u7ea7\u7684\u4e3b\u5e72\u76f8\u7ed3\u5408\uff0c\u4ee5\u4ece\u56fe\u50cf\u548c\u5206\u5272\u7ed3\u679c\u4e2d\u8fdb\u884c\u5168\u9762\u7684\u7279\u5f81\u63d0\u53d6\u3002\u7136\u540e\u901a\u8fc7\u4e32\u8054\u3001\u5377\u79ef\u5c42\u548c\u6b8b\u5dee\u8fde\u63a5\u7684\u7b80\u5355\u7ec4\u5408\u6765\u878d\u5408\u8fd9\u4e9b\u7279\u5f81\u3002\u6b64\u5916\uff0cISR-AQSNet \u8fd8\u91c7\u7528\u4e86\u591a\u5c3a\u5ea6\u5dee\u5206\u8d28\u91cf\u8bc4\u4f30\u89e3\u7801\u5668\uff0c\u80fd\u591f\u51c6\u786e\u5b9a\u4f4d\u5206\u5272\u7ed3\u679c\u4e22\u5931\u6216\u9519\u8bef\u7684\u533a\u57df\u3002\u5728\u65b0\u5efa\u7684 EVLab-BGZ \u6570\u636e\u96c6\uff08\u5305\u542b\u8d85\u8fc7 39,198 \u5ea7\u5efa\u7b51\u7269\uff09\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u81ea\u52a8\u5206\u5272\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u4ece\u800c\u5728\u8be5\u9886\u57df\u6811\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002|[2401.09828v1](http://arxiv.org/pdf/2401.09828v1)|null|\n", "2401.09826": "|**2024-01-18**|**Boosting Few-Shot Semantic Segmentation Via Segment Anything Model**|\u901a\u8fc7 Segment Anything \u6a21\u578b\u4fc3\u8fdb\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272|Chen-Bin Feng, Qi Lai, Kangdao Liu, Houcheng Su, Chi-Man Vong|In semantic segmentation, accurate prediction masks are crucial for downstream tasks such as medical image analysis and image editing. Due to the lack of annotated data, few-shot semantic segmentation (FSS) performs poorly in predicting masks with precise contours. Recently, we have noticed that the large foundation model segment anything model (SAM) performs well in processing detailed features. Inspired by SAM, we propose FSS-SAM to boost FSS methods by addressing the issue of inaccurate contour. The FSS-SAM is training-free. It works as a post-processing tool for any FSS methods and can improve the accuracy of predicted masks. Specifically, we use predicted masks from FSS methods to generate prompts and then use SAM to predict new masks. To avoid predicting wrong masks with SAM, we propose a prediction result selection (PRS) algorithm. The algorithm can remarkably decrease wrong predictions. Experiment results on public datasets show that our method is superior to base FSS methods in both quantitative and qualitative aspects.|\u5728\u8bed\u4e49\u5206\u5272\u4e2d\uff0c\u51c6\u786e\u7684\u9884\u6d4b\u63a9\u6a21\u5bf9\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u7f3a\u4e4f\u6ce8\u91ca\u6570\u636e\uff0c\u5c11\u955c\u5934\u8bed\u4e49\u5206\u5272\uff08FSS\uff09\u5728\u9884\u6d4b\u5177\u6709\u7cbe\u786e\u8f6e\u5ed3\u7684\u63a9\u6a21\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u6700\u8fd1\uff0c\u6211\u4eec\u6ce8\u610f\u5230\u5927\u578b\u57fa\u7840\u6a21\u578b\u5206\u6bb5\u4efb\u4f55\u6a21\u578b\uff08SAM\uff09\u5728\u5904\u7406\u7ec6\u8282\u7279\u5f81\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002\u53d7 SAM \u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa FSS-SAM\uff0c\u901a\u8fc7\u89e3\u51b3\u8f6e\u5ed3\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u6765\u589e\u5f3a FSS \u65b9\u6cd5\u3002 FSS-SAM \u65e0\u9700\u57f9\u8bad\u3002\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u4efb\u4f55 FSS \u65b9\u6cd5\u7684\u540e\u5904\u7406\u5de5\u5177\uff0c\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u63a9\u6a21\u7684\u51c6\u786e\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4f7f\u7528 FSS \u65b9\u6cd5\u9884\u6d4b\u7684\u63a9\u6a21\u6765\u751f\u6210\u63d0\u793a\uff0c\u7136\u540e\u4f7f\u7528 SAM \u9884\u6d4b\u65b0\u7684\u63a9\u6a21\u3002\u4e3a\u4e86\u907f\u514d\u4f7f\u7528 SAM \u9884\u6d4b\u9519\u8bef\u7684\u63a9\u6a21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u7ed3\u679c\u9009\u62e9\uff08PRS\uff09\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u53ef\u4ee5\u663e\u7740\u51cf\u5c11\u9519\u8bef\u9884\u6d4b\u3002\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u9762\u90fd\u4f18\u4e8e\u57fa\u672c FSS \u65b9\u6cd5\u3002|[2401.09826v1](http://arxiv.org/pdf/2401.09826v1)|null|\n", "2401.09823": "|**2024-01-18**|**Enhancing Small Object Encoding in Deep Neural Networks: Introducing Fast&Focused-Net with Volume-wise Dot Product Layer**|\u589e\u5f3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5c0f\u5bf9\u8c61\u7f16\u7801\uff1a\u5f15\u5165\u5177\u6709\u4f53\u79ef\u70b9\u79ef\u5c42\u7684 Fast&Focused-Net|Ali Tofik, Roy Partha Pratim|In this paper, we introduce Fast&Focused-Net, a novel deep neural network architecture tailored for efficiently encoding small objects into fixed-length feature vectors. Contrary to conventional Convolutional Neural Networks (CNNs), Fast&Focused-Net employs a series of our newly proposed layer, the Volume-wise Dot Product (VDP) layer, designed to address several inherent limitations of CNNs. Specifically, CNNs often exhibit a smaller effective receptive field than their theoretical counterparts, limiting their vision span. Additionally, the initial layers in CNNs produce low-dimensional feature vectors, presenting a bottleneck for subsequent learning. Lastly, the computational overhead of CNNs, particularly in capturing diverse image regions by parameter sharing, is significantly high. The VDP layer, at the heart of Fast&Focused-Net, aims to remedy these issues by efficiently covering the entire image patch information with reduced computational demand. Experimental results demonstrate the prowess of Fast&Focused-Net in a variety of applications. For small object classification tasks, our network outperformed state-of-the-art methods on datasets such as CIFAR-10, CIFAR-100, STL-10, SVHN-Cropped, and Fashion-MNIST. In the context of larger image classification, when combined with a transformer encoder (ViT), Fast&Focused-Net produced competitive results for OpenImages V6, ImageNet-1K, and Places365 datasets. Moreover, the same combination showcased unparalleled performance in text recognition tasks across SVT, IC15, SVTP, and HOST datasets. This paper presents the architecture, the underlying motivation, and extensive empirical evidence suggesting that Fast&Focused-Net is a promising direction for efficient and focused deep learning.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Fast&Focused-Net\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4e13\u4e3a\u5c06\u5c0f\u5bf9\u8c61\u6709\u6548\u7f16\u7801\u4e3a\u56fa\u5b9a\u957f\u5ea6\u7684\u7279\u5f81\u5411\u91cf\u800c\u8bbe\u8ba1\u3002\u4e0e\u4f20\u7edf\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u76f8\u53cd\uff0cFast&Focused-Net \u91c7\u7528\u4e86\u4e00\u7cfb\u5217\u6211\u4eec\u65b0\u63d0\u51fa\u7684\u5c42\uff0c\u5373\u4f53\u79ef\u70b9\u79ef (VDP) \u5c42\uff0c\u65e8\u5728\u89e3\u51b3 CNN \u7684\u51e0\u4e2a\u56fa\u6709\u5c40\u9650\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0cCNN \u901a\u5e38\u8868\u73b0\u51fa\u6bd4\u7406\u8bba\u5bf9\u5e94\u7269\u66f4\u5c0f\u7684\u6709\u6548\u611f\u53d7\u91ce\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u89c6\u91ce\u8303\u56f4\u3002\u6b64\u5916\uff0cCNN \u7684\u521d\u59cb\u5c42\u4f1a\u4ea7\u751f\u4f4e\u7ef4\u7279\u5f81\u5411\u91cf\uff0c\u4e3a\u540e\u7eed\u5b66\u4e60\u5e26\u6765\u74f6\u9888\u3002\u6700\u540e\uff0cCNN \u7684\u8ba1\u7b97\u5f00\u9500\uff08\u5c24\u5176\u662f\u901a\u8fc7\u53c2\u6570\u5171\u4eab\u6355\u83b7\u4e0d\u540c\u56fe\u50cf\u533a\u57df\uff09\u975e\u5e38\u9ad8\u3002 VDP \u5c42\u662f Fast&Focused-Net \u7684\u6838\u5fc3\uff0c\u65e8\u5728\u901a\u8fc7\u6709\u6548\u8986\u76d6\u6574\u4e2a\u56fe\u50cf\u5757\u4fe1\u606f\u5e76\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86 Fast&Focused-Net \u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u5f3a\u5927\u80fd\u529b\u3002\u5bf9\u4e8e\u5c0f\u5bf9\u8c61\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u7684\u7f51\u7edc\u5728 CIFAR-10\u3001CIFAR-100\u3001STL-10\u3001SVHN-Cropped \u548c Fashion-MNIST \u7b49\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u5728\u8f83\u5927\u56fe\u50cf\u5206\u7c7b\u7684\u80cc\u666f\u4e0b\uff0c\u5f53\u4e0e Transformer Encoder (ViT) \u7ed3\u5408\u4f7f\u7528\u65f6\uff0cFast&Focused-Net \u4e3a OpenImages V6\u3001ImageNet-1K \u548c Places365 \u6570\u636e\u96c6\u4ea7\u751f\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u76f8\u540c\u7684\u7ec4\u5408\u5728 SVT\u3001IC15\u3001SVTP \u548c HOST \u6570\u636e\u96c6\u7684\u6587\u672c\u8bc6\u522b\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u65e0\u4e0e\u4f26\u6bd4\u7684\u6027\u80fd\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u5176\u67b6\u6784\u3001\u6f5c\u5728\u52a8\u673a\u548c\u5e7f\u6cdb\u7684\u7ecf\u9a8c\u8bc1\u636e\uff0c\u8868\u660e Fast&Focused-Net \u662f\u9ad8\u6548\u4e14\u4e13\u6ce8\u7684\u6df1\u5ea6\u5b66\u4e60\u7684\u4e00\u4e2a\u6709\u524d\u9014\u7684\u65b9\u5411\u3002|[2401.09823v1](http://arxiv.org/pdf/2401.09823v1)|null|\n", "2401.09802": "|**2024-01-18**|**Multilingual Visual Speech Recognition with a Single Model by Learning with Discrete Visual Speech Units**|\u901a\u8fc7\u5b66\u4e60\u79bb\u6563\u89c6\u89c9\u8bed\u97f3\u5355\u5143\uff0c\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u8fdb\u884c\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b|Minsu Kim, Jeong Hun Yeo, Jeongsoo Choi, Se Jin Park, Yong Man Ro|This paper explores sentence-level Multilingual Visual Speech Recognition with a single model for the first time. As the massive multilingual modeling of visual data requires huge computational costs, we propose a novel strategy, processing with visual speech units. Motivated by the recent success of the audio speech unit, the proposed visual speech unit is obtained by discretizing the visual speech features extracted from the self-supervised visual speech model. To correctly capture multilingual visual speech, we first train the self-supervised visual speech model on 5,512 hours of multilingual audio-visual data. Through analysis, we verify that the visual speech units mainly contain viseme information while suppressing non-linguistic information. By using the visual speech units as the inputs of our system, we pre-train the model to predict corresponding text outputs on massive multilingual data constructed by merging several VSR databases. As both the inputs and outputs are discrete, we can greatly improve the training efficiency compared to the standard VSR training. Specifically, the input data size is reduced to 0.016% of the original video inputs. In order to complement the insufficient visual information in speech recognition, we apply curriculum learning where the inputs of the system begin with audio-visual speech units and gradually change to visual speech units. After pre-training, the model is finetuned on continuous features. We set new state-of-the-art multilingual VSR performances by achieving comparable performances to the previous language-specific VSR models, with a single trained model.|\u672c\u6587\u9996\u6b21\u63a2\u8ba8\u4e86\u5355\u4e00\u6a21\u578b\u7684\u53e5\u5b50\u7ea7\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\u3002\u7531\u4e8e\u89c6\u89c9\u6570\u636e\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u5efa\u6a21\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b56\u7565\uff0c\u5373\u4f7f\u7528\u89c6\u89c9\u8bed\u97f3\u5355\u5143\u8fdb\u884c\u5904\u7406\u3002\u53d7\u97f3\u9891\u8bed\u97f3\u5355\u5143\u6700\u8fd1\u6210\u529f\u7684\u542f\u53d1\uff0c\u6240\u63d0\u51fa\u7684\u89c6\u89c9\u8bed\u97f3\u5355\u5143\u662f\u901a\u8fc7\u79bb\u6563\u5316\u4ece\u81ea\u76d1\u7763\u89c6\u89c9\u8bed\u97f3\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u89c6\u89c9\u8bed\u97f3\u7279\u5f81\u6765\u83b7\u5f97\u7684\u3002\u4e3a\u4e86\u6b63\u786e\u6355\u83b7\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u97f3\uff0c\u6211\u4eec\u9996\u5148\u5728 5,512 \u5c0f\u65f6\u7684\u591a\u8bed\u8a00\u89c6\u542c\u6570\u636e\u4e0a\u8bad\u7ec3\u81ea\u76d1\u7763\u89c6\u89c9\u8bed\u97f3\u6a21\u578b\u3002\u901a\u8fc7\u5206\u6790\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u89c6\u89c9\u8bed\u97f3\u5355\u5143\u4e3b\u8981\u5305\u542b\u89c6\u4f4d\u4fe1\u606f\uff0c\u540c\u65f6\u6291\u5236\u975e\u8bed\u8a00\u4fe1\u606f\u3002\u901a\u8fc7\u4f7f\u7528\u89c6\u89c9\u8bed\u97f3\u5355\u5143\u4f5c\u4e3a\u7cfb\u7edf\u7684\u8f93\u5165\uff0c\u6211\u4eec\u5bf9\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u9884\u6d4b\u901a\u8fc7\u5408\u5e76\u591a\u4e2a VSR \u6570\u636e\u5e93\u6784\u5efa\u7684\u6d77\u91cf\u591a\u8bed\u8a00\u6570\u636e\u7684\u76f8\u5e94\u6587\u672c\u8f93\u51fa\u3002\u7531\u4e8e\u8f93\u5165\u548c\u8f93\u51fa\u90fd\u662f\u79bb\u6563\u7684\uff0c\u56e0\u6b64\u4e0e\u6807\u51c6 VSR \u8bad\u7ec3\u76f8\u6bd4\uff0c\u6211\u4eec\u53ef\u4ee5\u5927\u5927\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8f93\u5165\u6570\u636e\u5927\u5c0f\u51cf\u5c11\u5230\u539f\u59cb\u89c6\u9891\u8f93\u5165\u7684 0.016%\u3002\u4e3a\u4e86\u5f25\u8865\u8bed\u97f3\u8bc6\u522b\u4e2d\u89c6\u89c9\u4fe1\u606f\u7684\u4e0d\u8db3\uff0c\u6211\u4eec\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u7cfb\u7edf\u7684\u8f93\u5165\u4ece\u89c6\u542c\u8bed\u97f3\u5355\u5143\u5f00\u59cb\uff0c\u9010\u6e10\u8f6c\u53d8\u4e3a\u89c6\u89c9\u8bed\u97f3\u5355\u5143\u3002\u9884\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u9488\u5bf9\u8fde\u7eed\u7279\u5f81\u8fdb\u884c\u5fae\u8c03\u3002\u6211\u4eec\u901a\u8fc7\u5355\u4e00\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u4e86\u4e0e\u4e4b\u524d\u7684\u7279\u5b9a\u8bed\u8a00 VSR \u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4ece\u800c\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00 VSR \u6027\u80fd\u3002|[2401.09802v1](http://arxiv.org/pdf/2401.09802v1)|null|\n", "2401.09791": "|**2024-01-18**|**BreastRegNet: A Deep Learning Framework for Registration of Breast Faxitron and Histopathology Images**|BreastRegNet\uff1a\u7528\u4e8e\u6ce8\u518c\u4e73\u623f Faxitron \u548c\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6|Negar Golestani, Aihui Wang, Gregory R Bean, Mirabela Rusu|A standard treatment protocol for breast cancer entails administering neoadjuvant therapy followed by surgical removal of the tumor and surrounding tissue. Pathologists typically rely on cabinet X-ray radiographs, known as Faxitron, to examine the excised breast tissue and diagnose the extent of residual disease. However, accurately determining the location, size, and focality of residual cancer can be challenging, and incorrect assessments can lead to clinical consequences. The utilization of automated methods can improve the histopathology process, allowing pathologists to choose regions for sampling more effectively and precisely. Despite the recognized necessity, there are currently no such methods available. Training such automated detection models require accurate ground truth labels on ex-vivo radiology images, which can be acquired through registering Faxitron and histopathology images and mapping the extent of cancer from histopathology to x-ray images. This study introduces a deep learning-based image registration approach trained on mono-modal synthetic image pairs. The models were trained using data from 50 women who received neoadjuvant chemotherapy and underwent surgery. The results demonstrate that our method is faster and yields significantly lower average landmark error ($2.1\\pm1.96$ mm) over the state-of-the-art iterative ($4.43\\pm4.1$ mm) and deep learning ($4.02\\pm3.15$ mm) approaches. Improved performance of our approach in integrating radiology and pathology information facilitates generating large datasets, which allows training models for more accurate breast cancer detection.|\u4e73\u817a\u764c\u7684\u6807\u51c6\u6cbb\u7597\u65b9\u6848\u9700\u8981\u8fdb\u884c\u65b0\u8f85\u52a9\u6cbb\u7597\uff0c\u7136\u540e\u624b\u672f\u5207\u9664\u80bf\u7624\u548c\u5468\u56f4\u7ec4\u7ec7\u3002\u75c5\u7406\u5b66\u5bb6\u901a\u5e38\u4f9d\u9760\u67dc\u5f0f X \u5c04\u7ebf\u7167\u7247\uff08\u79f0\u4e3a Faxitron\uff09\u6765\u68c0\u67e5\u5207\u9664\u7684\u4e73\u817a\u7ec4\u7ec7\u5e76\u8bca\u65ad\u6b8b\u7559\u75be\u75c5\u7684\u7a0b\u5ea6\u3002\u7136\u800c\uff0c\u51c6\u786e\u786e\u5b9a\u6b8b\u7559\u764c\u75c7\u7684\u4f4d\u7f6e\u3001\u5927\u5c0f\u548c\u75c5\u7076\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u4e14\u4e0d\u6b63\u786e\u7684\u8bc4\u4f30\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e34\u5e8a\u540e\u679c\u3002\u81ea\u52a8\u5316\u65b9\u6cd5\u7684\u5229\u7528\u53ef\u4ee5\u6539\u5584\u7ec4\u7ec7\u75c5\u7406\u5b66\u8fc7\u7a0b\uff0c\u4f7f\u75c5\u7406\u5b66\u5bb6\u80fd\u591f\u66f4\u6709\u6548\u3001\u66f4\u7cbe\u786e\u5730\u9009\u62e9\u91c7\u6837\u533a\u57df\u3002\u5c3d\u7ba1\u8ba4\u8bc6\u5230\u5fc5\u8981\u6027\uff0c\u4f46\u76ee\u524d\u8fd8\u6ca1\u6709\u8fd9\u6837\u7684\u65b9\u6cd5\u53ef\u7528\u3002\u8bad\u7ec3\u6b64\u7c7b\u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u5728\u79bb\u4f53\u653e\u5c04\u5b66\u56fe\u50cf\u4e0a\u8fdb\u884c\u51c6\u786e\u7684\u5730\u9762\u5b9e\u51b5\u6807\u7b7e\uff0c\u8fd9\u4e9b\u6807\u7b7e\u53ef\u4ee5\u901a\u8fc7\u914d\u51c6 Faxitron \u548c\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5e76\u5c06\u764c\u75c7\u8303\u56f4\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u6620\u5c04\u5230 X \u5c04\u7ebf\u56fe\u50cf\u6765\u83b7\u53d6\u3002\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5355\u6a21\u6001\u5408\u6210\u56fe\u50cf\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u4e9b\u6a21\u578b\u4f7f\u7528 50 \u540d\u63a5\u53d7\u65b0\u8f85\u52a9\u5316\u7597\u5e76\u63a5\u53d7\u624b\u672f\u7684\u5973\u6027\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u8fed\u4ee3\uff08$4.43\\pm4.1$mm\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08$4.02\\pm3\uff09\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u66f4\u5feb\uff0c\u5e76\u4e14\u4ea7\u751f\u663e\u7740\u66f4\u4f4e\u7684\u5e73\u5747\u5730\u6807\u8bef\u5dee\uff08$2.1\\pm1.96$mm\uff09 .15$ mm) \u63a5\u8fd1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6574\u5408\u653e\u5c04\u5b66\u548c\u75c5\u7406\u5b66\u4fe1\u606f\u65b9\u9762\u7684\u6027\u80fd\u6539\u8fdb\u6709\u52a9\u4e8e\u751f\u6210\u5927\u578b\u6570\u636e\u96c6\uff0c\u4ece\u800c\u53ef\u4ee5\u8bad\u7ec3\u6a21\u578b\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u4e73\u817a\u764c\u68c0\u6d4b\u3002|[2401.09791v1](http://arxiv.org/pdf/2401.09791v1)|null|\n", "2401.09786": "|**2024-01-18**|**Adaptive Self-training Framework for Fine-grained Scene Graph Generation**|\u7528\u4e8e\u7ec6\u7c92\u5ea6\u573a\u666f\u56fe\u751f\u6210\u7684\u81ea\u9002\u5e94\u81ea\u8bad\u7ec3\u6846\u67b6|Kibum Kim, Kanghoon Yoon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park|Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is beneficial when adopting our proposed self-training framework to the state-of-the-art message-passing neural network (MPNN)-based SGG models. Our extensive experiments verify the effectiveness of ST-SGG on various SGG models, particularly in enhancing the performance on fine-grained predicate classes.|\u573a\u666f\u56fe\u751f\u6210\uff08SGG\uff09\u6a21\u578b\u5b58\u5728\u57fa\u51c6\u6570\u636e\u96c6\u7684\u56fa\u6709\u95ee\u9898\uff0c\u4f8b\u5982\u957f\u5c3e\u8c13\u8bcd\u5206\u5e03\u548c\u7f3a\u5931\u6ce8\u91ca\u95ee\u9898\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u5229\u7528\u672a\u6ce8\u91ca\u7684\u4e09\u5143\u7ec4\u6765\u7f13\u89e3 SGG \u7684\u957f\u5c3e\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SGG (ST-SGG) \u7684\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e3a\u672a\u6ce8\u91ca\u7684\u4e09\u5143\u7ec4\u5206\u914d\u4f2a\u6807\u7b7e\uff0c\u5e76\u57fa\u4e8e\u8be5\u4f2a\u6807\u7b7e\u8bad\u7ec3 SGG \u6a21\u578b\u3002\u867d\u7136\u56fe\u50cf\u8bc6\u522b\u7684\u81ea\u8bad\u7ec3\u5df2\u7ecf\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u5176\u56fa\u6709\u7684\u6027\u8d28\uff08\u4f8b\u5982\u8bed\u4e49\u6a21\u7cca\u6027\u548c\u8c13\u8bcd\u7c7b\u7684\u957f\u5c3e\u5206\u5e03\uff09\uff0c\u4e3a SGG \u4efb\u52a1\u8bbe\u8ba1\u81ea\u8bad\u7ec3\u6846\u67b6\u66f4\u5177\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4e3a SGG \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f2a\u6807\u8bb0\u6280\u672f\uff0c\u79f0\u4e3a\u7c7b\u7279\u5b9a\u52a8\u91cf\u81ea\u9002\u5e94\u9608\u503c\uff08CATM\uff09\uff0c\u5b83\u662f\u4e00\u4e2a\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u73b0\u6709\u7684 SGG \u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u56fe\u7ed3\u6784\u5b66\u4e60\u5668\uff08GSL\uff09\uff0c\u5f53\u5c06\u6211\u4eec\u63d0\u51fa\u7684\u81ea\u8bad\u7ec3\u6846\u67b6\u91c7\u7528\u5230\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc\uff08MPNN\uff09\u7684 SGG \u6a21\u578b\u65f6\uff0c\u5b83\u662f\u6709\u76ca\u7684\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 ST-SGG \u5728\u5404\u79cd SGG \u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u589e\u5f3a\u7ec6\u7c92\u5ea6\u8c13\u8bcd\u7c7b\u7684\u6027\u80fd\u65b9\u9762\u3002|[2401.09786v1](http://arxiv.org/pdf/2401.09786v1)|null|\n", "2401.09774": "|**2024-01-18**|**On the Audio Hallucinations in Large Audio-Video Language Models**|\u8bba\u5927\u578b\u97f3\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u542c|Taichi Nishimura, Shota Nakada, Masayoshi Kondo|Large audio-video language models can generate descriptions for both video and audio. However, they sometimes ignore audio content, producing audio descriptions solely reliant on visual information. This paper refers to this as audio hallucinations and analyzes them in large audio-video language models. We gather 1,000 sentences by inquiring about audio information and annotate them whether they contain hallucinations. If a sentence is hallucinated, we also categorize the type of hallucination. The results reveal that 332 sentences are hallucinated with distinct trends observed in nouns and verbs for each hallucination type. Based on this, we tackle a task of audio hallucination classification using pre-trained audio-text models in the zero-shot and fine-tuning settings. Our experimental results reveal that the zero-shot models achieve higher performance (52.2% in F1) than the random (40.3%) and the fine-tuning models achieve 87.9%, outperforming the zero-shot models.|\u5927\u578b\u97f3\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u89c6\u9891\u548c\u97f3\u9891\u7684\u63cf\u8ff0\u3002\u7136\u800c\uff0c\u4ed6\u4eec\u6709\u65f6\u4f1a\u5ffd\u7565\u97f3\u9891\u5185\u5bb9\uff0c\u4ea7\u751f\u4ec5\u4f9d\u8d56\u4e8e\u89c6\u89c9\u4fe1\u606f\u7684\u97f3\u9891\u63cf\u8ff0\u3002\u672c\u6587\u5c06\u5176\u79f0\u4e3a\u5e7b\u542c\uff0c\u5e76\u5728\u5927\u578b\u97f3\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u5bf9\u5176\u8fdb\u884c\u5206\u6790\u3002\u6211\u4eec\u901a\u8fc7\u67e5\u8be2\u97f3\u9891\u4fe1\u606f\u6536\u96c6\u4e86 1000 \u4e2a\u53e5\u5b50\uff0c\u5e76\u6ce8\u91ca\u5b83\u4eec\u662f\u5426\u5305\u542b\u5e7b\u89c9\u3002\u5982\u679c\u4e00\u4e2a\u53e5\u5b50\u662f\u5e7b\u89c9\u7684\uff0c\u6211\u4eec\u4e5f\u4f1a\u5bf9\u5e7b\u89c9\u7684\u7c7b\u578b\u8fdb\u884c\u5206\u7c7b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6709 332 \u4e2a\u53e5\u5b50\u51fa\u73b0\u5e7b\u89c9\uff0c\u6bcf\u79cd\u5e7b\u89c9\u7c7b\u578b\u7684\u540d\u8bcd\u548c\u52a8\u8bcd\u90fd\u6709\u4e0d\u540c\u7684\u8d8b\u52bf\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e2d\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u97f3\u9891\u6587\u672c\u6a21\u578b\u6765\u89e3\u51b3\u97f3\u9891\u5e7b\u89c9\u5206\u7c7b\u7684\u4efb\u52a1\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u96f6\u6837\u672c\u6a21\u578b\u5b9e\u73b0\u4e86\u6bd4\u968f\u673a\u6a21\u578b\uff0840.3\uff05\uff09\u66f4\u9ad8\u7684\u6027\u80fd\uff08F1 \u4e2d\u4e3a 52.2\uff05\uff09\uff0c\u800c\u5fae\u8c03\u6a21\u578b\u5b9e\u73b0\u4e86 87.9\uff05\uff0c\u4f18\u4e8e\u96f6\u6837\u672c\u6a21\u578b\u3002|[2401.09774v1](http://arxiv.org/pdf/2401.09774v1)|null|\n", "2401.09773": "|**2024-01-18**|**SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation**|SEINE\uff1a\u7528\u4e8e\u6838\u5b9e\u4f8b\u5206\u5272\u7684\u7ed3\u6784\u7f16\u7801\u548c\u4ea4\u4e92\u7f51\u7edc|Ye Zhang, Linghan Cai, Ziyue Wang, Yongbing Zhang|Nuclei instance segmentation in histopathological images is of great importance for biological analysis and cancer diagnosis but remains challenging for two reasons. (1) Similar visual presentation of intranuclear and extranuclear regions of chromophobe nuclei often causes under-segmentation, and (2) current methods lack the exploration of nuclei structure, resulting in fragmented instance predictions. To address these problems, this paper proposes a structure encoding and interaction network, termed SEINE, which develops the structure modeling scheme of nuclei and exploits the structure similarity between nuclei to improve the integrality of each segmented instance. Concretely, SEINE introduces a contour-based structure encoding (SE) that considers the correlation between nuclei structure and semantics, realizing a reasonable representation of the nuclei structure. Based on the encoding, we propose a structure-guided attention (SGA) that takes the clear nuclei as prototypes to enhance the structure learning for the fuzzy nuclei. To strengthen the structural learning ability, a semantic feature fusion (SFF) is presented to boost the semantic consistency of semantic and structure branches. Furthermore, a position enhancement (PE) method is applied to suppress incorrect nuclei boundary predictions. Extensive experiments demonstrate the superiority of our approaches, and SEINE achieves state-of-the-art (SOTA) performance on four datasets. The code is available at \\href{https://github.com/zhangye-zoe/SEINE}{https://github.com/zhangye-zoe/SEINE}.|\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u7684\u7ec6\u80de\u6838\u5b9e\u4f8b\u5206\u5272\u5bf9\u4e8e\u751f\u7269\u5206\u6790\u548c\u764c\u75c7\u8bca\u65ad\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u4e24\u4e2a\u539f\u56e0\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002 \uff081\uff09\u5acc\u8272\u6838\u7684\u6838\u5185\u548c\u6838\u5916\u533a\u57df\u7684\u76f8\u4f3c\u89c6\u89c9\u5448\u73b0\u901a\u5e38\u4f1a\u5bfc\u81f4\u5206\u5272\u4e0d\u8db3\uff0c\uff082\uff09\u5f53\u524d\u7684\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6838\u7ed3\u6784\u7684\u63a2\u7d22\uff0c\u5bfc\u81f4\u5b9e\u4f8b\u9884\u6d4b\u652f\u79bb\u7834\u788e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u7f16\u7801\u548c\u4ea4\u4e92\u7f51\u7edc\uff0c\u79f0\u4e3aSEINE\uff0c\u5b83\u5f00\u53d1\u4e86\u6838\u7684\u7ed3\u6784\u5efa\u6a21\u65b9\u6848\uff0c\u5e76\u5229\u7528\u6838\u4e4b\u95f4\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u6765\u63d0\u9ad8\u6bcf\u4e2a\u5206\u5272\u5b9e\u4f8b\u7684\u5b8c\u6574\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0cSEINE\u5f15\u5165\u4e86\u57fa\u4e8e\u8f6e\u5ed3\u7684\u7ed3\u6784\u7f16\u7801\uff08SE\uff09\uff0c\u8003\u8651\u4e86\u7ec6\u80de\u6838\u7ed3\u6784\u548c\u8bed\u4e49\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5b9e\u73b0\u4e86\u7ec6\u80de\u6838\u7ed3\u6784\u7684\u5408\u7406\u8868\u793a\u3002\u57fa\u4e8e\u7f16\u7801\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5f15\u5bfc\u6ce8\u610f\u529b\uff08SGA\uff09\uff0c\u4ee5\u6e05\u6670\u7684\u6838\u4e3a\u539f\u578b\u6765\u589e\u5f3a\u6a21\u7cca\u6838\u7684\u7ed3\u6784\u5b66\u4e60\u3002\u4e3a\u4e86\u589e\u5f3a\u7ed3\u6784\u5b66\u4e60\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u8bed\u4e49\u7279\u5f81\u878d\u5408\uff08SFF\uff09\u6765\u63d0\u9ad8\u8bed\u4e49\u548c\u7ed3\u6784\u5206\u652f\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u5e94\u7528\u4f4d\u7f6e\u589e\u5f3a\uff08PE\uff09\u65b9\u6cd5\u6765\u6291\u5236\u4e0d\u6b63\u786e\u7684\u6838\u8fb9\u754c\u9884\u6d4b\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0cSEINE \u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684 (SOTA) \u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728 \\href{https://github.com/zhangye-zoe/SEINE}{https://github.com/zhangye-zoe/SEINE} \u83b7\u53d6\u3002|[2401.09773v1](http://arxiv.org/pdf/2401.09773v1)|null|\n", "2401.09759": "|**2024-01-18**|**SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech Recognition**|SlideAVSR\uff1a\u7528\u4e8e\u89c6\u542c\u8bed\u97f3\u8bc6\u522b\u7684\u8bba\u6587\u8bb2\u89e3\u89c6\u9891\u6570\u636e\u96c6|Hao Wang, Shuhei Kurita, Shuichiro Shimizu, Daisuke Kawahara|Audio-visual speech recognition (AVSR) is a multimodal extension of automatic speech recognition (ASR), using video as a complement to audio. In AVSR, considerable efforts have been directed at datasets for facial features such as lip-readings, while they often fall short in evaluating the image comprehension capabilities in broader contexts. In this paper, we construct SlideAVSR, an AVSR dataset using scientific paper explanation videos. SlideAVSR provides a new benchmark where models transcribe speech utterances with texts on the slides on the presentation recordings. As technical terminologies that are frequent in paper explanations are notoriously challenging to transcribe without reference texts, our SlideAVSR dataset spotlights a new aspect of AVSR problems. As a simple yet effective baseline, we propose DocWhisper, an AVSR model that can refer to textual information from slides, and confirm its effectiveness on SlideAVSR.|\u89c6\u542c\u8bed\u97f3\u8bc6\u522b (AVSR) \u662f\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b (ASR) \u7684\u591a\u6a21\u5f0f\u6269\u5c55\uff0c\u4f7f\u7528\u89c6\u9891\u4f5c\u4e3a\u97f3\u9891\u7684\u8865\u5145\u3002\u5728 AVSR \u4e2d\uff0c\u4eba\u4eec\u5728\u5507\u8bfb\u7b49\u9762\u90e8\u7279\u5f81\u6570\u636e\u96c6\u4e0a\u6295\u5165\u4e86\u5927\u91cf\u7cbe\u529b\uff0c\u4f46\u5728\u8bc4\u4f30\u66f4\u5e7f\u6cdb\u7684\u80cc\u666f\u4e0b\u7684\u56fe\u50cf\u7406\u89e3\u80fd\u529b\u65b9\u9762\u5f80\u5f80\u5b58\u5728\u4e0d\u8db3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6784\u5efa\u4e86 SlideAVSR\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528\u79d1\u5b66\u8bba\u6587\u89e3\u91ca\u89c6\u9891\u7684 AVSR \u6570\u636e\u96c6\u3002 SlideAVSR \u63d0\u4f9b\u200b\u200b\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u6f14\u793a\u6587\u7a3f\u5f55\u97f3\u7684\u5e7b\u706f\u7247\u4e0a\u8f6c\u5f55\u8bed\u97f3\u548c\u6587\u672c\u3002\u7531\u4e8e\u8bba\u6587\u89e3\u91ca\u4e2d\u5e38\u89c1\u7684\u6280\u672f\u672f\u8bed\u5728\u6ca1\u6709\u53c2\u8003\u6587\u672c\u7684\u60c5\u51b5\u4e0b\u8f6c\u5f55\u8d77\u6765\u975e\u5e38\u56f0\u96be\uff0c\u56e0\u6b64\u6211\u4eec\u7684 SlideAVSR \u6570\u636e\u96c6\u7a81\u51fa\u4e86 AVSR \u95ee\u9898\u7684\u4e00\u4e2a\u65b0\u65b9\u9762\u3002\u4f5c\u4e3a\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u57fa\u7ebf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DocWhisper\uff0c\u4e00\u79cd AVSR \u6a21\u578b\uff0c\u53ef\u4ee5\u5f15\u7528\u5e7b\u706f\u7247\u4e2d\u7684\u6587\u672c\u4fe1\u606f\uff0c\u5e76\u786e\u8ba4\u5176\u5728 SlideAVSR \u4e0a\u7684\u6709\u6548\u6027\u3002|[2401.09759v1](http://arxiv.org/pdf/2401.09759v1)|null|\n", "2401.09732": "|**2024-01-18**|**Instance Brownian Bridge as Texts for Open-vocabulary Video Instance Segmentation**|\u5b9e\u4f8b\u5e03\u6717\u6865\u4f5c\u4e3a\u5f00\u653e\u8bcd\u6c47\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u7684\u6587\u672c|Zesen Cheng, Kehan Li, Hao Li, Peng Jin, Chang Liu, Xiawu Zheng, Rongrong Ji, Jie Chen|Temporally locating objects with arbitrary class texts is the primary pursuit of open-vocabulary Video Instance Segmentation (VIS). Because of the insufficient vocabulary of video data, previous methods leverage image-text pretraining model for recognizing object instances by separately aligning each frame and class texts, ignoring the correlation between frames. As a result, the separation breaks the instance movement context of videos, causing inferior alignment between video and text. To tackle this issue, we propose to link frame-level instance representations as a Brownian Bridge to model instance dynamics and align bridge-level instance representation to class texts for more precisely open-vocabulary VIS (BriVIS). Specifically, we build our system upon a frozen video segmentor to generate frame-level instance queries, and design Temporal Instance Resampler (TIR) to generate queries with temporal context from frame queries. To mold instance queries to follow Brownian bridge and accomplish alignment with class texts, we design Bridge-Text Alignment (BTA) to learn discriminative bridge-level representations of instances via contrastive objectives. Setting MinVIS as the basic video segmentor, BriVIS surpasses the Open-vocabulary SOTA (OV2Seg) by a clear margin. For example, on the challenging large-vocabulary VIS dataset (BURST), BriVIS achieves 7.43 mAP and exhibits 49.49% improvement compared to OV2Seg (4.97 mAP).|\u4e34\u65f6\u5b9a\u4f4d\u5177\u6709\u4efb\u610f\u7c7b\u6587\u672c\u7684\u5bf9\u8c61\u662f\u5f00\u653e\u8bcd\u6c47\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\uff08VIS\uff09\u7684\u4e3b\u8981\u8ffd\u6c42\u3002\u7531\u4e8e\u89c6\u9891\u6570\u636e\u7684\u8bcd\u6c47\u91cf\u4e0d\u8db3\uff0c\u4ee5\u524d\u7684\u65b9\u6cd5\u5229\u7528\u56fe\u50cf\u6587\u672c\u9884\u8bad\u7ec3\u6a21\u578b\u901a\u8fc7\u5355\u72ec\u5bf9\u9f50\u6bcf\u4e2a\u5e27\u548c\u7c7b\u6587\u672c\u6765\u8bc6\u522b\u5bf9\u8c61\u5b9e\u4f8b\uff0c\u5ffd\u7565\u5e27\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u7ed3\u679c\uff0c\u8fd9\u79cd\u5206\u79bb\u7834\u574f\u4e86\u89c6\u9891\u7684\u5b9e\u4f8b\u79fb\u52a8\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u89c6\u9891\u548c\u6587\u672c\u4e4b\u95f4\u7684\u5bf9\u9f50\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u5c06\u5e27\u7ea7\u5b9e\u4f8b\u8868\u793a\u4f5c\u4e3a\u5e03\u6717\u6865\u94fe\u63a5\u8d77\u6765\uff0c\u4ee5\u5bf9\u5b9e\u4f8b\u52a8\u6001\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5c06\u6865\u7ea7\u5b9e\u4f8b\u8868\u793a\u4e0e\u7c7b\u6587\u672c\u5bf9\u9f50\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u5f00\u653e\u8bcd\u6c47 VIS (BriVIS)\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5728\u51bb\u7ed3\u89c6\u9891\u5206\u6bb5\u5668\u4e0a\u6784\u5efa\u7cfb\u7edf\u4ee5\u751f\u6210\u5e27\u7ea7\u5b9e\u4f8b\u67e5\u8be2\uff0c\u5e76\u8bbe\u8ba1\u65f6\u6001\u5b9e\u4f8b\u91cd\u91c7\u6837\u5668\uff08TIR\uff09\u4ee5\u4ece\u5e27\u67e5\u8be2\u4e2d\u751f\u6210\u5177\u6709\u65f6\u6001\u4e0a\u4e0b\u6587\u7684\u67e5\u8be2\u3002\u4e3a\u4e86\u5851\u9020\u5b9e\u4f8b\u67e5\u8be2\u4ee5\u9075\u5faa\u5e03\u6717\u6865\u5e76\u5b9e\u73b0\u4e0e\u7c7b\u6587\u672c\u7684\u5bf9\u9f50\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u6865\u6587\u672c\u5bf9\u9f50\uff08BTA\uff09\u4ee5\u901a\u8fc7\u5bf9\u6bd4\u76ee\u6807\u5b66\u4e60\u5b9e\u4f8b\u7684\u5224\u522b\u6027\u6865\u7ea7\u8868\u793a\u3002\u4ee5 MinVIS \u4f5c\u4e3a\u57fa\u672c\u89c6\u9891\u5206\u5272\u5668\uff0cBriVIS \u660e\u663e\u8d85\u8d8a\u4e86 Open-vocabulary SOTA (OV2Seg)\u3002\u4f8b\u5982\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5927\u8bcd\u6c47\u91cf VIS \u6570\u636e\u96c6 (BURST) \u4e0a\uff0cBriVIS \u8fbe\u5230\u4e86 7.43 mAP\uff0c\u4e0e OV2Seg (4.97 mAP) \u76f8\u6bd4\uff0c\u63d0\u9ad8\u4e86 49.49%\u3002|[2401.09732v1](http://arxiv.org/pdf/2401.09732v1)|null|\n", "2401.09709": "|**2024-01-18**|**P2Seg: Pointly-supervised Segmentation via Mutual Distillation**|P2Seg\uff1a\u901a\u8fc7\u76f8\u4e92\u84b8\u998f\u8fdb\u884c\u70b9\u76d1\u7763\u5206\u5272|Zipeng Wang, Xuehui Yu, Xumeng Han, Wenwen Yu, Zhixun Huang, Jianbin Jiao, Zhenjun Han|Point-level Supervised Instance Segmentation (PSIS) aims to enhance the applicability and scalability of instance segmentation by utilizing low-cost yet instance-informative annotations. Existing PSIS methods usually rely on positional information to distinguish objects, but predicting precise boundaries remains challenging due to the lack of contour annotations. Nevertheless, weakly supervised semantic segmentation methods are proficient in utilizing intra-class feature consistency to capture the boundary contours of the same semantic regions. In this paper, we design a Mutual Distillation Module (MDM) to leverage the complementary strengths of both instance position and semantic information and achieve accurate instance-level object perception. The MDM consists of Semantic to Instance (S2I) and Instance to Semantic (I2S). S2I is guided by the precise boundaries of semantic regions to learn the association between annotated points and instance contours. I2S leverages discriminative relationships between instances to facilitate the differentiation of various objects within the semantic map. Extensive experiments substantiate the efficacy of MDM in fostering the synergy between instance and semantic information, consequently improving the quality of instance-level object representations. Our method achieves 55.7 mAP$_{50}$ and 17.6 mAP on the PASCAL VOC and MS COCO datasets, significantly outperforming recent PSIS methods and several box-supervised instance segmentation competitors.|\u70b9\u7ea7\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\uff08PSIS\uff09\u65e8\u5728\u901a\u8fc7\u5229\u7528\u4f4e\u6210\u672c\u4f46\u5b9e\u4f8b\u4fe1\u606f\u4e30\u5bcc\u7684\u6ce8\u91ca\u6765\u589e\u5f3a\u5b9e\u4f8b\u5206\u5272\u7684\u9002\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u7684 PSIS \u65b9\u6cd5\u901a\u5e38\u4f9d\u9760\u4f4d\u7f6e\u4fe1\u606f\u6765\u533a\u5206\u5bf9\u8c61\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u8f6e\u5ed3\u6ce8\u91ca\uff0c\u9884\u6d4b\u7cbe\u786e\u7684\u8fb9\u754c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u7136\u800c\uff0c\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u64c5\u957f\u5229\u7528\u7c7b\u5185\u7279\u5f81\u4e00\u81f4\u6027\u6765\u6355\u83b7\u76f8\u540c\u8bed\u4e49\u533a\u57df\u7684\u8fb9\u754c\u8f6e\u5ed3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u76f8\u4e92\u84b8\u998f\u6a21\u5757\uff08MDM\uff09\uff0c\u4ee5\u5229\u7528\u5b9e\u4f8b\u4f4d\u7f6e\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u5b9e\u73b0\u51c6\u786e\u7684\u5b9e\u4f8b\u7ea7\u5bf9\u8c61\u611f\u77e5\u3002 MDM \u7531\u8bed\u4e49\u5230\u5b9e\u4f8b (S2I) \u548c\u5b9e\u4f8b\u5230\u8bed\u4e49 (I2S) \u7ec4\u6210\u3002 S2I \u4ee5\u8bed\u4e49\u533a\u57df\u7684\u7cbe\u786e\u8fb9\u754c\u4e3a\u6307\u5bfc\u6765\u5b66\u4e60\u6ce8\u91ca\u70b9\u548c\u5b9e\u4f8b\u8f6e\u5ed3\u4e4b\u95f4\u7684\u5173\u8054\u3002 I2S \u5229\u7528\u5b9e\u4f8b\u4e4b\u95f4\u7684\u533a\u522b\u5173\u7cfb\u6765\u4fc3\u8fdb\u8bed\u4e49\u56fe\u4e2d\u5404\u79cd\u5bf9\u8c61\u7684\u533a\u5206\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\u4e86 MDM \u5728\u4fc3\u8fdb\u5b9e\u4f8b\u548c\u8bed\u4e49\u4fe1\u606f\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u65b9\u9762\u7684\u529f\u6548\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5b9e\u4f8b\u7ea7\u5bf9\u8c61\u8868\u793a\u7684\u8d28\u91cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 PASCAL VOC \u548c MS COCO \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 55.7 mAP$_{50}$ \u548c 17.6 mAP\uff0c\u663e\u7740\u4f18\u4e8e\u6700\u8fd1\u7684 PSIS \u65b9\u6cd5\u548c\u51e0\u4e2a\u6846\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u7ade\u4e89\u5bf9\u624b\u3002|[2401.09709v1](http://arxiv.org/pdf/2401.09709v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2401.10139": "|**2024-01-18**|**Model Compression Techniques in Biometrics Applications: A Survey**|\u751f\u7269\u8bc6\u522b\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff1a\u8c03\u67e5|Eduarda Caldeira, Pedro C. Neto, Marco Huber, Naser Damer, Ana F. Sequeira|The development of deep learning algorithms has extensively empowered humanity's task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.|\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u53d1\u5c55\u5e7f\u6cdb\u589e\u5f3a\u4e86\u4eba\u7c7b\u4efb\u52a1\u81ea\u52a8\u5316\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u6027\u80fd\u7684\u5de8\u5927\u6539\u8fdb\u4e0e\u5b83\u4eec\u4e0d\u65ad\u589e\u52a0\u7684\u590d\u6742\u6027\u9ad8\u5ea6\u76f8\u5173\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u901a\u5e38\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e2d\u7684\u4ee5\u4eba\u4e3a\u672c\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u7684\u6709\u7528\u6027\u3002\u8fd9\u5bfc\u81f4\u4e86\u538b\u7f29\u6280\u672f\u7684\u53d1\u5c55\uff0c\u8be5\u6280\u672f\u5927\u5927\u964d\u4f4e\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u800c\u4e0d\u4f1a\u663e\u7740\u964d\u4f4e\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5bf9\u751f\u7269\u8bc6\u522b\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff08\u5373\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u526a\u679d\uff09\u8fdb\u884c\u5168\u9762\u8c03\u67e5\uff0c\u7cfb\u7edf\u5316\u5f53\u524d\u6709\u5173\u8be5\u4e3b\u9898\u7684\u6587\u732e\u3002\u6211\u4eec\u5bf9\u8fd9\u4e9b\u6280\u672f\u7684\u6bd4\u8f83\u4ef7\u503c\u8fdb\u884c\u6279\u5224\u6027\u5206\u6790\uff0c\u91cd\u70b9\u5206\u6790\u5b83\u4eec\u7684\u4f18\u70b9\u548c\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u53ef\u80fd\u6539\u8fdb\u5f53\u524d\u65b9\u6cd5\u7684\u672a\u6765\u5de5\u4f5c\u65b9\u5411\u7684\u5efa\u8bae\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8ba8\u8bba\u548c\u5206\u6790\u4e86\u6a21\u578b\u504f\u5dee\u548c\u6a21\u578b\u538b\u7f29\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5f3a\u8c03\u4e86\u5728\u672a\u6765\u7684\u5de5\u4f5c\u4e2d\u5c06\u538b\u7f29\u7814\u7a76\u5bfc\u5411\u6a21\u578b\u516c\u5e73\u6027\u7684\u5fc5\u8981\u6027\u3002|[2401.10139v1](http://arxiv.org/pdf/2401.10139v1)|null|\n"}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2401.10232": "|**2024-01-18**|**ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions**|ParaHome\uff1a\u53c2\u6570\u5316\u65e5\u5e38\u5bb6\u5ead\u6d3b\u52a8\u4ee5\u5b9e\u73b0\u4eba\u673a\u4ea4\u4e92\u7684 3D \u751f\u6210\u6a21\u578b|Jeonghwan Kim, Jisoo Kim, Jeonghyeon Na, Hanbyul Joo|To enable machines to learn how humans interact with the physical world in our daily activities, it is crucial to provide rich data that encompasses the 3D motion of humans as well as the motion of objects in a learnable 3D representation. Ideally, this data should be collected in a natural setup, capturing the authentic dynamic 3D signals during human-object interactions. To address this challenge, we introduce the ParaHome system, designed to capture and parameterize dynamic 3D movements of humans and objects within a common home environment. Our system consists of a multi-view setup with 70 synchronized RGB cameras, as well as wearable motion capture devices equipped with an IMU-based body suit and hand motion capture gloves. By leveraging the ParaHome system, we collect a novel large-scale dataset of human-object interaction. Notably, our dataset offers key advancement over existing datasets in three main aspects: (1) capturing 3D body and dexterous hand manipulation motion alongside 3D object movement within a contextual home environment during natural activities; (2) encompassing human interaction with multiple objects in various episodic scenarios with corresponding descriptions in texts; (3) including articulated objects with multiple parts expressed with parameterized articulations. Building upon our dataset, we introduce new research tasks aimed at building a generative model for learning and synthesizing human-object interactions in a real-world room setting.|\u4e3a\u4e86\u4f7f\u673a\u5668\u80fd\u591f\u4e86\u89e3\u4eba\u7c7b\u5728\u65e5\u5e38\u6d3b\u52a8\u4e2d\u5982\u4f55\u4e0e\u7269\u7406\u4e16\u754c\u4ea4\u4e92\uff0c\u63d0\u4f9b\u5305\u542b\u4eba\u7c7b 3D \u8fd0\u52a8\u4ee5\u53ca\u53ef\u5b66\u4e60 3D \u8868\u793a\u5f62\u5f0f\u7684\u7269\u4f53\u8fd0\u52a8\u7684\u4e30\u5bcc\u6570\u636e\u81f3\u5173\u91cd\u8981\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u6570\u636e\u5e94\u8be5\u5728\u81ea\u7136\u8bbe\u7f6e\u4e2d\u6536\u96c6\uff0c\u5728\u4eba\u4e0e\u7269\u4f53\u4ea4\u4e92\u671f\u95f4\u6355\u83b7\u771f\u5b9e\u7684\u52a8\u6001 3D \u4fe1\u53f7\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 ParaHome \u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u65e8\u5728\u6355\u83b7\u548c\u53c2\u6570\u5316\u516c\u5171\u5bb6\u5ead\u73af\u5883\u4e2d\u4eba\u7c7b\u548c\u7269\u4f53\u7684\u52a8\u6001 3D \u8fd0\u52a8\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u7531\u5177\u6709 70 \u4e2a\u540c\u6b65 RGB \u6444\u50cf\u673a\u7684\u591a\u89c6\u56fe\u8bbe\u7f6e\u4ee5\u53ca\u914d\u5907\u57fa\u4e8e IMU \u7684\u8fde\u4f53\u8863\u548c\u624b\u90e8\u52a8\u4f5c\u6355\u6349\u624b\u5957\u7684\u53ef\u7a7f\u6234\u52a8\u4f5c\u6355\u6349\u8bbe\u5907\u7ec4\u6210\u3002\u901a\u8fc7\u5229\u7528 ParaHome \u7cfb\u7edf\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5927\u89c4\u6a21\u4eba\u673a\u4ea4\u4e92\u6570\u636e\u96c6\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u6570\u636e\u96c6\u5728\u4e09\u4e2a\u4e3b\u8981\u65b9\u9762\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u8fdb\u6b65\uff1a\uff081\uff09\u5728\u81ea\u7136\u6d3b\u52a8\u671f\u95f4\u6355\u6349 3D \u8eab\u4f53\u548c\u7075\u5de7\u7684\u624b\u90e8\u64cd\u4f5c\u8fd0\u52a8\u4ee5\u53ca\u5bb6\u5ead\u73af\u5883\u4e2d\u7684 3D \u5bf9\u8c61\u79fb\u52a8\uff1b \uff082\uff09\u5305\u542b\u4eba\u7c7b\u5728\u5404\u79cd\u60c5\u666f\u573a\u666f\u4e2d\u4e0e\u591a\u4e2a\u5bf9\u8c61\u7684\u4ea4\u4e92\uff0c\u5e76\u5728\u6587\u672c\u4e2d\u8fdb\u884c\u76f8\u5e94\u7684\u63cf\u8ff0\uff1b (3)\u5305\u62ec\u7528\u53c2\u6570\u5316\u5173\u8282\u8868\u8fbe\u7684\u5177\u6709\u591a\u4e2a\u90e8\u5206\u7684\u5173\u8282\u5bf9\u8c61\u3002\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65b0\u7684\u7814\u7a76\u4efb\u52a1\uff0c\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u623f\u95f4\u73af\u5883\u4e2d\u5b66\u4e60\u548c\u7efc\u5408\u4eba\u4e0e\u7269\u4f53\u7684\u4ea4\u4e92\u3002|[2401.10232v1](http://arxiv.org/pdf/2401.10232v1)|null|\n", "2401.10219": "|**2024-01-18**|**Edit One for All: Interactive Batch Image Editing**|\u7f16\u8f91\u4e00\u5e94\u4ff1\u5168\uff1a\u4ea4\u4e92\u5f0f\u6279\u91cf\u56fe\u50cf\u7f16\u8f91|Thao Nguyen, Utkarsh Ojha, Yuheng Li, Haotian Liu, Yong Jae Lee|In recent years, image editing has advanced remarkably. With increased human control, it is now possible to edit an image in a plethora of ways; from specifying in text what we want to change, to straight up dragging the contents of the image in an interactive point-based manner. However, most of the focus has remained on editing single images at a time. Whether and how we can simultaneously edit large batches of images has remained understudied. With the goal of minimizing human supervision in the editing process, this paper presents a novel method for interactive batch image editing using StyleGAN as the medium. Given an edit specified by users in an example image (e.g., make the face frontal), our method can automatically transfer that edit to other test images, so that regardless of their initial state (pose), they all arrive at the same final state (e.g., all facing front). Extensive experiments demonstrate that edits performed using our method have similar visual quality to existing single-image-editing methods, while having more visual consistency and saving significant time and human effort.|\u8fd1\u5e74\u6765\uff0c\u56fe\u50cf\u7f16\u8f91\u53d6\u5f97\u4e86\u663e\u7740\u8fdb\u6b65\u3002\u968f\u7740\u4eba\u7c7b\u63a7\u5236\u529b\u7684\u589e\u5f3a\uff0c\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7f16\u8f91\u56fe\u50cf\uff1b\u4ece\u5728\u6587\u672c\u4e2d\u6307\u5b9a\u6211\u4eec\u60f3\u8981\u66f4\u6539\u7684\u5185\u5bb9\uff0c\u5230\u4ee5\u57fa\u4e8e\u70b9\u7684\u4ea4\u4e92\u5f0f\u65b9\u5f0f\u76f4\u63a5\u62d6\u52a8\u56fe\u50cf\u7684\u5185\u5bb9\u3002\u7136\u800c\uff0c\u5927\u90e8\u5206\u6ce8\u610f\u529b\u4ecd\u7136\u96c6\u4e2d\u5728\u4e00\u6b21\u7f16\u8f91\u5355\u4e2a\u56fe\u50cf\u4e0a\u3002\u6211\u4eec\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u540c\u65f6\u7f16\u8f91\u5927\u6279\u91cf\u56fe\u50cf\u4ecd\u6709\u5f85\u7814\u7a76\u3002\u4e3a\u4e86\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u4eba\u5de5\u76d1\u7763\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528 StyleGAN \u4f5c\u4e3a\u5a92\u4ecb\u7684\u4ea4\u4e92\u5f0f\u6279\u91cf\u56fe\u50cf\u7f16\u8f91\u7684\u65b0\u65b9\u6cd5\u3002\u7ed9\u5b9a\u7528\u6237\u5728\u793a\u4f8b\u56fe\u50cf\u4e2d\u6307\u5b9a\u7684\u7f16\u8f91\uff08\u4f8b\u5982\uff0c\u5c06\u8138\u90e8\u8bbe\u4e3a\u6b63\u9762\uff09\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u81ea\u52a8\u5c06\u8be5\u7f16\u8f91\u8f6c\u79fb\u5230\u5176\u4ed6\u6d4b\u8bd5\u56fe\u50cf\uff0c\u4ee5\u4fbf\u65e0\u8bba\u5b83\u4eec\u7684\u521d\u59cb\u72b6\u6001\uff08\u59ff\u52bf\uff09\u5982\u4f55\uff0c\u5b83\u4eec\u90fd\u4f1a\u8fbe\u5230\u76f8\u540c\u7684\u6700\u7ec8\u72b6\u6001\uff08\u4f8b\u5982\uff0c\u5168\u90e8\u9762\u5411\u524d\u65b9\uff09\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u7684\u7f16\u8f91\u4e0e\u73b0\u6709\u7684\u5355\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5177\u6709\u76f8\u4f3c\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u7684\u89c6\u89c9\u4e00\u81f4\u6027\u5e76\u8282\u7701\u5927\u91cf\u65f6\u95f4\u548c\u4eba\u529b\u3002|[2401.10219v1](http://arxiv.org/pdf/2401.10219v1)|null|\n", "2401.10208": "|**2024-01-18**|**MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer**|MM-Interleaved\uff1a\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u540c\u6b65\u5668\u8fdb\u884c\u4ea4\u9519\u56fe\u50cf\u6587\u672c\u751f\u6210\u5efa\u6a21|Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, et.al.|Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions. Code and models are available at \\url{https://github.com/OpenGVLab/MM-Interleaved}.|\u5f00\u53d1\u4ea4\u9519\u56fe\u50cf\u6587\u672c\u6570\u636e\u7684\u751f\u6210\u6a21\u578b\u5177\u6709\u7814\u7a76\u548c\u5b9e\u7528\u4ef7\u503c\u3002\u5b83\u9700\u8981\u6a21\u578b\u6765\u7406\u89e3\u4ea4\u9519\u5e8f\u5217\u5e76\u968f\u540e\u751f\u6210\u56fe\u50cf\u548c\u6587\u672c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5c1d\u8bd5\u53d7\u5230\u56fa\u5b9a\u6570\u91cf\u7684\u89c6\u89c9\u6807\u8bb0\u65e0\u6cd5\u6709\u6548\u6355\u83b7\u56fe\u50cf\u7ec6\u8282\u7684\u95ee\u9898\u7684\u9650\u5236\uff0c\u8fd9\u5728\u591a\u56fe\u50cf\u573a\u666f\u4e2d\u5c24\u5176\u6210\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86 MM-Interleaved\uff0c\u4e00\u79cd\u7528\u4e8e\u4ea4\u9519\u56fe\u50cf\u6587\u672c\u6570\u636e\u7684\u7aef\u5230\u7aef\u751f\u6210\u6a21\u578b\u3002\u5b83\u5f15\u5165\u4e86\u591a\u5c3a\u5ea6\u548c\u591a\u56fe\u50cf\u7279\u5f81\u540c\u6b65\u5668\u6a21\u5757\uff0c\u5141\u8bb8\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u8bbf\u95ee\u5148\u524d\u4e0a\u4e0b\u6587\u4e2d\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7279\u5f81\u3002 MM-Interleaved \u5728\u914d\u5bf9\u548c\u4ea4\u9519\u56fe\u50cf\u6587\u672c\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u4e86\u7aef\u5230\u7aef\u9884\u8bad\u7ec3\u3002\u5b83\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u5f97\u5230\u8fdb\u4e00\u6b65\u589e\u5f3a\uff0c\u5176\u4e2d\u6a21\u578b\u63d0\u9ad8\u4e86\u9075\u5faa\u590d\u6742\u591a\u6a21\u6001\u6307\u4ee4\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86 MM-Interleaved \u5728\u9075\u5faa\u591a\u6a21\u5f0f\u6307\u4ee4\u8bc6\u522b\u89c6\u89c9\u7ec6\u8282\u4ee5\u53ca\u9075\u5faa\u6587\u672c\u548c\u89c6\u89c9\u6761\u4ef6\u751f\u6210\u4e00\u81f4\u56fe\u50cf\u65b9\u9762\u7684\u591a\u529f\u80fd\u6027\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 \\url{https://github.com/OpenGVLab/MM-Interleaved} \u83b7\u53d6\u3002|[2401.10208v1](http://arxiv.org/pdf/2401.10208v1)|null|\n", "2401.10150": "|**2024-01-18**|**Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation**|Motion-Zero\uff1a\u7528\u4e8e\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u7684\u96f6\u955c\u5934\u79fb\u52a8\u5bf9\u8c61\u63a7\u5236\u6846\u67b6|Changgu Chen, Junwei Shu, Lianggangxu Chen, Gaoqi He, Changbo Wang, Yang Li|Recent large-scale pre-trained diffusion models have demonstrated a powerful generative ability to produce high-quality videos from detailed text descriptions. However, exerting control over the motion of objects in videos generated by any video diffusion model is a challenging problem. In this paper, we propose a novel zero-shot moving object trajectory control framework, Motion-Zero, to enable a bounding-box-trajectories-controlled text-to-video diffusion model.To this end, an initial noise prior module is designed to provide a position-based prior to improve the stability of the appearance of the moving object and the accuracy of position. In addition, based on the attention map of the U-net, spatial constraints are directly applied to the denoising process of diffusion models, which further ensures the positional and spatial consistency of moving objects during the inference. Furthermore, temporal consistency is guaranteed with a proposed shift temporal attention mechanism. Our method can be flexibly applied to various state-of-the-art video diffusion models without any training process. Extensive experiments demonstrate our proposed method can control the motion trajectories of objects and generate high-quality videos.|\u6700\u8fd1\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5df2\u7ecf\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u53ef\u4ee5\u6839\u636e\u8be6\u7ec6\u7684\u6587\u672c\u63cf\u8ff0\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u3002\u7136\u800c\uff0c\u5bf9\u4efb\u4f55\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u89c6\u9891\u4e2d\u7684\u5bf9\u8c61\u8fd0\u52a8\u8fdb\u884c\u63a7\u5236\u90fd\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u6837\u672c\u79fb\u52a8\u7269\u4f53\u8f68\u8ff9\u63a7\u5236\u6846\u67b6Motion-Zero\uff0c\u4ee5\u5b9e\u73b0\u8fb9\u754c\u6846\u8f68\u8ff9\u63a7\u5236\u7684\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u3002\u4e3a\u6b64\uff0c\u8bbe\u8ba1\u4e86\u521d\u59cb\u566a\u58f0\u5148\u9a8c\u6a21\u5757\u63d0\u4f9b\u57fa\u4e8e\u4f4d\u7f6e\u7684\u5148\u9a8c\uff0c\u63d0\u9ad8\u8fd0\u52a8\u7269\u4f53\u5916\u89c2\u7684\u7a33\u5b9a\u6027\u548c\u4f4d\u7f6e\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u57fa\u4e8eU-net\u7684\u6ce8\u610f\u529b\u56fe\uff0c\u5c06\u7a7a\u95f4\u7ea6\u675f\u76f4\u63a5\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u8fdb\u4e00\u6b65\u4fdd\u8bc1\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fd0\u52a8\u7269\u4f53\u7684\u4f4d\u7f6e\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u63d0\u51fa\u7684\u8f6c\u79fb\u65f6\u95f4\u6ce8\u610f\u529b\u673a\u5236\u4fdd\u8bc1\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u7075\u6d3b\u5730\u5e94\u7528\u4e8e\u5404\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u8fc7\u7a0b\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u63a7\u5236\u7269\u4f53\u7684\u8fd0\u52a8\u8f68\u8ff9\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u3002|[2401.10150v1](http://arxiv.org/pdf/2401.10150v1)|null|\n", "2401.10061": "|**2024-01-18**|**DiffusionGPT: LLM-Driven Text-to-Image Generation System**|DiffusionGPT\uff1a\u6cd5\u5b66\u7855\u58eb\u9a71\u52a8\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7cfb\u7edf|Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, Shilei Wen|Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional performance across diverse domains. Moreover, we introduce Advantage Databases, where the Tree-of-Thought is enriched with human feedback, aligning the model selection process with human preferences. Through extensive experiments and comparisons, we demonstrate the effectiveness of DiffusionGPT, showcasing its potential for pushing the boundaries of image synthesis in diverse domains.|\u6269\u6563\u6a21\u578b\u4e3a\u56fe\u50cf\u751f\u6210\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5bfc\u81f4\u5f00\u6e90\u5e73\u53f0\u4e0a\u5171\u4eab\u7684\u9ad8\u8d28\u91cf\u6a21\u578b\u6fc0\u589e\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u4ecd\u7136\u5b58\u5728\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u901a\u5e38\u65e0\u6cd5\u5904\u7406\u4e0d\u540c\u7684\u8f93\u5165\uff0c\u6216\u8005\u4ec5\u9650\u4e8e\u5355\u4e00\u6a21\u578b\u7ed3\u679c\u3002\u76ee\u524d\u7684\u7edf\u4e00\u5c1d\u8bd5\u5f80\u5f80\u5206\u4e3a\u4e24\u4e2a\u6b63\u4ea4\u7684\u65b9\u9762\uff1ai\uff09\u5728\u8f93\u5165\u9636\u6bb5\u89e3\u6790\u591a\u6837\u5316\u7684\u63d0\u793a\uff1b ii) \u6fc0\u6d3b\u4e13\u5bb6\u6a21\u578b\u8fdb\u884c\u8f93\u51fa\u3002\u4e3a\u4e86\u7ed3\u5408\u4e24\u4e2a\u9886\u57df\u7684\u4f18\u70b9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DiffusionGPT\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u6210\u7cfb\u7edf\uff0c\u80fd\u591f\u65e0\u7f1d\u5730\u5bb9\u7eb3\u5404\u79cd\u7c7b\u578b\u7684\u63d0\u793a\u5e76\u96c6\u6210\u9886\u57df\u4e13\u5bb6\u6a21\u578b\u3002 DiffusionGPT \u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u4e3a\u5404\u79cd\u751f\u6210\u6a21\u578b\u6784\u5efa\u7279\u5b9a\u9886\u57df\u7684\u6811\u3002\u5f53\u63d0\u4f9b\u8f93\u5165\u65f6\uff0c\u6cd5\u5b66\u7855\u58eb\u4f1a\u89e3\u6790\u63d0\u793a\u5e76\u4f7f\u7528\u601d\u60f3\u6811\u6765\u6307\u5bfc\u9009\u62e9\u9002\u5f53\u7684\u6a21\u578b\uff0c\u4ece\u800c\u653e\u5bbd\u8f93\u5165\u7ea6\u675f\u5e76\u786e\u4fdd\u5728\u4e0d\u540c\u9886\u57df\u7684\u5353\u8d8a\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4f18\u52bf\u6570\u636e\u5e93\uff0c\u5176\u4e2d\u601d\u60f3\u6811\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u4e30\u5bcc\uff0c\u4f7f\u6a21\u578b\u9009\u62e9\u8fc7\u7a0b\u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\u548c\u6bd4\u8f83\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 DiffusionGPT \u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u9886\u57df\u7a81\u7834\u56fe\u50cf\u5408\u6210\u754c\u9650\u7684\u6f5c\u529b\u3002|[2401.10061v1](http://arxiv.org/pdf/2401.10061v1)|null|\n", "2401.09836": "|**2024-01-18**|**Exploring Latent Cross-Channel Embedding for Accurate 3D Human Pose Reconstruction in a Diffusion Framework**|\u63a2\u7d22\u6f5c\u5728\u8de8\u901a\u9053\u5d4c\u5165\uff0c\u4ee5\u5728\u6269\u6563\u6846\u67b6\u4e2d\u5b9e\u73b0\u51c6\u786e\u7684 3D \u4eba\u4f53\u59ff\u52bf\u91cd\u5efa|Junkun Jiang, Jie Chen|Monocular 3D human pose estimation poses significant challenges due to the inherent depth ambiguities that arise during the reprojection process from 2D to 3D. Conventional approaches that rely on estimating an over-fit projection matrix struggle to effectively address these challenges and often result in noisy outputs. Recent advancements in diffusion models have shown promise in incorporating structural priors to address reprojection ambiguities. However, there is still ample room for improvement as these methods often overlook the exploration of correlation between the 2D and 3D joint-level features. In this study, we propose a novel cross-channel embedding framework that aims to fully explore the correlation between joint-level features of 3D coordinates and their 2D projections. In addition, we introduce a context guidance mechanism to facilitate the propagation of joint graph attention across latent channels during the iterative diffusion process. To evaluate the effectiveness of our proposed method, we conduct experiments on two benchmark datasets, namely Human3.6M and MPI-INF-3DHP. Our results demonstrate a significant improvement in terms of reconstruction accuracy compared to state-of-the-art methods. The code for our method will be made available online for further reference.|\u7531\u4e8e\u4ece 2D \u5230 3D \u7684\u91cd\u6295\u5f71\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u56fa\u6709\u6df1\u5ea6\u6a21\u7cca\u6027\uff0c\u5355\u76ee 3D \u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u4f9d\u8d56\u4e8e\u4f30\u8ba1\u8fc7\u5ea6\u62df\u5408\u6295\u5f71\u77e9\u9635\u7684\u4f20\u7edf\u65b9\u6cd5\u5f88\u96be\u6709\u6548\u5730\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u5e76\u4e14\u5e38\u5e38\u4f1a\u5bfc\u81f4\u8f93\u51fa\u566a\u58f0\u3002\u6269\u6563\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u793a\u51fa\u5728\u7ed3\u5408\u7ed3\u6784\u5148\u9a8c\u6765\u89e3\u51b3\u91cd\u6295\u5f71\u6a21\u7cca\u6027\u65b9\u9762\u7684\u524d\u666f\u3002\u7136\u800c\uff0c\u4ecd\u7136\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u65b9\u6cd5\u7ecf\u5e38\u5ffd\u89c6 2D \u548c 3D \u8054\u5408\u7ea7\u7279\u5f81\u4e4b\u95f4\u76f8\u5173\u6027\u7684\u63a2\u7d22\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u901a\u9053\u5d4c\u5165\u6846\u67b6\uff0c\u65e8\u5728\u5145\u5206\u63a2\u7d22 3D \u5750\u6807\u7684\u8054\u5408\u7ea7\u7279\u5f81\u4e0e\u5176 2D \u6295\u5f71\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e0a\u4e0b\u6587\u5f15\u5bfc\u673a\u5236\uff0c\u4ee5\u4fc3\u8fdb\u8fed\u4ee3\u6269\u6563\u8fc7\u7a0b\u4e2d\u8054\u5408\u56fe\u6ce8\u610f\u529b\u8de8\u6f5c\u5728\u901a\u9053\u7684\u4f20\u64ad\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6 Human3.6M \u548c MPI-INF-3DHP \u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u91cd\u5efa\u7cbe\u5ea6\u6709\u4e86\u663e\u7740\u63d0\u9ad8\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u4ee3\u7801\u5c06\u5728\u7ebf\u63d0\u4f9b\u4ee5\u4f9b\u8fdb\u4e00\u6b65\u53c2\u8003\u3002|[2401.09836v1](http://arxiv.org/pdf/2401.09836v1)|null|\n", "2401.09794": "|**2024-01-18**|**Wavelet-Guided Acceleration of Text Inversion in Diffusion-Based Image Editing**|\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u7f16\u8f91\u4e2d\u6587\u672c\u53cd\u8f6c\u7684\u5c0f\u6ce2\u5f15\u5bfc\u52a0\u901f|Gwanhyeong Koo, Sunjae Yoon, Chang D. Yoo|In the field of image editing, Null-text Inversion (NTI) enables fine-grained editing while preserving the structure of the original image by optimizing null embeddings during the DDIM sampling process. However, the NTI process is time-consuming, taking more than two minutes per image. To address this, we introduce an innovative method that maintains the principles of the NTI while accelerating the image editing process. We propose the WaveOpt-Estimator, which determines the text optimization endpoint based on frequency characteristics. Utilizing wavelet transform analysis to identify the image's frequency characteristics, we can limit text optimization to specific timesteps during the DDIM sampling process. By adopting the Negative-Prompt Inversion (NPI) concept, a target prompt representing the original image serves as the initial text value for optimization. This approach maintains performance comparable to NTI while reducing the average editing time by over 80% compared to the NTI method. Our method presents a promising approach for efficient, high-quality image editing based on diffusion models.|\u5728\u56fe\u50cf\u7f16\u8f91\u9886\u57df\uff0c\u7a7a\u6587\u672c\u53cd\u8f6c\uff08NTI\uff09\u901a\u8fc7\u5728 DDIM \u91c7\u6837\u8fc7\u7a0b\u4e2d\u4f18\u5316\u7a7a\u5d4c\u5165\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u56fe\u50cf\u7684\u7ed3\u6784\u3002\u7136\u800c\uff0cNTI \u8fc7\u7a0b\u975e\u5e38\u8017\u65f6\uff0c\u6bcf\u5f20\u56fe\u50cf\u9700\u8981\u4e24\u5206\u949f\u4ee5\u4e0a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u52a0\u901f\u56fe\u50cf\u7f16\u8f91\u8fc7\u7a0b\u7684\u540c\u65f6\u4fdd\u6301\u4e86 NTI \u7684\u539f\u5219\u3002\u6211\u4eec\u63d0\u51fa\u4e86 WaveOpt-Estimator\uff0c\u5b83\u6839\u636e\u9891\u7387\u7279\u5f81\u786e\u5b9a\u6587\u672c\u4f18\u5316\u7aef\u70b9\u3002\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u5206\u6790\u6765\u8bc6\u522b\u56fe\u50cf\u7684\u9891\u7387\u7279\u5f81\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u6587\u672c\u4f18\u5316\u9650\u5236\u5728 DDIM \u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u7279\u5b9a\u65f6\u95f4\u6b65\u957f\u3002\u91c7\u7528\u8d1f\u63d0\u793a\u53cd\u8f6c\uff08NPI\uff09\u6982\u5ff5\uff0c\u4ee3\u8868\u539f\u59cb\u56fe\u50cf\u7684\u76ee\u6807\u63d0\u793a\u4f5c\u4e3a\u4f18\u5316\u7684\u521d\u59cb\u6587\u672c\u503c\u3002\u8fd9\u79cd\u65b9\u6cd5\u4fdd\u6301\u4e86\u4e0e NTI \u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e0e NTI \u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e73\u5747\u7f16\u8f91\u65f6\u95f4\u51cf\u5c11\u4e86 80% \u4ee5\u4e0a\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u56fe\u50cf\u7f16\u8f91\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002|[2401.09794v1](http://arxiv.org/pdf/2401.09794v1)|null|\n", "2401.09763": "|**2024-01-18**|**CLIP Model for Images to Textual Prompts Based on Top-k Neighbors**|\u57fa\u4e8eTop-k\u90bb\u5c45\u7684\u56fe\u50cf\u5230\u6587\u672c\u63d0\u793a\u7684CLIP\u6a21\u578b|Xin Zhang, Xin Zhang, YeMing Cai, Tianzhi Jia|Text-to-image synthesis, a subfield of multimodal generation, has gained significant attention in recent years. We propose a cost-effective approach for image-to-prompt generation that leverages generative models to generate textual prompts without the need for large amounts of annotated data. We divide our method into two stages: online stage and offline stage. We use a combination of the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system consists of two main parts: an offline task and an online task. Our method owns the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011 higher than Clip, Clip + KNN(top 10) respectively.|\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u662f\u591a\u6a21\u6001\u751f\u6210\u7684\u4e00\u4e2a\u5b50\u9886\u57df\uff0c\u8fd1\u5e74\u6765\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u56fe\u50cf\u5230\u63d0\u793a\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u6a21\u578b\u751f\u6210\u6587\u672c\u63d0\u793a\uff0c\u800c\u4e0d\u9700\u8981\u5927\u91cf\u6ce8\u91ca\u6570\u636e\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u5728\u7ebf\u9636\u6bb5\u548c\u79bb\u7ebf\u9636\u6bb5\u3002\u6211\u4eec\u4f7f\u7528 CLIP \u6a21\u578b\u548c K \u6700\u8fd1\u90bb\uff08KNN\uff09\u7b97\u6cd5\u7684\u7ec4\u5408\u3002\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u7531\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\u7ec4\u6210\uff1a\u79bb\u7ebf\u4efb\u52a1\u548c\u5728\u7ebf\u4efb\u52a1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8fd9\u4e9b\u6a21\u578b\u4e2d\u62e5\u6709\u6700\u9ad8\u7684\u5ea6\u91cf 0.612\uff0c\u5206\u522b\u6bd4 Clip\u3001Clip + KNN\uff08\u524d 10\uff09\u9ad8 0.013\u30010.055\u30010.011\u3002|[2401.09763v1](http://arxiv.org/pdf/2401.09763v1)|null|\n", "2401.09742": "|**2024-01-18**|**Image Translation as Diffusion Visual Programmers**|\u4f5c\u4e3a\u6269\u6563\u89c6\u89c9\u7a0b\u5e8f\u5458\u7684\u56fe\u50cf\u7ffb\u8bd1|Cheng Han, James C. Liang, Qifan Wang, Majid Rabbani, Sohail Dianat, Raghuveer Rao, Ying Nian Wu, Dongfang Liu|We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework. Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture, orchestrating a coherent sequence of visual programs (i.e., computer vision models) for various pro-symbolic steps, which span RoI identification, style transfer, and position manipulation, facilitating transparent and controllable image translation processes. Extensive experiments demonstrate DVP's remarkable performance, surpassing concurrent arts. This success can be attributed to several key features of DVP: First, DVP achieves condition-flexible translation via instance normalization, enabling the model to eliminate sensitivity caused by the manual guidance and optimally focus on textual descriptions for high-quality content generation. Second, the framework enhances in-context reasoning by deciphering intricate high-dimensional concepts in feature spaces into more accessible low-dimensional symbols (e.g., [Prompt], [RoI object]), allowing for localized, context-free editing while maintaining overall coherence. Last but not least, DVP improves systemic controllability and explainability by offering explicit symbolic representations at each programming stage, empowering users to intuitively interpret and modify results. Our research marks a substantial step towards harmonizing artificial image translation processes with cognitive intelligence, promising broader applications.|\u6211\u4eec\u4ecb\u7ecd\u4e86\u65b0\u9896\u7684\u6269\u6563\u89c6\u89c9\u7f16\u7a0b\u5668\uff08DVP\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u56fe\u50cf\u7ffb\u8bd1\u6846\u67b6\u3002\u6211\u4eec\u63d0\u51fa\u7684 DVP \u5728 GPT \u67b6\u6784\u4e2d\u65e0\u7f1d\u5d4c\u5165\u4e86\u6761\u4ef6\u7075\u6d3b\u7684\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u5404\u79cd\u4eb2\u7b26\u53f7\u6b65\u9aa4\u7f16\u6392\u4e86\u8fde\u8d2f\u7684\u89c6\u89c9\u7a0b\u5e8f\u5e8f\u5217\uff08\u5373\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff09\uff0c\u8fd9\u4e9b\u6b65\u9aa4\u6db5\u76d6\u4e86 RoI \u8bc6\u522b\u3001\u98ce\u683c\u8f6c\u79fb\u548c\u4f4d\u7f6e\u64cd\u7eb5\uff0c\u4ece\u800c\u4fc3\u8fdb\u900f\u660e\u4e14\u53ef\u63a7\u7684\u56fe\u50cf\u7ffb\u8bd1\u8fc7\u7a0b\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86DVP\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5e76\u884c\u827a\u672f\u3002\u8fd9\u4e00\u6210\u529f\u5f52\u529f\u4e8e DVP \u7684\u51e0\u4e2a\u5173\u952e\u7279\u6027\uff1a\u9996\u5148\uff0cDVP \u901a\u8fc7\u5b9e\u4f8b\u5f52\u4e00\u5316\u5b9e\u73b0\u4e86\u6761\u4ef6\u7075\u6d3b\u7684\u7ffb\u8bd1\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6d88\u9664\u4eba\u5de5\u6307\u5bfc\u5e26\u6765\u7684\u654f\u611f\u6027\uff0c\u5e76\u6700\u4f73\u5730\u5173\u6ce8\u6587\u672c\u63cf\u8ff0\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5185\u5bb9\u3002\u5176\u6b21\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u7279\u5f81\u7a7a\u95f4\u4e2d\u590d\u6742\u7684\u9ad8\u7ef4\u6982\u5ff5\u89e3\u8bfb\u4e3a\u66f4\u5bb9\u6613\u8bbf\u95ee\u7684\u4f4e\u7ef4\u7b26\u53f7\uff08\u4f8b\u5982\uff0c[Prompt]\u3001[RoI object]\uff09\u6765\u589e\u5f3a\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u5141\u8bb8\u672c\u5730\u5316\u3001\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u8fde\u8d2f\u6027\u3002\u6700\u540e\u4f46\u5e76\u975e\u6700\u4e0d\u91cd\u8981\u7684\u4e00\u70b9\u662f\uff0cDVP \u901a\u8fc7\u5728\u6bcf\u4e2a\u7f16\u7a0b\u9636\u6bb5\u63d0\u4f9b\u660e\u786e\u7684\u7b26\u53f7\u8868\u793a\u6765\u63d0\u9ad8\u7cfb\u7edf\u7684\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u76f4\u89c2\u5730\u89e3\u91ca\u548c\u4fee\u6539\u7ed3\u679c\u3002\u6211\u4eec\u7684\u7814\u7a76\u6807\u5fd7\u7740\u5728\u534f\u8c03\u4eba\u5de5\u56fe\u50cf\u7ffb\u8bd1\u8fc7\u7a0b\u4e0e\u8ba4\u77e5\u667a\u80fd\u65b9\u9762\u8fc8\u51fa\u4e86\u5b9e\u8d28\u6027\u7684\u4e00\u6b65\uff0c\u6709\u671b\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002|[2401.09742v1](http://arxiv.org/pdf/2401.09742v1)|null|\n", "2401.09671": "|**2024-01-18**|**Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach**|\u8fc8\u5411\u53ef\u8bc6\u522b\u7684\u65e0\u76d1\u7763\u9886\u57df\u7ffb\u8bd1\uff1a\u591a\u6837\u5316\u7684\u5206\u5e03\u5339\u914d\u65b9\u6cd5|Sagar Shrestha, Xiao Fu|Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism\" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysis shows that MPA is unlikely to exist, if multiple pairs of diverse cross-domain conditional distributions are matched by the learning function. Our theory leads to a UDT learner using distribution matching over auxiliary variable-induced subsets of the domains -- other than over the entire data domains as in the classical approaches. The proposed framework is the first to rigorously establish translation identifiability under reasonable UDT settings, to our best knowledge. Experiments corroborate with our theoretical claims.|\u65e0\u76d1\u7763\u57df\u7ffb\u8bd1\uff08UDT\uff09\u65e8\u5728\u627e\u5230\u5c06\u6837\u672c\u4ece\u4e00\u4e2a\u57df\uff08\u4f8b\u5982\u8349\u56fe\uff09\u8f6c\u6362\u5230\u53e6\u4e00\u4e2a\u57df\uff08\u4f8b\u5982\u7167\u7247\uff09\u800c\u4e0d\u6539\u53d8\u9ad8\u7ea7\u8bed\u4e49\uff08\u4e5f\u79f0\u4e3a\u201c\u5185\u5bb9\u201d\uff09\u7684\u51fd\u6570\u3002\u5e73\u79fb\u51fd\u6570\u901a\u5e38\u901a\u8fc7\u53d8\u6362\u540e\u7684\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u6982\u7387\u5206\u5e03\u5339\u914d\u6765\u5bfb\u6c42\u3002 CycleGAN \u53ef\u4ee5\u8bf4\u662f\u8fd9\u4e00\u9886\u57df\u4e2d\u6700\u5177\u4ee3\u8868\u6027\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u6587\u732e\u4e2d\u6ce8\u610f\u5230\uff0cCycleGAN \u53ca\u5176\u53d8\u4f53\u53ef\u80fd\u65e0\u6cd5\u8bc6\u522b\u6240\u9700\u7684\u7ffb\u8bd1\u529f\u80fd\u5e76\u4ea7\u751f\u5185\u5bb9\u4e0d\u4e00\u81f4\u7684\u7ffb\u8bd1\u3002\u8fd9\u79cd\u9650\u5236\u662f\u7531\u4e8e\u5b66\u4e60\u6807\u51c6\u7684\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u4e2d\u5b58\u5728\u591a\u4e2a\u7ffb\u8bd1\u51fd\u6570\uff08\u79f0\u4e3a\u201c\u6d4b\u91cf\u4fdd\u7559\u81ea\u540c\u6784\u201d\uff08MPA\uff09\uff09\u800c\u51fa\u73b0\u7684\u3002\u5c3d\u7ba1\u610f\u8bc6\u5230\u4e86\u6b64\u7c7b\u53ef\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u4f46\u89e3\u51b3\u65b9\u6848\u4ecd\u7136\u96be\u4ee5\u6349\u6478\u3002\u7814\u7a76\u6df1\u5165\u7814\u7a76\u4e86\u6838\u5fc3\u53ef\u8bc6\u522b\u6027\u67e5\u8be2\uff0c\u5e76\u5f15\u5165\u4e86 MPA \u6d88\u9664\u7406\u8bba\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u5982\u679c\u5b66\u4e60\u51fd\u6570\u5339\u914d\u591a\u5bf9\u4e0d\u540c\u7684\u8de8\u57df\u6761\u4ef6\u5206\u5e03\uff0c\u5219 MPA \u4e0d\u592a\u53ef\u80fd\u5b58\u5728\u3002\u6211\u4eec\u7684\u7406\u8bba\u5bfc\u81f4\u4f7f\u7528\u5206\u5e03\u7684 UDT \u5b66\u4e60\u5668\u5339\u914d\u8f85\u52a9\u53d8\u91cf\u5f15\u8d77\u7684\u57df\u5b50\u96c6\u2014\u2014\u800c\u4e0d\u662f\u7ecf\u5178\u65b9\u6cd5\u4e2d\u7684\u6574\u4e2a\u6570\u636e\u57df\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u662f\u7b2c\u4e00\u4e2a\u5728\u5408\u7406\u7684 UDT \u8bbe\u7f6e\u4e0b\u4e25\u683c\u5efa\u7acb\u7ffb\u8bd1\u53ef\u8bc6\u522b\u6027\u7684\u6846\u67b6\u3002\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684\u7406\u8bba\u4e3b\u5f20\u3002|[2401.09671v1](http://arxiv.org/pdf/2401.09671v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.10226": "|**2024-01-18**|**Towards Language-Driven Video Inpainting via Multimodal Large Language Models**|\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u8bed\u8a00\u9a71\u52a8\u7684\u89c6\u9891\u4fee\u590d|Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, et.al.|We introduce a new task -- language-driven video inpainting, which uses natural language instructions to guide the inpainting process. This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive. We present the Remove Objects from Videos by Instructions (ROVI) dataset, containing 5,650 videos and 9,091 inpainting results, to support training and evaluation for this task. We also propose a novel diffusion-based language-driven video inpainting framework, the first end-to-end baseline for this task, integrating Multimodal Large Language Models to understand and execute complex language-based inpainting requests effectively. Our comprehensive results showcase the dataset's versatility and the model's effectiveness in various language-instructed inpainting scenarios. We will make datasets, code, and models publicly available.|\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u4efb\u52a1\u2014\u2014\u8bed\u8a00\u9a71\u52a8\u7684\u89c6\u9891\u4fee\u590d\uff0c\u5b83\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6765\u6307\u5bfc\u4fee\u590d\u8fc7\u7a0b\u3002\u8fd9\u79cd\u65b9\u6cd5\u514b\u670d\u4e86\u4f20\u7edf\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4f20\u7edf\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u52a8\u6807\u8bb0\u7684\u4e8c\u8fdb\u5236\u63a9\u6a21\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u901a\u5e38\u662f\u4e4f\u5473\u4e14\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u6309\u6307\u4ee4\u4ece\u89c6\u9891\u4e2d\u5220\u9664\u5bf9\u8c61\u201d(ROVI) \u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 5,650 \u4e2a\u89c6\u9891\u548c 9,091 \u4e2a\u4fee\u590d\u7ed3\u679c\uff0c\u4ee5\u652f\u6301\u8be5\u4efb\u52a1\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u8a00\u9a71\u52a8\u7684\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u8fd9\u662f\u8be5\u4efb\u52a1\u7684\u7b2c\u4e00\u4e2a\u7aef\u5230\u7aef\u57fa\u7ebf\uff0c\u96c6\u6210\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u6709\u6548\u5730\u7406\u89e3\u548c\u6267\u884c\u590d\u6742\u7684\u57fa\u4e8e\u8bed\u8a00\u7684\u4fee\u590d\u8bf7\u6c42\u3002\u6211\u4eec\u7684\u7efc\u5408\u7ed3\u679c\u5c55\u793a\u4e86\u6570\u636e\u96c6\u7684\u591a\u529f\u80fd\u6027\u548c\u6a21\u578b\u5728\u5404\u79cd\u8bed\u8a00\u6307\u5bfc\u7684\u4fee\u590d\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5c06\u516c\u5f00\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u6a21\u578b\u3002|[2401.10226v1](http://arxiv.org/pdf/2401.10226v1)|null|\n", "2401.10011": "|**2024-01-18**|**CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly Supervised Text-based Person Re-Identification**|CPCL\uff1a\u7528\u4e8e\u5f31\u76d1\u7763\u7684\u57fa\u4e8e\u6587\u672c\u7684\u4eba\u5458\u91cd\u65b0\u8bc6\u522b\u7684\u8de8\u6a21\u6001\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60|Yanwei Zheng, Xinpeng Zhao, Chuanlin Lan, Xiaowei Zhang, Bowen Huang, Jibin Yang, Dongxiao Yu|Weakly supervised text-based person re-identification (TPRe-ID) seeks to retrieve images of a target person using textual descriptions, without relying on identity annotations and is more challenging and practical. The primary challenge is the intra-class differences, encompassing intra-modal feature variations and cross-modal semantic gaps. Prior works have focused on instance-level samples and ignored prototypical features of each person which are intrinsic and invariant. Toward this, we propose a Cross-Modal Prototypical Contrastive Learning (CPCL) method. In practice, the CPCL introduces the CLIP model to weakly supervised TPRe-ID for the first time, mapping visual and textual instances into a shared latent space. Subsequently, the proposed Prototypical Multi-modal Memory (PMM) module captures associations between heterogeneous modalities of image-text pairs belonging to the same person through the Hybrid Cross-modal Matching (HCM) module in a many-to-many mapping fashion. Moreover, the Outlier Pseudo Label Mining (OPLM) module further distinguishes valuable outlier samples from each modality, enhancing the creation of more reliable clusters by mining implicit relationships between image-text pairs. Experimental results demonstrate that our proposed CPCL attains state-of-the-art performance on all three public datasets, with a significant improvement of 11.58%, 8.77% and 5.25% in Rank@1 accuracy on CUHK-PEDES, ICFG-PEDES and RSTPReid datasets, respectively. The code is available at https://github.com/codeGallery24/CPCL.|\u57fa\u4e8e\u5f31\u76d1\u7763\u6587\u672c\u7684\u884c\u4eba\u91cd\u8bc6\u522b\uff08TPRe-ID\uff09\u5bfb\u6c42\u4f7f\u7528\u6587\u672c\u63cf\u8ff0\u6765\u68c0\u7d22\u76ee\u6807\u4eba\u7684\u56fe\u50cf\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u8eab\u4efd\u6ce8\u91ca\uff0c\u66f4\u5177\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u3002\u4e3b\u8981\u6311\u6218\u662f\u7c7b\u5185\u5dee\u5f02\uff0c\u5305\u62ec\u6a21\u6001\u5185\u7279\u5f81\u53d8\u5316\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u5dee\u8ddd\u3002\u5148\u524d\u7684\u5de5\u4f5c\u96c6\u4e2d\u4e8e\u5b9e\u4f8b\u7ea7\u6837\u672c\uff0c\u800c\u5ffd\u7565\u4e86\u6bcf\u4e2a\u4eba\u56fa\u6709\u4e14\u4e0d\u53d8\u7684\u539f\u578b\u7279\u5f81\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\uff08CPCL\uff09\u65b9\u6cd5\u3002\u5728\u5b9e\u8df5\u4e2d\uff0cCPCL \u9996\u6b21\u5c06 CLIP \u6a21\u578b\u5f15\u5165\u5f31\u76d1\u7763 TPRe-ID\uff0c\u5c06\u89c6\u89c9\u548c\u6587\u672c\u5b9e\u4f8b\u6620\u5c04\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u3002\u968f\u540e\uff0c\u6240\u63d0\u51fa\u7684\u539f\u578b\u591a\u6a21\u6001\u8bb0\u5fc6\uff08PMM\uff09\u6a21\u5757\u901a\u8fc7\u6df7\u5408\u8de8\u6a21\u6001\u5339\u914d\uff08HCM\uff09\u6a21\u5757\u4ee5\u591a\u5bf9\u591a\u6620\u5c04\u65b9\u5f0f\u6355\u83b7\u5c5e\u4e8e\u540c\u4e00\u4e2a\u4eba\u7684\u56fe\u50cf\u6587\u672c\u5bf9\u7684\u5f02\u6784\u6a21\u6001\u4e4b\u95f4\u7684\u5173\u8054\u3002\u6b64\u5916\uff0c\u5f02\u5e38\u503c\u4f2a\u6807\u7b7e\u6316\u6398\uff08OPLM\uff09\u6a21\u5757\u8fdb\u4e00\u6b65\u533a\u5206\u6bcf\u79cd\u6a21\u6001\u4e2d\u6709\u4ef7\u503c\u7684\u5f02\u5e38\u503c\u6837\u672c\uff0c\u901a\u8fc7\u6316\u6398\u56fe\u50cf-\u6587\u672c\u5bf9\u4e4b\u95f4\u7684\u9690\u5f0f\u5173\u7cfb\u6765\u589e\u5f3a\u66f4\u53ef\u9760\u805a\u7c7b\u7684\u521b\u5efa\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 CPCL \u5728\u6240\u6709\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728 CUHK-PEDES\u3001ICFG-PEDES \u548c RSTPReid \u4e0a\u7684 Rank@1 \u51c6\u786e\u7387\u663e\u7740\u63d0\u9ad8\u4e86 11.58%\u30018.77% \u548c 5.25%\u6570\u636e\u96c6\uff0c\u5206\u522b\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/codeGallery24/CPCL \u83b7\u53d6\u3002|[2401.10011v1](http://arxiv.org/pdf/2401.10011v1)|null|\n", "2401.10005": "|**2024-01-18**|**Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation**|\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u94fe\u548c\u89c6\u89c9\u95ee\u9898\u751f\u6210\u63a8\u8fdb\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b|Kohei Uehara, Nabarun Goswami, Hanqin Wang, Toshiaki Baba, Kohtaro Tanaka, Tomohiro Hashimoto, Kai Wang, Rei Ito, Takagi Naoya, Ryo Umagami, et.al.|The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of Large Multi-Modal Models (LMMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. Our method comprises the development of a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. We designed an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment. The model undergoes a three-stage training phase, starting with large-scale image-text alignment using a large-scale datasets, followed by instruction tuning, and fine-tuning with a focus on chain-of-thought reasoning. The results demonstrate a stride toward a more robust, accurate, and interpretable LMM, capable of reasoning explicitly and seeking information proactively when confronted with ambiguous visual input.|\u5bf9\u80fd\u591f\u89e3\u91ca\u548c\u63a8\u7406\u89c6\u89c9\u5185\u5bb9\u7684\u667a\u80fd\u7cfb\u7edf\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u9700\u8981\u5f00\u53d1\u4e0d\u4ec5\u51c6\u786e\u800c\u4e14\u5177\u6709\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u4f7f LMM \u80fd\u591f\u57fa\u4e8e\u89c6\u89c9\u5185\u5bb9\u548c\u6587\u672c\u6307\u4ee4\u8fdb\u884c\u663e\u5f0f\u63a8\u7406\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u4ee5\u63d0\u51fa\u95ee\u9898\u6765\u83b7\u53d6\u5fc5\u8981\u77e5\u8bc6\u7684\u7cfb\u7edf\uff0c\u4ece\u800c\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\u7684\u7a33\u5065\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u5f00\u53d1\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u65b0\u9896\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u4e0e\u63d0\u95ee\u673a\u5236\u76f8\u7ed3\u5408\u7684\u601d\u60f3\u94fe\u63a8\u7406\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a LMM\uff0c\u5b83\u5177\u6709\u5f88\u5f3a\u7684\u533a\u57df\u611f\u77e5\u80fd\u529b\uff0c\u53ef\u4ee5\u6ee1\u8db3\u56fe\u50cf\u6587\u672c\u5bf9\u9f50\u7684\u590d\u6742\u8981\u6c42\u3002\u8be5\u6a21\u578b\u7ecf\u5386\u4e86\u4e09\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\u9636\u6bb5\uff0c\u9996\u5148\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u5927\u89c4\u6a21\u56fe\u50cf\u6587\u672c\u5bf9\u9f50\uff0c\u7136\u540e\u8fdb\u884c\u6307\u4ee4\u8c03\u6574\uff0c\u6700\u540e\u8fdb\u884c\u4ee5\u601d\u60f3\u94fe\u63a8\u7406\u4e3a\u91cd\u70b9\u7684\u5fae\u8c03\u3002\u7ed3\u679c\u8868\u660e\uff0cLMM \u671d\u7740\u66f4\u7a33\u5065\u3001\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u89e3\u91ca\u7684\u65b9\u5411\u8fc8\u8fdb\u4e86\u4e00\u6b65\uff0c\u80fd\u591f\u5728\u9762\u5bf9\u6a21\u7cca\u7684\u89c6\u89c9\u8f93\u5165\u65f6\u8fdb\u884c\u660e\u786e\u7684\u63a8\u7406\u5e76\u4e3b\u52a8\u5bfb\u627e\u4fe1\u606f\u3002|[2401.10005v1](http://arxiv.org/pdf/2401.10005v1)|null|\n", "2401.09985": "|**2024-01-18**|**WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens**|WorldDreamer\uff1a\u901a\u8fc7\u9884\u6d4b\u5c4f\u853d\u4ee4\u724c\u5b9e\u73b0\u89c6\u9891\u751f\u6210\u7684\u901a\u7528\u4e16\u754c\u6a21\u578b|Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, Jiwen Lu|World models play a crucial role in understanding and predicting the dynamics of the world, which is essential for video generation. However, existing world models are confined to specific scenarios such as gaming or driving, limiting their ability to capture the complexity of general world dynamic environments. Therefore, we introduce WorldDreamer, a pioneering world model to foster a comprehensive comprehension of general world physics and motions, which significantly enhances the capabilities of video generation. Drawing inspiration from the success of large language models, WorldDreamer frames world modeling as an unsupervised visual sequence modeling challenge. This is achieved by mapping visual inputs to discrete tokens and predicting the masked ones. During this process, we incorporate multi-modal prompts to facilitate interaction within the world model. Our experiments show that WorldDreamer excels in generating videos across different scenarios, including natural scenes and driving environments. WorldDreamer showcases versatility in executing tasks such as text-to-video conversion, image-tovideo synthesis, and video editing. These results underscore WorldDreamer's effectiveness in capturing dynamic elements within diverse general world environments.|\u4e16\u754c\u6a21\u578b\u5728\u7406\u89e3\u548c\u9884\u6d4b\u4e16\u754c\u52a8\u6001\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u8fd9\u5bf9\u4e8e\u89c6\u9891\u751f\u6210\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4e16\u754c\u6a21\u578b\u4ec5\u9650\u4e8e\u6e38\u620f\u6216\u9a7e\u9a76\u7b49\u7279\u5b9a\u573a\u666f\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u6355\u6349\u4e00\u822c\u4e16\u754c\u52a8\u6001\u73af\u5883\u590d\u6742\u6027\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86WorldDreamer\uff0c\u8fd9\u662f\u4e00\u79cd\u5f00\u521b\u6027\u7684\u4e16\u754c\u6a21\u578b\uff0c\u65e8\u5728\u4fc3\u8fdb\u5bf9\u4e00\u822c\u4e16\u754c\u7269\u7406\u548c\u8fd0\u52a8\u7684\u5168\u9762\u7406\u89e3\uff0c\u4ece\u800c\u663e\u7740\u589e\u5f3a\u89c6\u9891\u751f\u6210\u7684\u80fd\u529b\u3002 WorldDreamer \u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u5c06\u4e16\u754c\u5efa\u6a21\u89c6\u4e3a\u65e0\u76d1\u7763\u7684\u89c6\u89c9\u5e8f\u5217\u5efa\u6a21\u6311\u6218\u3002\u8fd9\u662f\u901a\u8fc7\u5c06\u89c6\u89c9\u8f93\u5165\u6620\u5c04\u5230\u79bb\u6563\u6807\u8bb0\u5e76\u9884\u6d4b\u88ab\u5c4f\u853d\u7684\u6807\u8bb0\u6765\u5b9e\u73b0\u7684\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u591a\u6a21\u5f0f\u63d0\u793a\u6765\u4fc3\u8fdb\u4e16\u754c\u6a21\u578b\u5185\u7684\u4ea4\u4e92\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWorldDreamer \u64c5\u957f\u751f\u6210\u4e0d\u540c\u573a\u666f\u7684\u89c6\u9891\uff0c\u5305\u62ec\u81ea\u7136\u573a\u666f\u548c\u9a7e\u9a76\u73af\u5883\u3002 WorldDreamer \u5c55\u793a\u4e86\u6267\u884c\u6587\u672c\u5230\u89c6\u9891\u8f6c\u6362\u3001\u56fe\u50cf\u5230\u89c6\u9891\u5408\u6210\u548c\u89c6\u9891\u7f16\u8f91\u7b49\u4efb\u52a1\u7684\u591a\u529f\u80fd\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86 WorldDreamer \u5728\u6355\u6349\u4e0d\u540c\u7684\u4e00\u822c\u4e16\u754c\u73af\u5883\u4e2d\u7684\u52a8\u6001\u5143\u7d20\u65b9\u9762\u7684\u6709\u6548\u6027\u3002|[2401.09985v1](http://arxiv.org/pdf/2401.09985v1)|null|\n", "2401.09861": "|**2024-01-18**|**Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models**|\u65f6\u95f4\u6d1e\u5bdf\u529b\u589e\u5f3a\uff1a\u51cf\u8f7b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u65f6\u95f4\u5e7b\u89c9|Li Sun, Liuan Wang, Jun Sun, Takayuki Okatani|Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrates a significant reduction in temporal hallucinations and an improvement in the quality of event-related responses. This research not only provides a new perspective in addressing a critical limitation of MLLMs but also contributes a quantitatively measurable method for evaluating MLLMs in the context of temporal-related questions.|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u6700\u65b0\u8fdb\u5c55\u663e\u7740\u589e\u5f3a\u4e86\u5bf9\u591a\u5a92\u4f53\u5185\u5bb9\u7684\u7406\u89e3\uff0c\u5c06\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u7b49\u591a\u79cd\u6a21\u6001\u7ed3\u5408\u5728\u4e00\u8d77\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u9762\u4e34\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u89c6\u9891\u8f93\u5165\u65f6\uff0c\u662f\u5e7b\u89c9\u7684\u53d1\u751f\u2014\u2014\u9519\u8bef\u7684\u611f\u77e5\u6216\u89e3\u91ca\uff0c\u7279\u522b\u662f\u5728\u4e8b\u4ef6\u5c42\u9762\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3 MLLM \u4e2d\u7684\u4e8b\u4ef6\u7ea7\u5e7b\u89c9\uff0c\u91cd\u70b9\u5173\u6ce8\u89c6\u9891\u5185\u5bb9\u4e2d\u7684\u7279\u5b9a\u65f6\u95f4\u7406\u89e3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4ece\u4e8b\u4ef6\u67e5\u8be2\u548c\u63d0\u4f9b\u7684\u89c6\u9891\u4e2d\u63d0\u53d6\u5e76\u5229\u7528\u7279\u5b9a\u4e8e\u4e8b\u4ef6\u7684\u4fe1\u606f\u6765\u5b8c\u5584 MLLM \u7684\u54cd\u5e94\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u72ec\u7279\u7684\u673a\u5236\uff0c\u53ef\u5c06\u6309\u9700\u4e8b\u4ef6\u67e5\u8be2\u5206\u89e3\u4e3a\u6807\u5fd7\u6027\u64cd\u4f5c\u3002\u968f\u540e\uff0c\u6211\u4eec\u4f7f\u7528 CLIP \u548c BLIP2 \u7b49\u6a21\u578b\u6765\u9884\u6d4b\u4e8b\u4ef6\u53d1\u751f\u7684\u7279\u5b9a\u65f6\u95f4\u6233\u3002\u6211\u4eec\u4f7f\u7528 Charades-STA \u6570\u636e\u96c6\u8fdb\u884c\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u65f6\u95f4\u5e7b\u89c9\u663e\u7740\u51cf\u5c11\uff0c\u4e8b\u4ef6\u76f8\u5173\u53cd\u5e94\u7684\u8d28\u91cf\u6709\u6240\u63d0\u9ad8\u3002\u8fd9\u9879\u7814\u7a76\u4e0d\u4ec5\u4e3a\u89e3\u51b3 MLLM \u7684\u5173\u952e\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u800c\u4e14\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u65f6\u95f4\u76f8\u5173\u95ee\u9898\u7684\u80cc\u666f\u4e0b\u8bc4\u4f30 MLLM \u7684\u5b9a\u91cf\u53ef\u6d4b\u91cf\u65b9\u6cd5\u3002|[2401.09861v1](http://arxiv.org/pdf/2401.09861v1)|null|\n", "2401.09712": "|**2024-01-18**|**SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model**|SkyEyeGPT\uff1a\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u8c03\u6574\u6765\u7edf\u4e00\u9065\u611f\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1|Yang Zhan, Zhitong Xiong, Yuan Yuan|Large language models (LLMs) have recently been extended to the vision-language realm, obtaining impressive general multi-modal capabilities. However, the exploration of multi-modal large language models (MLLMs) for remote sensing (RS) data is still in its infancy, and the performance is not satisfactory. In this work, we introduce SkyEyeGPT, a unified multi-modal large language model specifically designed for RS vision-language understanding. To this end, we meticulously curate an RS multi-modal instruction tuning dataset, including single-task and multi-task conversation instructions. After manual verification, we obtain a high-quality RS instruction-following dataset with 968k samples. Our research demonstrates that with a simple yet effective design, SkyEyeGPT works surprisingly well on considerably different tasks without the need for extra encoding modules. Specifically, after projecting RS visual features to the language domain via an alignment layer, they are fed jointly with task-specific instructions into an LLM-based RS decoder to predict answers for RS open-ended tasks. In addition, we design a two-stage tuning method to enhance instruction-following and multi-turn dialogue ability at different granularities. Experiments on 8 datasets for RS vision-language tasks demonstrate SkyEyeGPT's superiority in image-level and region-level tasks, such as captioning and visual grounding. In particular, SkyEyeGPT exhibits encouraging results compared to GPT-4V in some qualitative tests. The online demo, code, and dataset will be released in https://github.com/ZhanYang-nwpu/SkyEyeGPT.|\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6700\u8fd1\u5df2\u6269\u5c55\u5230\u89c6\u89c9\u8bed\u8a00\u9886\u57df\uff0c\u83b7\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u901a\u7528\u591a\u6a21\u5f0f\u529f\u80fd\u3002\u7136\u800c\uff0c\u9488\u5bf9\u9065\u611f\uff08RS\uff09\u6570\u636e\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u63a2\u7d22\u4ecd\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\uff0c\u6027\u80fd\u5e76\u4e0d\u4ee4\u4eba\u6ee1\u610f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 SkyEyeGPT\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a RS \u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u800c\u8bbe\u8ba1\u7684\u7edf\u4e00\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u7cbe\u5fc3\u7b56\u5212\u4e86 RS \u591a\u6a21\u6001\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u5305\u62ec\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u5bf9\u8bdd\u6307\u4ee4\u3002\u7ecf\u8fc7\u624b\u52a8\u9a8c\u8bc1\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u5305\u542b 968k \u6837\u672c\u7684\u9ad8\u8d28\u91cf RS \u6307\u4ee4\u8ddf\u8e2a\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7b80\u5355\u800c\u6709\u6548\u7684\u8bbe\u8ba1\uff0cSkyEyeGPT \u5728\u76f8\u5f53\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u5f97\u975e\u5e38\u597d\uff0c\u800c\u4e0d\u9700\u8981\u989d\u5916\u7684\u7f16\u7801\u6a21\u5757\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u901a\u8fc7\u5bf9\u9f50\u5c42\u5c06 RS \u89c6\u89c9\u7279\u5f81\u6295\u5f71\u5230\u8bed\u8a00\u57df\u540e\uff0c\u5b83\u4eec\u4e0e\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u6307\u4ee4\u8054\u5408\u8f93\u5165\u5230\u57fa\u4e8e LLM \u7684 RS \u89e3\u7801\u5668\u4e2d\uff0c\u4ee5\u9884\u6d4b RS \u5f00\u653e\u5f0f\u4efb\u52a1\u7684\u7b54\u6848\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8c03\u6574\u65b9\u6cd5\u6765\u589e\u5f3a\u4e0d\u540c\u7c92\u5ea6\u7684\u6307\u4ee4\u8ddf\u8e2a\u548c\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\u3002\u5728 RS \u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684 8 \u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86 SkyEyeGPT \u5728\u56fe\u50cf\u7ea7\u548c\u533a\u57df\u7ea7\u4efb\u52a1\uff08\u4f8b\u5982\u5b57\u5e55\u548c\u89c6\u89c9\u57fa\u7840\uff09\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002\u7279\u522b\u662f\uff0c\u4e0e GPT-4V \u76f8\u6bd4\uff0cSkyEyeGPT \u5728\u4e00\u4e9b\u5b9a\u6027\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u7ed3\u679c\u3002\u5728\u7ebf\u6f14\u793a\u3001\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728 https://github.com/ZhanYang-nwpu/SkyEyeGPT \u53d1\u5e03\u3002|[2401.09712v1](http://arxiv.org/pdf/2401.09712v1)|null|\n"}, "LLM": {}, "Transformer": {"2401.10222": "|**2024-01-18**|**Supervised Fine-tuning in turn Improves Visual Foundation Models**|\u6709\u76d1\u7763\u7684\u5fae\u8c03\u53cd\u8fc7\u6765\u6539\u8fdb\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b|Xiaohu Jiang, Yixiao Ge, Yuying Ge, Chun Yuan, Ying Shan|Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and vision-linguistic scenarios.|\u8fd1\u5e74\u6765\uff0c\u50cf CLIP \u8fd9\u6837\u7684\u56fe\u6587\u8bad\u7ec3\u5728\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4e2d\u5360\u636e\u4e86\u4e3b\u5bfc\u5730\u4f4d\u3002\u968f\u540e\u4eba\u4eec\u52aa\u529b\u5c06\u533a\u57df\u7ea7\u89c6\u89c9\u5b66\u4e60\u5f15\u5165 CLIP \u7684\u9884\u8bad\u7ec3\u4e2d\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u533a\u57df\u7ea7\u6570\u636e\u96c6\u800c\u9762\u4e34\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u53d7\u5230\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08\u4f8b\u5982\u6307\u4ee4\u8c03\u4f18\uff09\u4e2d\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u7ec6\u7c92\u5ea6 SFT \u5728\u589e\u5f3a\u9884\u8bad\u7ec3\u540e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u751f\u6210\u65b9\u9762\u7684\u6f5c\u529b\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5 ViSFT\uff08Vision SFT\uff09\u6765\u91ca\u653e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u3002\u5728 ViSFT \u4e2d\uff0c\u901a\u8fc7\u5bf9\u4e00\u4e9b\u57df\u5185\u4efb\u52a1\u6267\u884c\u89c6\u89c9\u8054\u5408\u5b66\u4e60\u6765\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u7136\u540e\u5728\u57df\u5916\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u901a\u8fc7\u5728 8 V100 GPU \u4e0a\u4f7f\u7528 ViSFT \u5728\u4e0d\u5230 2 \u5929\u5185\u8fdb\u884c\u66f4\u65b0\uff0c\u5177\u6709\u8d85\u8fc7 4.4B \u53c2\u6570\u7684\u89c6\u89c9\u8f6c\u6362\u5668\u5728\u5404\u79cd\u57df\u5916\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u89c6\u89c9\u548c\u89c6\u89c9\u8bed\u8a00\u573a\u666f\uff09\u4e0a\u663e\u793a\u51fa\u6539\u8fdb\u3002|[2401.10222v1](http://arxiv.org/pdf/2401.10222v1)|null|\n", "2401.10215": "|**2024-01-18**|**GPAvatar: Generalizable and Precise Head Avatar from Image(s)**|GPAvatar\uff1a\u6765\u81ea\u56fe\u50cf\u7684\u53ef\u6982\u62ec\u4e14\u7cbe\u786e\u7684\u5934\u50cf|Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, Lijian Lin, Yunfei Liu, Tatsuya Harada|Head avatar reconstruction, crucial for applications in virtual reality, online meetings, gaming, and film industries, has garnered substantial attention within the computer vision community. The fundamental objective of this field is to faithfully recreate the head avatar and precisely control expressions and postures. Existing methods, categorized into 2D-based warping, mesh-based, and neural rendering approaches, present challenges in maintaining multi-view consistency, incorporating non-facial information, and generalizing to new identities. In this paper, we propose a framework named GPAvatar that reconstructs 3D head avatars from one or several images in a single forward pass. The key idea of this work is to introduce a dynamic point-based expression field driven by a point cloud to precisely and effectively capture expressions. Furthermore, we use a Multi Tri-planes Attention (MTA) fusion module in the tri-planes canonical field to leverage information from multiple input images. The proposed method achieves faithful identity reconstruction, precise expression control, and multi-view consistency, demonstrating promising results for free-viewpoint rendering and novel view synthesis.|\u5934\u90e8\u5934\u50cf\u91cd\u5efa\u5bf9\u4e8e\u865a\u62df\u73b0\u5b9e\u3001\u5728\u7ebf\u4f1a\u8bae\u3001\u6e38\u620f\u548c\u7535\u5f71\u884c\u4e1a\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5df2\u7ecf\u5f15\u8d77\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u754c\u7684\u5e7f\u6cdb\u5173\u6ce8\u3002\u8be5\u9886\u57df\u7684\u6839\u672c\u76ee\u6807\u662f\u5fe0\u5b9e\u5730\u518d\u73b0\u5934\u90e8\u865a\u62df\u5f62\u8c61\u5e76\u7cbe\u786e\u63a7\u5236\u8868\u60c5\u548c\u59ff\u52bf\u3002\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u57fa\u4e8e 2D \u7684\u626d\u66f2\u3001\u57fa\u4e8e\u7f51\u683c\u548c\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u5728\u7ef4\u6301\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3001\u5408\u5e76\u975e\u9762\u90e8\u4fe1\u606f\u4ee5\u53ca\u63a8\u5e7f\u5230\u65b0\u8eab\u4efd\u65b9\u9762\u63d0\u51fa\u4e86\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a GPAvatar \u7684\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u6839\u636e\u4e00\u5f20\u6216\u591a\u5f20\u56fe\u50cf\u91cd\u5efa 3D \u5934\u90e8\u5934\u50cf\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u5173\u952e\u601d\u60f3\u662f\u5f15\u5165\u7531\u70b9\u4e91\u9a71\u52a8\u7684\u57fa\u4e8e\u70b9\u7684\u52a8\u6001\u8868\u60c5\u573a\uff0c\u4ee5\u7cbe\u786e\u6709\u6548\u5730\u6355\u83b7\u8868\u60c5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u4e09\u5e73\u9762\u89c4\u8303\u9886\u57df\u4e2d\u4f7f\u7528\u591a\u4e09\u5e73\u9762\u6ce8\u610f\u529b\uff08MTA\uff09\u878d\u5408\u6a21\u5757\u6765\u5229\u7528\u6765\u81ea\u591a\u4e2a\u8f93\u5165\u56fe\u50cf\u7684\u4fe1\u606f\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5fe0\u5b9e\u7684\u8eab\u4efd\u91cd\u5efa\u3001\u7cbe\u786e\u7684\u8868\u8fbe\u63a7\u5236\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\uff0c\u4e3a\u81ea\u7531\u89c6\u70b9\u6e32\u67d3\u548c\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u5c55\u793a\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002|[2401.10215v1](http://arxiv.org/pdf/2401.10215v1)|null|\n", "2401.10166": "|**2024-01-18**|**VMamba: Visual State Space Model**|VMamba\uff1a\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b|Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Yunfan Liu|Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as the two most popular foundation models for visual representation learning. While CNNs exhibit remarkable scalability with linear complexity w.r.t. image resolution, ViTs surpass them in fitting capabilities despite contending with quadratic complexity. A closer inspection reveals that ViTs achieve superior visual modeling performance through the incorporation of global receptive fields and dynamic weights. This observation motivates us to propose a novel architecture that inherits these components while enhancing computational efficiency. To this end, we draw inspiration from the recently introduced state space model and propose the Visual State Space Model (VMamba), which achieves linear complexity without sacrificing global receptive fields. To address the encountered direction-sensitive issue, we introduce the Cross-Scan Module (CSM) to traverse the spatial domain and convert any non-causal visual image into order patch sequences. Extensive experimental results substantiate that VMamba not only demonstrates promising capabilities across various visual perception tasks, but also exhibits more pronounced advantages over established benchmarks as the image resolution increases. Source code has been available at https://github.com/MzeroMiko/VMamba.|\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u662f\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u4e24\u79cd\u6700\u6d41\u884c\u7684\u57fa\u7840\u6a21\u578b\u3002\u867d\u7136 CNN \u8868\u73b0\u51fa\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u3002\u5c3d\u7ba1\u5728\u56fe\u50cf\u5206\u8fa8\u7387\u65b9\u9762\uff0cViT \u7684\u62df\u5408\u80fd\u529b\u8d85\u8fc7\u4e86\u5b83\u4eec\uff0c\u4f46\u5176\u590d\u6742\u5ea6\u5374\u662f\u4e8c\u6b21\u65b9\u3002\u4ed4\u7ec6\u89c2\u5bdf\u53d1\u73b0\uff0cViT \u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u611f\u53d7\u91ce\u548c\u52a8\u6001\u6743\u91cd\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u89c6\u89c9\u5efa\u6a21\u6027\u80fd\u3002\u8fd9\u4e00\u89c2\u5bdf\u4fc3\u4f7f\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u7ee7\u627f\u4e86\u8fd9\u4e9b\u7ec4\u4ef6\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u4ece\u6700\u8fd1\u5f15\u5165\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u63d0\u51fa\u4e86\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08VMamba\uff09\uff0c\u8be5\u6a21\u578b\u5728\u4e0d\u727a\u7272\u5168\u5c40\u611f\u53d7\u91ce\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u7ebf\u6027\u590d\u6742\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u6240\u9047\u5230\u7684\u65b9\u5411\u654f\u611f\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4ea4\u53c9\u626b\u63cf\u6a21\u5757\uff08CSM\uff09\u6765\u904d\u5386\u7a7a\u95f4\u57df\u5e76\u5c06\u4efb\u4f55\u975e\u56e0\u679c\u89c6\u89c9\u56fe\u50cf\u8f6c\u6362\u4e3a\u987a\u5e8f\u8865\u4e01\u5e8f\u5217\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\uff0cVMamba \u4e0d\u4ec5\u5728\u5404\u79cd\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u524d\u666f\u7684\u80fd\u529b\uff0c\u800c\u4e14\u968f\u7740\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u63d0\u9ad8\uff0c\u4e0e\u65e2\u5b9a\u57fa\u51c6\u76f8\u6bd4\u4e5f\u8868\u73b0\u51fa\u66f4\u660e\u663e\u7684\u4f18\u52bf\u3002\u6e90\u4ee3\u7801\u5df2\u5728 https://github.com/MzeroMiko/VMamba \u4e0a\u63d0\u4f9b\u3002|[2401.10166v1](http://arxiv.org/pdf/2401.10166v1)|null|\n", "2401.10148": "|**2024-01-18**|**Explicitly Disentangled Representations in Object-Centric Learning**|\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u4e2d\u7684\u663e\u5f0f\u89e3\u7f20\u8868\u793a|Riccardo Majellaro, Jonathan Collu, Aske Plaat, Thomas M. Moerland|Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal that our approach achieves the desired disentanglement while also numerically improving baseline performance in most cases. In addition, we show that our method can generate novel textures for a specific object or transfer textures between objects with distinct shapes.|\u4ece\u539f\u59cb\u89c6\u89c9\u6570\u636e\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u8868\u793a\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u4e00\u4e2a\u91cd\u8981\u4e14\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\u3002\u6700\u8fd1\uff0c\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8868\u793a\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6280\u672f\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5174\u8da3\u3002\u5728\u8fd9\u79cd\u80cc\u666f\u4e0b\uff0c\u589e\u5f3a\u6f5c\u5728\u7279\u5f81\u7684\u9c81\u68d2\u6027\u53ef\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u8bad\u7ec3\u7684\u6548\u7387\u548c\u6548\u679c\u3002\u671d\u8fd9\u4e2a\u65b9\u5411\u8fc8\u51fa\u7684\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u4e00\u6b65\u662f\u7406\u6e05\u5bfc\u81f4\u6570\u636e\u53d8\u5316\u7684\u56e0\u7d20\u3002\u6b64\u524d\uff0c\u4e0d\u53d8\u69fd\u6ce8\u610f\u529b\u5c06\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u548c\u65b9\u5411\u4e0e\u5176\u4f59\u7279\u5f81\u5206\u5f00\u3002\u6269\u5c55\u8fd9\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u4e13\u6ce8\u4e8e\u5206\u79bb\u5f62\u72b6\u548c\u7eb9\u7406\u7ec4\u4ef6\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u504f\u5411\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\uff0c\u5c06\u5f62\u72b6\u548c\u7eb9\u7406\u7ec4\u4ef6\u5206\u89e3\u4e3a\u6f5c\u5728\u7a7a\u95f4\u7ef4\u5ea6\u7684\u4e24\u4e2a\u4e0d\u91cd\u53e0\u7684\u5b50\u96c6\u3002\u8fd9\u4e9b\u5b50\u96c6\u662f\u5148\u9a8c\u5df2\u77e5\u7684\uff0c\u56e0\u6b64\u662f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e4b\u524d\u5df2\u77e5\u7684\u3002\u5bf9\u4e00\u7cfb\u5217\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6240\u9700\u7684\u89e3\u7f20\uff0c\u540c\u65f6\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8fd8\u4ece\u6570\u503c\u4e0a\u63d0\u9ad8\u4e86\u57fa\u7ebf\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u4e3a\u7279\u5b9a\u5bf9\u8c61\u751f\u6210\u65b0\u9896\u7684\u7eb9\u7406\u6216\u5728\u5177\u6709\u4e0d\u540c\u5f62\u72b6\u7684\u5bf9\u8c61\u4e4b\u95f4\u4f20\u8f93\u7eb9\u7406\u3002|[2401.10148v1](http://arxiv.org/pdf/2401.10148v1)|null|\n", "2401.10090": "|**2024-01-18**|**Cross-Modality Perturbation Synergy Attack for Person Re-identification**|\u7528\u4e8e\u4eba\u5458\u91cd\u65b0\u8bc6\u522b\u7684\u8de8\u6a21\u6001\u6270\u52a8\u534f\u540c\u653b\u51fb|Yunpeng Gong, others|In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images. However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention. The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities. For instance, infrared images are typically grayscale, unlike visible images that contain color information. Existing attack methods have primarily focused on the characteristics of the visible image modality, overlooking the features of other modalities and the variations in data distribution among different modalities. This oversight can potentially undermine the effectiveness of these methods in image retrieval across diverse modalities. This study represents the first exploration into the security of cross-modality ReID models and proposes a universal perturbation attack specifically designed for cross-modality ReID. This attack optimizes perturbations by leveraging gradients from diverse modality data, thereby disrupting the discriminator and reinforcing the differences between modalities. We conducted experiments on two widely used cross-modality datasets, namely RegDB and SYSU, which not only demonstrated the effectiveness of our method but also provided insights for future enhancements in the robustness of cross-modality ReID systems.|\u8fd1\u5e74\u6765\uff0c\u56f4\u7ed5\u89e3\u51b3\u57fa\u4e8e RGB \u56fe\u50cf\u7684\u5355\u6a21\u5f0f\u884c\u4eba\u91cd\u65b0\u8bc6\u522b (ReID) \u7cfb\u7edf\u7684\u5b89\u5168\u95ee\u9898\u5f00\u5c55\u4e86\u5927\u91cf\u7814\u7a76\u3002\u7136\u800c\uff0c\u5728\u6d89\u53ca\u7ea2\u5916\u76f8\u673a\u6355\u83b7\u56fe\u50cf\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5e38\u89c1\u7684\u8de8\u6a21\u6001\u573a\u666f\u7684\u5b89\u5168\u6027\u5c1a\u672a\u5f97\u5230\u8db3\u591f\u7684\u91cd\u89c6\u3002\u8de8\u6a21\u6001 ReID \u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u6709\u6548\u5904\u7406\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u89c6\u89c9\u5dee\u5f02\u3002\u4f8b\u5982\uff0c\u4e0e\u5305\u542b\u989c\u8272\u4fe1\u606f\u7684\u53ef\u89c1\u5149\u56fe\u50cf\u4e0d\u540c\uff0c\u7ea2\u5916\u56fe\u50cf\u901a\u5e38\u662f\u7070\u5ea6\u7684\u3002\u73b0\u6709\u7684\u653b\u51fb\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53ef\u89c1\u56fe\u50cf\u6a21\u6001\u7684\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u5176\u4ed6\u6a21\u6001\u7684\u7279\u5f81\u4ee5\u53ca\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u6570\u636e\u5206\u5e03\u7684\u53d8\u5316\u3002\u8fd9\u79cd\u758f\u5ffd\u53ef\u80fd\u4f1a\u7834\u574f\u8fd9\u4e9b\u65b9\u6cd5\u5728\u8de8\u591a\u79cd\u6a21\u5f0f\u7684\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u6709\u6548\u6027\u3002\u8fd9\u9879\u7814\u7a76\u4ee3\u8868\u4e86\u5bf9\u8de8\u6a21\u6001 ReID \u6a21\u578b\u5b89\u5168\u6027\u7684\u9996\u6b21\u63a2\u7d22\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u4e3a\u8de8\u6a21\u6001 ReID \u8bbe\u8ba1\u7684\u901a\u7528\u6270\u52a8\u653b\u51fb\u3002\u8fd9\u79cd\u653b\u51fb\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u6a21\u6001\u6570\u636e\u7684\u68af\u5ea6\u6765\u4f18\u5316\u6270\u52a8\uff0c\u4ece\u800c\u7834\u574f\u9274\u522b\u5668\u5e76\u5f3a\u5316\u6a21\u6001\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u8de8\u6a21\u6001\u6570\u636e\u96c6 RegDB \u548c SYSU \u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8fd9\u4e0d\u4ec5\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u800c\u4e14\u4e3a\u672a\u6765\u589e\u5f3a\u8de8\u6a21\u6001 ReID \u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002|[2401.10090v1](http://arxiv.org/pdf/2401.10090v1)|null|\n", "2401.09716": "|**2024-01-18**|**HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization**|HCVP\uff1a\u5229\u7528\u5206\u5c42\u5bf9\u6bd4\u89c6\u89c9\u63d0\u793a\u8fdb\u884c\u9886\u57df\u6cdb\u5316|Guanglin Zhou, Zhongyi Han, Shiming Chen, Biwei Huang, Liming Zhu, Tongliang Liu, Lina Yao, Kun Zhang|Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel \\textbf{H}ierarchical \\textbf{C}ontrastive \\textbf{V}isual \\textbf{P}rompt (HCVP) methodology. This represents a significant advancement in the field, setting itself apart with a unique generative approach to prompts, alongside an explicit model structure and specialized loss functions. Differing from traditional visual prompts that are often shared across entire datasets, HCVP utilizes a hierarchical prompt generation network enhanced by prompt contrastive learning. These generative prompts are instance-dependent, catering to the unique characteristics inherent to different domains and tasks. Additionally, we devise a prompt modulation network that serves as a bridge, effectively incorporating the generated visual prompts into the vision transformer backbone. Experiments conducted on five DG datasets demonstrate the effectiveness of HCVP, outperforming both established DG algorithms and adaptation protocols.|\u9886\u57df\u6cdb\u5316\uff08DG\uff09\u81f4\u529b\u4e8e\u901a\u8fc7\u5b66\u4e60\u4e0d\u53d8\u7279\u5f81\u6765\u521b\u5efa\u5728\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u5728 DG \u4e2d\uff0c\u5c06\u6a21\u578b\u7ea6\u675f\u4e3a\u56fa\u5b9a\u7ed3\u6784\u6216\u7edf\u4e00\u53c2\u6570\u5316\u4ee5\u5c01\u88c5\u4e0d\u53d8\u7279\u5f81\u7684\u666e\u904d\u505a\u6cd5\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u6df7\u5408\u7279\u5b9a\u65b9\u9762\u3002\u8fd9\u79cd\u65b9\u6cd5\u5f88\u96be\u533a\u5206\u57df\u95f4\u53d8\u5316\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u5e76\u4e14\u53ef\u80fd\u4f1a\u5bf9\u67d0\u4e9b\u57df\u8868\u73b0\u51fa\u504f\u89c1\uff0c\u4ece\u800c\u963b\u788d\u57df\u4e0d\u53d8\u7279\u5f81\u7684\u7cbe\u786e\u5b66\u4e60\u3002\u8ba4\u8bc6\u5230\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u8865\u5145\u6a21\u578b\u7684\u9886\u57df\u7ea7\u548c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u7279\u5f81\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e8\u5728\u6307\u5bfc\u6a21\u578b\u66f4\u6709\u6548\u5730\u5c06\u4e0d\u53d8\u7279\u5f81\u4e0e\u7279\u5b9a\u7279\u5f81\u5206\u5f00\uff0c\u4ece\u800c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u4e8e DG \u8303\u5f0f\u4e2d\u89c6\u89c9\u63d0\u793a\u7684\u65b0\u5174\u8d8b\u52bf\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u5f15\u5165\u4e86\u65b0\u9896\u7684 \\textbf{H}ierarchical \\textbf{C}contrastive \\textbf{V}isual \\textbf{P}rompt (HCVP) \u65b9\u6cd5\u3002\u8fd9\u4ee3\u8868\u4e86\u8be5\u9886\u57df\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u4ee5\u5176\u72ec\u7279\u7684\u751f\u6210\u63d0\u793a\u65b9\u6cd5\u3001\u660e\u786e\u7684\u6a21\u578b\u7ed3\u6784\u548c\u4e13\u95e8\u7684\u635f\u5931\u51fd\u6570\u800c\u8131\u9896\u800c\u51fa\u3002\u4e0e\u901a\u5e38\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e2d\u5171\u4eab\u7684\u4f20\u7edf\u89c6\u89c9\u63d0\u793a\u4e0d\u540c\uff0cHCVP \u5229\u7528\u901a\u8fc7\u63d0\u793a\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684\u5206\u5c42\u63d0\u793a\u751f\u6210\u7f51\u7edc\u3002\u8fd9\u4e9b\u751f\u6210\u63d0\u793a\u662f\u4f9d\u8d56\u4e8e\u5b9e\u4f8b\u7684\uff0c\u8fce\u5408\u4e0d\u540c\u9886\u57df\u548c\u4efb\u52a1\u56fa\u6709\u7684\u72ec\u7279\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63d0\u793a\u8c03\u5236\u7f51\u7edc\u4f5c\u4e3a\u6865\u6881\uff0c\u6709\u6548\u5730\u5c06\u751f\u6210\u7684\u89c6\u89c9\u63d0\u793a\u5408\u5e76\u5230\u89c6\u89c9\u53d8\u538b\u5668\u4e3b\u5e72\u4e2d\u3002\u5728\u4e94\u4e2a DG \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86 HCVP \u7684\u6709\u6548\u6027\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684 DG \u7b97\u6cd5\u548c\u9002\u5e94\u534f\u8bae\u3002|[2401.09716v1](http://arxiv.org/pdf/2401.09716v1)|null|\n"}, "Nerf": {}, "3DGS": {"2401.09720": "|**2024-01-18**|**GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting**|GaussianBody\uff1a\u901a\u8fc7 3d \u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u7a7f\u7740\u8863\u670d\u7684\u4eba\u4f53|Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang|In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction.|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e 3D Gaussian Splatting \u7684\u65b0\u578b\u670d\u88c5\u4eba\u4f53\u91cd\u5efa\u65b9\u6cd5\uff0c\u79f0\u4e3a GaussianBody\u3002\u4e0e\u6602\u8d35\u7684\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u7684\u6a21\u578b\u76f8\u6bd4\uff0c3D \u9ad8\u65af\u5206\u5e03\u6700\u8fd1\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u6e32\u67d3\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u7531\u4e8e\u590d\u6742\u7684\u975e\u521a\u6027\u53d8\u5f62\u548c\u4e30\u5bcc\u7684\u5e03\u6599\u7ec6\u8282\uff0c\u5c06\u9759\u6001 3D \u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\u5e94\u7528\u4e8e\u52a8\u6001\u4eba\u4f53\u91cd\u5efa\u95ee\u9898\u5e76\u975e\u6613\u4e8b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8003\u8651\u663e\u5f0f\u59ff\u52bf\u5f15\u5bfc\u53d8\u5f62\u6765\u5173\u8054\u89c4\u8303\u7a7a\u95f4\u548c\u89c2\u5bdf\u7a7a\u95f4\u4e2d\u7684\u52a8\u6001\u9ad8\u65af\uff0c\u5f15\u5165\u57fa\u4e8e\u7269\u7406\u7684\u5148\u9a8c\u548c\u6b63\u5219\u5316\u53d8\u6362\u6709\u52a9\u4e8e\u51cf\u8f7b\u4e24\u4e2a\u7a7a\u95f4\u4e4b\u95f4\u7684\u6a21\u7cca\u6027\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u59ff\u6001\u7ec6\u5316\u7b56\u7565\u6765\u66f4\u65b0\u59ff\u6001\u56de\u5f52\uff0c\u4ee5\u8865\u507f\u4e0d\u51c6\u786e\u7684\u521d\u59cb\u4f30\u8ba1\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5c3a\u5ea6\u5206\u5272\u673a\u5236\u6765\u589e\u5f3a\u56de\u5f52\u70b9\u4e91\u7684\u5bc6\u5ea6\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u771f\u5b9e\u611f\u5c0f\u8bf4\u89c6\u56fe\u6e32\u67d3\u7ed3\u679c\uff0c\u5177\u6709\u52a8\u6001\u670d\u88c5\u4eba\u4f53\u7684\u9ad8\u8d28\u91cf\u7ec6\u8282\uff0c\u4ee5\u53ca\u663e\u5f0f\u51e0\u4f55\u91cd\u5efa\u3002|[2401.09720v1](http://arxiv.org/pdf/2401.09720v1)|null|\n"}, "3D/CG": {"2401.10171": "|**2024-01-18**|**SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild**|SHINOBI\uff1a\u901a\u8fc7 BRDF \u4f18\u5316\u5728\u91ce\u5916\u4f7f\u7528\u795e\u7ecf\u5bf9\u8c61\u5206\u89e3\u7684\u5f62\u72b6\u548c\u7167\u660e|Andreas Engelhardt, Amit Raj, Mark Boss, Yunzhi Zhang, Abhishek Kar, Yuanzhen Li, Deqing Sun, Ricardo Martin Brualla, Jonathan T. Barron, Hendrik P. A. Lensch, et.al.|We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc. Project page: https://shinobi.aengelhardt.com Video: https://www.youtube.com/watch?v=iFENQ6AcYd8&feature=youtu.be|\u6211\u4eec\u63d0\u51fa\u4e86 SHINOBI\uff0c\u8fd9\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e\u4e0d\u540c\u5149\u7167\u3001\u59ff\u52bf\u548c\u80cc\u666f\u6355\u83b7\u7684\u7269\u4f53\u56fe\u50cf\u91cd\u5efa\u5f62\u72b6\u3001\u6750\u8d28\u548c\u5149\u7167\u3002\u57fa\u4e8e\u65e0\u7ea6\u675f\u56fe\u50cf\u96c6\u5408\u7684\u5bf9\u8c61\u7684\u9006\u6e32\u67d3\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u5f62\u9886\u57df\u7684\u4e00\u4e2a\u957f\u671f\u6311\u6218\uff0c\u9700\u8981\u5bf9\u5f62\u72b6\u3001\u8f90\u5c04\u5ea6\u548c\u59ff\u6001\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u57fa\u4e8e\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f16\u7801\u7684\u9690\u5f0f\u5f62\u72b6\u8868\u793a\u53ef\u4ee5\u901a\u8fc7\u8054\u5408\u76f8\u673a\u5bf9\u9f50\u4f18\u5316\u5b9e\u73b0\u66f4\u5feb\u3001\u66f4\u7a33\u5065\u7684\u5f62\u72b6\u91cd\u5efa\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5148\u524d\u7684\u5de5\u4f5c\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u80fd\u591f\u7f16\u8f91\u7167\u660e\u548c\u7269\u4f53\u53cd\u5c04\u7387\uff08\u5373\u6750\u8d28\uff09\uff0c\u6211\u4eec\u8054\u5408\u4f18\u5316 BRDF \u548c\u7167\u660e\u4ee5\u53ca\u7269\u4f53\u7684\u5f62\u72b6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u7c7b\u65e0\u5173\uff0c\u9002\u7528\u4e8e\u5bf9\u8c61\u7684\u91ce\u5916\u56fe\u50cf\u96c6\u5408\uff0c\u4e3a AR/VR\u3001\u7535\u5f71\u3001\u6e38\u620f\u7b49\u591a\u79cd\u7528\u4f8b\u751f\u6210\u53ef\u91cd\u65b0\u70b9\u4eae\u7684 3D \u8d44\u6e90\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://shinobi.aengelhardt\u3002 com \u89c6\u9891\uff1ahttps://www.youtube.com/watch?v=iFENQ6AcYd8&feature=youtu.be|[2401.10171v1](http://arxiv.org/pdf/2401.10171v1)|null|\n", "2401.09736": "|**2024-01-18**|**Measuring the Discrepancy between 3D Geometric Models using Directional Distance Fields**|\u4f7f\u7528\u5b9a\u5411\u8ddd\u79bb\u573a\u6d4b\u91cf 3D \u51e0\u4f55\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02|Siyu Ren, Junhui Hou, Xiaodong Chen, Hongkai Xiong, Wenping Wang|Qualifying the discrepancy between 3D geometric models, which could be represented with either point clouds or triangle meshes, is a pivotal issue with board applications. Existing methods mainly focus on directly establishing the correspondence between two models and then aggregating point-wise distance between corresponding points, resulting in them being either inefficient or ineffective. In this paper, we propose DirDist, an efficient, effective, robust, and differentiable distance metric for 3D geometry data. Specifically, we construct DirDist based on the proposed implicit representation of 3D models, namely directional distance field (DDF), which defines the directional distances of 3D points to a model to capture its local surface geometry. We then transfer the discrepancy between two 3D geometric models as the discrepancy between their DDFs defined on an identical domain, naturally establishing model correspondence. To demonstrate the advantage of our DirDist, we explore various distance metric-driven 3D geometric modeling tasks, including template surface fitting, rigid registration, non-rigid registration, scene flow estimation and human pose optimization. Extensive experiments show that our DirDist achieves significantly higher accuracy under all tasks. As a generic distance metric, DirDist has the potential to advance the field of 3D geometric modeling. The source code is available at \\url{https://github.com/rsy6318/DirDist}.|\u9a8c\u8bc1\u53ef\u4ee5\u7528\u70b9\u4e91\u6216\u4e09\u89d2\u5f62\u7f51\u683c\u8868\u793a\u7684 3D \u51e0\u4f55\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u662f\u7535\u8def\u677f\u5e94\u7528\u7684\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u76f4\u63a5\u5efa\u7acb\u4e24\u4e2a\u6a21\u578b\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u7136\u540e\u805a\u5408\u5bf9\u5e94\u70b9\u4e4b\u95f4\u7684\u9010\u70b9\u8ddd\u79bb\uff0c\u5bfc\u81f4\u5176\u6548\u7387\u4f4e\u4e0b\u6216\u65e0\u6548\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DirDist\uff0c\u4e00\u79cd\u9ad8\u6548\u3001\u6709\u6548\u3001\u9c81\u68d2\u4e14\u53ef\u5fae\u5206\u7684 3D \u51e0\u4f55\u6570\u636e\u8ddd\u79bb\u5ea6\u91cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u57fa\u4e8e\u6240\u63d0\u51fa\u7684 3D \u6a21\u578b\u9690\u5f0f\u8868\u793a\uff08\u5373\u65b9\u5411\u8ddd\u79bb\u573a\uff08DDF\uff09\uff09\u6784\u5efa DirDist\uff0c\u5b83\u5b9a\u4e49 3D \u70b9\u5230\u6a21\u578b\u7684\u65b9\u5411\u8ddd\u79bb\u4ee5\u6355\u83b7\u5176\u5c40\u90e8\u8868\u9762\u51e0\u4f55\u5f62\u72b6\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u4e24\u4e2a 3D \u51e0\u4f55\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u8f6c\u5316\u4e3a\u5728\u540c\u4e00\u57df\u4e0a\u5b9a\u4e49\u7684 DDF \u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u81ea\u7136\u5730\u5efa\u7acb\u6a21\u578b\u5bf9\u5e94\u5173\u7cfb\u3002\u4e3a\u4e86\u5c55\u793a DirDist \u7684\u4f18\u52bf\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u5404\u79cd\u8ddd\u79bb\u5ea6\u91cf\u9a71\u52a8\u7684 3D \u51e0\u4f55\u5efa\u6a21\u4efb\u52a1\uff0c\u5305\u62ec\u6a21\u677f\u66f2\u9762\u62df\u5408\u3001\u521a\u6027\u914d\u51c6\u3001\u975e\u521a\u6027\u914d\u51c6\u3001\u573a\u666f\u6d41\u4f30\u8ba1\u548c\u4eba\u4f53\u59ff\u52bf\u4f18\u5316\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 DirDist \u5728\u6240\u6709\u4efb\u52a1\u4e0b\u90fd\u5b9e\u73b0\u4e86\u663e\u7740\u66f4\u9ad8\u7684\u51c6\u786e\u5ea6\u3002\u4f5c\u4e3a\u901a\u7528\u8ddd\u79bb\u5ea6\u91cf\uff0cDirDist \u6709\u6f5c\u529b\u63a8\u52a8 3D \u51e0\u4f55\u5efa\u6a21\u9886\u57df\u7684\u53d1\u5c55\u3002\u6e90\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/rsy6318/DirDist} \u83b7\u53d6\u3002|[2401.09736v1](http://arxiv.org/pdf/2401.09736v1)|null|\n", "2401.09721": "|**2024-01-18**|**fast graph-based denoising for point cloud color information**|\u57fa\u4e8e\u56fe\u7684\u5feb\u901f\u70b9\u4e91\u989c\u8272\u4fe1\u606f\u53bb\u566a|Ryosuke Watanabe, Keisuke Nonaka, Eduardo Pavez, Tatsuya Kobayashi, Antonio Ortega|Point clouds are utilized in various 3D applications such as cross-reality (XR) and realistic 3D displays. In some applications, e.g., for live streaming using a 3D point cloud, real-time point cloud denoising methods are required to enhance the visual quality. However, conventional high-precision denoising methods cannot be executed in real time for large-scale point clouds owing to the complexity of graph constructions with K nearest neighbors and noise level estimation. This paper proposes a fast graph-based denoising (FGBD) for a large-scale point cloud. First, high-speed graph construction is achieved by scanning a point cloud in various directions and searching adjacent neighborhoods on the scanning lines. Second, we propose a fast noise level estimation method using eigenvalues of the covariance matrix on a graph. Finally, we also propose a new low-cost filter selection method to enhance denoising accuracy to compensate for the degradation caused by the acceleration algorithms. In our experiments, we succeeded in reducing the processing time dramatically while maintaining accuracy relative to conventional denoising methods. Denoising was performed at 30fps, with frames containing approximately 1 million points.|\u70b9\u4e91\u7528\u4e8e\u5404\u79cd 3D \u5e94\u7528\uff0c\u4f8b\u5982\u8de8\u73b0\u5b9e (XR) \u548c\u903c\u771f\u7684 3D \u663e\u793a\u3002\u5728\u67d0\u4e9b\u5e94\u7528\u4e2d\uff0c\u4f8b\u5982\uff0c\u5bf9\u4e8e\u4f7f\u7528 3D \u70b9\u4e91\u7684\u76f4\u64ad\uff0c\u9700\u8981\u5b9e\u65f6\u70b9\u4e91\u53bb\u566a\u65b9\u6cd5\u6765\u589e\u5f3a\u89c6\u89c9\u8d28\u91cf\u3002\u7136\u800c\uff0c\u7531\u4e8eK\u8fd1\u90bb\u56fe\u6784\u5efa\u548c\u566a\u58f0\u6c34\u5e73\u4f30\u8ba1\u7684\u590d\u6742\u6027\uff0c\u4f20\u7edf\u7684\u9ad8\u7cbe\u5ea6\u53bb\u566a\u65b9\u6cd5\u65e0\u6cd5\u5bf9\u5927\u89c4\u6a21\u70b9\u4e91\u5b9e\u65f6\u6267\u884c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u89c4\u6a21\u70b9\u4e91\u7684\u5feb\u901f\u57fa\u4e8e\u56fe\u7684\u53bb\u566a\uff08FGBD\uff09\u3002\u9996\u5148\uff0c\u901a\u8fc7\u5728\u5404\u4e2a\u65b9\u5411\u4e0a\u626b\u63cf\u70b9\u4e91\u5e76\u641c\u7d22\u626b\u63cf\u7ebf\u4e0a\u7684\u76f8\u90bb\u90bb\u57df\u6765\u5b9e\u73b0\u9ad8\u901f\u56fe\u5f62\u6784\u5efa\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u56fe\u4e0a\u534f\u65b9\u5dee\u77e9\u9635\u7684\u7279\u5f81\u503c\u7684\u5feb\u901f\u566a\u58f0\u6c34\u5e73\u4f30\u8ba1\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f4e\u6210\u672c\u6ee4\u6ce2\u5668\u9009\u62e9\u65b9\u6cd5\u6765\u63d0\u9ad8\u53bb\u566a\u7cbe\u5ea6\uff0c\u4ee5\u8865\u507f\u52a0\u901f\u7b97\u6cd5\u9020\u6210\u7684\u9000\u5316\u3002\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u6210\u529f\u5730\u663e\u7740\u51cf\u5c11\u4e86\u5904\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5bf9\u4e8e\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002\u53bb\u566a\u4ee5 30fps \u8fdb\u884c\uff0c\u5e27\u5305\u542b\u5927\u7ea6 100 \u4e07\u4e2a\u70b9\u3002|[2401.09721v1](http://arxiv.org/pdf/2401.09721v1)|null|\n", "2401.09677": "|**2024-01-18**|**Eye Motion Matters for 3D Face Reconstruction**|\u773c\u52a8\u5bf9\u4e8e 3D \u9762\u90e8\u91cd\u5efa\u5f88\u91cd\u8981|Xuan Wang, Mengyuan Liu|Recent advances in single-image 3D face reconstruction have shown remarkable progress in various applications. Nevertheless, prevailing techniques tend to prioritize the global facial contour and expression, often neglecting the nuanced dynamics of the eye region. In response, we introduce an Eye Landmark Adjustment Module, complemented by a Local Dynamic Loss, designed to capture the dynamic features of the eyes area. Our module allows for flexible adjustment of landmarks, resulting in accurate recreation of various eye states. In this paper, we present a comprehensive evaluation of our approach, conducting extensive experiments on two datasets. The results underscore the superior performance of our approach, highlighting its significant contributions in addressing this particular challenge.|\u5355\u56fe\u50cf 3D \u4eba\u8138\u91cd\u5efa\u7684\u6700\u65b0\u8fdb\u5c55\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u90fd\u663e\u793a\u51fa\u663e\u7740\u7684\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u6d41\u884c\u7684\u6280\u672f\u503e\u5411\u4e8e\u4f18\u5148\u8003\u8651\u6574\u4f53\u9762\u90e8\u8f6e\u5ed3\u548c\u8868\u60c5\uff0c\u5e38\u5e38\u5ffd\u7565\u773c\u775b\u533a\u57df\u7684\u7ec6\u5fae\u52a8\u6001\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u773c\u775b\u5730\u6807\u8c03\u6574\u6a21\u5757\uff0c\u5e76\u8f85\u4ee5\u5c40\u90e8\u52a8\u6001\u635f\u5931\uff0c\u65e8\u5728\u6355\u83b7\u773c\u775b\u533a\u57df\u7684\u52a8\u6001\u7279\u5f81\u3002\u6211\u4eec\u7684\u6a21\u5757\u5141\u8bb8\u7075\u6d3b\u8c03\u6574\u5730\u6807\uff0c\u4ece\u800c\u51c6\u786e\u5730\u518d\u73b0\u5404\u79cd\u773c\u775b\u72b6\u6001\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5bf9\u4e24\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u5f3a\u8c03\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u5b83\u5728\u5e94\u5bf9\u8fd9\u4e00\u7279\u6b8a\u6311\u6218\u65b9\u9762\u7684\u91cd\u5927\u8d21\u732e\u3002|[2401.09677v1](http://arxiv.org/pdf/2401.09677v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2401.10191": "|**2024-01-18**|**Divide and not forget: Ensemble of selectively trained experts in Continual Learning**|\u5206\u5f00\u4f46\u4e0d\u8981\u5fd8\u8bb0\uff1a\u7ecf\u8fc7\u9009\u62e9\u6027\u57f9\u8bad\u7684\u6301\u7eed\u5b66\u4e60\u4e13\u5bb6\u56e2\u961f|Grzegorz Rype\u015b\u0107, Sebastian Cygert, Valeriya Khan, Tomasz Trzci\u0144ski, Bartosz Zieli\u0144ski, Bart\u0142omiej Twardowski|Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performance in exemplar-free settings across various scenarios, showing the potential of expert diversification through data in continual learning.|\u73ed\u7ea7\u589e\u91cf\u5b66\u4e60\u53d8\u5f97\u8d8a\u6765\u8d8a\u6d41\u884c\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u6269\u5927\u5176\u9002\u7528\u6027\uff0c\u540c\u65f6\u4e0d\u4f1a\u5fd8\u8bb0\u4ed6\u4eec\u5df2\u7ecf\u77e5\u9053\u7684\u4e1c\u897f\u3002\u8be5\u9886\u57df\u7684\u8d8b\u52bf\u662f\u4f7f\u7528\u4e13\u5bb6\u6df7\u5408\u6280\u672f\uff0c\u5176\u4e2d\u4e0d\u540c\u7684\u6a21\u578b\u4e00\u8d77\u5de5\u4f5c\u6765\u89e3\u51b3\u4efb\u52a1\u3002\u7136\u800c\uff0c\u4e13\u5bb6\u901a\u5e38\u4f7f\u7528\u6574\u4e2a\u4efb\u52a1\u6570\u636e\u4e00\u6b21\u6027\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u4f7f\u5f97\u4ed6\u4eec\u5f88\u5bb9\u6613\u5fd8\u8bb0\u5e76\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a SEED \u7684\u65b0\u65b9\u6cd5\u3002 SEED \u4ec5\u9009\u62e9\u4e00\u4e2a\u6700\u9002\u5408\u6240\u8003\u8651\u4efb\u52a1\u7684\u4e13\u5bb6\uff0c\u5e76\u4f7f\u7528\u6765\u81ea\u8be5\u4efb\u52a1\u7684\u6570\u636e\u4ec5\u5fae\u8c03\u8be5\u4e13\u5bb6\u3002\u4e3a\u6b64\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u7528\u9ad8\u65af\u5206\u5e03\u4ee3\u8868\u6bcf\u4e2a\u7c7b\u522b\uff0c\u5e76\u6839\u636e\u8fd9\u4e9b\u5206\u5e03\u7684\u76f8\u4f3c\u6027\u9009\u62e9\u6700\u4f73\u4e13\u5bb6\u3002\u56e0\u6b64\uff0cSEED \u589e\u52a0\u4e86\u4e13\u5bb6\u5185\u90e8\u7684\u591a\u6837\u6027\u548c\u5f02\u8d28\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8fd9\u79cd\u96c6\u6210\u65b9\u6cd5\u7684\u9ad8\u5ea6\u7a33\u5b9a\u6027\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSEED \u5728\u5404\u79cd\u573a\u666f\u7684\u65e0\u6837\u672c\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5b9e\u73b0\u4e13\u5bb6\u591a\u6837\u5316\u7684\u6f5c\u529b\u3002|[2401.10191v1](http://arxiv.org/pdf/2401.10191v1)|null|\n"}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "\u5176\u4ed6": {"2401.10224": "|**2024-01-18**|**The Manga Whisperer: Automatically Generating Transcriptions for Comics**|\u6f2b\u753b\u4f4e\u8bed\u8005\uff1a\u81ea\u52a8\u751f\u6210\u6f2b\u753b\u8f6c\u5f55|Ragav Sachdeva, Andrew Zisserman|In the past few decades, Japanese comics, commonly referred to as Manga, have transcended both cultural and linguistic boundaries to become a true worldwide sensation. Yet, the inherent reliance on visual cues and illustration within manga renders it largely inaccessible to individuals with visual impairments. In this work, we seek to address this substantial barrier, with the aim of ensuring that manga can be appreciated and actively engaged by everyone. Specifically, we tackle the problem of diarisation i.e. generating a transcription of who said what and when, in a fully automatic way.   To this end, we make the following contributions: (1) we present a unified model, Magi, that is able to (a) detect panels, text boxes and character boxes, (b) cluster characters by identity (without knowing the number of clusters apriori), and (c) associate dialogues to their speakers; (2) we propose a novel approach that is able to sort the detected text boxes in their reading order and generate a dialogue transcript; (3) we annotate an evaluation benchmark for this task using publicly available [English] manga pages. The code, evaluation datasets and the pre-trained model can be found at: https://github.com/ragavsachdeva/magi.|\u5728\u8fc7\u53bb\u7684\u51e0\u5341\u5e74\u91cc\uff0c\u65e5\u672c\u6f2b\u753b\uff08\u901a\u5e38\u79f0\u4e3a\u6f2b\u753b\uff09\u5df2\u7ecf\u8d85\u8d8a\u4e86\u6587\u5316\u548c\u8bed\u8a00\u7684\u754c\u9650\uff0c\u6210\u4e3a\u771f\u6b63\u7684\u4e16\u754c\u6027\u8f70\u52a8\u3002\u7136\u800c\uff0c\u6f2b\u753b\u5bf9\u89c6\u89c9\u7ebf\u7d22\u548c\u63d2\u56fe\u7684\u56fa\u6709\u4f9d\u8d56\u4f7f\u5f97\u89c6\u529b\u969c\u788d\u4eba\u58eb\u57fa\u672c\u4e0a\u65e0\u6cd5\u7406\u89e3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u529b\u6c42\u89e3\u51b3\u8fd9\u4e00\u91cd\u5927\u969c\u788d\uff0c\u65e8\u5728\u786e\u4fdd\u6bcf\u4e2a\u4eba\u90fd\u80fd\u6b23\u8d4f\u5e76\u79ef\u6781\u53c2\u4e0e\u6f2b\u753b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u5206\u7c7b\u95ee\u9898\uff0c\u5373\u4ee5\u5168\u81ea\u52a8\u65b9\u5f0f\u751f\u6210\u8c01\u8bf4\u4e86\u4ec0\u4e48\u3001\u4f55\u65f6\u8bf4\u7684\u8f6c\u5f55\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u505a\u51fa\u4ee5\u4e0b\u8d21\u732e\uff1a\uff081\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b Magi\uff0c\u5b83\u80fd\u591f\uff08a\uff09\u68c0\u6d4b\u9762\u677f\u3001\u6587\u672c\u6846\u548c\u5b57\u7b26\u6846\uff0c\uff08b\uff09\u6309\u8eab\u4efd\u5bf9\u5b57\u7b26\u8fdb\u884c\u805a\u7c7b\uff08\u4e0d\u77e5\u9053\u5b57\u7b26\u7684\u6570\u91cf\uff09 (c) \u5c06\u5bf9\u8bdd\u4e0e\u5176\u53d1\u8a00\u8005\u8054\u7cfb\u8d77\u6765\uff1b \uff082\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6309\u7167\u9605\u8bfb\u987a\u5e8f\u5bf9\u68c0\u6d4b\u5230\u7684\u6587\u672c\u6846\u8fdb\u884c\u6392\u5e8f\u5e76\u751f\u6210\u5bf9\u8bdd\u8bb0\u5f55\uff1b \uff083\uff09\u6211\u4eec\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u7684[\u82f1\u6587]\u6f2b\u753b\u9875\u9762\u6ce8\u91ca\u6b64\u4efb\u52a1\u7684\u8bc4\u4f30\u57fa\u51c6\u3002\u4ee3\u7801\u3001\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u627e\u5230\uff1ahttps://github.com/ragavsachdeva/magi\u3002|[2401.10224v1](http://arxiv.org/pdf/2401.10224v1)|null|\n", "2401.10220": "|**2024-01-18**|**AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data**|AutoFT\uff1a\u901a\u8fc7\u4f18\u5316 OOD \u6570\u636e\u7684\u8d85\u53c2\u6570\u8fdb\u884c\u9c81\u68d2\u5fae\u8c03|Caroline Choi, Yoonho Lee, Annie Chen, Allan Zhou, Aditi Raghunathan, Chelsea Finn|Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different losses, in addition to learning rate and weight decay values. We evaluate AutoFT on nine natural distribution shifts which include domain shifts and subpopulation shifts. Our experiments show that AutoFT significantly improves generalization to new OOD data, outperforming existing robust fine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous best methods by $6.0\\%$ and $1.5\\%$, respectively.|\u57fa\u7840\u6a21\u578b\u7f16\u7801\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u53ef\u4ee5\u901a\u8fc7\u5bf9\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u6765\u9002\u5e94\u6240\u9700\u7684\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5728\u4e00\u79cd\u7279\u5b9a\u6570\u636e\u5206\u5e03\u4e0a\u5fae\u8c03\u6a21\u578b\u901a\u5e38\u4f1a\u635f\u5bb3\u6a21\u578b\u5728\u5176\u4ed6\u5206\u5e03\u4e0a\u7684\u539f\u59cb\u6027\u80fd\u3002\u5f53\u524d\u7684\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u5229\u7528\u624b\u5de5\u5236\u4f5c\u7684\u6b63\u5219\u5316\u6280\u672f\u6765\u9650\u5236\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u5fae\u8c03\u8fc7\u7a0b\u3002\u7136\u800c\uff0c\u5f88\u96be\u7cbe\u786e\u5730\u6307\u5b9a\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u8981\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u7684\u54ea\u4e9b\u7279\u5f81\uff0c\u56e0\u4e3a\u8fd9\u53d6\u51b3\u4e8e\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u8bc4\u4f30\u6570\u636e\u5206\u5e03\u5982\u4f55\u76f8\u4e92\u5173\u8054\u3002\u6211\u4eec\u63d0\u51fa\u4e86 AutoFT\uff0c\u4e00\u79cd\u7528\u4e8e\u6307\u5bfc\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002 AutoFT \u4f18\u5316\u5fae\u8c03\u8d85\u53c2\u6570\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u5c0f\u578b\u5206\u5e03\u5916 (OOD) \u9a8c\u8bc1\u96c6\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u4ee5\u7cbe\u7ec6\u7684\u65b9\u5f0f\u6307\u5bfc\u5fae\u8c03\uff0cAutoFT \u641c\u7d22\u4e00\u4e2a\u9ad8\u5ea6\u8868\u8fbe\u7684\u8d85\u53c2\u6570\u7a7a\u95f4\uff0c\u9664\u4e86\u5b66\u4e60\u7387\u548c\u6743\u91cd\u8870\u51cf\u503c\u4e4b\u5916\uff0c\u5176\u4e2d\u8fd8\u5305\u62ec\u8bb8\u591a\u4e0d\u540c\u635f\u5931\u7684\u6743\u91cd\u7cfb\u6570\u3002\u6211\u4eec\u8bc4\u4f30 AutoFT \u7684\u4e5d\u79cd\u81ea\u7136\u5206\u5e03\u53d8\u5316\uff0c\u5176\u4e2d\u5305\u62ec\u57df\u53d8\u5316\u548c\u5b50\u7fa4\u4f53\u53d8\u5316\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAutoFT \u663e\u7740\u63d0\u9ad8\u4e86\u5bf9\u65b0 OOD \u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u7a33\u5065\u5fae\u8c03\u65b9\u6cd5\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cAutoFT \u5728 WILDS-iWildCam \u548c WILDS-FMoW \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5206\u522b\u6bd4\u4e4b\u524d\u7684\u6700\u4f73\u65b9\u6cd5\u9ad8\u51fa 6.0\\%$ \u548c 1.5\\%$\u3002|[2401.10220v1](http://arxiv.org/pdf/2401.10220v1)|null|\n", "2401.10178": "|**2024-01-18**|**Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields**|\u795e\u7ecf\u56de\u58f0\uff1a\u6df1\u5ea6\u5377\u79ef\u6ee4\u6ce2\u5668\u590d\u5236\u751f\u7269\u611f\u53d7\u91ce|Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu|In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina. We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence. Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields. Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights. This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks.|\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u4f9b\u7684\u8bc1\u636e\u8868\u660e\u6df1\u5ea6\u5377\u79ef\u6838\u53ef\u4ee5\u6709\u6548\u5730\u590d\u5236\u5728\u54fa\u4e73\u52a8\u7269\u89c6\u7f51\u819c\u4e2d\u89c2\u5bdf\u5230\u7684\u751f\u7269\u611f\u53d7\u91ce\u7684\u590d\u6742\u7ed3\u6784\u3002\u6211\u4eec\u63d0\u4f9b\u6765\u81ea\u5404\u79cd\u6700\u5148\u8fdb\u6a21\u578b\u7684\u7ecf\u8fc7\u8bad\u7ec3\u7684\u5185\u6838\u7684\u5206\u6790\uff0c\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u8bc1\u636e\u3002\u53d7\u8fd9\u4e00\u6709\u8da3\u53d1\u73b0\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u751f\u7269\u611f\u53d7\u91ce\u4e2d\u6c72\u53d6\u7075\u611f\u7684\u521d\u59cb\u5316\u65b9\u6848\u3002\u5bf9\u5177\u6709\u591a\u4e2a\u5177\u6709\u6df1\u5ea6\u5377\u79ef\u7279\u5f81\u7684 CNN \u67b6\u6784\u7684 ImageNet \u6570\u636e\u96c6\u8fdb\u884c\u7684\u5b9e\u9a8c\u5206\u6790\u8868\u660e\uff0c\u5f53\u4f7f\u7528\u751f\u7269\u5b66\u884d\u751f\u7684\u6743\u91cd\u8fdb\u884c\u521d\u59cb\u5316\u65f6\uff0c\u5b66\u4e60\u6a21\u578b\u7684\u51c6\u786e\u6027\u663e\u7740\u589e\u5f3a\u3002\u8fd9\u5960\u5b9a\u4e86\u53d7\u751f\u7269\u5b66\u542f\u53d1\u7684\u8ba1\u200b\u200b\u7b97\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u52a0\u6df1\u6211\u4eec\u5bf9\u89c6\u89c9\u5904\u7406\u7cfb\u7edf\u7684\u7406\u89e3\u5e76\u63d0\u9ad8\u5377\u79ef\u7f51\u7edc\u7684\u6548\u7387\u3002|[2401.10178v1](http://arxiv.org/pdf/2401.10178v1)|null|\n", "2401.10128": "|**2024-01-18**|**Sub2Full: split spectrum to boost OCT despeckling without clean data**|Sub2Full\uff1a\u5206\u5272\u5149\u8c31\u4ee5\u589e\u5f3a OCT \u53bb\u6591\u6548\u679c\uff0c\u65e0\u9700\u5e72\u51c0\u6570\u636e|Lingyun Wang, Jose A Sahel, Shaohua Pi|Optical coherence tomography (OCT) suffers from speckle noise, causing the deterioration of image quality, especially in high-resolution modalities like visible light OCT (vis-OCT). The potential of conventional supervised deep learning denoising methods is limited by the difficulty of obtaining clean data. Here, we proposed an innovative self-supervised strategy called Sub2Full (S2F) for OCT despeckling without clean data. This approach works by acquiring two repeated B-scans, splitting the spectrum of the first repeat as a low-resolution input, and utilizing the full spectrum of the second repeat as the high-resolution target. The proposed method was validated on vis-OCT retinal images visualizing sublaminar structures in outer retina and demonstrated superior performance over conventional Noise2Noise and Noise2Void schemes. The code is available at https://github.com/PittOCT/Sub2Full-OCT-Denoising.|\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf (OCT) \u4f1a\u53d7\u5230\u6563\u6591\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u53ef\u89c1\u5149 OCT (vis-OCT) \u7b49\u9ad8\u5206\u8fa8\u7387\u6a21\u5f0f\u4e2d\u3002\u4f20\u7edf\u7684\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u53bb\u566a\u65b9\u6cd5\u7684\u6f5c\u529b\u53d7\u5230\u96be\u4ee5\u83b7\u5f97\u5e72\u51c0\u6570\u636e\u7684\u9650\u5236\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Sub2Full (S2F) \u7684\u521b\u65b0\u81ea\u6211\u76d1\u7763\u7b56\u7565\uff0c\u7528\u4e8e\u65e0\u9700\u5e72\u51c0\u6570\u636e\u7684 OCT \u53bb\u6591\u3002\u8be5\u65b9\u6cd5\u7684\u5de5\u4f5c\u539f\u7406\u662f\u91c7\u96c6\u4e24\u6b21\u91cd\u590d\u7684 B \u626b\u63cf\uff0c\u5c06\u7b2c\u4e00\u6b21\u91cd\u590d\u7684\u5149\u8c31\u5206\u5272\u4e3a\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\uff0c\u5e76\u5229\u7528\u7b2c\u4e8c\u6b21\u91cd\u590d\u7684\u5168\u5149\u8c31\u4f5c\u4e3a\u9ad8\u5206\u8fa8\u7387\u76ee\u6807\u3002\u8be5\u65b9\u6cd5\u5728\u53ef\u89c6\u5316\u5916\u89c6\u7f51\u819c\u5c42\u4e0b\u7ed3\u6784\u7684 vis-OCT \u89c6\u7f51\u819c\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u8bc1\u660e\u4e86\u4f18\u4e8e\u4f20\u7edf Noise2Noise \u548c Noise2Void \u65b9\u6848\u7684\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/PittOCT/Sub2Full-OCT-Denoising \u83b7\u53d6\u3002|[2401.10128v1](http://arxiv.org/pdf/2401.10128v1)|null|\n", "2401.09673": "|**2024-01-18**|**Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack**|\u4f7f\u7528\u5c40\u90e8\u81ea\u9002\u5e94\u5bf9\u6297\u6027\u989c\u8272\u653b\u51fb\u6765\u4fdd\u62a4\u827a\u672f\u54c1\u514d\u53d7\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb|Zhongliang Guo, Kaixuan Wang, Weiye Li, Yifei Qian, Ognjen Arandjelovi\u0107, Lei Fang|Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study confirm that by attacking NST using the proposed method results in visually worse neural style transfer, thus making it an effective solution for visual artwork protection.|\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\uff08NST\uff09\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4ee5\u751f\u6210\u4efb\u610f\u98ce\u683c\u7684\u65b0\u56fe\u50cf\u3002\u8be5\u8fc7\u7a0b\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5c06\u98ce\u683c\u56fe\u50cf\u7684\u7f8e\u5b66\u5143\u7d20\u4e0e\u5185\u5bb9\u56fe\u50cf\u7684\u7ed3\u6784\u65b9\u9762\u878d\u5408\u6210\u548c\u8c10\u96c6\u6210\u7684\u89c6\u89c9\u7ed3\u679c\u3002\u7136\u800c\uff0c\u672a\u7ecf\u6388\u6743\u7684 NST \u53ef\u4ee5\u5229\u7528\u827a\u672f\u54c1\u3002\u8fd9\u79cd\u6ee5\u7528\u5f15\u8d77\u4e86\u5bf9\u827a\u672f\u5bb6\u6743\u5229\u7684\u793e\u4f1a\u6280\u672f\u5173\u6ce8\uff0c\u5e76\u63a8\u52a8\u4e86\u4e3b\u52a8\u4fdd\u62a4\u539f\u521b\u4f5c\u54c1\u7684\u6280\u672f\u65b9\u6cd5\u7684\u53d1\u5c55\u3002\u5bf9\u6297\u6027\u653b\u51fb\u662f\u673a\u5668\u5b66\u4e60\u5b89\u5168\u4e2d\u4e3b\u8981\u63a2\u8ba8\u7684\u4e00\u4e2a\u6982\u5ff5\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5f15\u5165\u4e86\u8fd9\u79cd\u6280\u672f\u6765\u4fdd\u62a4\u827a\u672f\u5bb6\u7684\u77e5\u8bc6\u4ea7\u6743\u3002\u5728\u672c\u6587\u4e2d\uff0c\u5c40\u90e8\u81ea\u9002\u5e94\u5bf9\u6297\u6027\u989c\u8272\u653b\u51fb\uff08LAACA\uff09\u662f\u4e00\u79cd\u4ee5\u4eba\u773c\u65e0\u6cd5\u5bdf\u89c9\u4f46\u7834\u574f NST \u7684\u65b9\u5f0f\u6539\u53d8\u56fe\u50cf\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9488\u5bf9\u5bcc\u542b\u9ad8\u9891\u5185\u5bb9\u7684\u56fe\u50cf\u533a\u57df\u8bbe\u8ba1\u6270\u52a8\uff0c\u8fd9\u4e9b\u6270\u52a8\u662f\u901a\u8fc7\u7834\u574f\u4e2d\u95f4\u7279\u5f81\u800c\u751f\u6210\u7684\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\uff0c\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u653b\u51fb NST \u4f1a\u5bfc\u81f4\u89c6\u89c9\u4e0a\u66f4\u5dee\u7684\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\uff0c\u4ece\u800c\u4f7f\u5176\u6210\u4e3a\u89c6\u89c9\u827a\u672f\u54c1\u4fdd\u62a4\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002|[2401.09673v1](http://arxiv.org/pdf/2401.09673v1)|null|\n"}}