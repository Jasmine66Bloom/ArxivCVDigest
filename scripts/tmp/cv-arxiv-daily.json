{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.12979": "|**2024-01-23**|**GALA: Generating Animatable Layered Assets from a Single Scan**|GALA\uff1a\u901a\u8fc7\u5355\u6b21\u626b\u63cf\u751f\u6210\u53ef\u52a8\u753b\u5316\u7684\u5206\u5c42\u8d44\u6e90|Taeksoo Kim, Byungjun Kim, Shunsuke Saito, Hanbyul Joo|We present GALA, a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets. The outputs can then be combined with other assets to create novel clothed human avatars with any pose. Existing reconstruction approaches often treat clothed humans as a single-layer of geometry and overlook the inherent compositionality of humans with hairstyles, clothing, and accessories, thereby limiting the utility of the meshes for downstream applications. Decomposing a single-layer mesh into separate layers is a challenging task because it requires the synthesis of plausible geometry and texture for the severely occluded regions. Moreover, even with successful decomposition, meshes are not normalized in terms of poses and body shapes, failing coherent composition with novel identities and poses. To address these challenges, we propose to leverage the general knowledge of a pretrained 2D diffusion model as geometry and appearance prior for humans and other assets. We first separate the input mesh using the 3D surface segmentation extracted from multi-view 2D segmentations. Then we synthesize the missing geometry of different layers in both posed and canonical spaces using a novel pose-guided Score Distillation Sampling (SDS) loss. Once we complete inpainting high-fidelity 3D geometry, we also apply the same SDS loss to its texture to obtain the complete appearance including the initially occluded regions. Through a series of decomposition steps, we obtain multiple layers of 3D assets in a shared canonical space normalized in terms of poses and human shapes, hence supporting effortless composition to novel identities and reanimation with novel poses. Our experiments demonstrate the effectiveness of our approach for decomposition, canonicalization, and composition tasks compared to existing solutions.|\u6211\u4eec\u63d0\u51fa\u4e86 GALA\uff0c\u8fd9\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u5b83\u5c06\u5355\u5c42\u670d\u88c5 3D \u4eba\u4f53\u7f51\u683c\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u5c06\u5176\u5206\u89e3\u4e3a\u5b8c\u6574\u7684\u591a\u5c42 3D \u8d44\u4ea7\u3002\u7136\u540e\uff0c\u8f93\u51fa\u53ef\u4ee5\u4e0e\u5176\u4ed6\u8d44\u4ea7\u7ed3\u5408\u8d77\u6765\uff0c\u521b\u5efa\u5177\u6709\u4efb\u4f55\u59ff\u52bf\u7684\u65b0\u9896\u7684\u670d\u88c5\u4eba\u7c7b\u5316\u8eab\u3002\u73b0\u6709\u7684\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u5c06\u7a7f\u7740\u8863\u670d\u7684\u4eba\u7c7b\u89c6\u4e3a\u5355\u5c42\u51e0\u4f55\u4f53\uff0c\u800c\u5ffd\u7565\u4e86\u4eba\u7c7b\u4e0e\u53d1\u578b\u3001\u670d\u88c5\u548c\u914d\u9970\u7684\u56fa\u6709\u7ec4\u5408\u6027\uff0c\u4ece\u800c\u9650\u5236\u4e86\u7f51\u683c\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u5c06\u5355\u5c42\u7f51\u683c\u5206\u89e3\u4e3a\u5355\u72ec\u7684\u5c42\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u4e3a\u4e25\u91cd\u906e\u6321\u7684\u533a\u57df\u5408\u6210\u5408\u7406\u7684\u51e0\u4f55\u5f62\u72b6\u548c\u7eb9\u7406\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5206\u89e3\u6210\u529f\uff0c\u7f51\u683c\u5728\u59ff\u52bf\u548c\u8eab\u4f53\u5f62\u72b6\u65b9\u9762\u4e5f\u6ca1\u6709\u6807\u51c6\u5316\uff0c\u65e0\u6cd5\u4e0e\u65b0\u9896\u7684\u8eab\u4efd\u548c\u59ff\u52bf\u8fdb\u884c\u8fde\u8d2f\u7684\u7ec4\u5408\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528\u9884\u8bad\u7ec3\u7684\u4e8c\u7ef4\u6269\u6563\u6a21\u578b\u7684\u4e00\u822c\u77e5\u8bc6\u4f5c\u4e3a\u4eba\u7c7b\u548c\u5176\u4ed6\u8d44\u4ea7\u7684\u51e0\u4f55\u548c\u5916\u89c2\u5148\u9a8c\u3002\u6211\u4eec\u9996\u5148\u4f7f\u7528\u4ece\u591a\u89c6\u56fe 2D \u5206\u5272\u4e2d\u63d0\u53d6\u7684 3D \u8868\u9762\u5206\u5272\u6765\u5206\u79bb\u8f93\u5165\u7f51\u683c\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u65b0\u9896\u7684\u59ff\u52bf\u5f15\u5bfc\u5206\u6570\u84b8\u998f\u91c7\u6837\uff08SDS\uff09\u635f\u5931\u6765\u5408\u6210\u59ff\u52bf\u7a7a\u95f4\u548c\u89c4\u8303\u7a7a\u95f4\u4e2d\u4e0d\u540c\u5c42\u7684\u7f3a\u5931\u51e0\u4f55\u5f62\u72b6\u3002\u4e00\u65e6\u6211\u4eec\u5b8c\u6210\u4e86\u9ad8\u4fdd\u771f 3D \u51e0\u4f55\u4f53\u7684\u4fee\u590d\uff0c\u6211\u4eec\u8fd8\u4f1a\u5bf9\u5176\u7eb9\u7406\u5e94\u7528\u76f8\u540c\u7684 SDS \u635f\u5931\u4ee5\u83b7\u5f97\u5b8c\u6574\u7684\u5916\u89c2\uff0c\u5305\u62ec\u6700\u521d\u88ab\u906e\u6321\u7684\u533a\u57df\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u5206\u89e3\u6b65\u9aa4\uff0c\u6211\u4eec\u5728\u5171\u4eab\u89c4\u8303\u7a7a\u95f4\u4e2d\u83b7\u5f97\u4e86\u591a\u5c42 3D \u8d44\u4ea7\uff0c\u5e76\u6839\u636e\u59ff\u52bf\u548c\u4eba\u4f53\u5f62\u72b6\u8fdb\u884c\u4e86\u89c4\u8303\u5316\uff0c\u4ece\u800c\u652f\u6301\u8f7b\u677e\u7ec4\u5408\u65b0\u7684\u8eab\u4efd\u5e76\u4ee5\u65b0\u7684\u59ff\u52bf\u590d\u6d3b\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u4e0e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u5206\u89e3\u3001\u6807\u51c6\u5316\u548c\u7ec4\u5408\u4efb\u52a1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2401.12979v1](http://arxiv.org/pdf/2401.12979v1)|null|\n", "2401.12974": "|**2024-01-23**|**SegmentAnyBone: A Universal Model that Segments Any Bone at Any Location on MRI**|SegmentAnyBone\uff1a\u4e00\u79cd\u901a\u7528\u6a21\u578b\uff0c\u53ef\u5728 MRI \u4e0a\u7684\u4efb\u4f55\u4f4d\u7f6e\u5206\u5272\u4efb\u4f55\u9aa8\u9abc|Hanxue Gu, Roy Colglazier, Haoyu Dong, Jikai Zhang, Yaqian Chen, Zafer Yildiz, Yuwen Chen, Lin Li, Jichen Yang, Jay Willhite, et.al.|Magnetic Resonance Imaging (MRI) is pivotal in radiology, offering non-invasive and high-quality insights into the human body. Precise segmentation of MRIs into different organs and tissues would be highly beneficial since it would allow for a higher level of understanding of the image content and enable important measurements, which are essential for accurate diagnosis and effective treatment planning. Specifically, segmenting bones in MRI would allow for more quantitative assessments of musculoskeletal conditions, while such assessments are largely absent in current radiological practice. The difficulty of bone MRI segmentation is illustrated by the fact that limited algorithms are publicly available for use, and those contained in the literature typically address a specific anatomic area. In our study, we propose a versatile, publicly available deep-learning model for bone segmentation in MRI across multiple standard MRI locations. The proposed model can operate in two modes: fully automated segmentation and prompt-based segmentation. Our contributions include (1) collecting and annotating a new MRI dataset across various MRI protocols, encompassing over 300 annotated volumes and 8485 annotated slices across diverse anatomic regions; (2) investigating several standard network architectures and strategies for automated segmentation; (3) introducing SegmentAnyBone, an innovative foundational model-based approach that extends Segment Anything Model (SAM); (4) comparative analysis of our algorithm and previous approaches; and (5) generalization analysis of our algorithm across different anatomical locations and MRI sequences, as well as an external dataset. We publicly release our model at https://github.com/mazurowski-lab/SegmentAnyBone.|\u78c1\u5171\u632f\u6210\u50cf (MRI) \u5728\u653e\u5c04\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u63d0\u4f9b\u5bf9\u4eba\u4f53\u7684\u975e\u4fb5\u5165\u6027\u9ad8\u8d28\u91cf\u6d1e\u5bdf\u3002\u5c06 MRI \u7cbe\u786e\u5206\u5272\u6210\u4e0d\u540c\u7684\u5668\u5b98\u548c\u7ec4\u7ec7\u5c06\u975e\u5e38\u6709\u76ca\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u63d0\u9ad8\u5bf9\u56fe\u50cf\u5185\u5bb9\u7684\u7406\u89e3\u5e76\u5b9e\u73b0\u91cd\u8981\u7684\u6d4b\u91cf\uff0c\u8fd9\u5bf9\u4e8e\u51c6\u786e\u7684\u8bca\u65ad\u548c\u6709\u6548\u7684\u6cbb\u7597\u8ba1\u5212\u81f3\u5173\u91cd\u8981\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728 MRI \u4e2d\u5206\u5272\u9aa8\u9abc\u5c06\u5141\u8bb8\u5bf9\u808c\u8089\u9aa8\u9abc\u72b6\u51b5\u8fdb\u884c\u66f4\u5b9a\u91cf\u7684\u8bc4\u4f30\uff0c\u800c\u8fd9\u79cd\u8bc4\u4f30\u5728\u5f53\u524d\u7684\u653e\u5c04\u5b66\u5b9e\u8df5\u4e2d\u57fa\u672c\u4e0a\u4e0d\u5b58\u5728\u3002\u516c\u5f00\u4f7f\u7528\u7684\u7b97\u6cd5\u6709\u9650\uff0c\u5e76\u4e14\u6587\u732e\u4e2d\u5305\u542b\u7684\u7b97\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u7684\u89e3\u5256\u533a\u57df\uff0c\u8fd9\u4e00\u4e8b\u5b9e\u8bf4\u660e\u4e86\u9aa8 MRI \u5206\u5272\u7684\u56f0\u96be\u3002\u5728\u6211\u4eec\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u516c\u5f00\u53ef\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u8de8\u591a\u4e2a\u6807\u51c6 MRI \u4f4d\u7f6e\u7684 MRI \u9aa8\u5206\u5272\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u53ef\u4ee5\u4ee5\u4e24\u79cd\u6a21\u5f0f\u8fd0\u884c\uff1a\u5168\u81ea\u52a8\u5206\u5272\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u5206\u5272\u3002\u6211\u4eec\u7684\u8d21\u732e\u5305\u62ec (1) \u8de8\u5404\u79cd MRI \u534f\u8bae\u6536\u96c6\u548c\u6ce8\u91ca\u65b0\u7684 MRI \u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u89e3\u5256\u533a\u57df\u7684 300 \u591a\u4e2a\u5e26\u6ce8\u91ca\u7684\u5377\u548c 8485 \u4e2a\u5e26\u6ce8\u91ca\u7684\u5207\u7247\uff1b (2) \u7814\u7a76\u51e0\u79cd\u6807\u51c6\u7f51\u7edc\u67b6\u6784\u548c\u81ea\u52a8\u5206\u6bb5\u7b56\u7565\uff1b (3) \u5f15\u5165 SegmentAnyBone\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86 Segment Anything Model (SAM)\uff1b \uff084\uff09\u6211\u4eec\u7684\u7b97\u6cd5\u548c\u4ee5\u524d\u7684\u65b9\u6cd5\u7684\u6bd4\u8f83\u5206\u6790\uff1b (5) \u5bf9\u4e0d\u540c\u89e3\u5256\u4f4d\u7f6e\u548c MRI \u5e8f\u5217\u4ee5\u53ca\u5916\u90e8\u6570\u636e\u96c6\u7684\u7b97\u6cd5\u8fdb\u884c\u6cdb\u5316\u5206\u6790\u3002\u6211\u4eec\u5728 https://github.com/mazurowski-lab/SegmentAnyBone \u516c\u5f00\u53d1\u5e03\u6211\u4eec\u7684\u6a21\u578b\u3002|[2401.12974v1](http://arxiv.org/pdf/2401.12974v1)|null|\n", "2401.12938": "|**2024-01-23**|**Neural deformation fields for template-based reconstruction of cortical surfaces from MRI**|\u7528\u4e8e\u57fa\u4e8e MRI \u76ae\u8d28\u8868\u9762\u6a21\u677f\u91cd\u5efa\u7684\u795e\u7ecf\u53d8\u5f62\u573a|Fabian Bongratz, Anne-Marie Rickmann, Christian Wachinger|The reconstruction of cortical surfaces is a prerequisite for quantitative analyses of the cerebral cortex in magnetic resonance imaging (MRI). Existing segmentation-based methods separate the surface registration from the surface extraction, which is computationally inefficient and prone to distortions. We introduce Vox2Cortex-Flow (V2C-Flow), a deep mesh-deformation technique that learns a deformation field from a brain template to the cortical surfaces of an MRI scan. To this end, we present a geometric neural network that models the deformation-describing ordinary differential equation in a continuous manner. The network architecture comprises convolutional and graph-convolutional layers, which allows it to work with images and meshes at the same time. V2C-Flow is not only very fast, requiring less than two seconds to infer all four cortical surfaces, but also establishes vertex-wise correspondences to the template during reconstruction. In addition, V2C-Flow is the first approach for cortex reconstruction that models white matter and pial surfaces jointly, therefore avoiding intersections between them. Our comprehensive experiments on internal and external test data demonstrate that V2C-Flow results in cortical surfaces that are state-of-the-art in terms of accuracy. Moreover, we show that the established correspondences are more consistent than in FreeSurfer and that they can directly be utilized for cortex parcellation and group analyses of cortical thickness.|\u76ae\u8d28\u8868\u9762\u7684\u91cd\u5efa\u662f\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u4e2d\u5927\u8111\u76ae\u5c42\u5b9a\u91cf\u5206\u6790\u7684\u5148\u51b3\u6761\u4ef6\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5206\u5272\u7684\u65b9\u6cd5\u5c06\u8868\u9762\u914d\u51c6\u4e0e\u8868\u9762\u63d0\u53d6\u5206\u5f00\uff0c\u8fd9\u5728\u8ba1\u7b97\u4e0a\u6548\u7387\u4f4e\u4e0b\u5e76\u4e14\u5bb9\u6613\u5931\u771f\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86 Vox2Cortex-Flow (V2C-Flow)\uff0c\u8fd9\u662f\u4e00\u79cd\u6df1\u5ea6\u7f51\u683c\u53d8\u5f62\u6280\u672f\uff0c\u53ef\u4ee5\u5b66\u4e60\u4ece\u5927\u8111\u6a21\u677f\u5230 MRI \u626b\u63cf\u7684\u76ae\u8d28\u8868\u9762\u7684\u53d8\u5f62\u573a\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u8fde\u7eed\u65b9\u5f0f\u5bf9\u63cf\u8ff0\u53d8\u5f62\u7684\u5e38\u5fae\u5206\u65b9\u7a0b\u8fdb\u884c\u5efa\u6a21\u3002\u8be5\u7f51\u7edc\u67b6\u6784\u5305\u62ec\u5377\u79ef\u5c42\u548c\u56fe\u5377\u79ef\u5c42\uff0c\u8fd9\u4f7f\u5f97\u5b83\u53ef\u4ee5\u540c\u65f6\u5904\u7406\u56fe\u50cf\u548c\u7f51\u683c\u3002 V2C-Flow \u4e0d\u4ec5\u901f\u5ea6\u975e\u5e38\u5feb\uff0c\u53ea\u9700\u4e0d\u5230\u4e24\u79d2\u5373\u53ef\u63a8\u65ad\u51fa\u6240\u6709\u56db\u4e2a\u76ae\u8d28\u8868\u9762\uff0c\u800c\u4e14\u8fd8\u80fd\u5728\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u4e0e\u6a21\u677f\u5efa\u7acb\u9876\u70b9\u5bf9\u5e94\u5173\u7cfb\u3002\u6b64\u5916\uff0cV2C-Flow \u662f\u7b2c\u4e00\u4e2a\u8054\u5408\u6a21\u62df\u767d\u8d28\u548c\u8f6f\u8111\u819c\u8868\u9762\u7684\u76ae\u5c42\u91cd\u5efa\u65b9\u6cd5\uff0c\u4ece\u800c\u907f\u514d\u5b83\u4eec\u4e4b\u95f4\u7684\u4ea4\u53c9\u3002\u6211\u4eec\u5bf9\u5185\u90e8\u548c\u5916\u90e8\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cV2C-Flow \u4ea7\u751f\u7684\u76ae\u8d28\u8868\u9762\u5728\u51c6\u786e\u6027\u200b\u200b\u65b9\u9762\u662f\u6700\u5148\u8fdb\u7684\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8868\u660e\u6240\u5efa\u7acb\u7684\u5bf9\u5e94\u5173\u7cfb\u6bd4 FreeSurfer \u4e2d\u7684\u66f4\u52a0\u4e00\u81f4\uff0c\u5e76\u4e14\u5b83\u4eec\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u76ae\u5c42\u5206\u5272\u548c\u76ae\u5c42\u539a\u5ea6\u7684\u5206\u7ec4\u5206\u6790\u3002|[2401.12938v1](http://arxiv.org/pdf/2401.12938v1)|null|\n", "2401.12932": "|**2024-01-23**|**Segmentation of tibiofemoral joint tissues from knee MRI using MtRA-Unet and incorporating shape information: Data from the Osteoarthritis Initiative**|\u4f7f\u7528 MtRA-Unet \u5bf9\u819d MRI \u4e2d\u7684\u80eb\u80a1\u5173\u8282\u7ec4\u7ec7\u8fdb\u884c\u5206\u5272\u5e76\u7ed3\u5408\u5f62\u72b6\u4fe1\u606f\uff1a\u6765\u81ea\u9aa8\u5173\u8282\u708e\u5021\u8bae\u7684\u6570\u636e|Akshay Daydar, Alik Pramanick, Arijit Sur, Subramani Kanagaraj|Knee Osteoarthritis (KOA) is the third most prevalent Musculoskeletal Disorder (MSD) after neck and back pain. To monitor such a severe MSD, a segmentation map of the femur, tibia and tibiofemoral cartilage is usually accessed using the automated segmentation algorithm from the Magnetic Resonance Imaging (MRI) of the knee. But, in recent works, such segmentation is conceivable only from the multistage framework thus creating data handling issues and needing continuous manual inference rendering it unable to make a quick and precise clinical diagnosis. In order to solve these issues, in this paper the Multi-Resolution Attentive-Unet (MtRA-Unet) is proposed to segment the femur, tibia and tibiofemoral cartilage automatically. The proposed work has included a novel Multi-Resolution Feature Fusion (MRFF) and Shape Reconstruction (SR) loss that focuses on multi-contextual information and structural anatomical details of the femur, tibia and tibiofemoral cartilage. Unlike previous approaches, the proposed work is a single-stage and end-to-end framework producing a Dice Similarity Coefficient (DSC) of 98.5% for the femur, 98.4% for the tibia, 89.1% for Femoral Cartilage (FC) and 86.1% for Tibial Cartilage (TC) for critical MRI slices that can be helpful to clinicians for KOA grading. The time to segment MRI volume (160 slices) per subject is 22 sec. which is one of the fastest among state-of-the-art. Moreover, comprehensive experimentation on the segmentation of FC and TC which is of utmost importance for morphology-based studies to check KOA progression reveals that the proposed method has produced an excellent result with binary segmentation|\u819d\u9aa8\u5173\u8282\u708e (KOA) \u662f\u7ee7\u9888\u90e8\u548c\u80cc\u90e8\u75bc\u75db\u4e4b\u540e\u7b2c\u4e09\u5927\u6700\u5e38\u89c1\u7684\u808c\u8089\u9aa8\u9abc\u75be\u75c5 (MSD)\u3002\u4e3a\u4e86\u76d1\u6d4b\u5982\u6b64\u4e25\u91cd\u7684 MSD\uff0c\u901a\u5e38\u4f7f\u7528\u819d\u5173\u8282\u78c1\u5171\u632f\u6210\u50cf (MRI) \u7684\u81ea\u52a8\u5206\u5272\u7b97\u6cd5\u6765\u83b7\u53d6\u80a1\u9aa8\u3001\u80eb\u9aa8\u548c\u80eb\u80a1\u8f6f\u9aa8\u7684\u5206\u5272\u56fe\u3002\u4f46\u662f\uff0c\u5728\u6700\u8fd1\u7684\u5de5\u4f5c\u4e2d\uff0c\u8fd9\u79cd\u5206\u5272\u53ea\u80fd\u4ece\u591a\u7ea7\u6846\u67b6\u4e2d\u5b9e\u73b0\uff0c\u4ece\u800c\u4ea7\u751f\u6570\u636e\u5904\u7406\u95ee\u9898\uff0c\u5e76\u4e14\u9700\u8981\u6301\u7eed\u7684\u624b\u52a8\u63a8\u7406\uff0c\u4f7f\u5176\u65e0\u6cd5\u505a\u51fa\u5feb\u901f\u800c\u7cbe\u786e\u7684\u4e34\u5e8a\u8bca\u65ad\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u591a\u5206\u8fa8\u7387\u5173\u6ce8Unet\uff08MtRA-Unet\uff09\u6765\u81ea\u52a8\u5206\u5272\u80a1\u9aa8\u3001\u80eb\u9aa8\u548c\u80eb\u80a1\u8f6f\u9aa8\u3002\u62df\u8bae\u7684\u5de5\u4f5c\u5305\u62ec\u65b0\u9896\u7684\u591a\u5206\u8fa8\u7387\u7279\u5f81\u878d\u5408\uff08MRFF\uff09\u548c\u5f62\u72b6\u91cd\u5efa\uff08SR\uff09\u635f\u5931\uff0c\u91cd\u70b9\u5173\u6ce8\u80a1\u9aa8\u3001\u80eb\u9aa8\u548c\u80eb\u80a1\u8f6f\u9aa8\u7684\u591a\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u7ed3\u6784\u89e3\u5256\u7ec6\u8282\u3002\u4e0e\u4e4b\u524d\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6240\u63d0\u51fa\u7684\u5de5\u4f5c\u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u80a1\u9aa8\u7684 Dice \u76f8\u4f3c\u7cfb\u6570 (DSC) \u4e3a 98.5%\uff0c\u80eb\u9aa8\u4e3a 98.4%\uff0c\u80a1\u9aa8\u8f6f\u9aa8 (FC) \u4e3a 89.1%\uff0c\u80a1\u9aa8\u4e3a 86.1%\u3002\u5173\u952e MRI \u5207\u7247\u7684\u80eb\u9aa8\u8f6f\u9aa8 (TC) \u767e\u5206\u6bd4\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u533b\u751f\u8fdb\u884c KOA \u5206\u7ea7\u3002\u5bf9\u6bcf\u4e2a\u53d7\u8bd5\u8005\u7684 MRI \u4f53\u79ef\uff08160 \u4e2a\u5207\u7247\uff09\u8fdb\u884c\u5206\u6bb5\u7684\u65f6\u95f4\u4e3a 22 \u79d2\u3002\u8fd9\u662f\u6700\u5148\u8fdb\u7684\u6700\u5feb\u4e4b\u4e00\u3002\u6b64\u5916\uff0c\u5bf9 FC \u548c TC \u5206\u5272\u7684\u7efc\u5408\u5b9e\u9a8c\uff08\u5bf9\u4e8e\u68c0\u67e5 KOA \u8fdb\u5c55\u7684\u57fa\u4e8e\u5f62\u6001\u5b66\u7684\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff09\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e8c\u5143\u5206\u5272\u4e2d\u4ea7\u751f\u4e86\u51fa\u8272\u7684\u7ed3\u679c|[2401.12932v1](http://arxiv.org/pdf/2401.12932v1)|null|\n", "2401.12902": "|**2024-01-23**|**Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?**|\u9762\u5bf9\u623f\u95f4\u91cc\u7684\u5927\u8c61\uff1a\u89c6\u89c9\u63d0\u793a\u8c03\u6574\u8fd8\u662f\u5168\u9762\u5fae\u8c03\uff1f|Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang, Lifu Huang, Siyuan Qi, Dongfang Liu|As the scale of vision models continues to grow, the emergence of Visual Prompt Tuning (VPT) as a parameter-efficient transfer learning technique has gained attention due to its superior performance compared to traditional full-finetuning. However, the conditions favoring VPT (the ``when\") and the underlying rationale (the ``why\") remain unclear. In this paper, we conduct a comprehensive analysis across 19 distinct datasets and tasks. To understand the ``when\" aspect, we identify the scenarios where VPT proves favorable by two dimensions: task objectives and data distributions. We find that VPT is preferrable when there is 1) a substantial disparity between the original and the downstream task objectives (e.g., transitioning from classification to counting), or 2) a similarity in data distributions between the two tasks (e.g., both involve natural images). In exploring the ``why\" dimension, our results indicate VPT's success cannot be attributed solely to overfitting and optimization considerations. The unique way VPT preserves original features and adds parameters appears to be a pivotal factor. Our study provides insights into VPT's mechanisms, and offers guidance for its optimal utilization.|\u968f\u7740\u89c6\u89c9\u6a21\u578b\u89c4\u6a21\u7684\u4e0d\u65ad\u589e\u957f\uff0c\u89c6\u89c9\u63d0\u793a\u8c03\u6574\uff08VPT\uff09\u4f5c\u4e3a\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\u7684\u51fa\u73b0\uff0c\u7531\u4e8e\u5176\u76f8\u5bf9\u4e8e\u4f20\u7edf\u5168\u5fae\u8c03\u7684\u4f18\u8d8a\u6027\u80fd\u800c\u53d7\u5230\u5173\u6ce8\u3002\u7136\u800c\uff0c\u6709\u5229\u4e8e VPT \u7684\u6761\u4ef6\uff08\u201c\u4f55\u65f6\u201d\uff09\u548c\u57fa\u672c\u539f\u7406\uff08\u201c\u4e3a\u4ec0\u4e48\u201d\uff09\u4ecd\u4e0d\u6e05\u695a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9 19 \u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u548c\u4efb\u52a1\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002\u4e3a\u4e86\u7406\u89e3\u201c\u4f55\u65f6\u201d\u65b9\u9762\uff0c\u6211\u4eec\u901a\u8fc7\u4e24\u4e2a\u7ef4\u5ea6\u6765\u786e\u5b9a VPT \u88ab\u8bc1\u660e\u662f\u6709\u5229\u7684\u573a\u666f\uff1a\u4efb\u52a1\u76ee\u6807\u548c\u6570\u636e\u5206\u5e03\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5f53\u5b58\u5728 1) \u539f\u59cb\u4efb\u52a1\u76ee\u6807\u548c\u4e0b\u6e38\u4efb\u52a1\u76ee\u6807\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u5dee\u5f02\u65f6\uff0cVPT \u66f4\u53ef\u53d6\uff08\u4f8b\u5982\uff0c\u4ece\u5206\u7c7b\u8fc7\u6e21\u5230\u8ba1\u6570\uff09\uff0c\u6216 2\uff09\u4e24\u4e2a\u4efb\u52a1\u4e4b\u95f4\u6570\u636e\u5206\u5e03\u7684\u76f8\u4f3c\u6027\uff08\u4f8b\u5982\uff0c\u90fd\u6d89\u53ca\u81ea\u7136\u56fe\u50cf\uff09\u3002\u5728\u63a2\u7d22\u201c\u4e3a\u4ec0\u4e48\u201d\u7ef4\u5ea6\u65f6\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e VPT \u7684\u6210\u529f\u4e0d\u80fd\u4ec5\u4ec5\u5f52\u56e0\u4e8e\u8fc7\u5ea6\u62df\u5408\u548c\u4f18\u5316\u8003\u8651\u3002 VPT \u4fdd\u7559\u539f\u59cb\u7279\u5f81\u5e76\u6dfb\u52a0\u53c2\u6570\u7684\u72ec\u7279\u65b9\u5f0f\u4f3c\u4e4e\u662f\u4e00\u4e2a\u5173\u952e\u56e0\u7d20\u3002\u6211\u4eec\u7684\u7814\u7a76\u6df1\u5165\u4e86\u89e3\u4e86 VPT \u7684\u673a\u5236\uff0c\u5e76\u4e3a\u5176\u6700\u4f73\u5229\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002|[2401.12902v1](http://arxiv.org/pdf/2401.12902v1)|null|\n", "2401.12870": "|**2024-01-23**|**Unlocking the Potential: Multi-task Deep Learning for Spaceborne Quantitative Monitoring of Fugitive Methane Plumes**|\u91ca\u653e\u6f5c\u529b\uff1a\u7528\u4e8e\u661f\u8f7d\u9003\u9038\u7532\u70f7\u7fbd\u6d41\u5b9a\u91cf\u76d1\u6d4b\u7684\u591a\u4efb\u52a1\u6df1\u5ea6\u5b66\u4e60|Guoxin Si, Shiliang Fu, Wei Yao|With the intensification of global warming, the monitoring of methane emission and detection of gas plumes from landfills have increasingly received attention. We decompose methane emission monitoring into three sub-tasks: methane concentration inversion, plume segmentation, and emission rate estimation. Conventional algorithms have limitations: methane concentration inversion usually uses the matched filter, which is sensitive to global spectrum distribution and contains a large amount of noises. There is limited research on plume segmentation, with many studies resorting to manual segmentation that is likely to be subjective. The estimation of methane emission rate often utilizes IME algorithm, which relies on obtaining meteorological measurement data. Using the WENT landfill site in Hong Kong and PRISMA hyperspectral satellite imagery, we propose a new deep learning-based framework for quantitative monitoring of methane emissions from remote sensing images based on physical simulation. We generate simulated methane plumes using large eddy simulation (LES) and different concentration maps of fugitive emission using the radiative transfer equation (RTE), while combining augmentation techniques to create a simulated PRISMA dataset. We train a U-Net network for methane concentration inversion, a Mask R-CNN network for methane plume segmentation, and a ResNet-50 network for methane emission rate estimation. All three deep networks achieve higher validation accuracy compared to conventional algorithms. We further respectively combine the first two sub-tasks and the last two sub-tasks to design the multi-task learning models - MTL-01 and MTL-02, both of which achieve higher accuracy than single-task models. Our research serves as a demonstration of applying multi-task deep learning to quantitative methane monitoring and can be extended to a broad range of methane monitoring tasks.|\u968f\u7740\u5168\u7403\u53d8\u6696\u7684\u52a0\u5267\uff0c\u7532\u70f7\u6392\u653e\u7684\u76d1\u6d4b\u548c\u5783\u573e\u586b\u57cb\u573a\u6c14\u4f53\u7fbd\u6d41\u7684\u68c0\u6d4b\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u6211\u4eec\u5c06\u7532\u70f7\u6392\u653e\u76d1\u6d4b\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u4efb\u52a1\uff1a\u7532\u70f7\u6d53\u5ea6\u53cd\u6f14\u3001\u7fbd\u6d41\u5206\u5272\u548c\u6392\u653e\u7387\u4f30\u7b97\u3002\u4f20\u7edf\u7b97\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7532\u70f7\u6d53\u5ea6\u53cd\u6f14\u901a\u5e38\u91c7\u7528\u5339\u914d\u6ee4\u6ce2\u5668\uff0c\u8be5\u6ee4\u6ce2\u5668\u5bf9\u5168\u5c40\u8c31\u5206\u5e03\u654f\u611f\uff0c\u4e14\u5305\u542b\u5927\u91cf\u566a\u58f0\u3002\u5173\u4e8e\u7fbd\u6d41\u5206\u5272\u7684\u7814\u7a76\u6709\u9650\uff0c\u8bb8\u591a\u7814\u7a76\u91c7\u7528\u53ef\u80fd\u662f\u4e3b\u89c2\u7684\u624b\u52a8\u5206\u5272\u3002\u7532\u70f7\u6392\u653e\u7387\u7684\u4f30\u7b97\u901a\u5e38\u91c7\u7528IME\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u83b7\u53d6\u6c14\u8c61\u6d4b\u91cf\u6570\u636e\u3002\u5229\u7528\u9999\u6e2fWENT\u5783\u573e\u586b\u57cb\u573a\u548cPRISMA\u9ad8\u5149\u8c31\u536b\u661f\u56fe\u50cf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8e\u7269\u7406\u6a21\u62df\u7684\u9065\u611f\u56fe\u50cf\u5b9a\u91cf\u76d1\u6d4b\u7532\u70f7\u6392\u653e\u3002\u6211\u4eec\u4f7f\u7528\u5927\u6da1\u6a21\u62df (LES) \u751f\u6210\u6a21\u62df\u7532\u70f7\u7fbd\u6d41\uff0c\u5e76\u4f7f\u7528\u8f90\u5c04\u4f20\u8f93\u65b9\u7a0b (RTE) \u751f\u6210\u4e0d\u540c\u9038\u6563\u6392\u653e\u6d53\u5ea6\u56fe\uff0c\u540c\u65f6\u7ed3\u5408\u589e\u5f3a\u6280\u672f\u521b\u5efa\u6a21\u62df PRISMA \u6570\u636e\u96c6\u3002\u6211\u4eec\u8bad\u7ec3\u7528\u4e8e\u7532\u70f7\u6d53\u5ea6\u53cd\u6f14\u7684 U-Net \u7f51\u7edc\u3001\u7528\u4e8e\u7532\u70f7\u7fbd\u6d41\u5206\u5272\u7684 Mask R-CNN \u7f51\u7edc\u4ee5\u53ca\u7528\u4e8e\u7532\u70f7\u6392\u653e\u7387\u4f30\u8ba1\u7684 ResNet-50 \u7f51\u7edc\u3002\u4e0e\u4f20\u7edf\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8fd9\u4e09\u4e2a\u6df1\u5ea6\u7f51\u7edc\u90fd\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u9a8c\u8bc1\u7cbe\u5ea6\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5206\u522b\u7ed3\u5408\u524d\u4e24\u4e2a\u5b50\u4efb\u52a1\u548c\u540e\u4e24\u4e2a\u5b50\u4efb\u52a1\u6765\u8bbe\u8ba1\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\u2014\u2014MTL-01\u548cMTL-02\uff0c\u5b83\u4eec\u90fd\u6bd4\u5355\u4efb\u52a1\u6a21\u578b\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002\u6211\u4eec\u7684\u7814\u7a76\u662f\u5c06\u591a\u4efb\u52a1\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e8e\u5b9a\u91cf\u7532\u70f7\u76d1\u6d4b\u7684\u793a\u8303\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u5e7f\u6cdb\u7684\u7532\u70f7\u76d1\u6d4b\u4efb\u52a1\u3002|[2401.12870v1](http://arxiv.org/pdf/2401.12870v1)|null|\n", "2401.12851": "|**2024-01-23**|**Classification of grapevine varieties using UAV hyperspectral imaging**|\u5229\u7528\u65e0\u4eba\u673a\u9ad8\u5149\u8c31\u6210\u50cf\u5bf9\u8461\u8404\u54c1\u79cd\u8fdb\u884c\u5206\u7c7b|Alfonso L\u00f3pez, Carlos Javier Ogayar, Francisco Ram\u00f3n Feito, Joaquim Jo\u00e3o Sousa|The classification of different grapevine varieties is a relevant phenotyping task in Precision Viticulture since it enables estimating the growth of vineyard rows dedicated to different varieties, among other applications concerning the wine industry. This task can be performed with destructive methods that require time-consuming tasks, including data collection and analysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a more efficient and less prohibitive approach to collecting hyperspectral data, despite acquiring noisier data. Therefore, the first task is the processing of these data to correct and downsample large amounts of data. In addition, the hyperspectral signatures of grape varieties are very similar. In this work, a Convolutional Neural Network (CNN) is proposed for classifying seventeen varieties of red and white grape variants. Rather than classifying single samples, these are processed together with their neighbourhood. Hence, the extraction of spatial and spectral features is addressed with 1) a spatial attention layer and 2) Inception blocks. The pipeline goes from processing to dataset elaboration, finishing with the training phase. The fitted model is evaluated in terms of response time, accuracy and data separability, and compared with other state-of-the-art CNNs for classifying hyperspectral data. Our network was proven to be much more lightweight with a reduced number of input bands, a lower number of trainable weights and therefore, reduced training time. Despite this, the evaluated metrics showed much better results for our network (~99% overall accuracy), in comparison with previous works barely achieving 81% OA.|\u4e0d\u540c\u8461\u8404\u54c1\u79cd\u7684\u5206\u7c7b\u662f\u7cbe\u51c6\u8461\u8404\u683d\u57f9\u4e2d\u7684\u4e00\u9879\u76f8\u5173\u8868\u578b\u5206\u6790\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u4f30\u8ba1\u4e13\u95e8\u7528\u4e8e\u4e0d\u540c\u54c1\u79cd\u7684\u8461\u8404\u56ed\u884c\u7684\u751f\u957f\uff0c\u4ee5\u53ca\u4e0e\u8461\u8404\u9152\u884c\u4e1a\u76f8\u5173\u7684\u5176\u4ed6\u5e94\u7528\u3002\u8be5\u4efb\u52a1\u53ef\u4ee5\u901a\u8fc7\u7834\u574f\u6027\u65b9\u6cd5\u6765\u6267\u884c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u8017\u65f6\u7684\u4efb\u52a1\uff0c\u5305\u62ec\u5b9e\u9a8c\u5ba4\u4e2d\u7684\u6570\u636e\u6536\u96c6\u548c\u5206\u6790\u3002\u7136\u800c\uff0c\u65e0\u4eba\u673a (UAV) \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u4e14\u66f4\u5bbd\u677e\u7684\u65b9\u6cd5\u6765\u6536\u96c6\u9ad8\u5149\u8c31\u6570\u636e\uff0c\u5c3d\u7ba1\u5b83\u83b7\u53d6\u7684\u6570\u636e\u566a\u58f0\u66f4\u5927\u3002\u56e0\u6b64\uff0c\u9996\u8981\u4efb\u52a1\u662f\u5bf9\u8fd9\u4e9b\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u5bf9\u5927\u91cf\u6570\u636e\u8fdb\u884c\u6821\u6b63\u548c\u4e0b\u91c7\u6837\u3002\u6b64\u5916\uff0c\u8461\u8404\u54c1\u79cd\u7684\u9ad8\u5149\u8c31\u7279\u5f81\u975e\u5e38\u76f8\u4f3c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6765\u5bf9 17 \u4e2a\u7ea2\u8461\u8404\u54c1\u79cd\u548c\u767d\u8461\u8404\u54c1\u79cd\u8fdb\u884c\u5206\u7c7b\u3002\u8fd9\u4e9b\u6837\u672c\u4e0d\u662f\u5bf9\u5355\u4e2a\u6837\u672c\u8fdb\u884c\u5206\u7c7b\uff0c\u800c\u662f\u4e0e\u5176\u90bb\u8fd1\u6837\u672c\u4e00\u8d77\u8fdb\u884c\u5904\u7406\u3002\u56e0\u6b64\uff0c\u7a7a\u95f4\u548c\u5149\u8c31\u7279\u5f81\u7684\u63d0\u53d6\u901a\u8fc7 1) \u7a7a\u95f4\u6ce8\u610f\u529b\u5c42\u548c 2) Inception \u5757\u6765\u89e3\u51b3\u3002\u8be5\u7ba1\u9053\u4ece\u5904\u7406\u5230\u6570\u636e\u96c6\u7ec6\u5316\uff0c\u6700\u540e\u5230\u8bad\u7ec3\u9636\u6bb5\u3002\u62df\u5408\u6a21\u578b\u5728\u54cd\u5e94\u65f6\u95f4\u3001\u51c6\u786e\u6027\u548c\u6570\u636e\u53ef\u5206\u79bb\u6027\u65b9\u9762\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e0e\u5176\u4ed6\u7528\u4e8e\u9ad8\u5149\u8c31\u6570\u636e\u5206\u7c7b\u7684\u6700\u5148\u8fdb\u7684 CNN \u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u4e8b\u5b9e\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u7f51\u7edc\u66f4\u52a0\u8f7b\u91cf\u7ea7\uff0c\u8f93\u5165\u9891\u6bb5\u6570\u91cf\u51cf\u5c11\uff0c\u53ef\u8bad\u7ec3\u6743\u91cd\u6570\u91cf\u51cf\u5c11\uff0c\u56e0\u6b64\u8bad\u7ec3\u65f6\u95f4\u4e5f\u51cf\u5c11\u4e86\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u8bc4\u4f30\u6307\u6807\u663e\u793a\u6211\u4eec\u7684\u7f51\u7edc\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff08\u603b\u4f53\u51c6\u786e\u7387\u7ea6\u4e3a 99%\uff09\uff0c\u800c\u4e4b\u524d\u7684\u5de5\u4f5c\u4ec5\u52c9\u5f3a\u5b9e\u73b0 81% OA\u3002|[2401.12851v1](http://arxiv.org/pdf/2401.12851v1)|null|\n", "2401.12820": "|**2024-01-23**|**DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer**|DatUS^2\uff1a\u6570\u636e\u9a71\u52a8\u7684\u65e0\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4e0e\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u89c6\u89c9 Transformer|Sonal Kumar, Arijit Sur, Rashmi Dutta Baruah|Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model. In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme. However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer. Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data. We compare these pseudo-annotated segmentation masks with ground truth masks for evaluating recent self-supervised training schemes to learn shared semantic properties at the patch level and discriminative semantic properties at the segment level. Finally, we evaluate existing state-of-the-art self-supervised training schemes with our proposed downstream task, i.e., DatUS^2. Also, the best version of DatUS^2 outperforms the existing state-of-the-art method for the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47% Pixel accuracy on the SUIM dataset. It also achieves a competitive level of accuracy for a large-scale and complex dataset, i.e., the COCO dataset.|\u591a\u4e2a\u81ea\u76d1\u7763\u57f9\u8bad\u65b9\u6848\u7684\u63d0\u6848\u4e0d\u65ad\u6d8c\u73b0\uff0c\u8ddd\u79bb\u5f00\u53d1\u901a\u7528\u57fa\u7840\u6a21\u578b\u53c8\u8fd1\u4e86\u4e00\u6b65\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u76d1\u7763\u4e0b\u6e38\u4efb\u52a1\u88ab\u8ba4\u4e3a\u662f\u9a8c\u8bc1\u901a\u8fc7\u81ea\u76d1\u7763\u8bad\u7ec3\u65b9\u6848\u5b66\u4e60\u7684\u89c6\u89c9\u7279\u5f81\u8d28\u91cf\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e4b\u4e00\u3002\u7136\u800c\uff0c\u65e0\u76d1\u7763\u5bc6\u96c6\u8bed\u4e49\u5206\u5272\u5c1a\u672a\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u63a2\u7d22\uff0c\u5b83\u53ef\u4ee5\u5728\u89c6\u89c9\u53d8\u6362\u5668\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u671f\u95f4\u5229\u7528\u548c\u8bc4\u4f30\u5757\u7ea7\u7279\u5f81\u8868\u793a\u4e2d\u5f15\u5165\u7684\u8bed\u4e49\u4fe1\u606f\u7684\u8d28\u91cf\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u9a71\u52a8\u7684\u65e0\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff08DatUS^2\uff09\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u3002 DatUS^2 \u4e3a\u672a\u6807\u8bb0\u7684\u56fe\u50cf\u6570\u636e\u96c6\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u4e14\u5bc6\u96c6\u7684\u4f2a\u6ce8\u91ca\u5206\u5272\u63a9\u6a21\uff0c\u800c\u4e0d\u4f7f\u7528\u4efb\u4f55\u89c6\u89c9\u5148\u9a8c\u6216\u540c\u6b65\u6570\u636e\u3002\u6211\u4eec\u5c06\u8fd9\u4e9b\u4f2a\u6ce8\u91ca\u7684\u5206\u5272\u63a9\u7801\u4e0e\u771f\u5b9e\u63a9\u7801\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u8bc4\u4f30\u6700\u8fd1\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u65b9\u6848\uff0c\u4ee5\u5b66\u4e60\u8865\u4e01\u7ea7\u522b\u7684\u5171\u4eab\u8bed\u4e49\u5c5e\u6027\u548c\u5206\u6bb5\u7ea7\u522b\u7684\u5224\u522b\u8bed\u4e49\u5c5e\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684\u4e0b\u6e38\u4efb\u52a1\uff08\u5373 DatUS^2\uff09\u8bc4\u4f30\u73b0\u6709\u6700\u5148\u8fdb\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u65b9\u6848\u3002\u6b64\u5916\uff0cDatUS^2 \u7684\u6700\u4f73\u7248\u672c\u5728\u65e0\u76d1\u7763\u5bc6\u96c6\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728 SUIM \u6570\u636e\u96c6\u4e0a\u5177\u6709 15.02% MiOU \u548c 21.47% \u50cf\u7d20\u7cbe\u5ea6\u3002\u5bf9\u4e8e\u5927\u89c4\u6a21\u4e14\u590d\u6742\u7684\u6570\u636e\u96c6\uff08\u5373 COCO \u6570\u636e\u96c6\uff09\uff0c\u5b83\u8fd8\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u6c34\u5e73\u3002|[2401.12820v1](http://arxiv.org/pdf/2401.12820v1)|null|\n", "2401.12761": "|**2024-01-23**|**MUSES: The Multi-Sensor Semantic Perception Dataset for Driving under Uncertainty**|MUSES\uff1a\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u9a7e\u9a76\u7684\u591a\u4f20\u611f\u5668\u8bed\u4e49\u611f\u77e5\u6570\u636e\u96c6|Tim Br\u00f6dermann, David Bruggemann, Christos Sakaridis, Kevin Ta, Odysseas Liagouris, Jason Corkill, Luc Van Gool|Achieving level-5 driving automation in autonomous vehicles necessitates a robust semantic visual perception system capable of parsing data from different sensors across diverse conditions. However, existing semantic perception datasets often lack important non-camera modalities typically used in autonomous vehicles, or they do not exploit such modalities to aid and improve semantic annotations in challenging conditions. To address this, we introduce MUSES, the MUlti-SEnsor Semantic perception dataset for driving in adverse conditions under increased uncertainty. MUSES includes synchronized multimodal recordings with 2D panoptic annotations for 2500 images captured under diverse weather and illumination. The dataset integrates a frame camera, a lidar, a radar, an event camera, and an IMU/GNSS sensor. Our new two-stage panoptic annotation protocol captures both class-level and instance-level uncertainty in the ground truth and enables the novel task of uncertainty-aware panoptic segmentation we introduce, along with standard semantic and panoptic segmentation. MUSES proves both effective for training and challenging for evaluating models under diverse visual conditions, and it opens new avenues for research in multimodal and uncertainty-aware dense semantic perception. Our dataset and benchmark will be made publicly available.|\u8981\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e2d\u5b9e\u73b0 5 \u7ea7\u9a7e\u9a76\u81ea\u52a8\u5316\uff0c\u9700\u8981\u4e00\u4e2a\u5f3a\u5927\u7684\u8bed\u4e49\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u89e3\u6790\u6765\u81ea\u4e0d\u540c\u4f20\u611f\u5668\u7684\u6570\u636e\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u8bed\u4e49\u611f\u77e5\u6570\u636e\u96c6\u901a\u5e38\u7f3a\u4e4f\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e2d\u901a\u5e38\u4f7f\u7528\u7684\u91cd\u8981\u7684\u975e\u76f8\u673a\u6a21\u6001\uff0c\u6216\u8005\u5b83\u4eec\u6ca1\u6709\u5229\u7528\u6b64\u7c7b\u6a21\u6001\u6765\u5e2e\u52a9\u548c\u6539\u8fdb\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u7684\u8bed\u4e49\u6ce8\u91ca\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 MUSES\uff0c\u5373\u591a\u4f20\u611f\u5668\u8bed\u4e49\u611f\u77e5\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5728\u4e0d\u786e\u5b9a\u6027\u589e\u52a0\u7684\u4e0d\u5229\u6761\u4ef6\u4e0b\u9a7e\u9a76\u3002 MUSES \u5305\u62ec\u540c\u6b65\u591a\u6a21\u6001\u8bb0\u5f55\u548c 2D \u5168\u666f\u6ce8\u91ca\uff0c\u53ef\u8bb0\u5f55\u5728\u4e0d\u540c\u5929\u6c14\u548c\u7167\u660e\u4e0b\u6355\u83b7\u7684 2500 \u5f20\u56fe\u50cf\u3002\u8be5\u6570\u636e\u96c6\u96c6\u6210\u4e86\u5e27\u76f8\u673a\u3001\u6fc0\u5149\u96f7\u8fbe\u3001\u96f7\u8fbe\u3001\u4e8b\u4ef6\u76f8\u673a\u548c IMU/GNSS \u4f20\u611f\u5668\u3002\u6211\u4eec\u65b0\u7684\u4e24\u9636\u6bb5\u5168\u666f\u6ce8\u91ca\u534f\u8bae\u6355\u83b7\u4e86\u771f\u5b9e\u60c5\u51b5\u4e2d\u7684\u7c7b\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u6211\u4eec\u5f15\u5165\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5168\u666f\u5206\u5272\u7684\u65b0\u4efb\u52a1\uff0c\u4ee5\u53ca\u6807\u51c6\u8bed\u4e49\u548c\u5168\u666f\u5206\u5272\u3002\u4e8b\u5b9e\u8bc1\u660e\uff0cMUSES \u5bf9\u4e8e\u8bad\u7ec3\u6709\u6548\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u89c6\u89c9\u6761\u4ef6\u4e0b\u8bc4\u4f30\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u5b83\u4e3a\u591a\u6a21\u6001\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5bc6\u96c6\u8bed\u4e49\u611f\u77e5\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5c06\u516c\u5f00\u3002|[2401.12761v1](http://arxiv.org/pdf/2401.12761v1)|null|\n", "2401.12743": "|**2024-01-23**|**Correlation-Embedded Transformer Tracking: A Single-Branch Framework**|\u76f8\u5173\u5d4c\u5165\u5f0f\u53d8\u538b\u5668\u8ddf\u8e2a\uff1a\u5355\u5206\u652f\u6846\u67b6|Fei Xie, Wankou Yang, Chunyu Wang, Lei Chu, Yue Cao, Chao Ma, Wenjun Zeng|Developing robust and discriminative appearance models has been a long-standing research challenge in visual object tracking. In the prevalent Siamese-based paradigm, the features extracted by the Siamese-like networks are often insufficient to model the tracked targets and distractor objects, thereby hindering them from being robust and discriminative simultaneously. While most Siamese trackers focus on designing robust correlation operations, we propose a novel single-branch tracking framework inspired by the transformer. Unlike the Siamese-like feature extraction, our tracker deeply embeds cross-image feature correlation in multiple layers of the feature network. By extensively matching the features of the two images through multiple layers, it can suppress non-target features, resulting in target-aware feature extraction. The output features can be directly used for predicting target locations without additional correlation steps. Thus, we reformulate the two-branch Siamese tracking as a conceptually simple, fully transformer-based Single-Branch Tracking pipeline, dubbed SBT. After conducting an in-depth analysis of the SBT baseline, we summarize many effective design principles and propose an improved tracker dubbed SuperSBT. SuperSBT adopts a hierarchical architecture with a local modeling layer to enhance shallow-level features. A unified relation modeling is proposed to remove complex handcrafted layer pattern designs. SuperSBT is further improved by masked image modeling pre-training, integrating temporal modeling, and equipping with dedicated prediction heads. Thus, SuperSBT outperforms the SBT baseline by 4.7%,3.0%, and 4.5% AUC scores in LaSOT, TrackingNet, and GOT-10K. Notably, SuperSBT greatly raises the speed of SBT from 37 FPS to 81 FPS. Extensive experiments show that our method achieves superior results on eight VOT benchmarks.|\u5f00\u53d1\u9c81\u68d2\u4e14\u5177\u6709\u8fa8\u522b\u529b\u7684\u5916\u89c2\u6a21\u578b\u4e00\u76f4\u662f\u89c6\u89c9\u5bf9\u8c61\u8ddf\u8e2a\u9886\u57df\u957f\u671f\u5b58\u5728\u7684\u7814\u7a76\u6311\u6218\u3002\u5728\u6d41\u884c\u7684\u57fa\u4e8e\u66b9\u7f57\u7684\u8303\u5f0f\u4e2d\uff0c\u7c7b\u66b9\u7f57\u7f51\u7edc\u63d0\u53d6\u7684\u7279\u5f81\u901a\u5e38\u4e0d\u8db3\u4ee5\u5bf9\u8ddf\u8e2a\u76ee\u6807\u548c\u5e72\u6270\u5bf9\u8c61\u8fdb\u884c\u5efa\u6a21\uff0c\u4ece\u800c\u963b\u788d\u4e86\u5b83\u4eec\u540c\u65f6\u5177\u6709\u9c81\u68d2\u6027\u548c\u5224\u522b\u6027\u3002\u867d\u7136\u5927\u591a\u6570\u66b9\u7f57\u8ddf\u8e2a\u5668\u4e13\u6ce8\u4e8e\u8bbe\u8ba1\u5f3a\u5927\u7684\u76f8\u5173\u64cd\u4f5c\uff0c\u4f46\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u53d8\u538b\u5668\u542f\u53d1\u7684\u65b0\u9896\u7684\u5355\u5206\u652f\u8ddf\u8e2a\u6846\u67b6\u3002\u4e0e\u7c7b\u4f3c Siamese \u7684\u7279\u5f81\u63d0\u53d6\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u8ddf\u8e2a\u5668\u5c06\u8de8\u56fe\u50cf\u7279\u5f81\u76f8\u5173\u6027\u6df1\u5ea6\u5d4c\u5165\u5230\u7279\u5f81\u7f51\u7edc\u7684\u591a\u5c42\u4e2d\u3002\u901a\u8fc7\u591a\u5c42\u5bf9\u4e24\u5e45\u56fe\u50cf\u7684\u7279\u5f81\u8fdb\u884c\u5e7f\u6cdb\u5339\u914d\uff0c\u53ef\u4ee5\u6291\u5236\u975e\u76ee\u6807\u7279\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u76ee\u6807\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u3002\u8f93\u51fa\u7279\u5f81\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u9884\u6d4b\u76ee\u6807\u4f4d\u7f6e\uff0c\u65e0\u9700\u989d\u5916\u7684\u76f8\u5173\u6b65\u9aa4\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u4e24\u5206\u652f\u8fde\u4f53\u8ddf\u8e2a\u91cd\u65b0\u8868\u8ff0\u4e3a\u6982\u5ff5\u4e0a\u7b80\u5355\u3001\u5b8c\u5168\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u5355\u5206\u652f\u8ddf\u8e2a\u7ba1\u9053\uff0c\u79f0\u4e3a SBT\u3002\u5728\u5bf9 SBT \u57fa\u7ebf\u8fdb\u884c\u6df1\u5165\u5206\u6790\u540e\uff0c\u6211\u4eec\u603b\u7ed3\u4e86\u8bb8\u591a\u6709\u6548\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8ddf\u8e2a\u5668 SuperSBT\u3002 SuperSBT\u91c7\u7528\u5177\u6709\u5c40\u90e8\u5efa\u6a21\u5c42\u7684\u5206\u5c42\u67b6\u6784\u6765\u589e\u5f3a\u6d45\u5c42\u7279\u5f81\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5173\u7cfb\u5efa\u6a21\u6765\u6d88\u9664\u590d\u6742\u7684\u624b\u5de5\u5c42\u6a21\u5f0f\u8bbe\u8ba1\u3002 SuperSBT\u901a\u8fc7\u63a9\u6a21\u56fe\u50cf\u5efa\u6a21\u9884\u8bad\u7ec3\u3001\u96c6\u6210\u65f6\u95f4\u5efa\u6a21\u4ee5\u53ca\u914d\u5907\u4e13\u7528\u9884\u6d4b\u5934\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002\u56e0\u6b64\uff0cSuperSBT \u5728 LaSOT\u3001TrackingNet \u548c GOT-10K \u4e2d\u7684 AUC \u5206\u6570\u6bd4 SBT \u57fa\u7ebf\u9ad8\u51fa 4.7%\u30013.0% \u548c 4.5%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSuperSBT \u5c06 SBT \u7684\u901f\u5ea6\u4ece 37 FPS \u5927\u5e45\u63d0\u5347\u81f3 81 FPS\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u516b\u4e2a VOT \u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002|[2401.12743v1](http://arxiv.org/pdf/2401.12743v1)|null|\n", "2401.12729": "|**2024-01-23**|**Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios**|\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u6bd4\u4f8b\u7c7b\u5e73\u8861\u6280\u672f\u589e\u5f3a\u5c0f\u7269\u4f53\u7684\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\uff1a\u5de5\u4e1a\u573a\u666f\u7684\u6bd4\u8f83\u7814\u7a76|Jibinraj Antony, Vinit Hegiste, Ali Nazeri, Hooman Tavakoli, Snehal Walunj, Christiane Plociennik, Martin Ruskowski|Object Detection (OD) has proven to be a significant computer vision method in extracting localized class information and has multiple applications in the industry. Although many of the state-of-the-art (SOTA) OD models perform well on medium and large sized objects, they seem to under perform on small objects. In most of the industrial use cases, it is difficult to collect and annotate data for small objects, as it is time-consuming and prone to human errors. Additionally, those datasets are likely to be unbalanced and often result in an inefficient model convergence. To tackle this challenge, this study presents a novel approach that injects additional data points to improve the performance of the OD models. Using synthetic data generation, the difficulties in data collection and annotations for small object data points can be minimized and to create a dataset with balanced distribution. This paper discusses the effects of a simple proportional class-balancing technique, to enable better anchor matching of the OD models. A comparison was carried out on the performances of the SOTA OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and synthetic datasets within an industrial use case.|\u5bf9\u8c61\u68c0\u6d4b\uff08OD\uff09\u5df2\u88ab\u8bc1\u660e\u662f\u63d0\u53d6\u672c\u5730\u5316\u7c7b\u522b\u4fe1\u606f\u7684\u91cd\u8981\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u884c\u4e1a\u4e2d\u5177\u6709\u591a\u79cd\u5e94\u7528\u3002\u5c3d\u7ba1\u8bb8\u591a\u6700\u5148\u8fdb\u7684 (SOTA) OD \u6a21\u578b\u5728\u4e2d\u578b\u548c\u5927\u578b\u7269\u4f53\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u4f3c\u4e4e\u5728\u5c0f\u578b\u7269\u4f53\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u5728\u5927\u591a\u6570\u5de5\u4e1a\u7528\u4f8b\u4e2d\uff0c\u6536\u96c6\u548c\u6ce8\u91ca\u5c0f\u5bf9\u8c61\u7684\u6570\u636e\u5f88\u56f0\u96be\uff0c\u56e0\u4e3a\u8fd9\u65e2\u8017\u65f6\u53c8\u5bb9\u6613\u51fa\u73b0\u4eba\u4e3a\u9519\u8bef\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u53ef\u80fd\u4e0d\u5e73\u8861\uff0c\u5e76\u4e14\u5e38\u5e38\u5bfc\u81f4\u6a21\u578b\u6536\u655b\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u6ce8\u5165\u989d\u5916\u7684\u6570\u636e\u70b9\u6765\u63d0\u9ad8 OD \u6a21\u578b\u7684\u6027\u80fd\u3002\u4f7f\u7528\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5c0f\u5bf9\u8c61\u6570\u636e\u70b9\u7684\u6570\u636e\u6536\u96c6\u548c\u6ce8\u91ca\u7684\u96be\u5ea6\uff0c\u5e76\u521b\u5efa\u5206\u5e03\u5747\u8861\u7684\u6570\u636e\u96c6\u3002\u672c\u6587\u8ba8\u8bba\u4e86\u7b80\u5355\u7684\u6bd4\u4f8b\u7c7b\u5e73\u8861\u6280\u672f\u7684\u6548\u679c\uff0c\u4ee5\u5b9e\u73b0 OD \u6a21\u578b\u66f4\u597d\u7684\u951a\u5339\u914d\u3002\u9488\u5bf9\u5de5\u4e1a\u7528\u4f8b\u4e2d\u771f\u5b9e\u6570\u636e\u96c6\u548c\u5408\u6210\u6570\u636e\u96c6\u7684\u7ec4\u5408\uff0c\u5bf9 SOTA OD \u6a21\u578b\uff08YOLOv5\u3001YOLOv7 \u548c SSD\uff09\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002|[2401.12729v1](http://arxiv.org/pdf/2401.12729v1)|null|\n", "2401.12725": "|**2024-01-23**|**Two-View Topogram-Based Anatomy-Guided CT Reconstruction for Prospective Risk Minimization**|\u57fa\u4e8e\u53cc\u89c6\u56fe\u62d3\u6251\u56fe\u7684\u89e3\u5256\u5f15\u5bfc CT \u91cd\u5efa\uff0c\u5b9e\u73b0\u524d\u77bb\u6027\u98ce\u9669\u6700\u5c0f\u5316|Chang Liu, Laura Klein, Yixing Huang, Edith Baader, Michael Lell, Marc Kachelrie\u00df, Andreas Maier|To facilitate a prospective estimation of CT effective dose and risk minimization process, a prospective spatial dose estimation and the known anatomical structures are expected. To this end, a CT reconstruction method is required to reconstruct CT volumes from as few projections as possible, i.e. by using the topograms, with anatomical structures as correct as possible. In this work, an optimized CT reconstruction model based on a generative adversarial network (GAN) is proposed. The GAN is trained to reconstruct 3D volumes from an anterior-posterior and a lateral CT projection. To enhance anatomical structures, a pre-trained organ segmentation network and the 3D perceptual loss are applied during the training phase, so that the model can then generate both organ-enhanced CT volume and the organ segmentation mask. The proposed method can reconstruct CT volumes with PSNR of 26.49, RMSE of 196.17, and SSIM of 0.64, compared to 26.21, 201.55 and 0.63 using the baseline method. In terms of the anatomical structure, the proposed method effectively enhances the organ shape and boundary and allows for a straight-forward identification of the relevant anatomical structures. We note that conventional reconstruction metrics fail to indicate the enhancement of anatomical structures. In addition to such metrics, the evaluation is expanded with assessing the organ segmentation performance. The average organ dice of the proposed method is 0.71 compared with 0.63 in baseline model, indicating the enhancement of anatomical structures.|\u4e3a\u4e86\u4fc3\u8fdb CT \u6709\u6548\u5242\u91cf\u548c\u98ce\u9669\u6700\u5c0f\u5316\u8fc7\u7a0b\u7684\u524d\u77bb\u6027\u4f30\u8ba1\uff0c\u9700\u8981\u524d\u77bb\u6027\u7a7a\u95f4\u5242\u91cf\u4f30\u8ba1\u548c\u5df2\u77e5\u7684\u89e3\u5256\u7ed3\u6784\u3002\u4e3a\u6b64\uff0c\u9700\u8981\u4e00\u79cd CT \u91cd\u5efa\u65b9\u6cd5\u6765\u4ece\u5c3d\u53ef\u80fd\u5c11\u7684\u6295\u5f71\uff08\u5373\u4f7f\u7528\u5730\u5f62\u56fe\uff09\u91cd\u5efa CT \u4f53\u79ef\uff0c\u5e76\u5177\u6709\u5c3d\u53ef\u80fd\u6b63\u786e\u7684\u89e3\u5256\u7ed3\u6784\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u4f18\u5316CT\u91cd\u5efa\u6a21\u578b\u3002 GAN \u7ecf\u8fc7\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6839\u636e\u524d\u540e\u548c\u6a2a\u5411 CT \u6295\u5f71\u91cd\u5efa 3D \u4f53\u79ef\u3002\u4e3a\u4e86\u589e\u5f3a\u89e3\u5256\u7ed3\u6784\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u5e94\u7528\u9884\u8bad\u7ec3\u7684\u5668\u5b98\u5206\u5272\u7f51\u7edc\u548c 3D \u611f\u77e5\u635f\u5931\uff0c\u4ee5\u4fbf\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u5668\u5b98\u589e\u5f3a CT \u4f53\u79ef\u548c\u5668\u5b98\u5206\u5272\u63a9\u6a21\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u91cd\u5efa CT \u4f53\u79ef\uff0cPSNR \u4e3a 26.49\uff0cRMSE \u4e3a 196.17\uff0cSSIM \u4e3a 0.64\uff0c\u800c\u4f7f\u7528\u57fa\u7ebf\u65b9\u6cd5\u7684\u7ed3\u679c\u4e3a 26.21\u3001201.55 \u548c 0.63\u3002\u5728\u89e3\u5256\u7ed3\u6784\u65b9\u9762\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86\u5668\u5b98\u5f62\u72b6\u548c\u8fb9\u754c\uff0c\u5e76\u53ef\u4ee5\u76f4\u63a5\u8bc6\u522b\u76f8\u5173\u89e3\u5256\u7ed3\u6784\u3002\u6211\u4eec\u6ce8\u610f\u5230\uff0c\u4f20\u7edf\u7684\u91cd\u5efa\u6307\u6807\u65e0\u6cd5\u8868\u660e\u89e3\u5256\u7ed3\u6784\u7684\u589e\u5f3a\u3002\u9664\u4e86\u8fd9\u4e9b\u6307\u6807\u4e4b\u5916\uff0c\u8fd8\u901a\u8fc7\u8bc4\u4f30\u5668\u5b98\u5206\u5272\u6027\u80fd\u6765\u6269\u5c55\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u7684\u5e73\u5747\u5668\u5b98\u9ab0\u5b50\u4e3a0.71\uff0c\u800c\u57fa\u7ebf\u6a21\u578b\u4e3a0.63\uff0c\u8868\u660e\u89e3\u5256\u7ed3\u6784\u5f97\u5230\u589e\u5f3a\u3002|[2401.12725v1](http://arxiv.org/pdf/2401.12725v1)|null|\n", "2401.12694": "|**2024-01-23**|**Pragmatic Communication in Multi-Agent Collaborative Perception**|\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u8bed\u7528\u6c9f\u901a|Yue Hu, Xianghe Pang, Xiaoqi Qin, Yonina C. Eldar, Siheng Chen, Ping Zhang, Wenjun Zhang|Collaborative perception allows each agent to enhance its perceptual abilities by exchanging messages with others. It inherently results in a trade-off between perception ability and communication costs. Previous works transmit complete full-frame high-dimensional feature maps among agents, resulting in substantial communication costs. To promote communication efficiency, we propose only transmitting the information needed for the collaborator's downstream task. This pragmatic communication strategy focuses on three key aspects: i) pragmatic message selection, which selects task-critical parts from the complete data, resulting in spatially and temporally sparse feature vectors; ii) pragmatic message representation, which achieves pragmatic approximation of high-dimensional feature vectors with a task-adaptive dictionary, enabling communicating with integer indices; iii) pragmatic collaborator selection, which identifies beneficial collaborators, pruning unnecessary communication links. Following this strategy, we first formulate a mathematical optimization framework for the perception-communication trade-off and then propose PragComm, a multi-agent collaborative perception system with two key components: i) single-agent detection and tracking and ii) pragmatic collaboration. The proposed PragComm promotes pragmatic communication and adapts to a wide range of communication conditions. We evaluate PragComm for both collaborative 3D object detection and tracking tasks in both real-world, V2V4Real, and simulation datasets, OPV2V and V2X-SIM2.0. PragComm consistently outperforms previous methods with more than 32.7K times lower communication volume on OPV2V. Code is available at github.com/PhyllisH/PragComm.|\u534f\u4f5c\u611f\u77e5\u5141\u8bb8\u6bcf\u4e2a\u4ee3\u7406\u901a\u8fc7\u4e0e\u5176\u4ed6\u4ee3\u7406\u4ea4\u6362\u6d88\u606f\u6765\u589e\u5f3a\u5176\u611f\u77e5\u80fd\u529b\u3002\u5b83\u672c\u8d28\u4e0a\u5bfc\u81f4\u611f\u77e5\u80fd\u529b\u548c\u901a\u4fe1\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u5728\u4ee3\u7406\u4e4b\u95f4\u4f20\u8f93\u5b8c\u6574\u7684\u5168\u5e27\u9ad8\u7ef4\u7279\u5f81\u56fe\uff0c\u5bfc\u81f4\u5927\u91cf\u7684\u901a\u4fe1\u6210\u672c\u3002\u4e3a\u4e86\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\uff0c\u6211\u4eec\u5efa\u8bae\u4ec5\u4f20\u8f93\u534f\u4f5c\u8005\u4e0b\u6e38\u4efb\u52a1\u6240\u9700\u7684\u4fe1\u606f\u3002\u8fd9\u79cd\u5b9e\u7528\u7684\u901a\u4fe1\u7b56\u7565\u4fa7\u91cd\u4e8e\u4e09\u4e2a\u5173\u952e\u65b9\u9762\uff1ai\uff09\u5b9e\u7528\u7684\u6d88\u606f\u9009\u62e9\uff0c\u4ece\u5b8c\u6574\u7684\u6570\u636e\u4e2d\u9009\u62e9\u4efb\u52a1\u5173\u952e\u90e8\u5206\uff0c\u4ece\u800c\u4ea7\u751f\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u7a00\u758f\u7684\u7279\u5f81\u5411\u91cf\uff1b ii) \u5b9e\u7528\u6d88\u606f\u8868\u793a\uff0c\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u5b57\u5178\u5b9e\u73b0\u9ad8\u7ef4\u7279\u5f81\u5411\u91cf\u7684\u5b9e\u7528\u8fd1\u4f3c\uff0c\u4ece\u800c\u80fd\u591f\u4e0e\u6574\u6570\u7d22\u5f15\u8fdb\u884c\u901a\u4fe1\uff1b iii) \u52a1\u5b9e\u7684\u5408\u4f5c\u8005\u9009\u62e9\uff0c\u8bc6\u522b\u6709\u76ca\u7684\u5408\u4f5c\u8005\uff0c\u4fee\u526a\u4e0d\u5fc5\u8981\u7684\u6c9f\u901a\u94fe\u63a5\u3002\u6309\u7167\u8fd9\u4e00\u7b56\u7565\uff0c\u6211\u4eec\u9996\u5148\u5236\u5b9a\u4e00\u4e2a\u7528\u4e8e\u611f\u77e5-\u901a\u4fe1\u6743\u8861\u7684\u6570\u5b66\u4f18\u5316\u6846\u67b6\uff0c\u7136\u540e\u63d0\u51fa PragComm\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u7cfb\u7edf\uff0c\u5177\u6709\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1ai\uff09\u5355\u667a\u80fd\u4f53\u68c0\u6d4b\u548c\u8ddf\u8e2a\u4ee5\u53ca ii\uff09\u52a1\u5b9e\u534f\u4f5c\u3002\u62df\u8bae\u7684 PragComm \u63d0\u5021\u52a1\u5b9e\u7684\u6c9f\u901a\u5e76\u9002\u5e94\u5e7f\u6cdb\u7684\u6c9f\u901a\u6761\u4ef6\u3002\u6211\u4eec\u8bc4\u4f30\u4e86 PragComm \u5728\u73b0\u5b9e\u4e16\u754c\uff08V2V4Real\uff09\u548c\u6a21\u62df\u6570\u636e\u96c6\uff08OPV2V \u548c V2X-SIM2.0\uff09\u4e2d\u7684\u534f\u4f5c 3D \u5bf9\u8c61\u68c0\u6d4b\u548c\u8ddf\u8e2a\u4efb\u52a1\u3002 PragComm \u7684\u6027\u80fd\u59cb\u7ec8\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff0cOPV2V \u4e0a\u7684\u901a\u4fe1\u91cf\u964d\u4f4e\u4e86 32.7K \u500d\u4ee5\u4e0a\u3002\u4ee3\u7801\u53ef\u5728 github.com/PhyllisH/PragComm \u83b7\u53d6\u3002|[2401.12694v1](http://arxiv.org/pdf/2401.12694v1)|null|\n", "2401.12689": "|**2024-01-23**|**Energy-based Automated Model Evaluation**|\u57fa\u4e8e\u80fd\u91cf\u7684\u81ea\u52a8\u5316\u6a21\u578b\u8bc4\u4f30|Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, Junbo Zhao|The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive experiments across modalities, datasets and different architectural backbones to validate MDE's validity, together with its superiority compared with prior approaches. We also prove MDE's versatility by showing its seamless integration with large-scale models, and easy adaption to learning scenarios with noisy- or imbalanced- labels.|\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u4f20\u7edf\u8bc4\u4f30\u534f\u8bae\u4e25\u91cd\u4f9d\u8d56\u4e8e\u5e26\u6807\u7b7e\u7684\u3001\u72ec\u7acb\u540c\u5206\u5e03\u5047\u8bbe\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u800c\u8fd9\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\u5e76\u4e0d\u5e38\u89c1\u3002\u81ea\u52a8\u6a21\u578b\u8bc4\u4f30 (AutoEval) \u5c55\u793a\u4e86\u8fd9\u79cd\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u4e0d\u5b58\u5728\u771f\u5b9e\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u5f62\u6210\u6d4b\u8bd5\u6027\u80fd\u7684\u8fd1\u7aef\u9884\u6d4b\u7ba1\u9053\u3002\u5c3d\u7ba1 AutoEval \u6846\u67b6\u6700\u8fd1\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\u3001\u5927\u91cf\u5b58\u50a8\u548c\u8ba1\u7b97\u6210\u672c\u3002\u5728\u8fd9\u65b9\u9762\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63aa\u65bd\u2014\u2014\u5143\u5206\u5e03\u5f0f\u80fd\u6e90\uff08MDE\uff09\u2014\u2014\u4f7f AutoEval \u6846\u67b6\u66f4\u52a0\u9ad8\u6548\u548c\u6709\u6548\u3002 MDE \u7684\u6838\u5fc3\u662f\u9488\u5bf9\u4e0e\u5404\u4e2a\u6837\u672c\u76f8\u5173\u7684\u4fe1\u606f\uff08\u80fd\u91cf\uff09\u5efa\u7acb\u5143\u5206\u5e03\u7edf\u8ba1\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8e\u80fd\u91cf\u7684\u5b66\u4e60\u63d0\u4f9b\u66f4\u5e73\u6ed1\u7684\u8868\u793a\u3002\u6211\u4eec\u901a\u8fc7\u5c06 MDE \u4e0e\u5206\u7c7b\u635f\u5931\u8054\u7cfb\u8d77\u6765\uff0c\u8fdb\u4e00\u6b65\u63d0\u4f9b\u6211\u4eec\u7684\u7406\u8bba\u89c1\u89e3\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u8de8\u6a21\u5f0f\u3001\u6570\u636e\u96c6\u548c\u4e0d\u540c\u67b6\u6784\u4e3b\u5e72\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u4ee5\u9a8c\u8bc1 MDE \u7684\u6709\u6548\u6027\u4ee5\u53ca\u4e0e\u5148\u524d\u65b9\u6cd5\u76f8\u6bd4\u7684\u4f18\u8d8a\u6027\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5c55\u793a MDE \u4e0e\u5927\u578b\u6a21\u578b\u7684\u65e0\u7f1d\u96c6\u6210\u4ee5\u53ca\u8f7b\u677e\u9002\u5e94\u5e26\u6709\u566a\u58f0\u6216\u4e0d\u5e73\u8861\u6807\u7b7e\u7684\u5b66\u4e60\u573a\u666f\u6765\u8bc1\u660e MDE \u7684\u591a\u529f\u80fd\u6027\u3002|[2401.12689v1](http://arxiv.org/pdf/2401.12689v1)|**[link](https://github.com/pengr/energy_autoeval)**|\n", "2401.12665": "|**2024-01-23**|**ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation**|ClipSAM\uff1aCLIP \u548c SAM \u534f\u4f5c\u8fdb\u884c\u96f6\u6837\u672c\u5f02\u5e38\u5206\u5272|Shengze Li, Jianjian Cao, Peng Ye, Yuhan Ding, Chongjun Tu, Tao Chen|Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple scales of CLIP to reason anomaly positions. Then, we design a novel Multi-level Mask Refinement (MMR) module, which utilizes the positional information as multi-level prompts for SAM to acquire hierarchical levels of masks and merges them. Extensive experiments validate the effectiveness of our approach, achieving the optimal segmentation performance on the MVTec-AD and VisA datasets.|\u6700\u8fd1\uff0cCLIP \u548c SAM \u7b49\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u5f02\u5e38\u5206\u5272\uff08ZSAS\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u65e0\u8bba\u662f\u57fa\u4e8eCLIP\u8fd8\u662f\u57fa\u4e8eSAM\u7684ZSAS\u65b9\u6cd5\u4ecd\u7136\u5b58\u5728\u4e0d\u53ef\u5ffd\u89c6\u7684\u5173\u952e\u7f3a\u70b9\uff1a1\uff09CLIP\u4e3b\u8981\u5173\u6ce8\u4e0d\u540c\u8f93\u5165\u4e4b\u95f4\u7684\u5168\u5c40\u7279\u5f81\u5bf9\u9f50\uff0c\u5bfc\u81f4\u5c40\u90e8\u5f02\u5e38\u90e8\u5206\u7684\u5206\u5272\u4e0d\u7cbe\u786e\uff1b 2) SAM \u5728\u6ca1\u6709\u9002\u5f53\u63d0\u793a\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u5f80\u5f80\u4f1a\u751f\u6210\u5927\u91cf\u5197\u4f59\u63a9\u6a21\uff0c\u4ece\u800c\u5bfc\u81f4\u590d\u6742\u7684\u540e\u5904\u7406\u8981\u6c42\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u521b\u65b0\u6027\u5730\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a ClipSAM for ZSAS \u7684 CLIP \u548c SAM \u534f\u4f5c\u6846\u67b6\u3002 ClipSAM\u80cc\u540e\u7684\u89c1\u89e3\u662f\u5229\u7528CLIP\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u8fdb\u884c\u5f02\u5e38\u5b9a\u4f4d\u548c\u7c97\u5206\u5272\uff0c\u8fdb\u4e00\u6b65\u7528\u4f5cSAM\u7684\u63d0\u793a\u7ea6\u675f\u6765\u7ec6\u5316\u5f02\u5e38\u5206\u5272\u7ed3\u679c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u7edf\u4e00\u591a\u5c3a\u5ea6\u8de8\u6a21\u6001\u4ea4\u4e92\uff08UMCI\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u5728 CLIP \u7684\u591a\u4e2a\u5c3a\u5ea6\u4e0a\u5c06\u8bed\u8a00\u4e0e\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u4ea4\u4e92\uff0c\u4ee5\u63a8\u7406\u5f02\u5e38\u4f4d\u7f6e\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u7ea7\u63a9\u6a21\u7ec6\u5316\uff08MMR\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5229\u7528\u4f4d\u7f6e\u4fe1\u606f\u4f5c\u4e3aSAM\u7684\u591a\u7ea7\u63d0\u793a\u6765\u83b7\u53d6\u63a9\u6a21\u7684\u5c42\u6b21\u7ea7\u522b\u5e76\u5c06\u5b83\u4eec\u5408\u5e76\u3002\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728 MVTec-AD \u548c VisA \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u5206\u5272\u6027\u80fd\u3002|[2401.12665v1](http://arxiv.org/pdf/2401.12665v1)|null|\n", "2401.12535": "|**2024-01-23**|**Self-Supervised Vision Transformers Are Efficient Segmentation Learners for Imperfect Labels**|\u81ea\u76d1\u7763\u89c6\u89c9\u53d8\u538b\u5668\u662f\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u7684\u6709\u6548\u5206\u5272\u5b66\u4e60\u5668|Seungho Lee, Seoungyoon Kang, Hyunjung Shim|This study demonstrates a cost-effective approach to semantic segmentation using self-supervised vision transformers (SSVT). By freezing the SSVT backbone and training a lightweight segmentation head, our approach effectively utilizes imperfect labels, thereby improving robustness to label imperfections. Empirical experiments show significant performance improvements over existing methods for various annotation types, including scribble, point-level, and image-level labels. The research highlights the effectiveness of self-supervised vision transformers in dealing with imperfect labels, providing a practical and efficient solution for semantic segmentation while reducing annotation costs. Through extensive experiments, we confirm that our method outperforms baseline models for all types of imperfect labels. Especially under the zero-shot vision-language-model-based label, our model exhibits 11.5\\%p performance gain compared to the baseline.|\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u4f7f\u7528\u81ea\u76d1\u7763\u89c6\u89c9\u8f6c\u6362\u5668\uff08SSVT\uff09\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u7684\u7ecf\u6d4e\u6709\u6548\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u51bb\u7ed3 SSVT \u4e3b\u5e72\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u5272\u5934\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u5229\u7528\u4e86\u4e0d\u5b8c\u7f8e\u7684\u6807\u7b7e\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5bf9\u6807\u7b7e\u7f3a\u9677\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u8bc1\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5404\u79cd\u6ce8\u91ca\u7c7b\u578b\uff08\u5305\u62ec\u6d82\u9e26\u3001\u70b9\u7ea7\u548c\u56fe\u50cf\u7ea7\u6807\u7b7e\uff09\u7684\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6027\u80fd\u6709\u4e86\u663e\u7740\u63d0\u9ad8\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u81ea\u76d1\u7763\u89c6\u89c9\u8f6c\u6362\u5668\u5728\u5904\u7406\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6ce8\u91ca\u6210\u672c\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u786e\u8ba4\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u4e8e\u6240\u6709\u7c7b\u578b\u7684\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u90fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002\u7279\u522b\u662f\u5728\u57fa\u4e8e\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6807\u7b7e\u4e0b\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u57fa\u7ebf\u76f8\u6bd4\u8868\u73b0\u51fa 11.5\\%p \u7684\u6027\u80fd\u589e\u76ca\u3002|[2401.12535v1](http://arxiv.org/pdf/2401.12535v1)|null|\n", "2401.12513": "|**2024-01-23**|**Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT and SimCLR**|\u4f7f\u7528 YOLOv8\u3001DeiT \u548c SimCLR \u68c0\u6d4b\u548c\u8bc6\u522b\u5e0c\u814a\u7eb8\u838e\u8349\u4e2d\u7684\u5b57\u7b26|Robert Turnbull, Evelyn Mannix|The capacity to isolate and recognize individual characters from facsimile images of papyrus manuscripts yields rich opportunities for digital analysis. For this reason the `ICDAR 2023 Competition on Detection and Recognition of Greek Letters on Papyri' was held as part of the 17th International Conference on Document Analysis and Recognition. This paper discusses our submission to the competition. We used an ensemble of YOLOv8 models to detect and classify individual characters and employed two different approaches for refining the character predictions, including a transformer based DeiT approach and a ResNet-50 model trained on a large corpus of unlabelled data using SimCLR, a self-supervised learning method. Our submission won the recognition challenge with a mAP of 42.2%, and was runner-up in the detection challenge with a mean average precision (mAP) of 51.4%. At the more relaxed intersection over union threshold of 0.5, we achieved the highest mean average precision and mean average recall results for both detection and classification. We ran our prediction pipeline on more than 4,500 images from the Oxyrhynchus Papyri to illustrate the utility of our approach, and we release the results publicly in multiple formats.|\u4ece\u7eb8\u838e\u8349\u624b\u7a3f\u7684\u4f20\u771f\u56fe\u50cf\u4e2d\u5206\u79bb\u548c\u8bc6\u522b\u5355\u4e2a\u5b57\u7b26\u7684\u80fd\u529b\u4e3a\u6570\u5b57\u5206\u6790\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u673a\u4f1a\u3002\u56e0\u6b64\uff0c\u4f5c\u4e3a\u7b2c 17 \u5c4a\u56fd\u9645\u6587\u732e\u5206\u6790\u4e0e\u8bc6\u522b\u4f1a\u8bae\u7684\u4e00\u90e8\u5206\uff0c\u4e3e\u529e\u4e86\u201cICDAR 2023 \u7eb8\u838e\u8349\u5e0c\u814a\u5b57\u6bcd\u68c0\u6d4b\u548c\u8bc6\u522b\u7ade\u8d5b\u201d\u3002\u672c\u6587\u8ba8\u8bba\u4e86\u6211\u4eec\u63d0\u4ea4\u7684\u7ade\u8d5b\u5185\u5bb9\u3002\u6211\u4eec\u4f7f\u7528 YOLOv8 \u6a21\u578b\u96c6\u5408\u6765\u68c0\u6d4b\u548c\u5206\u7c7b\u5355\u4e2a\u5b57\u7b26\uff0c\u5e76\u91c7\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u6765\u7ec6\u5316\u5b57\u7b26\u9884\u6d4b\uff0c\u5305\u62ec\u57fa\u4e8e Transformer \u7684 DeiT \u65b9\u6cd5\u548c\u4f7f\u7528 SimCLR\uff08\u4e00\u79cd\u81ea\u5b66\u4e60\u5de5\u5177\uff09\u5728\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u4e0a\u8bad\u7ec3\u7684 ResNet-50 \u6a21\u578b\u3002\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u6211\u4eec\u63d0\u4ea4\u7684\u4f5c\u54c1\u4ee5 42.2% \u7684 mAP \u8d62\u5f97\u4e86\u8bc6\u522b\u6311\u6218\u8d5b\uff0c\u5e76\u4ee5 51.4% \u7684\u5e73\u5747\u7cbe\u5ea6 (mAP) \u5728\u68c0\u6d4b\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4e9a\u519b\u3002\u5728\u6bd4\u5e76\u96c6\u9608\u503c 0.5 \u66f4\u5bbd\u677e\u7684\u4ea4\u96c6\u5904\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u68c0\u6d4b\u548c\u5206\u7c7b\u7684\u6700\u9ad8\u5e73\u5747\u7cbe\u5ea6\u548c\u5e73\u5747\u53ec\u56de\u7387\u7ed3\u679c\u3002\u6211\u4eec\u5bf9\u6765\u81ea Oxyrhynchus Papyri \u7684 4,500 \u591a\u5f20\u56fe\u50cf\u8fd0\u884c\u4e86\u9884\u6d4b\u7ba1\u9053\uff0c\u4ee5\u8bf4\u660e\u6211\u4eec\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u4ee5\u591a\u79cd\u683c\u5f0f\u516c\u5f00\u53d1\u5e03\u7ed3\u679c\u3002|[2401.12513v1](http://arxiv.org/pdf/2401.12513v1)|null|\n", "2401.12507": "|**2024-01-23**|**Open-Set Facial Expression Recognition**|\u5f00\u653e\u96c6\u9762\u90e8\u8868\u60c5\u8bc6\u522b|Yuhang Zhang, Yue Yao, Xuannan Liu, Lixiong Qin, Wenjing Wang, Weihong Deng|Facial expression recognition (FER) models are typically trained on datasets with a fixed number of seven basic classes. However, recent research works point out that there are far more expressions than the basic ones. Thus, when these models are deployed in the real world, they may encounter unknown classes, such as compound expressions that cannot be classified into existing basic classes. To address this issue, we propose the open-set FER task for the first time. Though there are many existing open-set recognition methods, we argue that they do not work well for open-set FER because FER data are all human faces with very small inter-class distances, which makes the open-set samples very similar to close-set samples. In this paper, we are the first to transform the disadvantage of small inter-class distance into an advantage by proposing a new way for open-set FER. Specifically, we find that small inter-class distance allows for sparsely distributed pseudo labels of open-set samples, which can be viewed as symmetric noisy labels. Based on this novel observation, we convert the open-set FER to a noisy label detection problem. We further propose a novel method that incorporates attention map consistency and cycle training to detect the open-set samples. Extensive experiments on various FER datasets demonstrate that our method clearly outperforms state-of-the-art open-set recognition methods by large margins. Code is available at https://github.com/zyh-uaiaaaa.|\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u6a21\u578b\u901a\u5e38\u5728\u5177\u6709\u56fa\u5b9a\u6570\u91cf\u7684\u4e03\u4e2a\u57fa\u672c\u7c7b\u522b\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u5de5\u4f5c\u6307\u51fa\uff0c\u8868\u8fbe\u65b9\u5f0f\u8fdc\u591a\u4e8e\u57fa\u672c\u8868\u8fbe\u65b9\u5f0f\u3002\u56e0\u6b64\uff0c\u5f53\u8fd9\u4e9b\u6a21\u578b\u90e8\u7f72\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u65f6\uff0c\u5b83\u4eec\u53ef\u80fd\u4f1a\u9047\u5230\u672a\u77e5\u7684\u7c7b\uff0c\u4f8b\u5982\u65e0\u6cd5\u5206\u7c7b\u5230\u73b0\u6709\u57fa\u672c\u7c7b\u7684\u590d\u5408\u8868\u8fbe\u5f0f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9996\u6b21\u63d0\u51fa\u4e86\u5f00\u653e\u96c6 FER \u4efb\u52a1\u3002\u5c3d\u7ba1\u73b0\u6709\u7684\u5f00\u653e\u96c6\u8bc6\u522b\u65b9\u6cd5\u6709\u5f88\u591a\uff0c\u4f46\u6211\u4eec\u8ba4\u4e3a\u5b83\u4eec\u5bf9\u4e8e\u5f00\u653e\u96c6 FER \u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a FER \u6570\u636e\u90fd\u662f\u4eba\u8138\uff0c\u7c7b\u95f4\u8ddd\u79bb\u975e\u5e38\u5c0f\uff0c\u8fd9\u4f7f\u5f97\u5f00\u653e\u96c6\u6837\u672c\u4e0e\u5c01\u95ed\u6837\u672c\u975e\u5e38\u76f8\u4f3c- \u8bbe\u7f6e\u6837\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u653e\u96c6 FER \u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u7c7b\u95f4\u8ddd\u79bb\u8f83\u5c0f\u7684\u7f3a\u70b9\u8f6c\u5316\u4e3a\u4f18\u52bf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u53d1\u73b0\u8f83\u5c0f\u7684\u7c7b\u95f4\u8ddd\u79bb\u5141\u8bb8\u5f00\u653e\u96c6\u6837\u672c\u7684\u7a00\u758f\u5206\u5e03\u7684\u4f2a\u6807\u7b7e\uff0c\u8fd9\u53ef\u4ee5\u88ab\u89c6\u4e3a\u5bf9\u79f0\u566a\u58f0\u6807\u7b7e\u3002\u57fa\u4e8e\u8fd9\u4e00\u65b0\u9896\u7684\u89c2\u5bdf\uff0c\u6211\u4eec\u5c06\u5f00\u653e\u96c6 FER \u8f6c\u6362\u4e3a\u566a\u58f0\u6807\u7b7e\u68c0\u6d4b\u95ee\u9898\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6ce8\u610f\u529b\u56fe\u4e00\u81f4\u6027\u548c\u5faa\u73af\u8bad\u7ec3\u6765\u68c0\u6d4b\u5f00\u653e\u96c6\u6837\u672c\u3002\u5bf9\u5404\u79cd FER \u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u660e\u663e\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5f00\u653e\u96c6\u8bc6\u522b\u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/zyh-uaiaaaa \u83b7\u53d6\u3002|[2401.12507v1](http://arxiv.org/pdf/2401.12507v1)|null|\n", "2401.12503": "|**2024-01-23**|**Small Language Model Meets with Reinforced Vision Vocabulary**|\u5c0f\u8bed\u8a00\u6a21\u578b\u4e0e\u5f3a\u5316\u89c6\u89c9\u8bcd\u6c47\u7684\u7ed3\u5408|Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, Xiangyu Zhang|Playing Large Vision Language Models (LVLMs) in 2023 is trendy among the AI community. However, the relatively large number of parameters (more than 7B) of popular LVLMs makes it difficult to train and deploy on consumer GPUs, discouraging many researchers with limited resources. Imagine how cool it would be to experience all the features of current LVLMs on an old GTX1080ti (our only game card). Accordingly, we present Vary-toy in this report, a small-size Vary along with Qwen-1.8B as the base ``large'' language model. In Vary-toy, we introduce an improved vision vocabulary, allowing the model to not only possess all features of Vary but also gather more generality. Specifically, we replace negative samples of natural images with positive sample data driven by object detection in the procedure of generating vision vocabulary, more sufficiently utilizing the capacity of the vocabulary network and enabling it to efficiently encode visual information corresponding to natural objects. For experiments, Vary-toy can achieve 65.6% ANLS on DocVQA, 59.1% accuracy on ChartQA, 88.1% accuracy on RefCOCO, and 29% on MMVet. The code will be publicly available on the homepage.|2023 \u5e74\uff0c\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728 AI \u793e\u533a\u4e2d\u5f88\u6d41\u884c\u3002\u7136\u800c\uff0c\u6d41\u884c\u7684 LVLM \u7684\u53c2\u6570\u6570\u91cf\u76f8\u5bf9\u8f83\u591a\uff08\u8d85\u8fc7 7B\uff09\uff0c\u56e0\u6b64\u5f88\u96be\u5728\u6d88\u8d39\u7ea7 GPU \u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u90e8\u7f72\uff0c\u8fd9\u8ba9\u8bb8\u591a\u8d44\u6e90\u6709\u9650\u7684\u7814\u7a76\u4eba\u5458\u671b\u800c\u5374\u6b65\u3002\u60f3\u8c61\u4e00\u4e0b\uff0c\u5728\u65e7\u7684 GTX1080ti\uff08\u6211\u4eec\u552f\u4e00\u7684\u6e38\u620f\u5361\uff09\u4e0a\u4f53\u9a8c\u5f53\u524d LVLM \u7684\u6240\u6709\u529f\u80fd\u4f1a\u6709\u591a\u9177\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5728\u672c\u62a5\u544a\u4e2d\u4ecb\u7ecd\u4e86 Vary-toy\uff0c\u4e00\u4e2a\u5c0f\u578b Vary \u4ee5\u53ca Qwen-1.8B \u4f5c\u4e3a\u57fa\u7840\u201c\u5927\u578b\u201d\u8bed\u8a00\u6a21\u578b\u3002\u5728Vary-toy\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6539\u8fdb\u7684\u89c6\u89c9\u8bcd\u6c47\u8868\uff0c\u4f7f\u6a21\u578b\u4e0d\u4ec5\u62e5\u6709Vary\u7684\u6240\u6709\u7279\u5f81\uff0c\u800c\u4e14\u8fd8\u5177\u6709\u66f4\u591a\u7684\u901a\u7528\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5728\u751f\u6210\u89c6\u89c9\u8bcd\u6c47\u7684\u8fc7\u7a0b\u4e2d\u7528\u76ee\u6807\u68c0\u6d4b\u9a71\u52a8\u7684\u6b63\u6837\u672c\u6570\u636e\u66ff\u6362\u81ea\u7136\u56fe\u50cf\u7684\u8d1f\u6837\u672c\uff0c\u66f4\u5145\u5206\u5730\u5229\u7528\u8bcd\u6c47\u7f51\u7edc\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u6709\u6548\u5730\u7f16\u7801\u4e0e\u81ea\u7136\u7269\u4f53\u76f8\u5bf9\u5e94\u7684\u89c6\u89c9\u4fe1\u606f\u3002\u5bf9\u4e8e\u5b9e\u9a8c\uff0cVary-toy \u5728 DocVQA \u4e0a\u53ef\u4ee5\u8fbe\u5230 65.6% \u7684 ANLS\uff0c\u5728 ChartQA \u4e0a\u8fbe\u5230 59.1% \u7684\u51c6\u786e\u7387\uff0c\u5728 RefCOCO \u4e0a\u8fbe\u5230 88.1% \u7684\u51c6\u786e\u7387\uff0c\u5728 MMVet \u4e0a\u8fbe\u5230 29% \u7684\u51c6\u786e\u7387\u3002\u8be5\u4ee3\u7801\u5c06\u5728\u4e3b\u9875\u4e0a\u516c\u5f00\u3002|[2401.12503v1](http://arxiv.org/pdf/2401.12503v1)|null|\n", "2401.12488": "|**2024-01-23**|**An Automated Real-Time Approach for Image Processing and Segmentation of Fluoroscopic Images and Videos Using a Single Deep Learning Network**|\u4f7f\u7528\u5355\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u5bf9\u8367\u5149\u56fe\u50cf\u548c\u89c6\u9891\u8fdb\u884c\u56fe\u50cf\u5904\u7406\u548c\u5206\u5272\u7684\u81ea\u52a8\u5316\u5b9e\u65f6\u65b9\u6cd5|Viet Dung Nguyen, Michael T. LaCour, Richard D. Komistek|Image segmentation in total knee arthroplasty is crucial for precise preoperative planning and accurate implant positioning, leading to improved surgical outcomes and patient satisfaction. The biggest challenges of image segmentation in total knee arthroplasty include accurately delineating complex anatomical structures, dealing with image artifacts and noise, and developing robust algorithms that can handle anatomical variations and pathologies commonly encountered in patients. The potential of using machine learning for image segmentation in total knee arthroplasty lies in its ability to improve segmentation accuracy, automate the process, and provide real-time assistance to surgeons, leading to enhanced surgical planning, implant placement, and patient outcomes. This paper proposes a methodology to use deep learning for robust and real-time total knee arthroplasty image segmentation. The deep learning model, trained on a large dataset, demonstrates outstanding performance in accurately segmenting both the implanted femur and tibia, achieving an impressive mean-Average-Precision (mAP) of 88.83 when compared to the ground truth while also achieving a real-time segmented speed of 20 frames per second (fps). We have introduced a novel methodology for segmenting implanted knee fluoroscopic or x-ray images that showcases remarkable levels of accuracy and speed, paving the way for various potential extended applications.|\u5168\u819d\u5173\u8282\u7f6e\u6362\u672f\u4e2d\u7684\u56fe\u50cf\u5206\u5272\u5bf9\u4e8e\u7cbe\u786e\u7684\u672f\u524d\u8ba1\u5212\u548c\u51c6\u786e\u7684\u690d\u5165\u7269\u5b9a\u4f4d\u81f3\u5173\u91cd\u8981\uff0c\u4ece\u800c\u6539\u5584\u624b\u672f\u7ed3\u679c\u548c\u60a3\u8005\u6ee1\u610f\u5ea6\u3002\u5168\u819d\u5173\u8282\u7f6e\u6362\u672f\u4e2d\u56fe\u50cf\u5206\u5272\u7684\u6700\u5927\u6311\u6218\u5305\u62ec\u51c6\u786e\u63cf\u7ed8\u590d\u6742\u7684\u89e3\u5256\u7ed3\u6784\u3001\u5904\u7406\u56fe\u50cf\u4f2a\u5f71\u548c\u566a\u58f0\uff0c\u4ee5\u53ca\u5f00\u53d1\u80fd\u591f\u5904\u7406\u60a3\u8005\u5e38\u89c1\u7684\u89e3\u5256\u53d8\u5316\u548c\u75c5\u7406\u7684\u5f3a\u5927\u7b97\u6cd5\u3002\u5728\u5168\u819d\u5173\u8282\u7f6e\u6362\u672f\u4e2d\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u56fe\u50cf\u5206\u5272\u7684\u6f5c\u529b\u5728\u4e8e\u5176\u80fd\u591f\u63d0\u9ad8\u5206\u5272\u51c6\u786e\u6027\u3001\u81ea\u52a8\u5316\u6d41\u7a0b\u5e76\u4e3a\u5916\u79d1\u533b\u751f\u63d0\u4f9b\u5b9e\u65f6\u5e2e\u52a9\uff0c\u4ece\u800c\u6539\u5584\u624b\u672f\u8ba1\u5212\u3001\u690d\u5165\u7269\u653e\u7f6e\u548c\u60a3\u8005\u7ed3\u679c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u7a33\u5065\u4e14\u5b9e\u65f6\u7684\u5168\u819d\u5173\u8282\u7f6e\u6362\u672f\u56fe\u50cf\u5206\u5272\u7684\u65b9\u6cd5\u3002\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u51c6\u786e\u5206\u5272\u690d\u5165\u7684\u80a1\u9aa8\u548c\u80eb\u9aa8\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u6027\u80fd\uff0c\u4e0e\u5730\u9762\u5b9e\u51b5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684 88.83 \u7684\u5e73\u5747\u7cbe\u5ea6 (mAP)\uff0c\u540c\u65f6\u8fd8\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5206\u5272\u5206\u6bb5\u901f\u5ea6\u4e3a\u6bcf\u79d2 20 \u5e27 (fps)\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u5206\u5272\u690d\u5165\u7684\u819d\u5173\u8282\u900f\u89c6\u6216 X \u5c04\u7ebf\u56fe\u50cf\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\uff0c\u4e3a\u5404\u79cd\u6f5c\u5728\u7684\u6269\u5c55\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2401.12488v1](http://arxiv.org/pdf/2401.12488v1)|null|\n", "2401.12480": "|**2024-01-23**|**Explore Synergistic Interaction Across Frames for Interactive Video Object Segmentation**|\u63a2\u7d22\u4ea4\u4e92\u5f0f\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u8de8\u5e27\u534f\u540c\u4ea4\u4e92|Kexin Li, Tao Jiang, Zongxin Yang, Yi Yang, Yueting Zhuang, Jun Xiao|Interactive Video Object Segmentation (iVOS) is a challenging task that requires real-time human-computer interaction. To improve the user experience, it is important to consider the user's input habits, segmentation quality, running time and memory consumption.However, existing methods compromise user experience with single input mode and slow running speed. Specifically, these methods only allow the user to interact with one single frame, which limits the expression of the user's intent.To overcome these limitations and better align with people's usage habits, we propose a framework that can accept multiple frames simultaneously and explore synergistic interaction across frames (SIAF). Concretely, we designed the Across-Frame Interaction Module that enables users to annotate different objects freely on multiple frames. The AFI module will migrate scribble information among multiple interactive frames and generate multi-frame masks. Additionally, we employ the id-queried mechanism to process multiple objects in batches. Furthermore, for a more efficient propagation and lightweight model, we design a truncated re-propagation strategy to replace the previous multi-round fusion module, which employs an across-round memory that stores important interaction information. Our SwinB-SIAF achieves new state-of-the-art performance on DAVIS 2017 (89.6%, J&F@60). Moreover, our R50-SIAF is more than 3 faster than the state-of-the-art competitor under challenging multi-object scenarios.|\u4ea4\u4e92\u5f0f\u89c6\u9891\u5bf9\u8c61\u5206\u5272\uff08iVOS\uff09\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u5b9e\u65f6\u4eba\u673a\u4ea4\u4e92\u3002\u4e3a\u4e86\u63d0\u9ad8\u7528\u6237\u4f53\u9a8c\uff0c\u9700\u8981\u8003\u8651\u7528\u6237\u7684\u8f93\u5165\u4e60\u60ef\u3001\u5206\u5272\u8d28\u91cf\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u8f93\u5165\u6a21\u5f0f\u5355\u4e00\u3001\u8fd0\u884c\u901f\u5ea6\u6162\uff0c\u635f\u5bb3\u4e86\u7528\u6237\u4f53\u9a8c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ea\u5141\u8bb8\u7528\u6237\u4e0e\u5355\u4e2a\u6846\u67b6\u8fdb\u884c\u4ea4\u4e92\uff0c\u8fd9\u9650\u5236\u4e86\u7528\u6237\u610f\u56fe\u7684\u8868\u8fbe\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u5e76\u66f4\u597d\u5730\u7b26\u5408\u4eba\u4eec\u7684\u4f7f\u7528\u4e60\u60ef\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u4ee5\u540c\u65f6\u63a5\u53d7\u591a\u4e2a\u6846\u67b6\u5e76\u63a2\u7d22\u534f\u540c\u4ea4\u4e92\u7684\u6846\u67b6\u8de8\u6846\u67b6\uff08SIAF\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u8de8\u5e27\u4ea4\u4e92\u6a21\u5757\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u5728\u591a\u4e2a\u5e27\u4e0a\u81ea\u7531\u6ce8\u91ca\u4e0d\u540c\u7684\u5bf9\u8c61\u3002 AFI\u6a21\u5757\u5c06\u5728\u591a\u4e2a\u4ea4\u4e92\u5e27\u4e4b\u95f4\u8fc1\u79fb\u6d82\u9e26\u4fe1\u606f\u5e76\u751f\u6210\u591a\u5e27\u63a9\u6a21\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528id\u67e5\u8be2\u673a\u5236\u6765\u6279\u91cf\u5904\u7406\u591a\u4e2a\u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u66f4\u9ad8\u6548\u7684\u4f20\u64ad\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u622a\u65ad\u7684\u91cd\u65b0\u4f20\u64ad\u7b56\u7565\u6765\u53d6\u4ee3\u4e4b\u524d\u7684\u591a\u8f6e\u878d\u5408\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u91c7\u7528\u8de8\u8f6e\u5b58\u50a8\u5668\u6765\u5b58\u50a8\u91cd\u8981\u7684\u4ea4\u4e92\u4fe1\u606f\u3002\u6211\u4eec\u7684 SwinB-SIAF \u5728 DAVIS 2017 \u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0889.6%\uff0cJ&F@60\uff09\u3002\u6b64\u5916\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u76ee\u6807\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u7684 R50-SIAF \u6bd4\u6700\u5148\u8fdb\u7684\u7ade\u4e89\u5bf9\u624b\u5feb 3 \u500d\u4ee5\u4e0a\u3002|[2401.12480v1](http://arxiv.org/pdf/2401.12480v1)|null|\n", "2401.12479": "|**2024-01-23**|**TD^2-Net: Toward Denoising and Debiasing for Dynamic Scene Graph Generation**|TD^2-Net\uff1a\u52a8\u6001\u573a\u666f\u56fe\u751f\u6210\u7684\u53bb\u566a\u548c\u53bb\u504f|Xin Lin, Chong Shi, Yibing Zhan, Zuopeng Yang, Yaqi Wu, Dacheng Tao|Dynamic scene graph generation (SGG) focuses on detecting objects in a video and determining their pairwise relationships. Existing dynamic SGG methods usually suffer from several issues, including 1) Contextual noise, as some frames might contain occluded and blurred objects. 2) Label bias, primarily due to the high imbalance between a few positive relationship samples and numerous negative ones. Additionally, the distribution of relationships exhibits a long-tailed pattern. To address the above problems, in this paper, we introduce a network named TD$^2$-Net that aims at denoising and debiasing for dynamic SGG. Specifically, we first propose a denoising spatio-temporal transformer module that enhances object representation with robust contextual information. This is achieved by designing a differentiable Top-K object selector that utilizes the gumbel-softmax sampling strategy to select the relevant neighborhood for each object. Second, we introduce an asymmetrical reweighting loss to relieve the issue of label bias. This loss function integrates asymmetry focusing factors and the volume of samples to adjust the weights assigned to individual samples. Systematic experimental results demonstrate the superiority of our proposed TD$^2$-Net over existing state-of-the-art approaches on Action Genome databases. In more detail, TD$^2$-Net outperforms the second-best competitors by 12.7 \\% on mean-Recall@10 for predicate classification.|\u52a8\u6001\u573a\u666f\u56fe\u751f\u6210\uff08SGG\uff09\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u89c6\u9891\u4e2d\u7684\u5bf9\u8c61\u5e76\u786e\u5b9a\u5b83\u4eec\u7684\u6210\u5bf9\u5173\u7cfb\u3002\u73b0\u6709\u7684\u52a8\u6001 SGG \u65b9\u6cd5\u901a\u5e38\u4f1a\u9047\u5230\u51e0\u4e2a\u95ee\u9898\uff0c\u5305\u62ec 1\uff09\u4e0a\u4e0b\u6587\u566a\u58f0\uff0c\u56e0\u4e3a\u67d0\u4e9b\u5e27\u53ef\u80fd\u5305\u542b\u88ab\u906e\u6321\u548c\u6a21\u7cca\u7684\u5bf9\u8c61\u3002 2\uff09\u6807\u7b7e\u504f\u5dee\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u5c11\u6570\u6b63\u5173\u7cfb\u6837\u672c\u4e0e\u5927\u91cf\u8d1f\u5173\u7cfb\u6837\u672c\u4e4b\u95f4\u7684\u9ad8\u5ea6\u4e0d\u5e73\u8861\u9020\u6210\u7684\u3002\u6b64\u5916\uff0c\u5173\u7cfb\u7684\u5206\u5e03\u5448\u73b0\u51fa\u957f\u5c3e\u6a21\u5f0f\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a TD$^2$-Net \u7684\u7f51\u7edc\uff0c\u65e8\u5728\u5bf9\u52a8\u6001 SGG \u8fdb\u884c\u53bb\u566a\u548c\u53bb\u504f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u566a\u65f6\u7a7a\u53d8\u6362\u5668\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u901a\u8fc7\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u589e\u5f3a\u5bf9\u8c61\u8868\u793a\u3002\u8fd9\u662f\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u53ef\u5fae\u5206\u7684 Top-K \u5bf9\u8c61\u9009\u62e9\u5668\u6765\u5b9e\u73b0\u7684\uff0c\u8be5\u9009\u62e9\u5668\u5229\u7528gumbel-softmax \u91c7\u6837\u7b56\u7565\u4e3a\u6bcf\u4e2a\u5bf9\u8c61\u9009\u62e9\u76f8\u5173\u90bb\u57df\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f15\u5165\u4e0d\u5bf9\u79f0\u91cd\u65b0\u52a0\u6743\u635f\u5931\u6765\u7f13\u89e3\u6807\u7b7e\u504f\u5dee\u95ee\u9898\u3002\u8be5\u635f\u5931\u51fd\u6570\u96c6\u6210\u4e86\u4e0d\u5bf9\u79f0\u805a\u7126\u56e0\u5b50\u548c\u6837\u672c\u91cf\uff0c\u4ee5\u8c03\u6574\u5206\u914d\u7ed9\u5404\u4e2a\u6837\u672c\u7684\u6743\u91cd\u3002\u7cfb\u7edf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684 TD$^2$-Net \u76f8\u5bf9\u4e8e Action Genome \u6570\u636e\u5e93\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u66f4\u8be6\u7ec6\u5730\u8bf4\uff0c\u5728\u8c13\u8bcd\u5206\u7c7b\u7684mean-Recall@10 \u4e0a\uff0cTD$^2$-Net \u6bd4\u7b2c\u4e8c\u597d\u7684\u7ade\u4e89\u5bf9\u624b\u9ad8\u51fa 12.7%\u3002|[2401.12479v1](http://arxiv.org/pdf/2401.12479v1)|null|\n", "2401.12471": "|**2024-01-23**|**Zero Shot Open-ended Video Inference**|\u96f6\u955c\u5934\u5f00\u653e\u5f0f\u89c6\u9891\u63a8\u7406|Ee Yeo Keat, Zhang Hao, Alexander Matyasko, Basura Fernando|Zero-shot open-ended inference on untrimmed videos poses a significant challenge, especially when no annotated data is utilized to navigate the inference direction. In this work, we aim to address this underexplored domain by introducing an adaptable framework that efficiently combines both the frozen vision-language (VL) model and off-the-shelf large language model (LLM) for conducting zero-shot open-ended inference tasks without requiring any additional training or fine-tuning. Our comprehensive experiments span various video action datasets for goal inference and action recognition tasks. The results demonstrate the framework's superior performance in goal inference compared to conventional vision-language models in open-ended and close-ended scenarios. Notably, the proposed framework exhibits the capability to generalize effectively to action recognition tasks, underscoring its versatility and potential contributions to advancing the video-based zero-shot understanding.|\u5bf9\u672a\u4fee\u526a\u89c6\u9891\u7684\u96f6\u6837\u672c\u5f00\u653e\u5f0f\u63a8\u7406\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u6ca1\u6709\u4f7f\u7528\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u6765\u5bfc\u822a\u63a8\u7406\u65b9\u5411\u65f6\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u9002\u5e94\u6027\u5f3a\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e2a\u5c1a\u672a\u5f00\u53d1\u7684\u9886\u57df\uff0c\u8be5\u6846\u67b6\u6709\u6548\u5730\u7ed3\u5408\u4e86\u51bb\u7ed3\u89c6\u89c9\u8bed\u8a00\uff08VL\uff09\u6a21\u578b\u548c\u73b0\u6210\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4ee5\u8fdb\u884c\u96f6\u6837\u672c\u5f00\u653e\u5f0f\u63a8\u7406\u4efb\u52a1\u65e0\u9700\u4efb\u4f55\u989d\u5916\u7684\u57f9\u8bad\u6216\u5fae\u8c03\u3002\u6211\u4eec\u7684\u7efc\u5408\u5b9e\u9a8c\u6db5\u76d6\u5404\u79cd\u89c6\u9891\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u76ee\u6807\u63a8\u7406\u548c\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5f00\u653e\u5f0f\u548c\u5c01\u95ed\u5f0f\u573a\u666f\u4e2d\u7684\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u76ee\u6807\u63a8\u7406\u65b9\u9762\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5c55\u793a\u4e86\u6709\u6548\u63a8\u5e7f\u5230\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u5bf9\u63a8\u8fdb\u57fa\u4e8e\u89c6\u9891\u7684\u96f6\u955c\u5934\u7406\u89e3\u7684\u6f5c\u5728\u8d21\u732e\u3002|[2401.12471v1](http://arxiv.org/pdf/2401.12471v1)|null|\n", "2401.12452": "|**2024-01-23**|**Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural Calibration**|\u901a\u8fc7 2D-3D \u795e\u7ecf\u6821\u51c6\u8fdb\u884c LiDAR 3D \u70b9\u4e91\u7684\u81ea\u76d1\u7763\u5b66\u4e60|Yifan Zhang, Siyu Ren, Junhui Hou, Jinjian Wu, Guangming Shi|This paper introduces a novel self-supervised learning framework for enhancing 3D perception in autonomous driving scenes. Specifically, our approach, named NCLR, focuses on 2D-3D neural calibration, a novel pretext task that estimates the rigid transformation aligning camera and LiDAR coordinate systems. First, we propose the learnable transformation alignment to bridge the domain gap between image and point cloud data, converting features into a unified representation space for effective comparison and matching. Second, we identify the overlapping area between the image and point cloud with the fused features. Third, we establish dense 2D-3D correspondences to estimate the rigid transformation. The framework not only learns fine-grained matching from points to pixels but also achieves alignment of the image and point cloud at a holistic level, understanding their relative pose. We demonstrate NCLR's efficacy by applying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D semantic segmentation, object detection, and panoptic segmentation. Comprehensive experiments on various datasets illustrate the superiority of NCLR over existing self-supervised methods. The results confirm that joint learning from different modalities significantly enhances the network's understanding abilities and effectiveness of learned representation. Code will be available at \\url{https://github.com/Eaphan/NCLR}.|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684 3D \u611f\u77e5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u540d\u4e3a NCLR\uff0c\u4e13\u6ce8\u4e8e 2D-3D \u795e\u7ecf\u6821\u51c6\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u501f\u53e3\u4efb\u52a1\uff0c\u7528\u4e8e\u4f30\u8ba1\u521a\u6027\u53d8\u6362\u5bf9\u9f50\u76f8\u673a\u548c LiDAR \u5750\u6807\u7cfb\u3002\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u53ef\u5b66\u4e60\u7684\u53d8\u6362\u5bf9\u9f50\u6765\u5f25\u5408\u56fe\u50cf\u548c\u70b9\u4e91\u6570\u636e\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\uff0c\u5c06\u7279\u5f81\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684\u8868\u793a\u7a7a\u95f4\u4ee5\u8fdb\u884c\u6709\u6548\u7684\u6bd4\u8f83\u548c\u5339\u914d\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5229\u7528\u878d\u5408\u7279\u5f81\u8bc6\u522b\u56fe\u50cf\u548c\u70b9\u4e91\u4e4b\u95f4\u7684\u91cd\u53e0\u533a\u57df\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u5efa\u7acb\u5bc6\u96c6\u7684 2D-3D \u5bf9\u5e94\u5173\u7cfb\u6765\u4f30\u8ba1\u521a\u6027\u53d8\u6362\u3002\u8be5\u6846\u67b6\u4e0d\u4ec5\u5b66\u4e60\u4ece\u70b9\u5230\u50cf\u7d20\u7684\u7ec6\u7c92\u5ea6\u5339\u914d\uff0c\u800c\u4e14\u8fd8\u5b9e\u73b0\u4e86\u56fe\u50cf\u548c\u70b9\u4e91\u7684\u6574\u4f53\u5bf9\u9f50\uff0c\u4e86\u89e3\u5b83\u4eec\u7684\u76f8\u5bf9\u59ff\u6001\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u7684\u4e3b\u5e72\u5e94\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff08\u4f8b\u5982\u57fa\u4e8e LiDAR \u7684 3D \u8bed\u4e49\u5206\u5272\u3001\u5bf9\u8c61\u68c0\u6d4b\u548c\u5168\u666f\u5206\u5272\uff09\u6765\u5c55\u793a NCLR \u7684\u529f\u6548\u3002\u5bf9\u5404\u79cd\u6570\u636e\u96c6\u7684\u7efc\u5408\u5b9e\u9a8c\u8bf4\u660e\u4e86 NCLR \u76f8\u5bf9\u4e8e\u73b0\u6709\u81ea\u6211\u76d1\u7763\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u7ed3\u679c\u8bc1\u5b9e\uff0c\u4e0d\u540c\u6a21\u5f0f\u7684\u8054\u5408\u5b66\u4e60\u663e\u7740\u589e\u5f3a\u4e86\u7f51\u7edc\u7684\u7406\u89e3\u80fd\u529b\u548c\u5b66\u4e60\u8868\u793a\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/Eaphan/NCLR} \u83b7\u53d6\u3002|[2401.12452v1](http://arxiv.org/pdf/2401.12452v1)|null|\n", "2401.12447": "|**2024-01-23**|**NIV-SSD: Neighbor IoU-Voting Single-Stage Object Detector From Point Cloud**|NIV-SSD\uff1a\u6765\u81ea\u70b9\u4e91\u7684\u90bb\u5c45 IoU \u6295\u7968\u5355\u7ea7\u7269\u4f53\u68c0\u6d4b\u5668|Shuai Liu, Di Wang, Quan Wang, Kai Huang|Previous single-stage detectors typically suffer the misalignment between localization accuracy and classification confidence. To solve the misalignment problem, we introduce a novel rectification method named neighbor IoU-voting (NIV) strategy. Typically, classification and regression are treated as separate branches, making it challenging to establish a connection between them. Consequently, the classification confidence cannot accurately reflect the regression quality. NIV strategy can serve as a bridge between classification and regression branches by calculating two types of statistical data from the regression output to correct the classification confidence. Furthermore, to alleviate the imbalance of detection accuracy for complete objects with dense points (easy objects) and incomplete objects with sparse points (difficult objects), we propose a new data augmentation scheme named object resampling. It undersamples easy objects and oversamples difficult objects by randomly transforming part of easy objects into difficult objects. Finally, combining the NIV strategy and object resampling augmentation, we design an efficient single-stage detector termed NIV-SSD. Extensive experiments on several datasets indicate the effectiveness of the NIV strategy and the competitive performance of the NIV-SSD detector. The code will be available at https://github.com/Say2L/NIV-SSD.|\u4ee5\u524d\u7684\u5355\u7ea7\u68c0\u6d4b\u5668\u901a\u5e38\u4f1a\u9047\u5230\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u5206\u7c7b\u7f6e\u4fe1\u5ea6\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u9519\u4f4d\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6821\u6b63\u65b9\u6cd5\uff0c\u79f0\u4e3a\u90bb\u5c45 IoU \u6295\u7968\uff08NIV\uff09\u7b56\u7565\u3002\u901a\u5e38\uff0c\u5206\u7c7b\u548c\u56de\u5f52\u88ab\u89c6\u4e3a\u72ec\u7acb\u7684\u5206\u652f\uff0c\u56e0\u6b64\u5f88\u96be\u5728\u5b83\u4eec\u4e4b\u95f4\u5efa\u7acb\u8054\u7cfb\u3002\u56e0\u6b64\uff0c\u5206\u7c7b\u7f6e\u4fe1\u5ea6\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u56de\u5f52\u8d28\u91cf\u3002 NIV\u7b56\u7565\u53ef\u4ee5\u4f5c\u4e3a\u5206\u7c7b\u548c\u56de\u5f52\u5206\u652f\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u901a\u8fc7\u4ece\u56de\u5f52\u8f93\u51fa\u8ba1\u7b97\u4e24\u7c7b\u7edf\u8ba1\u6570\u636e\u6765\u6821\u6b63\u5206\u7c7b\u7f6e\u4fe1\u5ea6\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u7f13\u89e3\u5177\u6709\u5bc6\u96c6\u70b9\u7684\u5b8c\u6574\u5bf9\u8c61\uff08\u5bb9\u6613\u5bf9\u8c61\uff09\u548c\u5177\u6709\u7a00\u758f\u70b9\u7684\u4e0d\u5b8c\u6574\u5bf9\u8c61\uff08\u56f0\u96be\u5bf9\u8c61\uff09\u7684\u68c0\u6d4b\u7cbe\u5ea6\u7684\u4e0d\u5e73\u8861\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6848\uff0c\u79f0\u4e3a\u5bf9\u8c61\u91cd\u91c7\u6837\u3002\u5b83\u901a\u8fc7\u5c06\u90e8\u5206\u7b80\u5355\u5bf9\u8c61\u968f\u673a\u8f6c\u6362\u4e3a\u56f0\u96be\u5bf9\u8c61\uff0c\u5bf9\u7b80\u5355\u5bf9\u8c61\u8fdb\u884c\u6b20\u91c7\u6837\uff0c\u5bf9\u56f0\u96be\u5bf9\u8c61\u8fdb\u884c\u8fc7\u91c7\u6837\u3002\u6700\u540e\uff0c\u7ed3\u5408 NIV \u7b56\u7565\u548c\u5bf9\u8c61\u91cd\u91c7\u6837\u589e\u5f3a\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u7ea7\u68c0\u6d4b\u5668\uff0c\u79f0\u4e3a NIV-SSD\u3002\u5bf9\u591a\u4e2a\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86 NIV \u7b56\u7565\u7684\u6709\u6548\u6027\u4ee5\u53ca NIV-SSD \u68c0\u6d4b\u5668\u7684\u7ade\u4e89\u6027\u80fd\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/Say2L/NIV-SSD \u4e0a\u83b7\u53d6\u3002|[2401.12447v1](http://arxiv.org/pdf/2401.12447v1)|**[link](https://github.com/say2l/niv-ssd)**|\n", "2401.12439": "|**2024-01-23**|**MAST: Video Polyp Segmentation with a Mixture-Attention Siamese Transformer**|MAST\uff1a\u4f7f\u7528\u6df7\u5408\u6ce8\u610f\u529b Siamese Transformer \u8fdb\u884c\u89c6\u9891\u606f\u8089\u5206\u5272|Geng Chen, Junqing Yang, Xiaozhou Pu, Ge-Peng Ji, Huan Xiong, Yongsheng Pan, Hengfei Cui, Yong Xia|Accurate segmentation of polyps from colonoscopy videos is of great significance to polyp treatment and early prevention of colorectal cancer. However, it is challenging due to the difficulties associated with modelling long-range spatio-temporal relationships within a colonoscopy video. In this paper, we address this challenging task with a novel Mixture-Attention Siamese Transformer (MAST), which explicitly models the long-range spatio-temporal relationships with a mixture-attention mechanism for accurate polyp segmentation. Specifically, we first construct a Siamese transformer architecture to jointly encode paired video frames for their feature representations. We then design a mixture-attention module to exploit the intra-frame and inter-frame correlations, enhancing the features with rich spatio-temporal relationships. Finally, the enhanced features are fed to two parallel decoders for predicting the segmentation maps. To the best of our knowledge, our MAST is the first transformer model dedicated to video polyp segmentation. Extensive experiments on the large-scale SUN-SEG benchmark demonstrate the superior performance of MAST in comparison with the cutting-edge competitors. Our code is publicly available at https://github.com/Junqing-Yang/MAST.|\u80a0\u955c\u89c6\u9891\u4e2d\u606f\u8089\u7684\u51c6\u786e\u5206\u5272\u5bf9\u4e8e\u606f\u8089\u6cbb\u7597\u548c\u7ed3\u76f4\u80a0\u764c\u7684\u65e9\u671f\u9884\u9632\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5728\u7ed3\u80a0\u955c\u68c0\u67e5\u89c6\u9891\u4e2d\u5efa\u6a21\u8fdc\u7a0b\u65f6\u7a7a\u5173\u7cfb\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u6b64\u5b83\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u6ce8\u610f\u66b9\u7f57\u53d8\u6362\u5668\uff08MAST\uff09\u6765\u89e3\u51b3\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5b83\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u673a\u5236\u660e\u786e\u5730\u6a21\u62df\u8fdc\u7a0b\u65f6\u7a7a\u5173\u7cfb\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u606f\u8089\u5206\u5272\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e00\u4e2a\u8fde\u4f53\u53d8\u538b\u5668\u67b6\u6784\u6765\u8054\u5408\u7f16\u7801\u914d\u5bf9\u89c6\u9891\u5e27\u7684\u7279\u5f81\u8868\u793a\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6df7\u5408\u6ce8\u610f\u6a21\u5757\u6765\u5229\u7528\u5e27\u5185\u548c\u5e27\u95f4\u76f8\u5173\u6027\uff0c\u589e\u5f3a\u5177\u6709\u4e30\u5bcc\u65f6\u7a7a\u5173\u7cfb\u7684\u7279\u5f81\u3002\u6700\u540e\uff0c\u589e\u5f3a\u7684\u7279\u5f81\u88ab\u9988\u9001\u5230\u4e24\u4e2a\u5e76\u884c\u89e3\u7801\u5668\u4ee5\u9884\u6d4b\u5206\u5272\u56fe\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u7684 MAST \u662f\u7b2c\u4e00\u4e2a\u4e13\u7528\u4e8e\u89c6\u9891\u606f\u8089\u5206\u5272\u7684 Transformer \u6a21\u578b\u3002\u5728\u5927\u578b SUN-SEG \u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86 MAST \u4e0e\u5c16\u7aef\u7ade\u4e89\u5bf9\u624b\u76f8\u6bd4\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/Junqing-Yang/MAST \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2401.12439v1](http://arxiv.org/pdf/2401.12439v1)|**[link](https://github.com/junqing-yang/mast)**|\n", "2401.12425": "|**2024-01-23**|**The Neglected Tails of Vision-Language Models**|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u88ab\u5ffd\u89c6\u7684\u5c3e\u5df4|Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, Shu Kong|Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $<$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-to-image generators, also struggle with the rare concepts identified by our method. To mitigate VLMs' imbalanced performance in zero-shot recognition, we propose REtrieval-Augmented Learning REAL. First, instead of prompting VLMs using the original class names, REAL uses their most frequent synonyms found in VLMs' pretraining texts. This already outperforms human-engineered and LLM-generated prompts over nine benchmark datasets, likely because VLMs have seen more images associated with the frequently used synonyms. Second, REAL uses all the concept synonyms to retrieve a small, class-balanced set of pretraining data to train a robust classifier. REAL surpasses the recent retrieval-augmented solution REACT, using 400x less storage and 10,000x less training time!|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u96f6\u6837\u672c\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89c6\u89c9\u6982\u5ff5\u4e4b\u95f4\u8868\u73b0\u51fa\u4e25\u91cd\u4e0d\u5e73\u8861\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5c3d\u7ba1 CLIP \u5728 ImageNet \u4e0a\u7684\u5e73\u5747\u96f6\u6837\u672c\u51c6\u786e\u7387\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0872.7%\uff09\uff0c\u4f46\u5728 10 \u4e2a\u6982\u5ff5\uff08\u4f8b\u5982\u9640\u87ba\u4eea\u548c\u591c\u86c7\uff09\u4e0a\u7684\u6536\u76ca\u7387\u4e0d\u5230 10%\uff0c\u5927\u6982\u662f\u56e0\u4e3a\u8fd9\u4e9b\u6982\u5ff5\u5728 VLM \u7684\u4e0d\u5e73\u8861\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u9884\u8bad\u7ec3\u6570\u636e\u3002\u7136\u800c\uff0c\u8bc4\u4f30\u8fd9\u79cd\u4e0d\u5e73\u8861\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u8ba1\u7b97 VLM \u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7279\u5b9a\u6982\u5ff5\u7684\u9891\u7387\u5e76\u975e\u6613\u4e8b\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u9996\u6b21\u5c1d\u8bd5\u901a\u8fc7\u5206\u6790\u9884\u8bad\u7ec3\u6587\u672c\u6765\u6d4b\u91cf\u6982\u5ff5\u9891\u7387\u3002\u6211\u4eec\u4f7f\u7528\u73b0\u6210\u7684\u8bed\u8a00\u6a21\u578b\u6765\u5e2e\u52a9\u8ba1\u7b97\u5305\u542b\u7ed9\u5b9a\u6982\u5ff5\u540c\u4e49\u8bcd\u7684\u76f8\u5173\u6587\u672c\u5e76\u89e3\u51b3\u8bed\u8a00\u6b67\u4e49\u3002\u6211\u4eec\u786e\u8ba4\u50cf LAION \u8fd9\u6837\u7684\u6d41\u884c VLM \u6570\u636e\u96c6\u786e\u5b9e\u8868\u73b0\u51fa\u957f\u5c3e\u6982\u5ff5\u5206\u5e03\uff0c\u8fd9\u4e0e\u6bcf\u7c7b\u7684\u51c6\u786e\u5ea6\u5bc6\u5207\u76f8\u5173\u3002\u6b64\u5916\uff0c\u5f53\u4ee3\u7684\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u4f8b\u5982\u89c6\u89c9\u804a\u5929\u673a\u5668\u4eba\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5668\uff0c\u4e5f\u96be\u4ee5\u89e3\u51b3\u6211\u4eec\u7684\u65b9\u6cd5\u8bc6\u522b\u7684\u7f55\u89c1\u6982\u5ff5\u3002\u4e3a\u4e86\u7f13\u89e3 VLM \u5728\u96f6\u6837\u672c\u8bc6\u522b\u4e2d\u7684\u4e0d\u5e73\u8861\u6027\u80fd\uff0c\u6211\u4eec\u63d0\u51fa REtrieval-Augmented Learning REAL\u3002\u9996\u5148\uff0cREAL \u6ca1\u6709\u63d0\u793a VLM \u4f7f\u7528\u539f\u59cb\u7c7b\u540d\uff0c\u800c\u662f\u4f7f\u7528 VLM \u9884\u8bad\u7ec3\u6587\u672c\u4e2d\u6700\u5e38\u89c1\u7684\u540c\u4e49\u8bcd\u3002\u5728\u4e5d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8fd9\u5df2\u7ecf\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u548c\u6cd5\u5b66\u7855\u58eb\u751f\u6210\u7684\u63d0\u793a\uff0c\u53ef\u80fd\u662f\u56e0\u4e3a VLM \u5df2\u7ecf\u770b\u5230\u4e86\u66f4\u591a\u4e0e\u5e38\u7528\u540c\u4e49\u8bcd\u76f8\u5173\u7684\u56fe\u50cf\u3002\u5176\u6b21\uff0cREAL \u4f7f\u7528\u6240\u6709\u6982\u5ff5\u540c\u4e49\u8bcd\u6765\u68c0\u7d22\u5c0f\u578b\u3001\u7c7b\u5e73\u8861\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ee5\u8bad\u7ec3\u9c81\u68d2\u7684\u5206\u7c7b\u5668\u3002 REAL \u8d85\u8d8a\u4e86\u6700\u8fd1\u7684\u68c0\u7d22\u589e\u5f3a\u89e3\u51b3\u65b9\u6848 REACT\uff0c\u4f7f\u7528\u7684\u5b58\u50a8\u7a7a\u95f4\u51cf\u5c11\u4e86 400 \u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e86 10,000 \u500d\uff01|[2401.12425v1](http://arxiv.org/pdf/2401.12425v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2401.12433": "|**2024-01-23**|**A Novel Garment Transfer Method Supervised by Distilled Knowledge of Virtual Try-on Model**|\u865a\u62df\u8bd5\u7a7f\u6a21\u578b\u84b8\u998f\u77e5\u8bc6\u76d1\u7763\u4e0b\u7684\u65b0\u578b\u670d\u88c5\u4f20\u8f93\u65b9\u6cd5|Naiyu Fang, Lemiao Qiu, Shuyou Zhang, Zili Wang, Kerui Hu, Jianrong Tan|When a shopper chooses garments online, garment transfer technology wears the garment from the model image onto the shopper's image, allowing the shopper to decide whether the garment is suitable for them. As garment transfer leverages wild and cheap person image as garment condition, it has attracted tremendous community attention and holds vast commercial potential. However, since the ground truth of garment transfer is almost unavailable in reality, previous studies have treated garment transfer as either pose transfer or garment-pose disentanglement, and trained garment transfer in self-supervised learning, yet do not cover garment transfer intentions completely. Therefore, the training supervising the garment transfer is a rock-hard issue. Notably, virtual try-on technology has exhibited superior performance using self-supervised learning. We supervise the garment transfer training via knowledge distillation from virtual try-on. Specifically, we first train the transfer parsing reasoning model at multi-phases to provide shape guidance for downstream tasks. The transfer parsing reasoning model learns the response and feature knowledge from the try-on parsing reasoning model and absorbs the hard knowledge from the ground truth. By leveraging the warping knowledge from virtual try-on, we estimate a progressive flow to precisely warp the garment by learning the shape and content correspondence. To enhance transfer realism, we propose a well-designed arm regrowth task to infer exposed skin pixel content. Experiments demonstrate that our method has state-of-the-art performance in transferring garments between person compared with other virtual try-on and garment transfer methods.|\u5f53\u8d2d\u7269\u8005\u5728\u7ebf\u9009\u62e9\u670d\u88c5\u65f6\uff0c\u670d\u88c5\u8f6c\u79fb\u6280\u672f\u5c06\u6a21\u7279\u56fe\u50cf\u4e2d\u7684\u670d\u88c5\u590d\u5236\u5230\u8d2d\u7269\u8005\u7684\u56fe\u50cf\u4e0a\uff0c\u8ba9\u8d2d\u7269\u8005\u51b3\u5b9a\u8be5\u670d\u88c5\u662f\u5426\u9002\u5408\u81ea\u5df1\u3002\u7531\u4e8e\u670d\u88c5\u8f6c\u79fb\u5229\u7528\u72c2\u91ce\u3001\u5ec9\u4ef7\u7684\u4eba\u7269\u5f62\u8c61\u4f5c\u4e3a\u670d\u88c5\u6761\u4ef6\uff0c\u5f15\u8d77\u4e86\u793e\u4f1a\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u5e76\u5177\u6709\u5de8\u5927\u7684\u5546\u4e1a\u6f5c\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u670d\u88c5\u8f6c\u79fb\u7684\u57fa\u672c\u4e8b\u5b9e\u5728\u73b0\u5b9e\u4e2d\u51e0\u4e4e\u4e0d\u53ef\u7528\uff0c\u4ee5\u524d\u7684\u7814\u7a76\u5c06\u670d\u88c5\u8f6c\u79fb\u89c6\u4e3a\u59ff\u52bf\u8f6c\u79fb\u6216\u670d\u88c5\u59ff\u52bf\u89e3\u5f00\uff0c\u5e76\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u8bad\u7ec3\u670d\u88c5\u8f6c\u79fb\uff0c\u4f46\u5e76\u6ca1\u6709\u5b8c\u5168\u6db5\u76d6\u670d\u88c5\u8f6c\u79fb\u610f\u56fe\u3002\u56e0\u6b64\uff0c\u76d1\u7763\u670d\u88c5\u8f6c\u79fb\u7684\u57f9\u8bad\u662f\u4e00\u4e2a\u68d8\u624b\u7684\u95ee\u9898\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u865a\u62df\u8bd5\u7a7f\u6280\u672f\u901a\u8fc7\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6211\u4eec\u901a\u8fc7\u865a\u62df\u8bd5\u7a7f\u7684\u77e5\u8bc6\u84b8\u998f\u6765\u76d1\u7763\u670d\u88c5\u8f6c\u79fb\u57f9\u8bad\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u5728\u591a\u9636\u6bb5\u8bad\u7ec3\u8f6c\u79fb\u89e3\u6790\u63a8\u7406\u6a21\u578b\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u5f62\u72b6\u6307\u5bfc\u3002\u8fc1\u79fb\u89e3\u6790\u63a8\u7406\u6a21\u578b\u4ece\u8bd5\u7a7f\u89e3\u6790\u63a8\u7406\u6a21\u578b\u4e2d\u5b66\u4e60\u54cd\u5e94\u548c\u7279\u5f81\u77e5\u8bc6\uff0c\u5e76\u4ece\u5730\u9762\u4e8b\u5b9e\u4e2d\u5438\u6536\u786c\u77e5\u8bc6\u3002\u901a\u8fc7\u5229\u7528\u865a\u62df\u8bd5\u7a7f\u4e2d\u7684\u53d8\u5f62\u77e5\u8bc6\uff0c\u6211\u4eec\u901a\u8fc7\u5b66\u4e60\u5f62\u72b6\u548c\u5185\u5bb9\u5bf9\u5e94\u5173\u7cfb\u6765\u4f30\u8ba1\u6e10\u8fdb\u6d41\u7a0b\uff0c\u4ee5\u7cbe\u786e\u5730\u53d8\u5f62\u670d\u88c5\u3002\u4e3a\u4e86\u589e\u5f3a\u8f6c\u79fb\u771f\u5b9e\u611f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u624b\u81c2\u518d\u751f\u4efb\u52a1\u6765\u63a8\u65ad\u66b4\u9732\u7684\u76ae\u80a4\u50cf\u7d20\u5185\u5bb9\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u865a\u62df\u8bd5\u7a7f\u548c\u670d\u88c5\u8f6c\u79fb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4eba\u4e0e\u4eba\u4e4b\u95f4\u8f6c\u79fb\u670d\u88c5\u65b9\u9762\u5177\u6709\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2401.12433v1](http://arxiv.org/pdf/2401.12433v1)|null|\n", "2401.12414": "|**2024-01-23**|**Icy Moon Surface Simulation and Stereo Depth Estimation for Sampling Autonomy**|\u7528\u4e8e\u91c7\u6837\u81ea\u4e3b\u6027\u7684\u51b0\u6708\u8868\u9762\u6a21\u62df\u548c\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1|Ramchander Bhaskara, Georgios Georgakis, Jeremy Nash, Marissa Cameron, Joseph Bowkett, Adnan Ansar, Manoranjan Majji, Paul Backes|Sampling autonomy for icy moon lander missions requires understanding of topographic and photometric properties of the sampling terrain. Unavailability of high resolution visual datasets (either bird-eye view or point-of-view from a lander) is an obstacle for selection, verification or development of perception systems. We attempt to alleviate this problem by: 1) proposing Graphical Utility for Icy moon Surface Simulations (GUISS) framework, for versatile stereo dataset generation that spans the spectrum of bulk photometric properties, and 2) focusing on a stereo-based visual perception system and evaluating both traditional and deep learning-based algorithms for depth estimation from stereo matching. The surface reflectance properties of icy moon terrains (Enceladus and Europa) are inferred from multispectral datasets of previous missions. With procedural terrain generation and physically valid illumination sources, our framework can fit a wide range of hypotheses with respect to visual representations of icy moon terrains. This is followed by a study over the performance of stereo matching algorithms under different visual hypotheses. Finally, we emphasize the standing challenges to be addressed for simulating perception data assets for icy moons such as Enceladus and Europa. Our code can be found here: https://github.com/nasa-jpl/guiss.|\u51b0\u51b7\u6708\u7403\u7740\u9646\u5668\u4efb\u52a1\u7684\u81ea\u4e3b\u91c7\u6837\u9700\u8981\u4e86\u89e3\u91c7\u6837\u5730\u5f62\u7684\u5730\u5f62\u548c\u5149\u5ea6\u7279\u6027\u3002\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u6570\u636e\u96c6\uff08\u65e0\u8bba\u662f\u9e1f\u77b0\u56fe\u8fd8\u662f\u7740\u9646\u5668\u7684\u89c6\u89d2\uff09\u7684\u4e0d\u53ef\u7528\u662f\u9009\u62e9\u3001\u9a8c\u8bc1\u6216\u5f00\u53d1\u611f\u77e5\u7cfb\u7edf\u7684\u969c\u788d\u3002\u6211\u4eec\u8bd5\u56fe\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff1a1\uff09\u63d0\u51fa\u51b0\u6708\u8868\u9762\u6a21\u62df\u56fe\u5f62\u5b9e\u7528\u7a0b\u5e8f\uff08GUISS\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u8de8\u8d8a\u4f53\u5149\u5ea6\u7279\u6027\u8303\u56f4\u7684\u901a\u7528\u7acb\u4f53\u6570\u636e\u96c6\uff0c2\uff09\u4e13\u6ce8\u4e8e\u57fa\u4e8e\u7acb\u4f53\u7684\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u548c\u8bc4\u4f30\u4f20\u7edf\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7acb\u4f53\u5339\u914d\u6df1\u5ea6\u4f30\u8ba1\u7b97\u6cd5\u3002\u51b0\u51b7\u7684\u536b\u661f\u5730\u5f62\uff08\u571f\u536b\u4e8c\u548c\u6b27\u7f57\u5df4\uff09\u7684\u8868\u9762\u53cd\u5c04\u7279\u6027\u662f\u6839\u636e\u4e4b\u524d\u4efb\u52a1\u7684\u591a\u5149\u8c31\u6570\u636e\u96c6\u63a8\u65ad\u51fa\u6765\u7684\u3002\u901a\u8fc7\u7a0b\u5e8f\u5730\u5f62\u751f\u6210\u548c\u7269\u7406\u4e0a\u6709\u6548\u7684\u7167\u660e\u6e90\uff0c\u6211\u4eec\u7684\u6846\u67b6\u53ef\u4ee5\u9002\u5e94\u6709\u5173\u51b0\u51b7\u6708\u7403\u5730\u5f62\u7684\u89c6\u89c9\u8868\u793a\u7684\u5404\u79cd\u5047\u8bbe\u3002\u63a5\u4e0b\u6765\u662f\u5bf9\u4e0d\u540c\u89c6\u89c9\u5047\u8bbe\u4e0b\u7acb\u4f53\u5339\u914d\u7b97\u6cd5\u6027\u80fd\u7684\u7814\u7a76\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f3a\u8c03\u6a21\u62df\u571f\u536b\u4e8c\u548c\u6728\u536b\u4e8c\u7b49\u51b0\u51b7\u536b\u661f\u7684\u611f\u77e5\u6570\u636e\u8d44\u4ea7\u9700\u8981\u89e3\u51b3\u7684\u957f\u671f\u6311\u6218\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u4ee5\u5728\u8fd9\u91cc\u627e\u5230\uff1ahttps://github.com/nasa-jpl/guiss\u3002|[2401.12414v1](http://arxiv.org/pdf/2401.12414v1)|**[link](https://github.com/nasa-jpl/guiss)**|\n"}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2401.12978": "|**2024-01-23**|**Zero-Shot Learning for the Primitives of 3D Affordance in General Objects**|\u4e00\u822c\u5bf9\u8c61\u4e2d 3D \u53ef\u4f9b\u6027\u57fa\u5143\u7684\u96f6\u6837\u672c\u5b66\u4e60|Hyeonwoo Kim, Sookwan Han, Patrick Kwon, Hanbyul Joo|One of the major challenges in AI is teaching machines to precisely respond and utilize environmental functionalities, thereby achieving the affordance awareness that humans possess. Despite its importance, the field has been lagging in terms of learning, especially in 3D, as annotating affordance accompanies a laborious process due to the numerous variations of human-object interaction. The low availability of affordance data limits the learning in terms of generalization for object categories, and also simplifies the representation of affordance, capturing only a fraction of the affordance. To overcome these challenges, we propose a novel, self-supervised method to generate the 3D affordance examples given only a 3D object, without any manual annotations. The method starts by capturing the 3D object into images and creating 2D affordance images by inserting humans into the image via inpainting diffusion models, where we present the Adaptive Mask algorithm to enable human insertion without altering the original details of the object. The method consequently lifts inserted humans back to 3D to create 3D human-object pairs, where the depth ambiguity is resolved within a depth optimization framework that utilizes pre-generated human postures from multiple viewpoints. We also provide a novel affordance representation defined on relative orientations and proximity between dense human and object points, that can be easily aggregated from any 3D HOI datasets. The proposed representation serves as a primitive that can be manifested to conventional affordance representations via simple transformations, ranging from physically exerted affordances to nonphysical ones. We demonstrate the efficacy of our method and representation by generating the 3D affordance samples and deriving high-quality affordance examples from the representation, including contact, orientation, and spatial occupancies.|\u4eba\u5de5\u667a\u80fd\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u662f\u6559\u4f1a\u673a\u5668\u7cbe\u786e\u54cd\u5e94\u548c\u5229\u7528\u73af\u5883\u529f\u80fd\uff0c\u4ece\u800c\u5b9e\u73b0\u4eba\u7c7b\u62e5\u6709\u7684\u53ef\u4f9b\u6027\u610f\u8bc6\u3002\u5c3d\u7ba1\u5b83\u5f88\u91cd\u8981\uff0c\u4f46\u8be5\u9886\u57df\u5728\u5b66\u4e60\u65b9\u9762\u4e00\u76f4\u843d\u540e\uff0c\u5c24\u5176\u662f\u5728 3D \u9886\u57df\uff0c\u56e0\u4e3a\u7531\u4e8e\u4eba\u4e0e\u7269\u4f53\u4ea4\u4e92\u7684\u4f17\u591a\u53d8\u5316\uff0c\u6ce8\u91ca\u53ef\u4f9b\u6027\u4f34\u968f\u7740\u4e00\u4e2a\u8d39\u529b\u7684\u8fc7\u7a0b\u3002\u53ef\u4f9b\u6027\u6570\u636e\u7684\u4f4e\u53ef\u7528\u6027\u9650\u5236\u4e86\u5bf9\u8c61\u7c7b\u522b\u6cdb\u5316\u65b9\u9762\u7684\u5b66\u4e60\uff0c\u5e76\u4e14\u8fd8\u7b80\u5316\u4e86\u53ef\u4f9b\u6027\u7684\u8868\u793a\uff0c\u4ec5\u6355\u83b7\u4e86\u53ef\u4f9b\u6027\u7684\u4e00\u5c0f\u90e8\u5206\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u4ec5\u5728\u7ed9\u5b9a 3D \u5bf9\u8c61\u7684\u60c5\u51b5\u4e0b\u751f\u6210 3D \u53ef\u4f9b\u6027\u793a\u4f8b\uff0c\u65e0\u9700\u4efb\u4f55\u624b\u52a8\u6ce8\u91ca\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5c06 3D \u5bf9\u8c61\u6355\u83b7\u5230\u56fe\u50cf\u4e2d\uff0c\u5e76\u901a\u8fc7\u4fee\u590d\u6269\u6563\u6a21\u578b\u5c06\u4eba\u4f53\u63d2\u5165\u5230\u56fe\u50cf\u4e2d\u6765\u521b\u5efa 2D \u53ef\u4f9b\u6027\u56fe\u50cf\uff0c\u5176\u4e2d\u6211\u4eec\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u63a9\u6a21\u7b97\u6cd5\uff0c\u4ee5\u5728\u4e0d\u6539\u53d8\u5bf9\u8c61\u539f\u59cb\u7ec6\u8282\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4eba\u4f53\u63d2\u5165\u3002\u56e0\u6b64\uff0c\u8be5\u65b9\u6cd5\u5c06\u63d2\u5165\u7684\u4eba\u4f53\u63d0\u5347\u56de 3D \u4ee5\u521b\u5efa 3D \u4eba\u4f53-\u7269\u4f53\u5bf9\uff0c\u5176\u4e2d\u6df1\u5ea6\u6a21\u7cca\u6027\u5728\u6df1\u5ea6\u4f18\u5316\u6846\u67b6\u5185\u5f97\u5230\u89e3\u51b3\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4ece\u591a\u4e2a\u89c6\u70b9\u9884\u5148\u751f\u6210\u7684\u4eba\u4f53\u59ff\u52bf\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u4f9b\u6027\u8868\u793a\uff0c\u5b83\u662f\u6839\u636e\u5bc6\u96c6\u7684\u4eba\u548c\u7269\u4f53\u70b9\u4e4b\u95f4\u7684\u76f8\u5bf9\u65b9\u5411\u548c\u63a5\u8fd1\u5ea6\u5b9a\u4e49\u7684\uff0c\u53ef\u4ee5\u8f7b\u677e\u5730\u4ece\u4efb\u4f55 3D HOI \u6570\u636e\u96c6\u4e2d\u805a\u5408\u3002\u6240\u63d0\u51fa\u7684\u8868\u793a\u4f5c\u4e3a\u4e00\u4e2a\u57fa\u5143\uff0c\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u8f6c\u6362\uff08\u4ece\u7269\u7406\u65bd\u52a0\u7684\u53ef\u4f9b\u6027\u5230\u975e\u7269\u7406\u7684\u53ef\u4f9b\u6027\uff09\u6765\u663e\u73b0\u4e3a\u4f20\u7edf\u7684\u53ef\u4f9b\u6027\u8868\u793a\u3002\u6211\u4eec\u901a\u8fc7\u751f\u6210 3D \u53ef\u4f9b\u6027\u6837\u672c\u5e76\u4ece\u8868\u793a\u4e2d\u5bfc\u51fa\u9ad8\u8d28\u91cf\u53ef\u4f9b\u6027\u793a\u4f8b\uff08\u5305\u62ec\u63a5\u89e6\u3001\u65b9\u5411\u548c\u7a7a\u95f4\u5360\u7528\uff09\u6765\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u548c\u8868\u793a\u7684\u6709\u6548\u6027\u3002|[2401.12978v1](http://arxiv.org/pdf/2401.12978v1)|null|\n", "2401.12945": "|**2024-01-23**|**Lumiere: A Space-Time Diffusion Model for Video Generation**|Lumiere\uff1a\u7528\u4e8e\u89c6\u9891\u751f\u6210\u7684\u65f6\u7a7a\u6269\u6563\u6a21\u578b|Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et.al.|We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.|\u6211\u4eec\u4ecb\u7ecd Lumiere\u2014\u2014\u4e00\u79cd\u6587\u672c\u5230\u89c6\u9891\u7684\u6269\u6563\u6a21\u578b\uff0c\u65e8\u5728\u5408\u6210\u63cf\u7ed8\u771f\u5b9e\u3001\u591a\u6837\u5316\u548c\u8fde\u8d2f\u8fd0\u52a8\u7684\u89c6\u9891\u2014\u2014\u8fd9\u662f\u89c6\u9891\u5408\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65f6\u7a7a U-Net \u67b6\u6784\uff0c\u8be5\u67b6\u6784\u901a\u8fc7\u6a21\u578b\u4e2d\u7684\u5355\u6b21\u4f20\u9012\u4e00\u6b21\u6027\u751f\u6210\u89c6\u9891\u7684\u6574\u4e2a\u65f6\u95f4\u6301\u7eed\u65f6\u95f4\u3002\u8fd9\u4e0e\u73b0\u6709\u7684\u89c6\u9891\u6a21\u578b\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u73b0\u6709\u7684\u89c6\u9891\u6a21\u578b\u5408\u6210\u9065\u8fdc\u7684\u5173\u952e\u5e27\uff0c\u7136\u540e\u8fdb\u884c\u65f6\u95f4\u8d85\u5206\u8fa8\u7387\u2014\u2014\u8fd9\u79cd\u65b9\u6cd5\u672c\u8d28\u4e0a\u4f7f\u5168\u5c40\u65f6\u95f4\u4e00\u81f4\u6027\u96be\u4ee5\u5b9e\u73b0\u3002\u901a\u8fc7\u90e8\u7f72\u7a7a\u95f4\u548c\uff08\u91cd\u8981\u7684\uff09\u65f6\u95f4\u4e0b\u91c7\u6837\u548c\u4e0a\u91c7\u6837\uff0c\u5e76\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5b66\u4e60\u901a\u8fc7\u5728\u591a\u4e2a\u65f6\u7a7a\u5c3a\u5ea6\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7ed3\u679c\uff0c\u5e76\u8868\u660e\u6211\u4eec\u7684\u8bbe\u8ba1\u53ef\u4ee5\u8f7b\u677e\u4fc3\u8fdb\u5404\u79cd\u5185\u5bb9\u521b\u5efa\u4efb\u52a1\u548c\u89c6\u9891\u7f16\u8f91\u5e94\u7528\u7a0b\u5e8f\uff0c\u5305\u62ec\u56fe\u50cf\u5230\u89c6\u9891\u3001\u89c6\u9891\u4fee\u590d\u548c\u98ce\u683c\u5316\u751f\u6210\u3002|[2401.12945v1](http://arxiv.org/pdf/2401.12945v1)|null|\n", "2401.12596": "|**2024-01-23**|**UniHDA: Towards Universal Hybrid Domain Adaptation of Image Generators**|UniHDA\uff1a\u8fc8\u5411\u56fe\u50cf\u751f\u6210\u5668\u7684\u901a\u7528\u6df7\u5408\u57df\u9002\u5e94|Hengjia Li, Yang Liu, Yuqi Lin, Zhanwei Zhang, Yibo Zhao, weihang Pan, Tu Zheng, Zheng Yang, Yuchun Jiang, Boxi Wu, et.al.|Generative domain adaptation has achieved remarkable progress, enabling us to adapt a pre-trained generator to a new target domain. However, existing methods simply adapt the generator to a single target domain and are limited to a single modality, either text-driven or image-driven. Moreover, they are prone to overfitting domain-specific attributes, which inevitably compromises cross-domain consistency. In this paper, we propose UniHDA, a unified and versatile framework for generative hybrid domain adaptation with multi-modal references from multiple domains. We use CLIP encoder to project multi-modal references into a unified embedding space and then linear interpolate the direction vectors from multiple target domains to achieve hybrid domain adaptation. To ensure the cross-domain consistency, we propose a novel cross-domain spatial structure (CSS) loss that maintains detailed spatial structure information between source and target generator. Experiments show that the adapted generator can synthesise realistic images with various attribute compositions. Additionally, our framework is versatile to multiple generators, \\eg, StyleGAN2 and Diffusion Models.|\u751f\u6210\u57df\u9002\u5e94\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u5c55\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u5c06\u9884\u8bad\u7ec3\u7684\u751f\u6210\u5668\u9002\u5e94\u65b0\u7684\u76ee\u6807\u57df\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u53ea\u662f\u7b80\u5355\u5730\u4f7f\u751f\u6210\u5668\u9002\u5e94\u5355\u4e2a\u76ee\u6807\u57df\uff0c\u5e76\u4e14\u4ec5\u9650\u4e8e\u5355\u4e00\u6a21\u5f0f\uff08\u6587\u672c\u9a71\u52a8\u6216\u56fe\u50cf\u9a71\u52a8\uff09\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u5f88\u5bb9\u6613\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u4e8e\u57df\u7684\u5c5e\u6027\uff0c\u8fd9\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u635f\u5bb3\u8de8\u57df\u7684\u4e00\u81f4\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 UniHDA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u4e14\u901a\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5177\u6709\u6765\u81ea\u591a\u4e2a\u9886\u57df\u7684\u591a\u6a21\u6001\u53c2\u8003\u7684\u751f\u6210\u6df7\u5408\u57df\u9002\u5e94\u3002\u6211\u4eec\u4f7f\u7528 CLIP \u7f16\u7801\u5668\u5c06\u591a\u6a21\u6001\u53c2\u8003\u6295\u5f71\u5230\u7edf\u4e00\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u7136\u540e\u5bf9\u6765\u81ea\u591a\u4e2a\u76ee\u6807\u57df\u7684\u65b9\u5411\u5411\u91cf\u8fdb\u884c\u7ebf\u6027\u63d2\u503c\u4ee5\u5b9e\u73b0\u6df7\u5408\u57df\u81ea\u9002\u5e94\u3002\u4e3a\u4e86\u786e\u4fdd\u8de8\u57df\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u57df\u7a7a\u95f4\u7ed3\u6784\uff08CSS\uff09\u635f\u5931\uff0c\u5b83\u53ef\u4ee5\u7ef4\u62a4\u6e90\u548c\u76ee\u6807\u751f\u6210\u5668\u4e4b\u95f4\u7684\u8be6\u7ec6\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u9002\u5e94\u7684\u751f\u6210\u5668\u53ef\u4ee5\u5408\u6210\u5177\u6709\u5404\u79cd\u5c5e\u6027\u7ec4\u5408\u7684\u903c\u771f\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u9002\u7528\u4e8e\u591a\u79cd\u751f\u6210\u5668\uff0c\u4f8b\u5982 StyleGAN2 \u548c\u6269\u6563\u6a21\u578b\u3002|[2401.12596v1](http://arxiv.org/pdf/2401.12596v1)|null|\n", "2401.12456": "|**2024-01-23**|**Exploration and Improvement of Nerf-based 3D Scene Editing Techniques**|\u57fa\u4e8eNerf\u76843D\u573a\u666f\u7f16\u8f91\u6280\u672f\u7684\u63a2\u7d22\u4e0e\u6539\u8fdb|Shun Fang, Ming Cui, Xing Feng, Yanan Zhang|NeRF's high-quality scene synthesis capability was quickly accepted by scholars in the years after it was proposed, and significant progress has been made in 3D scene representation and synthesis. However, the high computational cost limits intuitive and efficient editing of scenes, making NeRF's development in the scene editing field facing many challenges. This paper reviews the preliminary explorations of scholars on NeRF in the scene or object editing field in recent years, mainly changing the shape and texture of scenes or objects in new synthesized scenes; through the combination of residual models such as GaN and Transformer with NeRF, the generalization ability of NeRF scene editing has been further expanded, including realizing real-time new perspective editing feedback, multimodal editing of text synthesized 3D scenes, 4D synthesis performance, and in-depth exploration in light and shadow editing, initially achieving optimization of indirect touch editing and detail representation in complex scenes. Currently, most NeRF editing methods focus on the touch points and materials of indirect points, but when dealing with more complex or larger 3D scenes, it is difficult to balance accuracy, breadth, efficiency, and quality. Overcoming these challenges may become the direction of future NeRF 3D scene editing technology.|NeRF\u7684\u9ad8\u8d28\u91cf\u573a\u666f\u5408\u6210\u80fd\u529b\u5728\u63d0\u51fa\u540e\u7684\u51e0\u5e74\u91cc\u8fc5\u901f\u88ab\u5b66\u8005\u4eec\u63a5\u53d7\uff0c\u5e76\u57283D\u573a\u666f\u8868\u793a\u548c\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u573a\u666f\u7684\u76f4\u89c2\u9ad8\u6548\u7f16\u8f91\uff0c\u4f7f\u5f97NeRF\u5728\u573a\u666f\u7f16\u8f91\u9886\u57df\u7684\u53d1\u5c55\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002\u672c\u6587\u56de\u987e\u4e86\u8fd1\u5e74\u6765\u5b66\u8005\u4eec\u5bf9NeRF\u5728\u573a\u666f\u6216\u7269\u4f53\u7f16\u8f91\u9886\u57df\u7684\u521d\u6b65\u63a2\u7d22\uff0c\u4e3b\u8981\u662f\u6539\u53d8\u65b0\u5408\u6210\u573a\u666f\u4e2d\u573a\u666f\u6216\u7269\u4f53\u7684\u5f62\u72b6\u548c\u7eb9\u7406\uff1b\u901a\u8fc7GaN\u3001Transformer\u7b49\u6b8b\u5dee\u6a21\u578b\u4e0eNeRF\u7684\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86NeRF\u573a\u666f\u7f16\u8f91\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u5b9e\u73b0\u5b9e\u65f6\u65b0\u89c6\u89d2\u7f16\u8f91\u53cd\u9988\u3001\u6587\u672c\u5408\u62103D\u573a\u666f\u7684\u591a\u6a21\u6001\u7f16\u8f91\u30014D\u5408\u6210\u6027\u80fd\u4ee5\u53ca-\u5728\u5149\u5f71\u7f16\u8f91\u65b9\u9762\u8fdb\u884c\u6df1\u5165\u63a2\u7d22\uff0c\u521d\u6b65\u5b9e\u73b0\u590d\u6742\u573a\u666f\u4e0b\u95f4\u63a5\u89e6\u6478\u7f16\u8f91\u548c\u7ec6\u8282\u8868\u73b0\u7684\u4f18\u5316\u3002\u76ee\u524d\uff0c\u5927\u591a\u6570 NeRF \u7f16\u8f91\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u95f4\u63a5\u70b9\u7684\u63a5\u89e6\u70b9\u548c\u6750\u8d28\uff0c\u4f46\u5728\u5904\u7406\u66f4\u590d\u6742\u6216\u66f4\u5927\u7684 3D \u573a\u666f\u65f6\uff0c\u5f88\u96be\u5e73\u8861\u51c6\u786e\u6027\u3001\u5e7f\u5ea6\u3001\u6548\u7387\u548c\u8d28\u91cf\u3002\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u53ef\u80fd\u4f1a\u6210\u4e3a\u672a\u6765NeRF 3D\u573a\u666f\u7f16\u8f91\u6280\u672f\u7684\u65b9\u5411\u3002|[2401.12456v1](http://arxiv.org/pdf/2401.12456v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.12972": "|**2024-01-23**|**On the Efficacy of Text-Based Input Modalities for Action Anticipation**|\u57fa\u4e8e\u6587\u672c\u7684\u8f93\u5165\u65b9\u5f0f\u5bf9\u52a8\u4f5c\u9884\u671f\u7684\u529f\u6548|Apoorva Beedu, Karan Samel, Irfan Essa|Although the task of anticipating future actions is highly uncertain, information from additional modalities help to narrow down plausible action choices. Each modality provides different environmental context for the model to learn from. While previous multi-modal methods leverage information from modalities such as video and audio, we primarily explore how text inputs for actions and objects can also enable more accurate action anticipation. Therefore, we propose a Multi-modal Anticipative Transformer (MAT), an attention-based video transformer architecture that jointly learns from multi-modal features and text captions. We train our model in two-stages, where the model first learns to predict actions in the video clip by aligning with captions, and during the second stage, we fine-tune the model to predict future actions. Compared to existing methods, MAT has the advantage of learning additional environmental context from two kinds of text inputs: action descriptions during the pre-training stage, and the text inputs for detected objects and actions during modality feature fusion. Through extensive experiments, we evaluate the effectiveness of the pre-training stage, and show that our model outperforms previous methods on all datasets. In addition, we examine the impact of object and action information obtained via text and perform extensive ablations. We evaluate the performance on on three datasets: EpicKitchens-100, EpicKitchens-55 and EGTEA GAZE+; and show that text descriptions do indeed aid in more effective action anticipation.|\u5c3d\u7ba1\u9884\u6d4b\u672a\u6765\u884c\u52a8\u7684\u4efb\u52a1\u9ad8\u5ea6\u4e0d\u786e\u5b9a\uff0c\u4f46\u6765\u81ea\u5176\u4ed6\u6a21\u5f0f\u7684\u4fe1\u606f\u6709\u52a9\u4e8e\u7f29\u5c0f\u5408\u7406\u7684\u884c\u52a8\u9009\u62e9\u8303\u56f4\u3002\u6bcf\u79cd\u6a21\u5f0f\u90fd\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u73af\u5883\u80cc\u666f\u4f9b\u6a21\u578b\u5b66\u4e60\u3002\u867d\u7136\u4ee5\u524d\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5229\u7528\u6765\u81ea\u89c6\u9891\u548c\u97f3\u9891\u7b49\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u4f46\u6211\u4eec\u4e3b\u8981\u63a2\u8ba8\u52a8\u4f5c\u548c\u5bf9\u8c61\u7684\u6587\u672c\u8f93\u5165\u5982\u4f55\u4e5f\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u52a8\u4f5c\u9884\u671f\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u9884\u671f\u53d8\u6362\u5668\uff08MAT\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u89c6\u9891\u53d8\u6362\u5668\u67b6\u6784\uff0c\u53ef\u4ee5\u8054\u5408\u5b66\u4e60\u591a\u6a21\u6001\u7279\u5f81\u548c\u6587\u672c\u5b57\u5e55\u3002\u6211\u4eec\u5206\u4e24\u4e2a\u9636\u6bb5\u8bad\u7ec3\u6a21\u578b\uff0c\u6a21\u578b\u9996\u5148\u5b66\u4e60\u901a\u8fc7\u4e0e\u5b57\u5e55\u5bf9\u9f50\u6765\u9884\u6d4b\u89c6\u9891\u526a\u8f91\u4e2d\u7684\u52a8\u4f5c\uff0c\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u5fae\u8c03\u6a21\u578b\u4ee5\u9884\u6d4b\u672a\u6765\u7684\u52a8\u4f5c\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cMAT \u7684\u4f18\u70b9\u662f\u53ef\u4ee5\u4ece\u4e24\u79cd\u6587\u672c\u8f93\u5165\u4e2d\u5b66\u4e60\u989d\u5916\u7684\u73af\u5883\u4e0a\u4e0b\u6587\uff1a\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u52a8\u4f5c\u63cf\u8ff0\uff0c\u4ee5\u53ca\u6a21\u6001\u7279\u5f81\u878d\u5408\u671f\u95f4\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u548c\u52a8\u4f5c\u7684\u6587\u672c\u8f93\u5165\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8868\u660e\u6211\u4eec\u7684\u6a21\u578b\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u68c0\u67e5\u901a\u8fc7\u6587\u672c\u83b7\u5f97\u7684\u5bf9\u8c61\u548c\u52a8\u4f5c\u4fe1\u606f\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u5e7f\u6cdb\u7684\u6d88\u878d\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u4e09\u4e2a\u6570\u636e\u96c6\u7684\u6027\u80fd\uff1aEpicKitchens-100\u3001EpicKitchens-55 \u548c EGTEA GAZE+\uff1b\u5e76\u8868\u660e\u6587\u672c\u63cf\u8ff0\u786e\u5b9e\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u7684\u884c\u52a8\u9884\u671f\u3002|[2401.12972v1](http://arxiv.org/pdf/2401.12972v1)|null|\n", "2401.12915": "|**2024-01-23**|**Red Teaming Visual Language Models**|\u7ea2\u961f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b|Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, Qi Liu|VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.|VLM\uff08\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u6269\u5c55\u4e86 LLM\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u7684\u529f\u80fd\uff0c\u4ee5\u63a5\u53d7\u591a\u6a21\u5f0f\u8f93\u5165\u3002\u7531\u4e8e\u5df2\u7ecf\u8bc1\u5b9e LLM \u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u7684\u6d4b\u8bd5\u7528\u4f8b\uff08\u79f0\u4e3a\u7ea2\u961f\uff09\u8bf1\u5bfc\u751f\u6210\u6709\u5bb3\u6216\u4e0d\u51c6\u786e\u7684\u5185\u5bb9\uff0c\u56e0\u6b64 VLM \u5728\u7c7b\u4f3c\u573a\u666f\u4e2d\u7684\u8868\u73b0\u5982\u4f55\uff0c\u7279\u522b\u662f\u5728\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\u76f8\u7ed3\u5408\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u4e3a\u4e86\u63a2\u8ba8\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u7ea2\u961f\u6570\u636e\u96c6 RTVLM\uff0c\u5b83\u5305\u542b 4 \u4e2a\u4e3b\u8981\u65b9\u9762\uff08\u5fe0\u5b9e\u3001\u9690\u79c1\u3001\u5b89\u5168\u3001\u516c\u5e73\uff09\u4e0b\u7684 10 \u4e2a\u5b50\u4efb\u52a1\uff08\u4f8b\u5982\u56fe\u50cf\u8bef\u5bfc\u3001\u591a\u6a21\u5f0f\u8d8a\u72f1\u3001\u4eba\u8138\u516c\u5e73\u7b49\uff09\u3002\u6211\u4eec\u7684 RTVLM \u662f\u7b2c\u4e00\u4e2a\u5728\u8fd9 4 \u4e2a\u4e0d\u540c\u65b9\u9762\u5bf9\u5f53\u524d VLM \u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u7684\u7ea2\u961f\u6570\u636e\u96c6\u3002\u8be6\u7ec6\u5206\u6790\u663e\u793a\uff0c10 \u4e2a\u8457\u540d\u7684\u5f00\u6e90 VLM \u90fd\u4e0d\u540c\u7a0b\u5ea6\u5730\u4e0e\u7ea2\u961f\u4f5c\u6597\u4e89\uff0c\u4e0e GPT-4V \u7684\u6027\u80fd\u5dee\u8ddd\u9ad8\u8fbe 31%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7b80\u5355\u5730\u4f7f\u7528 RTVLM \u5c06\u7ea2\u961f\u5bf9\u9f50\u4e0e\u76d1\u7763\u5fae\u8c03 (SFT) \u5e94\u7528\u4e8e LLaVA-v1.5\uff0c\u8fd9\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728 RTVLM \u6d4b\u8bd5\u96c6\u4e2d\u63d0\u9ad8\u4e86 10%\uff0c\u5728 MM-Hal \u4e2d\u63d0\u9ad8\u4e86 13%\uff0c\u5e76\u4e14\u6ca1\u6709\u660e\u663e\u7684\u5f71\u54cdMM-Bench \u7684\u4e0b\u964d\uff0c\u8d85\u8fc7\u4e86\u5177\u6709\u5e38\u89c4\u5bf9\u9f50\u6570\u636e\u7684\u5176\u4ed6\u57fa\u4e8e LLaVA \u7684\u6a21\u578b\u3002\u8fd9\u8868\u660e\u5f53\u524d\u7684\u5f00\u6e90 VLM \u4ecd\u7136\u7f3a\u4e4f\u7ea2\u961f\u534f\u8c03\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u662f\u5f00\u6e90\u7684\u3002|[2401.12915v1](http://arxiv.org/pdf/2401.12915v1)|null|\n", "2401.12862": "|**2024-01-23**|**FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units**|FedRSU\uff1a\u8def\u8fb9\u573a\u666f\u6d41\u4f30\u8ba1\u7684\u8054\u90a6\u5b66\u4e60|Shaoheng Fang, Rui Ye, Wenhao Wang, Zuhong Liu, Yuxiao Wang, Yafei Wang, Siheng Chen, Yanfeng Wang|Roadside unit (RSU) can significantly improve the safety and robustness of autonomous vehicles through Vehicle-to-Everything (V2X) communication. Currently, the usage of a single RSU mainly focuses on real-time inference and V2X collaboration, while neglecting the potential value of the high-quality data collected by RSU sensors. Integrating the vast amounts of data from numerous RSUs can provide a rich source of data for model training. However, the absence of ground truth annotations and the difficulty of transmitting enormous volumes of data are two inevitable barriers to fully exploiting this hidden value. In this paper, we introduce FedRSU, an innovative federated learning framework for self-supervised scene flow estimation. In FedRSU, we present a recurrent self-supervision training paradigm, where for each RSU, the scene flow prediction of points at every timestamp can be supervised by its subsequent future multi-modality observation. Another key component of FedRSU is federated learning, where multiple devices collaboratively train an ML model while keeping the training data local and private. With the power of the recurrent self-supervised learning paradigm, FL is able to leverage innumerable underutilized data from RSU. To verify the FedRSU framework, we construct a large-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU clients, covering various scenarios, modalities, and sensor settings. Based on RSU-SF, we show that FedRSU can greatly improve model performance in ITS and provide a comprehensive benchmark under diverse FL scenarios. To the best of our knowledge, we provide the first real-world LiDAR-camera multi-modal dataset and benchmark for the FL community.|\u8def\u8fb9\u5355\u5143 (RSU) \u53ef\u4ee5\u901a\u8fc7\u8f66\u5bf9\u4e07\u7269 (V2X) \u901a\u4fe1\u663e\u7740\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6027\u548c\u7a33\u5065\u6027\u3002\u76ee\u524d\uff0c\u5355\u4e2aRSU\u7684\u4f7f\u7528\u4e3b\u8981\u96c6\u4e2d\u5728\u5b9e\u65f6\u63a8\u7406\u548cV2X\u534f\u4f5c\uff0c\u800c\u5ffd\u7565\u4e86RSU\u4f20\u611f\u5668\u6536\u96c6\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u6f5c\u5728\u4ef7\u503c\u3002\u6574\u5408\u6765\u81ea\u4f17\u591aRSU\u7684\u6d77\u91cf\u6570\u636e\u53ef\u4ee5\u4e3a\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e30\u5bcc\u7684\u6570\u636e\u6e90\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6ce8\u91ca\u548c\u4f20\u8f93\u5927\u91cf\u6570\u636e\u7684\u56f0\u96be\u662f\u5145\u5206\u5229\u7528\u8fd9\u4e00\u9690\u85cf\u4ef7\u503c\u7684\u4e24\u4e2a\u4e0d\u53ef\u907f\u514d\u7684\u969c\u788d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 FedRSU\uff0c\u4e00\u79cd\u7528\u4e8e\u81ea\u76d1\u7763\u573a\u666f\u6d41\u4f30\u8ba1\u7684\u521b\u65b0\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u3002\u5728FedRSU\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5faa\u73af\u81ea\u76d1\u7763\u8bad\u7ec3\u8303\u4f8b\uff0c\u5176\u4e2d\u5bf9\u4e8e\u6bcf\u4e2aRSU\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6233\u70b9\u7684\u573a\u666f\u6d41\u9884\u6d4b\u53ef\u4ee5\u901a\u8fc7\u5176\u540e\u7eed\u7684\u672a\u6765\u591a\u6a21\u6001\u89c2\u5bdf\u6765\u76d1\u7763\u3002 FedRSU \u7684\u53e6\u4e00\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\u662f\u8054\u5408\u5b66\u4e60\uff0c\u5176\u4e2d\u591a\u4e2a\u8bbe\u5907\u534f\u4f5c\u8bad\u7ec3 ML \u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u6570\u636e\u672c\u5730\u548c\u79c1\u6709\u3002\u51ed\u501f\u5faa\u73af\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u7684\u5f3a\u5927\u529f\u80fd\uff0cFL \u80fd\u591f\u5229\u7528 RSU \u4e2d\u65e0\u6570\u672a\u5145\u5206\u5229\u7528\u7684\u6570\u636e\u3002\u4e3a\u4e86\u9a8c\u8bc1 FedRSU \u6846\u67b6\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6 RSU-SF\u3002\u8be5\u6570\u636e\u96c6\u7531 17 \u4e2a RSU \u5ba2\u6237\u7aef\u7ec4\u6210\uff0c\u6db5\u76d6\u5404\u79cd\u573a\u666f\u3001\u6a21\u5f0f\u548c\u4f20\u611f\u5668\u8bbe\u7f6e\u3002\u57fa\u4e8eRSU-SF\uff0c\u6211\u4eec\u8bc1\u660eFedRSU\u53ef\u4ee5\u6781\u5927\u5730\u63d0\u9ad8ITS\u4e2d\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u540c\u7684FL\u573a\u666f\u4e0b\u63d0\u4f9b\u5168\u9762\u7684\u57fa\u51c6\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u4e3a FL \u793e\u533a\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684 LiDAR \u76f8\u673a\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002|[2401.12862v1](http://arxiv.org/pdf/2401.12862v1)|null|\n", "2401.12568": "|**2024-01-23**|**NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for Talking Face Synthesis**|NeRF-AD\uff1a\u5177\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u89e3\u5f00\u7684\u795e\u7ecf\u8f90\u5c04\u573a\uff0c\u7528\u4e8e\u8bf4\u8bdd\u4eba\u8138\u5408\u6210|Chongke Bi, Xiaoxing Liu, Zhilei Liu|Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to https://xiaoxingliu02.github.io/NeRF-AD.|\u97f3\u9891\u9a71\u52a8\u7684\u4eba\u8138\u5408\u6210\u662f\u5f53\u524d\u591a\u7ef4\u4fe1\u53f7\u5904\u7406\u548c\u591a\u5a92\u4f53\u9886\u57df\u7684\u7814\u7a76\u70ed\u70b9\u4e4b\u4e00\u3002\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u6700\u8fd1\u88ab\u5f15\u5165\u8fd9\u4e00\u7814\u7a76\u9886\u57df\uff0c\u4ee5\u589e\u5f3a\u751f\u6210\u7684\u9762\u90e8\u7684\u771f\u5b9e\u611f\u548c 3D \u6548\u679c\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u57fa\u4e8e NeRF \u7684\u65b9\u6cd5\u8981\u4e48\u7ed9 NeRF \u5e26\u6765\u590d\u6742\u7684\u5b66\u4e60\u4efb\u52a1\uff0c\u540c\u65f6\u7f3a\u4e4f\u76d1\u7763\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u7684\u65b9\u6cd5\uff0c\u8981\u4e48\u65e0\u6cd5\u7cbe\u786e\u5730\u5c06\u97f3\u9891\u6620\u5c04\u5230\u4e0e\u8bed\u97f3\u8fd0\u52a8\u76f8\u5173\u7684\u9762\u90e8\u533a\u57df\u3002\u8fd9\u4e9b\u539f\u56e0\u6700\u7ec8\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u4e0d\u51c6\u786e\u7684\u5507\u90e8\u5f62\u72b6\u3002\u672c\u6587\u5c06 NeRF \u5b66\u4e60\u4efb\u52a1\u7684\u4e00\u90e8\u5206\u5411\u524d\u63a8\u8fdb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e NeRF \u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u89e3\u7ea0\u7f20\u7684\u8bf4\u8bdd\u4eba\u8138\u5408\u6210\u65b9\u6cd5\uff08NeRF-AD\uff09\u3002\u7279\u522b\u662f\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u89e3\u7f20\u6a21\u5757\uff0c\u4f7f\u7528\u4e0e\u8bed\u97f3\u76f8\u5173\u7684\u9762\u90e8\u52a8\u4f5c\u5355\u5143\uff08AU\uff09\u4fe1\u606f\u5c06\u9762\u90e8\u5206\u89e3\u4e3a\u97f3\u9891\u9762\u90e8\u548c\u8eab\u4efd\u9762\u90e8\u3002\u4e3a\u4e86\u7cbe\u786e\u8c03\u8282\u97f3\u9891\u5982\u4f55\u5f71\u54cd\u8bf4\u8bdd\u7684\u9762\u5b54\uff0c\u6211\u4eec\u4ec5\u5c06\u97f3\u9891\u9762\u5b54\u4e0e\u97f3\u9891\u529f\u80fd\u878d\u5408\u3002\u6b64\u5916\uff0cAU\u4fe1\u606f\u4e5f\u88ab\u7528\u6765\u76d1\u7763\u8fd9\u4e24\u79cd\u6a21\u5f0f\u7684\u878d\u5408\u3002\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 NeRF-AD \u5728\u751f\u6210\u903c\u771f\u7684\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\uff08\u5305\u62ec\u56fe\u50cf\u8d28\u91cf\u548c\u5507\u5f62\u540c\u6b65\uff09\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u67e5\u770b\u89c6\u9891\u7ed3\u679c\u8bf7\u53c2\u8003https://xiaoxingliu02.github.io/NeRF-AD\u3002|[2401.12568v1](http://arxiv.org/pdf/2401.12568v1)|null|\n", "2401.12419": "|**2024-01-23**|**Multi-modal News Understanding with Professionally Labelled Videos (ReutersViLNews)**|\u901a\u8fc7\u4e13\u4e1a\u6807\u8bb0\u7684\u89c6\u9891\u8fdb\u884c\u591a\u6a21\u5f0f\u65b0\u95fb\u7406\u89e3 (ReutersViLNews)|Shih-Han Chou, Matthew Kowal, Yasmin Niknam, Diana Moyano, Shayaan Mehdi, Richard Pito, Cheng Zhang, Ian Knopke, Sedef Akinli Kocak, Leonid Sigal, et.al.|While progress has been made in the domain of video-language understanding, current state-of-the-art algorithms are still limited in their ability to understand videos at high levels of abstraction, such as news-oriented videos. Alternatively, humans easily amalgamate information from video and language to infer information beyond what is visually observable in the pixels. An example of this is watching a news story, where the context of the event can play as big of a role in understanding the story as the event itself. Towards a solution for designing this ability in algorithms, we present a large-scale analysis on an in-house dataset collected by the Reuters News Agency, called Reuters Video-Language News (ReutersViLNews) dataset which focuses on high-level video-language understanding with an emphasis on long-form news. The ReutersViLNews Dataset consists of long-form news videos collected and labeled by news industry professionals over several years and contains prominent news reporting from around the world. Each video involves a single story and contains action shots of the actual event, interviews with people associated with the event, footage from nearby areas, and more. ReutersViLNews dataset contains videos from seven subject categories: disaster, finance, entertainment, health, politics, sports, and miscellaneous with annotations from high-level to low-level, title caption, visual video description, high-level story description, keywords, and location. We first present an analysis of the dataset statistics of ReutersViLNews compared to previous datasets. Then we benchmark state-of-the-art approaches for four different video-language tasks. The results suggest that news-oriented videos are a substantial challenge for current video-language understanding algorithms and we conclude by providing future directions in designing approaches to solve the ReutersViLNews dataset.|\u5c3d\u7ba1\u5728\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u5728\u7406\u89e3\u9ad8\u62bd\u8c61\u7ea7\u522b\u7684\u89c6\u9891\uff08\u4f8b\u5982\u9762\u5411\u65b0\u95fb\u7684\u89c6\u9891\uff09\u7684\u80fd\u529b\u65b9\u9762\u4ecd\u7136\u53d7\u5230\u9650\u5236\u3002\u6216\u8005\uff0c\u4eba\u7c7b\u53ef\u4ee5\u8f7b\u677e\u5730\u5408\u5e76\u6765\u81ea\u89c6\u9891\u548c\u8bed\u8a00\u7684\u4fe1\u606f\uff0c\u4ee5\u63a8\u65ad\u8d85\u51fa\u50cf\u7d20\u4e2d\u89c6\u89c9\u53ef\u89c2\u5bdf\u5230\u7684\u4fe1\u606f\u3002\u4e00\u4e2a\u4f8b\u5b50\u662f\u89c2\u770b\u65b0\u95fb\u62a5\u9053\uff0c\u5176\u4e2d\u4e8b\u4ef6\u7684\u80cc\u666f\u5728\u7406\u89e3\u6545\u4e8b\u65b9\u9762\u4e0e\u4e8b\u4ef6\u672c\u8eab\u4e00\u6837\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\u3002\u4e3a\u4e86\u5728\u7b97\u6cd5\u4e2d\u8bbe\u8ba1\u8fd9\u79cd\u80fd\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6211\u4eec\u5bf9\u8def\u900f\u793e\u6536\u96c6\u7684\u5185\u90e8\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5206\u6790\uff0c\u8be5\u6570\u636e\u96c6\u79f0\u4e3a\u8def\u900f\u793e\u89c6\u9891\u8bed\u8a00\u65b0\u95fb (ReutersViLNews) \u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u4e13\u6ce8\u4e8e\u9ad8\u7ea7\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u91cd\u70b9\u662f\u957f\u7bc7\u65b0\u95fb\u3002 ReutersViLNews \u6570\u636e\u96c6\u7531\u65b0\u95fb\u884c\u4e1a\u4e13\u4e1a\u4eba\u58eb\u591a\u5e74\u6765\u6536\u96c6\u548c\u6807\u8bb0\u7684\u957f\u7bc7\u65b0\u95fb\u89c6\u9891\u7ec4\u6210\uff0c\u5305\u542b\u6765\u81ea\u4e16\u754c\u5404\u5730\u7684\u91cd\u8981\u65b0\u95fb\u62a5\u9053\u3002\u6bcf\u4e2a\u89c6\u9891\u90fd\u6d89\u53ca\u4e00\u4e2a\u6545\u4e8b\uff0c\u5e76\u5305\u542b\u5b9e\u9645\u4e8b\u4ef6\u7684\u52a8\u4f5c\u955c\u5934\u3001\u5bf9\u4e0e\u8be5\u4e8b\u4ef6\u76f8\u5173\u7684\u4eba\u5458\u7684\u91c7\u8bbf\u3001\u9644\u8fd1\u5730\u533a\u7684\u955c\u5934\u7b49\u7b49\u3002 ReutersViLNews \u6570\u636e\u96c6\u5305\u542b\u6765\u81ea\u4e03\u4e2a\u4e3b\u9898\u7c7b\u522b\u7684\u89c6\u9891\uff1a\u707e\u96be\u3001\u91d1\u878d\u3001\u5a31\u4e50\u3001\u5065\u5eb7\u3001\u653f\u6cbb\u3001\u4f53\u80b2\u548c\u6742\u9879\uff0c\u5e76\u5e26\u6709\u4ece\u9ad8\u7ea7\u5230\u4f4e\u7ea7\u7684\u6ce8\u91ca\u3001\u6807\u9898\u8bf4\u660e\u3001\u89c6\u89c9\u89c6\u9891\u63cf\u8ff0\u3001\u9ad8\u7ea7\u6545\u4e8b\u63cf\u8ff0\u3001\u5173\u952e\u5b57\u548c\u5730\u70b9\u3002\u6211\u4eec\u9996\u5148\u5bf9ReutersViLNews \u7684\u6570\u636e\u96c6\u7edf\u8ba1\u6570\u636e\u4e0e\u4e4b\u524d\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002\u7136\u540e\uff0c\u6211\u4eec\u5bf9\u56db\u79cd\u4e0d\u540c\u89c6\u9891\u8bed\u8a00\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u9762\u5411\u65b0\u95fb\u7684\u89c6\u9891\u5bf9\u5f53\u524d\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u7b97\u6cd5\u6765\u8bf4\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u4f9b\u89e3\u51b3ReutersViLNews\u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u65b9\u6cd5\u7684\u672a\u6765\u65b9\u5411\u6765\u5f97\u51fa\u7ed3\u8bba\u3002|[2401.12419v1](http://arxiv.org/pdf/2401.12419v1)|null|\n"}, "LLM": {"2401.12975": "|**2024-01-23**|**HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments**|HAZARD \u6311\u6218\uff1a\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e2d\u7684\u5177\u4f53\u51b3\u7b56|Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang, Yilun Du, Joshua B. Tenenbaum, Chuang Gan|Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.|\u9ad8\u4fdd\u771f\u865a\u62df\u73af\u5883\u7684\u6700\u65b0\u8fdb\u5c55\u662f\u6784\u5efa\u667a\u80fd\u5b9e\u4f53\u6765\u611f\u77e5\u3001\u63a8\u7406\u5e76\u4e0e\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u7684\u4e3b\u8981\u9a71\u52a8\u529b\u4e4b\u4e00\u3002\u901a\u5e38\uff0c\u8fd9\u4e9b\u73af\u5883\u4fdd\u6301\u4e0d\u53d8\uff0c\u9664\u975e\u4ee3\u7406\u4e0e\u5b83\u4eec\u4ea4\u4e92\u3002\u7136\u800c\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u4ee3\u7406\u8fd8\u53ef\u80fd\u9762\u4e34\u4ee5\u610f\u5916\u4e8b\u4ef6\u4e3a\u7279\u5f81\u7684\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\uff0c\u9700\u8981\u5feb\u901f\u91c7\u53d6\u76f8\u5e94\u7684\u884c\u52a8\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u62df\u4f53\u73b0\u57fa\u51c6\uff0c\u79f0\u4e3a HAZARD\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u52a8\u6001\u60c5\u51b5\u4e0b\u4f53\u73b0\u4e3b\u4f53\u7684\u51b3\u7b56\u80fd\u529b\u3002 HAZARD\u7531\u706b\u707e\u3001\u6d2a\u6c34\u548c\u98ce\u4e09\u79cd\u610f\u5916\u707e\u5bb3\u573a\u666f\u7ec4\u6210\uff0c\u7279\u522b\u652f\u6301\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u8f85\u52a9\u5e38\u8bc6\u63a8\u7406\u548c\u51b3\u7b56\u3002\u8be5\u57fa\u51c6\u4f7f\u6211\u4eec\u80fd\u591f\u8bc4\u4f30\u81ea\u4e3b\u4ee3\u7406\u8de8\u5404\u79cd\u7ba1\u9053\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u5305\u62ec\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3001\u57fa\u4e8e\u89c4\u5219\u548c\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\u3002\u4f5c\u4e3a\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u7684\u7b2c\u4e00\u6b65\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e LLM \u7684\u4ee3\u7406\uff0c\u5e76\u5bf9\u5176\u89e3\u51b3\u8fd9\u4e9b\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u7684\u627f\u8bfa\u548c\u6311\u6218\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002 HAZARD \u53ef\u5728 https://vis-www.cs.umass.edu/hazard/ \u83b7\u53d6\u3002|[2401.12975v1](http://arxiv.org/pdf/2401.12975v1)|**[link](https://github.com/umass-foundation-model/hazard)**|\n", "2401.12963": "|**2024-01-23**|**AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents**|AutoRT\uff1a\u673a\u5668\u4eba\u4ee3\u7406\u5927\u89c4\u6a21\u7f16\u6392\u7684\u5177\u4f53\u57fa\u7840\u6a21\u578b|Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, et.al.|Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such \"in-the-wild\" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.|\u7ed3\u5408\u4e86\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u6700\u8fd1\u7684\u884c\u52a8\u7684\u57fa\u7840\u6a21\u578b\u5f7b\u5e95\u6539\u53d8\u4e86\u5229\u7528\u4e92\u8054\u7f51\u89c4\u6a21\u6570\u636e\u6765\u63a8\u7406\u6709\u7528\u4efb\u52a1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u8bad\u7ec3\u5177\u4f53\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u6311\u6218\u4e4b\u4e00\u662f\u7f3a\u4e4f\u57fa\u4e8e\u7269\u7406\u4e16\u754c\u7684\u6570\u636e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AutoRT\uff0c\u8fd9\u662f\u4e00\u4e2a\u5229\u7528\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u5b8c\u5168\u770b\u4e0d\u89c1\u7684\u573a\u666f\u4e2d\u4ee5\u6700\u5c11\u7684\u4eba\u5de5\u76d1\u7763\u6765\u6269\u5927\u64cd\u4f5c\u673a\u5668\u4eba\u90e8\u7f72\u7684\u7cfb\u7edf\u3002 AutoRT \u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u8fdb\u884c\u573a\u666f\u7406\u89e3\u548c\u57fa\u7840\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u63d0\u51fa\u7531\u4e00\u7ec4\u673a\u5668\u4eba\u6267\u884c\u7684\u591a\u6837\u5316\u548c\u65b0\u9896\u7684\u6307\u4ee4\u3002\u901a\u8fc7\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u77e5\u8bc6\u6765\u6307\u5bfc\u6570\u636e\u6536\u96c6\uff0c\u4f7f AutoRT \u80fd\u591f\u6709\u6548\u5730\u63a8\u7406\u81ea\u4e3b\u6743\u8861\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u663e\u7740\u6269\u5927\u673a\u5668\u4eba\u5b66\u4e60\u7684\u6570\u636e\u6536\u96c6\u8303\u56f4\u3002\u6211\u4eec\u6f14\u793a\u4e86 AutoRT \u5411\u591a\u4e2a\u5efa\u7b51\u7269\u4e2d\u7684 20 \u591a\u4e2a\u673a\u5668\u4eba\u63d0\u51fa\u6307\u4ee4\uff0c\u5e76\u901a\u8fc7\u8fdc\u7a0b\u64cd\u4f5c\u548c\u81ea\u4e3b\u673a\u5668\u4eba\u7b56\u7565\u6536\u96c6 77,000 \u4e2a\u771f\u5b9e\u7684\u673a\u5668\u4eba\u4e8b\u4ef6\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0cAutoRT \u6536\u96c6\u7684\u6b64\u7c7b\u201c\u91ce\u5916\u201d\u6570\u636e\u660e\u663e\u66f4\u52a0\u591a\u6837\u5316\uff0c\u5e76\u4e14 AutoRT \u5bf9 LLM \u7684\u4f7f\u7528\u5141\u8bb8\u9075\u5faa\u53ef\u4ee5\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u6570\u636e\u6536\u96c6\u673a\u5668\u4eba\u7684\u6307\u4ee4\u3002|[2401.12963v1](http://arxiv.org/pdf/2401.12963v1)|null|\n"}, "Transformer": {"2401.12835": "|**2024-01-23**|**SGTR+: End-to-end Scene Graph Generation with Transformer**|SGTR+\uff1a\u4f7f\u7528 Transformer \u751f\u6210\u7aef\u5230\u7aef\u573a\u666f\u56fe|Rongjie Li, Songyang Zhang, Xuming He|Scene Graph Generation (SGG) remains a challenging visual understanding task due to its compositional property. Most previous works adopt a bottom-up, two-stage or point-based, one-stage approach, which often suffers from high time complexity or suboptimal designs. In this work, we propose a novel SGG method to address the aforementioned issues, formulating the task as a bipartite graph construction problem. To address the issues above, we create a transformer-based end-to-end framework to generate the entity and entity-aware predicate proposal set, and infer directed edges to form relation triplets. Moreover, we design a graph assembling module to infer the connectivity of the bipartite scene graph based on our entity-aware structure, enabling us to generate the scene graph in an end-to-end manner. Based on bipartite graph assembling paradigm, we further propose a new technical design to address the efficacy of entity-aware modeling and optimization stability of graph assembling. Equipped with the enhanced entity-aware design, our method achieves optimal performance and time-complexity. Extensive experimental results show that our design is able to achieve the state-of-the-art or comparable performance on three challenging benchmarks, surpassing most of the existing approaches and enjoying higher efficiency in inference. Code is available: https://github.com/Scarecrow0/SGTR|\u7531\u4e8e\u5176\u7ec4\u5408\u7279\u6027\uff0c\u573a\u666f\u56fe\u751f\u6210\uff08SGG\uff09\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u3002\u4ee5\u524d\u7684\u5927\u591a\u6570\u5de5\u4f5c\u90fd\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u4e24\u9636\u6bb5\u6216\u57fa\u4e8e\u70b9\u7684\u5355\u9636\u6bb5\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u901a\u5e38\u4f1a\u9047\u5230\u9ad8\u65f6\u95f4\u590d\u6742\u5ea6\u6216\u6b21\u4f18\u8bbe\u8ba1\u7684\u95ee\u9898\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 SGG \u65b9\u6cd5\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u5c06\u4efb\u52a1\u8868\u8ff0\u4e3a\u4e8c\u5206\u56fe\u6784\u5efa\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8f6c\u6362\u5668\u7684\u7aef\u5230\u7aef\u6846\u67b6\u6765\u751f\u6210\u5b9e\u4f53\u548c\u5b9e\u4f53\u611f\u77e5\u8c13\u8bcd\u5efa\u8bae\u96c6\uff0c\u5e76\u63a8\u65ad\u6709\u5411\u8fb9\u4ee5\u5f62\u6210\u5173\u7cfb\u4e09\u5143\u7ec4\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u56fe\u7ec4\u88c5\u6a21\u5757\u6765\u57fa\u4e8e\u5b9e\u4f53\u611f\u77e5\u7ed3\u6784\u63a8\u65ad\u4e8c\u5206\u573a\u666f\u56fe\u7684\u8fde\u63a5\u6027\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u751f\u6210\u573a\u666f\u56fe\u3002\u57fa\u4e8e\u4e8c\u5206\u56fe\u7ec4\u88c5\u8303\u5f0f\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6280\u672f\u8bbe\u8ba1\u6765\u89e3\u51b3\u5b9e\u4f53\u611f\u77e5\u5efa\u6a21\u7684\u6709\u6548\u6027\u548c\u56fe\u7ec4\u88c5\u7684\u4f18\u5316\u7a33\u5b9a\u6027\u3002\u914d\u5907\u589e\u5f3a\u7684\u5b9e\u4f53\u611f\u77e5\u8bbe\u8ba1\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u8bbe\u8ba1\u80fd\u591f\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6216\u53ef\u6bd4\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u5e76\u4eab\u6709\u66f4\u9ad8\u7684\u63a8\u7406\u6548\u7387\u3002\u4ee3\u7801\u53ef\u7528\uff1ahttps://github.com/Scarecrow0/SGTR|[2401.12835v1](http://arxiv.org/pdf/2401.12835v1)|**[link](https://github.com/scarecrow0/sgtr)**|\n", "2401.12736": "|**2024-01-23**|**Shift-ConvNets: Small Convolutional Kernel with Large Kernel Effects**|Shift-ConvNets\uff1a\u5177\u6709\u5927\u6838\u6548\u5e94\u7684\u5c0f\u5377\u79ef\u6838|Dachong Li, Li Li, Zhuangzhuang Chen, Jianqiang Li|Recent studies reveal that the remarkable performance of Vision transformers (ViTs) benefits from large receptive fields. For this reason, the large convolutional kernel design becomes an ideal solution to make Convolutional Neural Networks (CNNs) great again. However, the typical large convolutional kernels turn out to be hardware-unfriendly operators, resulting in discount compatibility of various hardware platforms. Thus, it is unwise to simply enlarge the convolutional kernel size. In this paper, we reveal that small convolutional kernels and convolution operations can achieve the closing effects of large kernel sizes. Then, we propose a shift-wise operator that ensures the CNNs capture long-range dependencies with the help of the sparse mechanism, while remaining hardware-friendly. Experimental results show that our shift-wise operator significantly improves the accuracy of a regular CNN while markedly reducing computational requirements. On the ImageNet-1k, our shift-wise enhanced CNN model outperforms the state-of-the-art models. Code & models at https://github.com/lidc54/shift-wiseConv.|\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u89c6\u89c9\u53d8\u538b\u5668\uff08ViT\uff09\u7684\u5353\u8d8a\u6027\u80fd\u5f97\u76ca\u4e8e\u5927\u7684\u611f\u53d7\u91ce\u3002\u56e0\u6b64\uff0c\u5927\u5377\u79ef\u6838\u8bbe\u8ba1\u6210\u4e3a\u8ba9\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u518d\u6b21\u4f1f\u5927\u7684\u7406\u60f3\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5178\u578b\u7684\u5927\u5377\u79ef\u6838\u7ed3\u679c\u662f\u786c\u4ef6\u4e0d\u53cb\u597d\u7684\u7b97\u5b50\uff0c\u5bfc\u81f4\u5404\u79cd\u786c\u4ef6\u5e73\u53f0\u7684\u517c\u5bb9\u6027\u6253\u6298\u6263\u3002\u56e0\u6b64\uff0c\u7b80\u5355\u5730\u589e\u5927\u5377\u79ef\u6838\u5927\u5c0f\u662f\u4e0d\u660e\u667a\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u5c0f\u5377\u79ef\u6838\u548c\u5377\u79ef\u8fd0\u7b97\u53ef\u4ee5\u5b9e\u73b0\u5927\u6838\u5c3a\u5bf8\u7684\u95ed\u5408\u6548\u679c\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79fb\u4f4d\u8fd0\u7b97\u7b26\uff0c\u786e\u4fdd CNN \u5728\u7a00\u758f\u673a\u5236\u7684\u5e2e\u52a9\u4e0b\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u786c\u4ef6\u53cb\u597d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u79fb\u4f4d\u7b97\u5b50\u663e\u7740\u63d0\u9ad8\u4e86\u5e38\u89c4 CNN \u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u7740\u964d\u4f4e\u4e86\u8ba1\u7b97\u8981\u6c42\u3002\u5728 ImageNet-1k \u4e0a\uff0c\u6211\u4eec\u7684\u79fb\u4f4d\u589e\u5f3a CNN \u6a21\u578b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002\u4ee3\u7801\u548c\u6a21\u578b\u4f4d\u4e8e https://github.com/lidc54/shift-wiseConv\u3002|[2401.12736v1](http://arxiv.org/pdf/2401.12736v1)|**[link](https://github.com/lidc54/shift-wiseconv)**|\n", "2401.12511": "|**2024-01-23**|**Convolutional Initialization for Data-Efficient Vision Transformers**|\u6570\u636e\u9ad8\u6548\u89c6\u89c9\u8f6c\u6362\u5668\u7684\u5377\u79ef\u521d\u59cb\u5316|Jianqiao Zheng, Xueqian Li, Simon Lucey|Training vision transformer networks on small datasets poses challenges. In contrast, convolutional neural networks (CNNs) can achieve state-of-the-art performance by leveraging their architectural inductive bias. In this paper, we investigate whether this inductive bias can be reinterpreted as an initialization bias within a vision transformer network. Our approach is motivated by the finding that random impulse filters can achieve almost comparable performance to learned filters in CNNs. We introduce a novel initialization strategy for transformer networks that can achieve comparable performance to CNNs on small datasets while preserving its architectural flexibility.|\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u89c6\u89c9\u53d8\u6362\u5668\u7f51\u7edc\u5e26\u6765\u4e86\u6311\u6218\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u53ef\u4ee5\u901a\u8fc7\u5229\u7528\u5176\u67b6\u6784\u5f52\u7eb3\u504f\u5dee\u6765\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u8fd9\u79cd\u5f52\u7eb3\u504f\u5dee\u662f\u5426\u53ef\u4ee5\u91cd\u65b0\u89e3\u91ca\u4e3a\u89c6\u89c9\u53d8\u6362\u5668\u7f51\u7edc\u4e2d\u7684\u521d\u59cb\u5316\u504f\u5dee\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u52a8\u673a\u662f\u53d1\u73b0\u968f\u673a\u8109\u51b2\u6ee4\u6ce2\u5668\u53ef\u4ee5\u5b9e\u73b0\u4e0e CNN \u4e2d\u7684\u5b66\u4e60\u6ee4\u6ce2\u5668\u51e0\u4e4e\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6211\u4eec\u4e3a Transformer \u7f51\u7edc\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u521d\u59cb\u5316\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u53ef\u4ee5\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e0e CNN \u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u67b6\u6784\u7075\u6d3b\u6027\u3002|[2401.12511v1](http://arxiv.org/pdf/2401.12511v1)|**[link](https://github.com/osiriszjq/impulse_init)**|\n", "2401.12451": "|**2024-01-23**|**Methods and strategies for improving the novel view synthesis quality of neural radiation field**|\u63d0\u9ad8\u795e\u7ecf\u8f90\u5c04\u573a\u65b0\u89c6\u5408\u6210\u8d28\u91cf\u7684\u65b9\u6cd5\u4e0e\u7b56\u7565|Shun Fang, Ming Cui, Xing Feng, Yanna Lv|Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a scene from 2D images and synthesize realistic novel view images. This technology has received widespread attention from the industry and has good application prospects. In response to the problem that the rendering quality of NeRF images needs to be improved, many researchers have proposed various methods to improve the rendering quality in the past three years. The latest relevant papers are classified and reviewed, the technical principles behind quality improvement are analyzed, and the future evolution direction of quality improvement methods is discussed. This study can help researchers quickly understand the current state and evolutionary context of technology in this field, which is helpful in inspiring the development of more efficient algorithms and promoting the application of NeRF technology in related fields.|\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u6280\u672f\u53ef\u4ee5\u4ece 2D \u56fe\u50cf\u4e2d\u5b66\u4e60\u573a\u666f\u7684 3D \u9690\u5f0f\u6a21\u578b\uff0c\u5e76\u5408\u6210\u903c\u771f\u7684\u65b0\u9896\u89c6\u56fe\u56fe\u50cf\u3002\u8be5\u6280\u672f\u53d7\u5230\u4e86\u4e1a\u754c\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002\u9488\u5bf9NeRF\u56fe\u50cf\u6e32\u67d3\u8d28\u91cf\u9700\u8981\u63d0\u9ad8\u7684\u95ee\u9898\uff0c\u8fd1\u4e09\u5e74\u6765\u8bb8\u591a\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u5404\u79cd\u63d0\u9ad8\u6e32\u67d3\u8d28\u91cf\u7684\u65b9\u6cd5\u3002\u5bf9\u6700\u65b0\u76f8\u5173\u8bba\u6587\u8fdb\u884c\u5206\u7c7b\u8bc4\u8ff0\uff0c\u5206\u6790\u8d28\u91cf\u6539\u8fdb\u80cc\u540e\u7684\u6280\u672f\u539f\u7406\uff0c\u63a2\u8ba8\u8d28\u91cf\u6539\u8fdb\u65b9\u6cd5\u672a\u6765\u7684\u6f14\u8fdb\u65b9\u5411\u3002\u8fd9\u9879\u7814\u7a76\u53ef\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5feb\u901f\u4e86\u89e3\u8be5\u9886\u57df\u6280\u672f\u7684\u73b0\u72b6\u548c\u6f14\u5316\u8109\u7edc\uff0c\u6709\u52a9\u4e8e\u542f\u53d1\u66f4\u9ad8\u6548\u7b97\u6cd5\u7684\u5f00\u53d1\uff0c\u63a8\u52a8NeRF\u6280\u672f\u5728\u76f8\u5173\u9886\u57df\u7684\u5e94\u7528\u3002|[2401.12451v1](http://arxiv.org/pdf/2401.12451v1)|null|\n", "2401.12422": "|**2024-01-23**|**InverseMatrixVT3D: An Efficient Projection Matrix-Based Approach for 3D Occupancy Prediction**|InverseMatrixVT3D\uff1a\u4e00\u79cd\u57fa\u4e8e\u6295\u5f71\u77e9\u9635\u7684\u9ad8\u6548 3D \u5360\u7528\u9884\u6d4b\u65b9\u6cd5|Zhenxing Ming, Julie Stephany Berrio, Mao Shan, Stewart Worrall|This paper introduces InverseMatrixVT3D, an efficient method for transforming multi-view image features into 3D feature volumes for 3D semantic occupancy prediction. Existing methods for constructing 3D volumes often rely on depth estimation, device-specific operators, or transformer queries, which hinders the widespread adoption of 3D occupancy models. In contrast, our approach leverages two projection matrices to store the static mapping relationships and matrix multiplications to efficiently generate global Bird's Eye View (BEV) features and local 3D feature volumes. Specifically, we achieve this by performing matrix multiplications between multi-view image feature maps and two sparse projection matrices. We introduce a sparse matrix handling technique for the projection matrices to optimise GPU memory usage. Moreover, a global-local attention fusion module is proposed to integrate the global BEV features with the local 3D feature volumes to obtain the final 3D volume. We also employ a multi-scale supervision mechanism to further enhance performance. Comprehensive experiments on the nuScenes dataset demonstrate the simplicity and effectiveness of our method. The code will be made available at:https://github.com/DanielMing123/InverseMatrixVT3D|\u672c\u6587\u4ecb\u7ecd\u4e86 InverseMatrixVT3D\uff0c\u8fd9\u662f\u4e00\u79cd\u5c06\u591a\u89c6\u56fe\u56fe\u50cf\u7279\u5f81\u8f6c\u6362\u4e3a 3D \u7279\u5f81\u91cf\u4ee5\u8fdb\u884c 3D \u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u6709\u6548\u65b9\u6cd5\u3002\u6784\u5efa 3D \u4f53\u79ef\u7684\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u3001\u7279\u5b9a\u4e8e\u8bbe\u5907\u7684\u8fd0\u7b97\u7b26\u6216\u53d8\u538b\u5668\u67e5\u8be2\uff0c\u8fd9\u963b\u788d\u4e86 3D \u5360\u7528\u6a21\u578b\u7684\u5e7f\u6cdb\u91c7\u7528\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4e24\u4e2a\u6295\u5f71\u77e9\u9635\u6765\u5b58\u50a8\u9759\u6001\u6620\u5c04\u5173\u7cfb\u548c\u77e9\u9635\u4e58\u6cd5\uff0c\u4ee5\u6709\u6548\u751f\u6210\u5168\u5c40\u9e1f\u77b0\u56fe (BEV) \u7279\u5f81\u548c\u5c40\u90e8 3D \u7279\u5f81\u4f53\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u591a\u89c6\u56fe\u56fe\u50cf\u7279\u5f81\u56fe\u548c\u4e24\u4e2a\u7a00\u758f\u6295\u5f71\u77e9\u9635\u4e4b\u95f4\u6267\u884c\u77e9\u9635\u4e58\u6cd5\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u6211\u4eec\u4e3a\u6295\u5f71\u77e9\u9635\u5f15\u5165\u7a00\u758f\u77e9\u9635\u5904\u7406\u6280\u672f\uff0c\u4ee5\u4f18\u5316 GPU \u5185\u5b58\u4f7f\u7528\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5168\u5c40-\u5c40\u90e8\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff0c\u5c06\u5168\u5c40 BEV \u7279\u5f81\u4e0e\u5c40\u90e8 3D \u7279\u5f81\u4f53\u79ef\u76f8\u96c6\u6210\uff0c\u4ee5\u83b7\u5f97\u6700\u7ec8\u7684 3D \u4f53\u79ef\u3002\u6211\u4eec\u8fd8\u91c7\u7528\u591a\u5c3a\u5ea6\u76d1\u7763\u673a\u5236\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7ee9\u6548\u3002 nuScenes \u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u7b80\u5355\u6027\u548c\u6709\u6548\u6027\u3002\u4ee3\u7801\u5c06\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u63d0\u4f9b\uff1ahttps://github.com/DanielMing123/InverseMatrixVT3D|[2401.12422v1](http://arxiv.org/pdf/2401.12422v1)|null|\n"}, "Nerf": {}, "3DGS": {"2401.12900": "|**2024-01-23**|**PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Creation with 3D Gaussian Splatting**|PSAvatar\uff1a\u57fa\u4e8e\u70b9\u7684\u53ef\u53d8\u5f62\u5f62\u72b6\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7 3D \u9ad8\u65af\u6cfc\u6e85\u521b\u5efa\u5b9e\u65f6\u5934\u90e8\u5934\u50cf|Zhongyuan Zhao, Zhenyu Bao, Qing Li, Guoping Qiu, Kanglin Liu|Despite much progress, creating real-time high-fidelity head avatar is still difficult and existing methods have to trade-off between speed and quality. 3DMM based methods often fail to model non-facial structures such as eyeglasses and hairstyles, while neural implicit models suffer from deformation inflexibility and rendering inefficiency.   Although 3D Gaussian has been demonstrated to possess promising capability for geometry representation and radiance field reconstruction, applying 3D Gaussian in head avatar creation remains a major challenge since it is difficult for 3D Gaussian to model the head shape variations caused by changing poses and expressions. In this paper, we introduce PSAvatar, a novel framework for animatable head avatar creation that utilizes discrete geometric primitive to create a parametric morphable shape model and employs 3D Gaussian for fine detail representation and high fidelity rendering. The parametric morphable shape model is a Point-based Morphable Shape Model (PMSM) which uses points instead of meshes for 3D representation to achieve enhanced representation flexibility. The PMSM first converts the FLAME mesh to points by sampling on the surfaces as well as off the meshes to enable the reconstruction of not only surface-like structures but also complex geometries such as eyeglasses and hairstyles. By aligning these points with the head shape in an analysis-by-synthesis manner, the PMSM makes it possible to utilize 3D Gaussian for fine detail representation and appearance modeling, thus enabling the creation of high-fidelity avatars. We show that PSAvatar can reconstruct high-fidelity head avatars of a variety of subjects and the avatars can be animated in real-time ($\\ge$ 25 fps at a resolution of 512 x 512 )|\u5c3d\u7ba1\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u5c55\uff0c\u4f46\u521b\u5efa\u5b9e\u65f6\u9ad8\u4fdd\u771f\u5934\u90e8\u5934\u50cf\u4ecd\u7136\u5f88\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u5fc5\u987b\u5728\u901f\u5ea6\u548c\u8d28\u91cf\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u57fa\u4e8e 3DMM \u7684\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u5bf9\u773c\u955c\u548c\u53d1\u578b\u7b49\u975e\u9762\u90e8\u7ed3\u6784\u8fdb\u884c\u5efa\u6a21\uff0c\u800c\u795e\u7ecf\u9690\u5f0f\u6a21\u578b\u5219\u5b58\u5728\u53d8\u5f62\u4e0d\u7075\u6d3b\u548c\u6e32\u67d3\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002\u5c3d\u7ba13D\u9ad8\u65af\u5df2\u88ab\u8bc1\u660e\u5728\u51e0\u4f55\u8868\u793a\u548c\u8f90\u5c04\u573a\u91cd\u5efa\u65b9\u9762\u5177\u6709\u826f\u597d\u7684\u80fd\u529b\uff0c\u4f46\u5c063D\u9ad8\u65af\u5e94\u7528\u4e8e\u5934\u90e8\u5934\u50cf\u521b\u5efa\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a3D\u9ad8\u65af\u5f88\u96be\u5bf9\u56e0\u59ff\u52bf\u548c\u8868\u60c5\u53d8\u5316\u800c\u5f15\u8d77\u7684\u5934\u90e8\u5f62\u72b6\u53d8\u5316\u8fdb\u884c\u5efa\u6a21\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 PSAvatar\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u521b\u5efa\u52a8\u753b\u5934\u90e8\u5934\u50cf\u7684\u65b0\u9896\u6846\u67b6\uff0c\u5b83\u5229\u7528\u79bb\u6563\u51e0\u4f55\u57fa\u5143\u521b\u5efa\u53c2\u6570\u5316\u53ef\u53d8\u5f62\u5f62\u72b6\u6a21\u578b\uff0c\u5e76\u91c7\u7528 3D \u9ad8\u65af\u8fdb\u884c\u7cbe\u7ec6\u7ec6\u8282\u8868\u793a\u548c\u9ad8\u4fdd\u771f\u5ea6\u6e32\u67d3\u3002\u53c2\u6570\u5316\u53ef\u53d8\u5f62\u5f62\u72b6\u6a21\u578b\u662f\u57fa\u4e8e\u70b9\u7684\u53ef\u53d8\u5f62\u5f62\u72b6\u6a21\u578b\uff08PMSM\uff09\uff0c\u5b83\u4f7f\u7528\u70b9\u800c\u4e0d\u662f\u7f51\u683c\u8fdb\u884c 3D \u8868\u793a\uff0c\u4ee5\u5b9e\u73b0\u589e\u5f3a\u7684\u8868\u793a\u7075\u6d3b\u6027\u3002 PMSM \u9996\u5148\u901a\u8fc7\u5728\u8868\u9762\u548c\u7f51\u683c\u5916\u8fdb\u884c\u91c7\u6837\uff0c\u5c06 FLAME \u7f51\u683c\u8f6c\u6362\u4e3a\u70b9\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u91cd\u5efa\u8868\u9762\u7ed3\u6784\uff0c\u8fd8\u53ef\u4ee5\u91cd\u5efa\u590d\u6742\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u4f8b\u5982\u773c\u955c\u548c\u53d1\u578b\u3002\u901a\u8fc7\u4ee5\u7efc\u5408\u5206\u6790\u7684\u65b9\u5f0f\u5c06\u8fd9\u4e9b\u70b9\u4e0e\u5934\u90e8\u5f62\u72b6\u5bf9\u9f50\uff0cPMSM \u4f7f\u5f97\u5229\u7528 3D \u9ad8\u65af\u8fdb\u884c\u7cbe\u7ec6\u7ec6\u8282\u8868\u793a\u548c\u5916\u89c2\u5efa\u6a21\u6210\u4e3a\u53ef\u80fd\uff0c\u4ece\u800c\u80fd\u591f\u521b\u5efa\u9ad8\u4fdd\u771f\u5316\u8eab\u3002\u6211\u4eec\u8bc1\u660e PSAvatar \u53ef\u4ee5\u91cd\u5efa\u5404\u79cd\u4e3b\u4f53\u7684\u9ad8\u4fdd\u771f\u5934\u90e8\u5934\u50cf\uff0c\u5e76\u4e14\u5934\u50cf\u53ef\u4ee5\u5b9e\u65f6\u52a8\u753b\uff08$\\ge$ 25 fps\uff0c\u5206\u8fa8\u7387\u4e3a 512 x 512 \uff09|[2401.12900v1](http://arxiv.org/pdf/2401.12900v1)|null|\n", "2401.12561": "|**2024-01-23**|**EndoGaussian: Gaussian Splatting for Deformable Surgical Scene Reconstruction**|EndoGaussian\uff1a\u7528\u4e8e\u53ef\u53d8\u5f62\u624b\u672f\u573a\u666f\u91cd\u5efa\u7684\u9ad8\u65af\u55b7\u5c04|Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan|Reconstructing deformable tissues from endoscopic stereo videos is essential in many downstream surgical applications. However, existing methods suffer from slow inference speed, which greatly limits their practical use. In this paper, we introduce EndoGaussian, a real-time surgical scene reconstruction framework that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical scenes as canonical Gaussians and a time-dependent deformation field, which predicts Gaussian deformations at novel timestamps. Due to the efficient Gaussian representation and parallel rendering pipeline, our framework significantly accelerates the rendering speed compared to previous methods. In addition, we design the deformation field as the combination of a lightweight encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian tracking with a minor rendering burden. Furthermore, we design a holistic Gaussian initialization method to fully leverage the surface distribution prior, achieved by searching informative points from across the input image sequence. Experiments on public endoscope datasets demonstrate that our method can achieve real-time rendering speed (195 FPS real-time, 100$\\times$ gain) while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and the fastest training speed (within 2 min/scene), showing significant promise for intraoperative surgery applications. Code is available at: \\url{https://yifliu3.github.io/EndoGaussian/}.|\u4ece\u5185\u7aa5\u955c\u7acb\u4f53\u89c6\u9891\u91cd\u5efa\u53ef\u53d8\u5f62\u7ec4\u7ec7\u5728\u8bb8\u591a\u4e0b\u6e38\u624b\u672f\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u8fd9\u6781\u5927\u5730\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u4f7f\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 EndoGaussian\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e 3D \u9ad8\u65af Splatting \u6784\u5efa\u7684\u5b9e\u65f6\u624b\u672f\u573a\u666f\u91cd\u5efa\u6846\u67b6\u3002\u6211\u4eec\u7684\u6846\u67b6\u5c06\u52a8\u6001\u624b\u672f\u573a\u666f\u8868\u793a\u4e3a\u89c4\u8303\u9ad8\u65af\u548c\u4e0e\u65f6\u95f4\u76f8\u5173\u7684\u53d8\u5f62\u573a\uff0c\u8be5\u53d8\u5f62\u573a\u9884\u6d4b\u65b0\u65f6\u95f4\u6233\u4e0b\u7684\u9ad8\u65af\u53d8\u5f62\u3002\u7531\u4e8e\u9ad8\u6548\u7684\u9ad8\u65af\u8868\u793a\u548c\u5e76\u884c\u6e32\u67d3\u7ba1\u9053\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\u663e\u7740\u52a0\u5feb\u4e86\u6e32\u67d3\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u53d8\u5f62\u573a\u8bbe\u8ba1\u4e3a\u8f7b\u91cf\u7ea7\u7f16\u7801\u4f53\u7d20\u548c\u6781\u5c0f\u7684 MLP \u7684\u7ec4\u5408\uff0c\u4ece\u800c\u53ef\u4ee5\u5728\u8f83\u5c0f\u7684\u6e32\u67d3\u8d1f\u62c5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u9ad8\u65af\u8ddf\u8e2a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6574\u4f53\u9ad8\u65af\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u4ee5\u5145\u5206\u5229\u7528\u8868\u9762\u5206\u5e03\u5148\u9a8c\uff0c\u8fd9\u662f\u901a\u8fc7\u4ece\u8f93\u5165\u56fe\u50cf\u5e8f\u5217\u4e2d\u641c\u7d22\u4fe1\u606f\u70b9\u6765\u5b9e\u73b0\u7684\u3002\u5728\u516c\u5171\u5185\u7aa5\u955c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\uff08195 FPS \u5b9e\u65f6\uff0c100$\\times$ \u589e\u76ca\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\uff0835.925 PSNR\uff09\u548c\u6700\u5feb\u7684\u8bad\u7ec3\u901f\u5ea6\uff082 \u5206\u949f\u5185/\u573a\u666f\uff09\uff0c\u5728\u672f\u4e2d\u624b\u672f\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u7684\u524d\u666f\u3002\u4ee3\u7801\u4f4d\u4e8e\uff1a\\url{https://yifliu3.github.io/EndoGaussian/}\u3002|[2401.12561v1](http://arxiv.org/pdf/2401.12561v1)|null|\n"}, "3D/CG": {"2401.12977": "|**2024-01-23**|**IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images**|IRIS\uff1a\u4f4e\u52a8\u6001\u8303\u56f4\u56fe\u50cf\u7684\u5ba4\u5185\u573a\u666f\u9006\u6e32\u67d3|Zhi-Hao Lin, Jia-Bin Huang, Zhengqin Li, Zhao Dong, Christian Richardt, Tuotuo Li, Michael Zollh\u00f6fer, Johannes Kopf, Shenlong Wang, Changil Kim|While numerous 3D reconstruction and novel-view synthesis methods allow for photorealistic rendering of a scene from multi-view images easily captured with consumer cameras, they bake illumination in their representations and fall short of supporting advanced applications like material editing, relighting, and virtual object insertion. The reconstruction of physically based material properties and lighting via inverse rendering promises to enable such applications.   However, most inverse rendering techniques require high dynamic range (HDR) images as input, a setting that is inaccessible to most users. We present a method that recovers the physically based material properties and spatially-varying HDR lighting of a scene from multi-view, low-dynamic-range (LDR) images. We model the LDR image formation process in our inverse rendering pipeline and propose a novel optimization strategy for material, lighting, and a camera response model. We evaluate our approach with synthetic and real scenes compared to the state-of-the-art inverse rendering methods that take either LDR or HDR input. Our method outperforms existing methods taking LDR images as input, and allows for highly realistic relighting and object insertion.|\u867d\u7136\u8bb8\u591a 3D \u91cd\u5efa\u548c\u65b0\u9896\u89c6\u56fe\u5408\u6210\u65b9\u6cd5\u5141\u8bb8\u4ece\u4f7f\u7528\u6d88\u8d39\u7ea7\u76f8\u673a\u8f7b\u677e\u6355\u83b7\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u4e2d\u5bf9\u573a\u666f\u8fdb\u884c\u771f\u5b9e\u611f\u6e32\u67d3\uff0c\u4f46\u5b83\u4eec\u5728\u8868\u793a\u4e2d\u70d8\u7119\u7167\u660e\uff0c\u5e76\u4e14\u65e0\u6cd5\u652f\u6301\u6750\u8d28\u7f16\u8f91\u3001\u91cd\u65b0\u7167\u660e\u548c\u865a\u62df\u5bf9\u8c61\u7b49\u9ad8\u7ea7\u5e94\u7528\u7a0b\u5e8f\u63d2\u5165\u3002\u901a\u8fc7\u9006\u5411\u6e32\u67d3\u91cd\u5efa\u57fa\u4e8e\u7269\u7406\u7684\u6750\u6599\u5c5e\u6027\u548c\u7167\u660e\u6709\u671b\u5b9e\u73b0\u6b64\u7c7b\u5e94\u7528\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u9006\u6e32\u67d3\u6280\u672f\u9700\u8981\u9ad8\u52a8\u6001\u8303\u56f4 (HDR) \u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u8fd9\u662f\u5927\u591a\u6570\u7528\u6237\u65e0\u6cd5\u8bbf\u95ee\u7684\u8bbe\u7f6e\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4ece\u591a\u89c6\u56fe\u3001\u4f4e\u52a8\u6001\u8303\u56f4 (LDR) \u56fe\u50cf\u4e2d\u6062\u590d\u573a\u666f\u7684\u57fa\u4e8e\u7269\u7406\u7684\u6750\u8d28\u5c5e\u6027\u548c\u7a7a\u95f4\u53d8\u5316\u7684 HDR \u7167\u660e\u3002\u6211\u4eec\u5728\u9006\u6e32\u67d3\u7ba1\u9053\u4e2d\u5bf9 LDR \u56fe\u50cf\u5f62\u6210\u8fc7\u7a0b\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u9488\u5bf9\u6750\u8d28\u3001\u7167\u660e\u548c\u76f8\u673a\u54cd\u5e94\u6a21\u578b\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f18\u5316\u7b56\u7565\u3002\u6211\u4eec\u4f7f\u7528\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e0e\u91c7\u7528 LDR \u6216 HDR \u8f93\u5165\u7684\u6700\u5148\u8fdb\u7684\u9006\u6e32\u67d3\u65b9\u6cd5\u6765\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u4ee5 LDR \u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5141\u8bb8\u9ad8\u5ea6\u903c\u771f\u7684\u91cd\u65b0\u7167\u660e\u548c\u5bf9\u8c61\u63d2\u5165\u3002|[2401.12977v1](http://arxiv.org/pdf/2401.12977v1)|null|\n", "2401.12946": "|**2024-01-23**|**Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization**|Coverage Axis++\uff1a3D \u5f62\u72b6\u9aa8\u67b6\u5316\u7684\u9ad8\u6548\u5185\u70b9\u9009\u62e9|Zimeng Wang, Zhiyang Dou, Rui Xu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Shiqing Xin, Lingjie Liu, Taku Komura, Xiaoming Yuan, et.al.|We introduce Coverage Axis++, a novel and efficient approach to 3D shape skeletonization. The current state-of-the-art approaches for this task often rely on the watertightness of the input or suffer from substantial computational costs, thereby limiting their practicality. To address this challenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal points, offering a high-accuracy approximation of the Medial Axis Transform (MAT) while significantly mitigating computational intensity for various shape representations. We introduce a simple yet effective strategy that considers both shape coverage and uniformity to derive skeletal points. The selection procedure enforces consistency with the shape structure while favoring the dominant medial balls, which thus introduces a compact underlying shape representation in terms of MAT. As a result, Coverage Axis++ allows for skeletonization for various shape representations (e.g., water-tight meshes, triangle soups, point clouds), specification of the number of skeletal points, few hyperparameters, and highly efficient computation with improved reconstruction accuracy. Extensive experiments across a wide range of 3D shapes validate the efficiency and effectiveness of Coverage Axis++. The code will be publicly available once the paper is published.|\u6211\u4eec\u63a8\u51fa\u4e86 Coverage Axis++\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684 3D \u5f62\u72b6\u9aa8\u67b6\u5316\u65b9\u6cd5\u3002\u5f53\u524d\u7528\u4e8e\u6b64\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u8f93\u5165\u7684\u9632\u6c34\u6027\u6216\u906d\u53d7\u5927\u91cf\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u7528\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0cCoverage Axis++ \u63d0\u51fa\u4e86\u4e00\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u6765\u9009\u62e9\u9aa8\u67b6\u70b9\uff0c\u63d0\u4f9b\u4e2d\u8f74\u53d8\u6362 (MAT) \u7684\u9ad8\u7cbe\u5ea6\u8fd1\u4f3c\uff0c\u540c\u65f6\u663e\u7740\u51cf\u8f7b\u5404\u79cd\u5f62\u72b6\u8868\u793a\u7684\u8ba1\u7b97\u5f3a\u5ea6\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u8003\u8651\u5f62\u72b6\u8986\u76d6\u8303\u56f4\u548c\u5747\u5300\u6027\u6765\u5bfc\u51fa\u9aa8\u67b6\u70b9\u3002\u9009\u62e9\u8fc7\u7a0b\u5f3a\u5236\u4e0e\u5f62\u72b6\u7ed3\u6784\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u6709\u5229\u4e8e\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u5185\u4fa7\u7403\uff0c\u4ece\u800c\u5728 MAT \u65b9\u9762\u5f15\u5165\u4e86\u7d27\u51d1\u7684\u57fa\u7840\u5f62\u72b6\u8868\u793a\u3002\u56e0\u6b64\uff0cCoverage Axis++ \u5141\u8bb8\u5bf9\u5404\u79cd\u5f62\u72b6\u8868\u793a\uff08\u4f8b\u5982\uff0c\u6c34\u5bc6\u7f51\u683c\u3001\u4e09\u89d2\u6c64\u3001\u70b9\u4e91\uff09\u8fdb\u884c\u9aa8\u67b6\u5316\uff0c\u6307\u5b9a\u9aa8\u67b6\u70b9\u7684\u6570\u91cf\uff0c\u5f88\u5c11\u7684\u8d85\u53c2\u6570\uff0c\u4ee5\u53ca\u63d0\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u7684\u9ad8\u6548\u8ba1\u7b97\u3002\u9488\u5bf9\u5404\u79cd 3D \u5f62\u72b6\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 Coverage Axis++ \u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002\u8bba\u6587\u53d1\u8868\u540e\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002|[2401.12946v1](http://arxiv.org/pdf/2401.12946v1)|null|\n", "2401.12751": "|**2024-01-23**|**PSDF: Prior-Driven Neural Implicit Surface Learning for Multi-view Reconstruction**|PSDF\uff1a\u7528\u4e8e\u591a\u89c6\u56fe\u91cd\u5efa\u7684\u5148\u9a8c\u9a71\u52a8\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u5b66\u4e60|Wanjuan Su, Chen Zhang, Qingshan Xu, Wenbing Tao|Surface reconstruction has traditionally relied on the Multi-View Stereo (MVS)-based pipeline, which often suffers from noisy and incomplete geometry. This is due to that although MVS has been proven to be an effective way to recover the geometry of the scenes, especially for locally detailed areas with rich textures, it struggles to deal with areas with low texture and large variations of illumination where the photometric consistency is unreliable. Recently, Neural Implicit Surface Reconstruction (NISR) combines surface rendering and volume rendering techniques and bypasses the MVS as an intermediate step, which has emerged as a promising alternative to overcome the limitations of traditional pipelines. While NISR has shown impressive results on simple scenes, it remains challenging to recover delicate geometry from uncontrolled real-world scenes which is caused by its underconstrained optimization. To this end, the framework PSDF is proposed which resorts to external geometric priors from a pretrained MVS network and internal geometric priors inherent in the NISR model to facilitate high-quality neural implicit surface learning. Specifically, the visibility-aware feature consistency loss and depth prior-assisted sampling based on external geometric priors are introduced. These proposals provide powerfully geometric consistency constraints and aid in locating surface intersection points, thereby significantly improving the accuracy and delicate reconstruction of NISR. Meanwhile, the internal prior-guided importance rendering is presented to enhance the fidelity of the reconstructed surface mesh by mitigating the biased rendering issue in NISR. Extensive experiments on the Tanks and Temples dataset show that PSDF achieves state-of-the-art performance on complex uncontrolled scenes.|\u8868\u9762\u91cd\u5efa\u4f20\u7edf\u4e0a\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u591a\u89c6\u56fe\u7acb\u4f53 (MVS) \u7684\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u7ecf\u5e38\u53d7\u5230\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u51e0\u4f55\u4f53\u7684\u5f71\u54cd\u3002\u8fd9\u662f\u56e0\u4e3a\uff0c\u867d\u7136 MVS \u5df2\u88ab\u8bc1\u660e\u662f\u6062\u590d\u573a\u666f\u51e0\u4f55\u5f62\u72b6\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7eb9\u7406\u4e30\u5bcc\u7684\u5c40\u90e8\u7ec6\u8282\u533a\u57df\uff0c\u4f46\u5b83\u5f88\u96be\u5904\u7406\u7eb9\u7406\u4f4e\u4e14\u5149\u7167\u53d8\u5316\u5927\u7684\u533a\u57df\uff0c\u8fd9\u4e9b\u533a\u57df\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u662f\u4e0d\u53ef\u9760\u7684\u3002\u6700\u8fd1\uff0c\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u91cd\u5efa\uff08NISR\uff09\u7ed3\u5408\u4e86\u8868\u9762\u6e32\u67d3\u548c\u4f53\u79ef\u6e32\u67d3\u6280\u672f\uff0c\u5e76\u7ed5\u8fc7 MVS \u4f5c\u4e3a\u4e2d\u95f4\u6b65\u9aa4\uff0c\u8fd9\u5df2\u6210\u4e3a\u514b\u670d\u4f20\u7edf\u7ba1\u9053\u9650\u5236\u7684\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u867d\u7136 NISR \u5728\u7b80\u5355\u573a\u666f\u4e0a\u663e\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u4ece\u4e0d\u53d7\u63a7\u5236\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u6062\u590d\u7cbe\u81f4\u7684\u51e0\u4f55\u56fe\u5f62\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u8fd9\u662f\u7531\u4e8e\u5176\u4f18\u5316\u4e0d\u8db3\u9020\u6210\u7684\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86 PSDF \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3\u7684 MVS \u7f51\u7edc\u7684\u5916\u90e8\u51e0\u4f55\u5148\u9a8c\u548c NISR \u6a21\u578b\u56fa\u6709\u7684\u5185\u90e8\u51e0\u4f55\u5148\u9a8c\u6765\u4fc3\u8fdb\u9ad8\u8d28\u91cf\u7684\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u5b66\u4e60\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5f15\u5165\u4e86\u53ef\u89c1\u6027\u611f\u77e5\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\u548c\u57fa\u4e8e\u5916\u90e8\u51e0\u4f55\u5148\u9a8c\u7684\u6df1\u5ea6\u5148\u9a8c\u8f85\u52a9\u91c7\u6837\u3002\u8fd9\u4e9b\u5efa\u8bae\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u7ea6\u675f\u5e76\u6709\u52a9\u4e8e\u5b9a\u4f4d\u8868\u9762\u4ea4\u70b9\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8 NISR \u7684\u51c6\u786e\u6027\u548c\u7cbe\u7ec6\u91cd\u5efa\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u5185\u90e8\u5148\u9a8c\u5f15\u5bfc\u7684\u91cd\u8981\u6027\u6e32\u67d3\uff0c\u901a\u8fc7\u51cf\u8f7b NISR \u4e2d\u7684\u504f\u5dee\u6e32\u67d3\u95ee\u9898\u6765\u589e\u5f3a\u91cd\u5efa\u8868\u9762\u7f51\u683c\u7684\u4fdd\u771f\u5ea6\u3002\u5bf9 Tanks \u548c Temples \u6570\u636e\u96c6\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPSDF \u5728\u590d\u6742\u7684\u4e0d\u53d7\u63a7\u5236\u7684\u573a\u666f\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2401.12751v1](http://arxiv.org/pdf/2401.12751v1)|null|\n", "2401.12592": "|**2024-01-23**|**RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos**|\u91ce\u5916 RGBD \u5bf9\u8c61\uff1a\u901a\u8fc7 RGB-D \u89c6\u9891\u7f29\u653e\u771f\u5b9e\u4e16\u754c 3D \u5bf9\u8c61\u5b66\u4e60|Hongchi Xia, Yang Fu, Sifei Liu, Xiaolong Wang|We introduce a new RGB-D object dataset captured in the wild called WildRGB-D. Unlike most existing real-world object-centric datasets which only come with RGB capturing, the direct capture of the depth channel allows better 3D annotations and broader downstream applications. WildRGB-D comprises large-scale category-level RGB-D object videos, which are taken using an iPhone to go around the objects in 360 degrees. It contains around 8500 recorded objects and nearly 20000 RGB-D videos across 46 common object categories. These videos are taken with diverse cluttered backgrounds with three setups to cover as many real-world scenarios as possible: (i) a single object in one video; (ii) multiple objects in one video; and (iii) an object with a static hand in one video. The dataset is annotated with object masks, real-world scale camera poses, and reconstructed aggregated point clouds from RGBD videos. We benchmark four tasks with WildRGB-D including novel view synthesis, camera pose estimation, object 6d pose estimation, and object surface reconstruction. Our experiments show that the large-scale capture of RGB-D objects provides a large potential to advance 3D object learning. Our project page is https://wildrgbd.github.io/.|\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5728\u91ce\u5916\u6355\u83b7\u7684\u65b0 RGB-D \u5bf9\u8c61\u6570\u636e\u96c6\uff0c\u79f0\u4e3a WildRGB-D\u3002\u4e0e\u5927\u591a\u6570\u73b0\u6709\u7684\u4ec5\u652f\u6301 RGB \u6355\u83b7\u7684\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\u4e0d\u540c\uff0c\u6df1\u5ea6\u901a\u9053\u7684\u76f4\u63a5\u6355\u83b7\u53ef\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684 3D \u6ce8\u91ca\u548c\u66f4\u5e7f\u6cdb\u7684\u4e0b\u6e38\u5e94\u7528\u3002 WildRGB-D \u5305\u542b\u5927\u89c4\u6a21\u7c7b\u522b\u7ea7 RGB-D \u5bf9\u8c61\u89c6\u9891\uff0c\u8fd9\u4e9b\u89c6\u9891\u662f\u4f7f\u7528 iPhone 360\u200b\u200b \u5ea6\u56f4\u7ed5\u5bf9\u8c61\u62cd\u6444\u7684\u3002\u5b83\u5305\u542b\u7ea6 8500 \u4e2a\u8bb0\u5f55\u7684\u5bf9\u8c61\u548c\u8fd1 20000 \u4e2a RGB-D \u89c6\u9891\uff0c\u6d89\u53ca 46 \u4e2a\u5e38\u89c1\u5bf9\u8c61\u7c7b\u522b\u3002\u8fd9\u4e9b\u89c6\u9891\u662f\u5728\u4e0d\u540c\u7684\u6742\u4e71\u80cc\u666f\u4e0b\u62cd\u6444\u7684\uff0c\u5e76\u91c7\u7528\u4e09\u79cd\u8bbe\u7f6e\u6765\u8986\u76d6\u5c3d\u53ef\u80fd\u591a\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\uff1a(i) \u4e00\u4e2a\u89c6\u9891\u4e2d\u7684\u5355\u4e2a\u5bf9\u8c61\uff1b (ii) \u4e00\u4e2a\u89c6\u9891\u4e2d\u6709\u591a\u4e2a\u5bf9\u8c61\uff1b (iii) \u4e00\u6bb5\u89c6\u9891\u4e2d\u5b58\u5728\u4e00\u53ea\u624b\u5904\u4e8e\u9759\u6b62\u72b6\u6001\u7684\u7269\u4f53\u3002\u8be5\u6570\u636e\u96c6\u4f7f\u7528\u5bf9\u8c61\u8499\u7248\u3001\u771f\u5b9e\u4e16\u754c\u6bd4\u4f8b\u76f8\u673a\u59ff\u52bf\u4ee5\u53ca\u4ece RGBD \u89c6\u9891\u91cd\u5efa\u7684\u805a\u5408\u70b9\u4e91\u8fdb\u884c\u6ce8\u91ca\u3002\u6211\u4eec\u4f7f\u7528 WildRGB-D \u5bf9\u56db\u4e2a\u4efb\u52a1\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u3001\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3001\u7269\u4f53 6d \u59ff\u6001\u4f30\u8ba1\u548c\u7269\u4f53\u8868\u9762\u91cd\u5efa\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u89c4\u6a21\u6355\u83b7 RGB-D \u5bf9\u8c61\u4e3a\u63a8\u8fdb 3D \u5bf9\u8c61\u5b66\u4e60\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u6f5c\u529b\u3002\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\u662f https://wildrgbd.github.io/\u3002|[2401.12592v1](http://arxiv.org/pdf/2401.12592v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2401.12648": "|**2024-01-23**|**Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning**|\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u57fa\u4e8e\u4e00\u81f4\u6027\u589e\u5f3a\u7684\u6df1\u5ea6\u591a\u89c6\u56fe\u805a\u7c7b|Hao Yang, Hua Mao, Wai Lok Woo, Jie Chen, Xi Peng|Multiview clustering (MVC) segregates data samples into meaningful clusters by synthesizing information across multiple views. Moreover, deep learning-based methods have demonstrated their strong feature learning capabilities in MVC scenarios. However, effectively generalizing feature representations while maintaining consistency is still an intractable problem. In addition, most existing deep clustering methods based on contrastive learning overlook the consistency of the clustering representations during the clustering process. In this paper, we show how the above problems can be overcome and propose a consistent enhancement-based deep MVC method via contrastive learning (CCEC). Specifically, semantic connection blocks are incorporated into a feature representation to preserve the consistent information among multiple views. Furthermore, the representation process for clustering is enhanced through spectral clustering, and the consistency across multiple views is improved. Experiments conducted on five datasets demonstrate the effectiveness and superiority of our method in comparison with the state-of-the-art (SOTA) methods. The code for this method can be accessed at https://anonymous.4open.science/r/CCEC-E84E/.|\u591a\u89c6\u56fe\u805a\u7c7b (MVC) \u901a\u8fc7\u7efc\u5408\u591a\u4e2a\u89c6\u56fe\u7684\u4fe1\u606f\uff0c\u5c06\u6570\u636e\u6837\u672c\u5206\u6210\u6709\u610f\u4e49\u7684\u7c07\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728MVC\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u7279\u5f81\u5b66\u4e60\u80fd\u529b\u3002\u7136\u800c\uff0c\u5728\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u540c\u65f6\u6709\u6548\u5730\u6982\u62ec\u7279\u5f81\u8868\u793a\u4ecd\u7136\u662f\u4e00\u4e2a\u68d8\u624b\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u5927\u591a\u6570\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\u90fd\u5ffd\u7565\u4e86\u805a\u7c7b\u8fc7\u7a0b\u4e2d\u805a\u7c7b\u8868\u793a\u7684\u4e00\u81f4\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u514b\u670d\u4e0a\u8ff0\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\uff08CCEC\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e00\u81f4\u589e\u5f3a\u7684\u6df1\u5ea6 MVC \u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8bed\u4e49\u8fde\u63a5\u5757\u88ab\u5408\u5e76\u5230\u7279\u5f81\u8868\u793a\u4e2d\u4ee5\u4fdd\u7559\u591a\u4e2a\u89c6\u56fe\u4e4b\u95f4\u7684\u4e00\u81f4\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8c31\u805a\u7c7b\u589e\u5f3a\u4e86\u805a\u7c7b\u7684\u8868\u793a\u8fc7\u7a0b\uff0c\u5e76\u4e14\u63d0\u9ad8\u4e86\u591a\u4e2a\u89c6\u56fe\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u65b9\u6cd5\u76f8\u6bd4\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002\u6b64\u65b9\u6cd5\u7684\u4ee3\u7801\u53ef\u4ee5\u8bbf\u95ee https://anonymous.4open.science/r/CCEC-E84E/\u3002|[2401.12648v1](http://arxiv.org/pdf/2401.12648v1)|null|\n", "2401.12609": "|**2024-01-23**|**Fast Semi-supervised Unmixing using Non-convex Optimization**|\u4f7f\u7528\u975e\u51f8\u4f18\u5316\u7684\u5feb\u901f\u534a\u76d1\u7763\u5206\u89e3|Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot|In this paper, we introduce a novel linear model tailored for semisupervised/library-based unmixing. Our model incorporates considerations for library mismatch while enabling the enforcement of the abundance sum-to-one constraint (ASC). Unlike conventional sparse unmixing methods, this model involves nonconvex optimization, presenting significant computational challenges. We demonstrate the efficacy of Alternating Methods of Multipliers (ADMM) in cyclically solving these intricate problems. We propose two semisupervised unmixing approaches, each relying on distinct priors applied to the new model in addition to the ASC: sparsity prior and convexity constraint. Our experimental results validate that enforcing the convexity constraint outperforms the sparsity prior for the endmember library. These results are corroborated across three simulated datasets (accounting for spectral variability and varying pixel purity levels) and the Cuprite dataset. Additionally, our comparison with conventional sparse unmixing methods showcases considerable advantages of our proposed model, which entails nonconvex optimization. Notably, our implementations of the proposed algorithms-fast semisupervised unmixing (FaSUn) and sparse unmixing using soft-shrinkage (SUnS)-prove considerably more efficient than traditional sparse unmixing methods. SUnS and FaSUn were implemented using PyTorch and provided in a dedicated Python package called Fast Semisupervised Unmixing (FUnmix), which is open-source and available at https://github.com/BehnoodRasti/FUnmix|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e13\u4e3a\u534a\u76d1\u7763/\u57fa\u4e8e\u5e93\u7684\u89e3\u6df7\u5408\u800c\u5b9a\u5236\u7684\u65b0\u578b\u7ebf\u6027\u6a21\u578b\u3002\u6211\u4eec\u7684\u6a21\u578b\u8003\u8651\u4e86\u6587\u5e93\u4e0d\u5339\u914d\uff0c\u540c\u65f6\u542f\u7528\u4e30\u5ea6\u548c\u5bf9\u4e00\u7ea6\u675f\uff08ASC\uff09\u3002\u4e0e\u4f20\u7edf\u7684\u7a00\u758f\u5206\u89e3\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u6a21\u578b\u6d89\u53ca\u975e\u51f8\u4f18\u5316\uff0c\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6311\u6218\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u4ea4\u66ff\u4e58\u5b50\u6cd5 (ADMM) \u5728\u5faa\u73af\u89e3\u51b3\u8fd9\u4e9b\u590d\u6742\u95ee\u9898\u65b9\u9762\u7684\u529f\u6548\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u534a\u76d1\u7763\u5206\u89e3\u65b9\u6cd5\uff0c\u9664\u4e86 ASC \u4e4b\u5916\uff0c\u6bcf\u79cd\u65b9\u6cd5\u8fd8\u4f9d\u8d56\u4e8e\u5e94\u7528\u4e8e\u65b0\u6a21\u578b\u7684\u4e0d\u540c\u5148\u9a8c\uff1a\u7a00\u758f\u5148\u9a8c\u548c\u51f8\u6027\u7ea6\u675f\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5bf9\u4e8e\u7aef\u5143\u5e93\u6765\u8bf4\uff0c\u5f3a\u5236\u51f8\u6027\u7ea6\u675f\u4f18\u4e8e\u7a00\u758f\u5148\u9a8c\u3002\u8fd9\u4e9b\u7ed3\u679c\u5728\u4e09\u4e2a\u6a21\u62df\u6570\u636e\u96c6\uff08\u8003\u8651\u5149\u8c31\u53d8\u5f02\u6027\u548c\u4e0d\u540c\u7684\u50cf\u7d20\u7eaf\u5ea6\u6c34\u5e73\uff09\u548c Cuprite \u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u8bc1\u5b9e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e0e\u4f20\u7edf\u7684\u7a00\u758f\u5206\u89e3\u65b9\u6cd5\u7684\u6bd4\u8f83\u5c55\u793a\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u7684\u76f8\u5f53\u5927\u7684\u4f18\u52bf\uff0c\u8fd9\u9700\u8981\u975e\u51f8\u4f18\u5316\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u7684\u5b9e\u73b0\u2014\u2014\u5feb\u901f\u534a\u76d1\u7763\u89e3\u6df7\u5408\uff08FaSUn\uff09\u548c\u4f7f\u7528\u8f6f\u6536\u7f29\u7684\u7a00\u758f\u89e3\u6df7\u5408\uff08SUnS\uff09\u2014\u2014\u88ab\u8bc1\u660e\u6bd4\u4f20\u7edf\u7684\u7a00\u758f\u89e3\u6df7\u5408\u65b9\u6cd5\u66f4\u6709\u6548\u3002 SUnS \u548c FaSUn \u662f\u4f7f\u7528 PyTorch \u5b9e\u73b0\u7684\uff0c\u5e76\u5728\u540d\u4e3a Fast Semisupervised Unmixing (FUnmix) \u7684\u4e13\u7528 Python \u5305\u4e2d\u63d0\u4f9b\uff0c\u8be5\u5305\u662f\u5f00\u6e90\u7684\uff0c\u53ef\u5728 https://github.com/BehnoodRasti/FUnmix \u4e0a\u83b7\u53d6|[2401.12609v1](http://arxiv.org/pdf/2401.12609v1)|null|\n", "2401.12421": "|**2024-01-23**|**AdaEmbed: Semi-supervised Domain Adaptation in the Embedding Space**|AdaEmbed\uff1a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u534a\u76d1\u7763\u57df\u9002\u5e94|Ali Mottaghi, Mohammad Abdullah Jamal, Serena Yeung, Omid Mohareri|Semi-supervised domain adaptation (SSDA) presents a critical hurdle in computer vision, especially given the frequent scarcity of labeled data in real-world settings. This scarcity often causes foundation models, trained on extensive datasets, to underperform when applied to new domains. AdaEmbed, our newly proposed methodology for SSDA, offers a promising solution to these challenges. Leveraging the potential of unlabeled data, AdaEmbed facilitates the transfer of knowledge from a labeled source domain to an unlabeled target domain by learning a shared embedding space. By generating accurate and uniform pseudo-labels based on the established embedding space, the model overcomes the limitations of conventional SSDA, thus enhancing performance significantly. Our method's effectiveness is validated through extensive experiments on benchmark datasets such as DomainNet, Office-Home, and VisDA-C, where AdaEmbed consistently outperforms all the baselines, setting a new state of the art for SSDA. With its straightforward implementation and high data efficiency, AdaEmbed stands out as a robust and pragmatic solution for real-world scenarios, where labeled data is scarce. To foster further research and application in this area, we are sharing the codebase of our unified framework for semi-supervised domain adaptation.|\u534a\u76d1\u7763\u57df\u9002\u5e94\uff08SSDA\uff09\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u969c\u788d\uff0c\u7279\u522b\u662f\u8003\u8651\u5230\u73b0\u5b9e\u4e16\u754c\u4e2d\u6807\u8bb0\u6570\u636e\u7ecf\u5e38\u7a00\u7f3a\u7684\u60c5\u51b5\u3002\u8fd9\u79cd\u7a00\u7f3a\u6027\u901a\u5e38\u4f1a\u5bfc\u81f4\u5728\u5e7f\u6cdb\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u5728\u5e94\u7528\u4e8e\u65b0\u9886\u57df\u65f6\u8868\u73b0\u4e0d\u4f73\u3002 AdaEmbed \u662f\u6211\u4eec\u65b0\u63d0\u51fa\u7684 SSDA \u65b9\u6cd5\uff0c\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u7684\u6f5c\u529b\uff0cAdaEmbed \u901a\u8fc7\u5b66\u4e60\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\uff0c\u4fc3\u8fdb\u77e5\u8bc6\u4ece\u6807\u8bb0\u6e90\u57df\u8f6c\u79fb\u5230\u672a\u6807\u8bb0\u76ee\u6807\u57df\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u57fa\u4e8e\u5efa\u7acb\u7684\u5d4c\u5165\u7a7a\u95f4\u751f\u6210\u51c6\u786e\u4e14\u5747\u5300\u7684\u4f2a\u6807\u7b7e\uff0c\u514b\u670d\u4e86\u4f20\u7edfSSDA\u7684\u5c40\u9650\u6027\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u901a\u8fc7\u5bf9 DomainNet\u3001Office-Home \u548c VisDA-C \u7b49\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u5f97\u5230\u9a8c\u8bc1\uff0c\u5176\u4e2d AdaEmbed \u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u51c6\uff0c\u4e3a SSDA \u8bbe\u5b9a\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002\u51ed\u501f\u5176\u7b80\u5355\u7684\u5b9e\u73b0\u548c\u9ad8\u6570\u636e\u6548\u7387\uff0cAdaEmbed \u6210\u4e3a\u9488\u5bf9\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u73b0\u5b9e\u573a\u666f\u7684\u5f3a\u5927\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4e3a\u4e86\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\uff0c\u6211\u4eec\u6b63\u5728\u5171\u4eab\u534a\u76d1\u7763\u57df\u9002\u5e94\u7edf\u4e00\u6846\u67b6\u7684\u4ee3\u7801\u5e93\u3002|[2401.12421v1](http://arxiv.org/pdf/2401.12421v1)|null|\n"}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "\u5176\u4ed6": {"2401.12888": "|**2024-01-23**|**Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big Data System, Data Mining, and Closed-Loop Technologies**|\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u6f14\u8fdb\uff1a\u5927\u6570\u636e\u7cfb\u7edf\u3001\u6570\u636e\u6316\u6398\u548c\u95ed\u73af\u6280\u672f\u7684\u5168\u9762\u7efc\u8ff0|Lincan Li, Wei Shao, Wei Dong, Yijun Tian, Kaixiang Yang, Wenjie Zhang|The aspiration of the next generation's autonomous driving (AD) technology relies on the dedicated integration and interaction among intelligent perception, prediction, planning, and low-level control. There has been a huge bottleneck regarding the upper bound of autonomous driving algorithm performance, a consensus from academia and industry believes that the key to surmount the bottleneck lies in data-centric autonomous driving technology. Recent advancement in AD simulation, closed-loop model training, and AD big data engine have gained some valuable experience. However, there is a lack of systematic knowledge and deep understanding regarding how to build efficient data-centric AD technology for AD algorithm self-evolution and better AD big data accumulation. To fill in the identified research gaps, this article will closely focus on reviewing the state-of-the-art data-driven autonomous driving technologies, with an emphasis on the comprehensive taxonomy of autonomous driving datasets characterized by milestone generations, key features, data acquisition settings, etc. Furthermore, we provide a systematic review of the existing benchmark closed-loop AD big data pipelines from the industrial frontier, including the procedure of closed-loop frameworks, key technologies, and empirical studies. Finally, the future directions, potential applications, limitations and concerns are discussed to arouse efforts from both academia and industry for promoting the further development of autonomous driving.|\u4e0b\u4e00\u4ee3\u81ea\u52a8\u9a7e\u9a76\uff08AD\uff09\u6280\u672f\u7684\u613f\u666f\u4f9d\u8d56\u4e8e\u667a\u80fd\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u5e95\u5c42\u63a7\u5236\u4e4b\u95f4\u7684\u4e13\u95e8\u96c6\u6210\u548c\u4ea4\u4e92\u3002\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u6027\u80fd\u4e0a\u9650\u5b58\u5728\u5de8\u5927\u74f6\u9888\uff0c\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u4e00\u81f4\u8ba4\u4e3a\uff0c\u7a81\u7834\u74f6\u9888\u7684\u5173\u952e\u5728\u4e8e\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u3002\u8fd1\u5e74\u6765\u5728AD\u4eff\u771f\u3001\u95ed\u73af\u6a21\u578b\u8bad\u7ec3\u3001AD\u5927\u6570\u636e\u5f15\u64ce\u7b49\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u4e9b\u8fdb\u5c55\uff0c\u79ef\u7d2f\u4e86\u4e00\u4e9b\u5b9d\u8d35\u7684\u7ecf\u9a8c\u3002\u7136\u800c\uff0c\u5982\u4f55\u6784\u5efa\u9ad8\u6548\u7684\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684AD\u6280\u672f\uff0c\u5b9e\u73b0AD\u7b97\u6cd5\u7684\u81ea\u6211\u8fdb\u5316\u548c\u66f4\u597d\u7684AD\u5927\u6570\u636e\u79ef\u7d2f\uff0c\u8fd8\u7f3a\u4e4f\u7cfb\u7edf\u7684\u8ba4\u8bc6\u548c\u6df1\u523b\u7684\u7406\u89e3\u3002\u4e3a\u4e86\u586b\u8865\u5df2\u786e\u5b9a\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u672c\u6587\u5c06\u5bc6\u5207\u5173\u6ce8\u6700\u5148\u8fdb\u7684\u6570\u636e\u9a71\u52a8\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\uff0c\u91cd\u70b9\u662f\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u7efc\u5408\u5206\u7c7b\uff0c\u4ee5\u91cc\u7a0b\u7891\u4ee3\u3001\u5173\u952e\u7279\u5f81\u3001\u6570\u636e\u4e3a\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ece\u4ea7\u4e1a\u524d\u6cbf\u5bf9\u73b0\u6709\u57fa\u51c6\u95ed\u73afAD\u5927\u6570\u636e\u7ba1\u9053\u8fdb\u884c\u4e86\u7cfb\u7edf\u56de\u987e\uff0c\u5305\u62ec\u95ed\u73af\u6846\u67b6\u6d41\u7a0b\u3001\u5173\u952e\u6280\u672f\u548c\u5b9e\u8bc1\u7814\u7a76\u3002\u6700\u540e\uff0c\u8ba8\u8bba\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u672a\u6765\u65b9\u5411\u3001\u6f5c\u5728\u5e94\u7528\u3001\u5c40\u9650\u6027\u548c\u5173\u6ce8\u70b9\uff0c\u4ee5\u5f15\u8d77\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u5171\u540c\u52aa\u529b\uff0c\u63a8\u52a8\u81ea\u52a8\u9a7e\u9a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002|[2401.12888v1](http://arxiv.org/pdf/2401.12888v1)|null|\n", "2401.12587": "|**2024-01-23**|**Fast Implicit Neural Representation Image Codec in Resource-limited Devices**|\u8d44\u6e90\u6709\u9650\u8bbe\u5907\u4e2d\u7684\u5feb\u901f\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u56fe\u50cf\u7f16\u89e3\u7801\u5668|Xiang Liu, Jiahong Chen, Bin Chen, Zimo Liu, Baoyi An, Shu-Tao Xia|Displaying high-quality images on edge devices, such as augmented reality devices, is essential for enhancing the user experience. However, these devices often face power consumption and computing resource limitations, making it challenging to apply many deep learning-based image compression algorithms in this field. Implicit Neural Representation (INR) for image compression is an emerging technology that offers two key benefits compared to cutting-edge autoencoder models: low computational complexity and parameter-free decoding. It also outperforms many traditional and early neural compression methods in terms of quality. In this study, we introduce a new Mixed Autoregressive Model (MARM) to significantly reduce the decoding time for the current INR codec, along with a new synthesis network to enhance reconstruction quality. MARM includes our proposed Autoregressive Upsampler (ARU) blocks, which are highly computationally efficient, and ARM from previous work to balance decoding time and reconstruction quality. We also propose enhancing ARU's performance using a checkerboard two-stage decoding strategy. Moreover, the ratio of different modules can be adjusted to maintain a balance between quality and speed. Comprehensive experiments demonstrate that our method significantly improves computational efficiency while preserving image quality. With different parameter settings, our method can outperform popular AE-based codecs in constrained environments in terms of both quality and decoding time, or achieve state-of-the-art reconstruction quality compared to other INR codecs.|\u5728\u589e\u5f3a\u73b0\u5b9e\u8bbe\u5907\u7b49\u8fb9\u7f18\u8bbe\u5907\u4e0a\u663e\u793a\u9ad8\u8d28\u91cf\u56fe\u50cf\u5bf9\u4e8e\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u8bbe\u5907\u901a\u5e38\u9762\u4e34\u529f\u8017\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9650\u5236\uff0c\u4f7f\u5f97\u5728\u8be5\u9886\u57df\u5e94\u7528\u8bb8\u591a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u538b\u7f29\u7b97\u6cd5\u5177\u6709\u6311\u6218\u6027\u3002\u7528\u4e8e\u56fe\u50cf\u538b\u7f29\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a (INR) \u662f\u4e00\u9879\u65b0\u5174\u6280\u672f\uff0c\u4e0e\u5c16\u7aef\u81ea\u52a8\u7f16\u7801\u5668\u6a21\u578b\u76f8\u6bd4\uff0c\u5b83\u5177\u6709\u4e24\u4e2a\u5173\u952e\u4f18\u52bf\uff1a\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u548c\u65e0\u53c2\u6570\u89e3\u7801\u3002\u5b83\u5728\u8d28\u91cf\u65b9\u9762\u4e5f\u4f18\u4e8e\u8bb8\u591a\u4f20\u7edf\u548c\u65e9\u671f\u7684\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u81ea\u56de\u5f52\u6a21\u578b\uff08MARM\uff09\uff0c\u4ee5\u663e\u7740\u51cf\u5c11\u5f53\u524d INR \u7f16\u89e3\u7801\u5668\u7684\u89e3\u7801\u65f6\u95f4\uff0c\u4ee5\u53ca\u4e00\u4e2a\u65b0\u7684\u5408\u6210\u7f51\u7edc\u6765\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002 MARM \u5305\u62ec\u6211\u4eec\u63d0\u51fa\u7684\u81ea\u56de\u5f52\u4e0a\u91c7\u6837\u5668 (ARU) \u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5177\u6709\u5f88\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u4e14\u5305\u542b\u4e4b\u524d\u5de5\u4f5c\u4e2d\u7684 ARM\uff0c\u7528\u4e8e\u5e73\u8861\u89e3\u7801\u65f6\u95f4\u548c\u91cd\u5efa\u8d28\u91cf\u3002\u6211\u4eec\u8fd8\u5efa\u8bae\u4f7f\u7528\u68cb\u76d8\u4e24\u9636\u6bb5\u89e3\u7801\u7b56\u7565\u6765\u589e\u5f3a ARU \u7684\u6027\u80fd\u3002\u800c\u4e14\uff0c\u53ef\u4ee5\u8c03\u6574\u4e0d\u540c\u6a21\u5757\u7684\u6bd4\u4f8b\uff0c\u4ee5\u4fdd\u6301\u8d28\u91cf\u548c\u901f\u5ea6\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u663e\u7740\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u901a\u8fc7\u4e0d\u540c\u7684\u53c2\u6570\u8bbe\u7f6e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u89e3\u7801\u65f6\u95f4\u65b9\u9762\u90fd\u53ef\u4ee5\u5728\u53d7\u9650\u73af\u5883\u4e2d\u4f18\u4e8e\u6d41\u884c\u7684\u57fa\u4e8e AE \u7684\u7f16\u89e3\u7801\u5668\uff0c\u6216\u8005\u4e0e\u5176\u4ed6 INR \u7f16\u89e3\u7801\u5668\u76f8\u6bd4\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\u3002|[2401.12587v1](http://arxiv.org/pdf/2401.12587v1)|null|\n", "2401.12438": "|**2024-01-23**|**Secure Federated Learning Approaches to Diagnosing COVID-19**|\u7528\u4e8e\u8bca\u65ad COVID-19 \u7684\u5b89\u5168\u8054\u5408\u5b66\u4e60\u65b9\u6cd5|Rittika Adhikari, Christopher Settles|The recent pandemic has underscored the importance of accurately diagnosing COVID-19 in hospital settings. A major challenge in this regard is differentiating COVID-19 from other respiratory illnesses based on chest X-rays, compounded by the restrictions of HIPAA compliance which limit the comparison of patient X-rays. This paper introduces a HIPAA-compliant model to aid in the diagnosis of COVID-19, utilizing federated learning. Federated learning is a distributed machine learning approach that allows for algorithm training across multiple decentralized devices using local data samples, without the need for data sharing. Our model advances previous efforts in chest X-ray diagnostic models. We examined leading models from established competitions in this domain and developed our own models tailored to be effective with specific hospital data. Considering the model's operation in a federated learning context, we explored the potential impact of biased data updates on the model's performance. To enhance hospital understanding of the model's decision-making process and to verify that the model is not focusing on irrelevant features, we employed a visualization technique that highlights key features in chest X-rays indicative of a positive COVID-19 diagnosis.|\u6700\u8fd1\u7684\u5927\u6d41\u884c\u51f8\u663e\u4e86\u5728\u533b\u9662\u73af\u5883\u4e2d\u51c6\u786e\u8bca\u65ad COVID-19 \u7684\u91cd\u8981\u6027\u3002\u8fd9\u65b9\u9762\u7684\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u662f\u6839\u636e\u80f8\u90e8 X \u5149\u68c0\u67e5\u5c06 COVID-19 \u4e0e\u5176\u4ed6\u547c\u5438\u9053\u75be\u75c5\u533a\u5206\u5f00\u6765\uff0c\u518d\u52a0\u4e0a HIPAA \u5408\u89c4\u6027\u7684\u9650\u5236\u9650\u5236\u4e86\u60a3\u8005 X \u5149\u68c0\u67e5\u7684\u6bd4\u8f83\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7b26\u5408 HIPAA \u8981\u6c42\u7684\u6a21\u578b\uff0c\u5229\u7528\u8054\u90a6\u5b66\u4e60\u6765\u5e2e\u52a9\u8bca\u65ad COVID-19\u3002\u8054\u90a6\u5b66\u4e60\u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5141\u8bb8\u4f7f\u7528\u672c\u5730\u6570\u636e\u6837\u672c\u8de8\u591a\u4e2a\u5206\u6563\u8bbe\u5907\u8fdb\u884c\u7b97\u6cd5\u8bad\u7ec3\uff0c\u800c\u65e0\u9700\u6570\u636e\u5171\u4eab\u3002\u6211\u4eec\u7684\u6a21\u578b\u63a8\u8fdb\u4e86\u4e4b\u524d\u5728\u80f8\u90e8 X \u5c04\u7ebf\u8bca\u65ad\u6a21\u578b\u65b9\u9762\u7684\u52aa\u529b\u3002\u6211\u4eec\u7814\u7a76\u4e86\u8be5\u9886\u57df\u5df2\u5efa\u7acb\u7684\u7ade\u4e89\u4e2d\u7684\u9886\u5148\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u6211\u4eec\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u4ee5\u4fbf\u5bf9\u7279\u5b9a\u7684\u533b\u9662\u6570\u636e\u6709\u6548\u3002\u8003\u8651\u5230\u6a21\u578b\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u8fd0\u884c\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u6709\u504f\u5dee\u7684\u6570\u636e\u66f4\u65b0\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u6f5c\u5728\u5f71\u54cd\u3002\u4e3a\u4e86\u589e\u5f3a\u533b\u9662\u5bf9\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u5e76\u9a8c\u8bc1\u6a21\u578b\u6ca1\u6709\u5173\u6ce8\u4e0d\u76f8\u5173\u7684\u7279\u5f81\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u53ef\u89c6\u5316\u6280\u672f\uff0c\u7a81\u51fa\u663e\u793a\u80f8\u90e8 X \u5149\u7247\u4e2d\u8868\u660e COVID-19 \u8bca\u65ad\u5448\u9633\u6027\u7684\u5173\u952e\u7279\u5f81\u3002|[2401.12438v1](http://arxiv.org/pdf/2401.12438v1)|null|\n"}}