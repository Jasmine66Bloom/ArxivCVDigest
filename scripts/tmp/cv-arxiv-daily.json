{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.08357": "|**2024-01-16**|**SAMF: Small-Area-Aware Multi-focus Image Fusion for Object Detection**|SAMF\uff1a\u7528\u4e8e\u7269\u4f53\u68c0\u6d4b\u7684\u5c0f\u533a\u57df\u611f\u77e5\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408|Xilai Li, Xiaosong Li, Haishu Tan, Jinyang Li|Existing multi-focus image fusion (MFIF) methods often fail to preserve the uncertain transition region and detect small focus areas within large defocused regions accurately. To address this issue, this study proposes a new small-area-aware MFIF algorithm for enhancing object detection capability. First, we enhance the pixel attributes within the small focus and boundary regions, which are subsequently combined with visual saliency detection to obtain the pre-fusion results used to discriminate the distribution of focused pixels. To accurately ensure pixel focus, we consider the source image as a combination of focused, defocused, and uncertain regions and propose a three-region segmentation strategy. Finally, we design an effective pixel selection rule to generate segmentation decision maps and obtain the final fusion results. Experiments demonstrated that the proposed method can accurately detect small and smooth focus areas while improving object detection performance, outperforming existing methods in both subjective and objective evaluations. The source code is available at https://github.com/ixilai/SAMF.|\u73b0\u6709\u7684\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408\uff08MFIF\uff09\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u4fdd\u7559\u4e0d\u786e\u5b9a\u7684\u8fc7\u6e21\u533a\u57df\u5e76\u51c6\u786e\u68c0\u6d4b\u5927\u6563\u7126\u533a\u57df\u5185\u7684\u5c0f\u7126\u70b9\u533a\u57df\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c0f\u533a\u57df\u611f\u77e5 MFIF \u7b97\u6cd5\u6765\u589e\u5f3a\u76ee\u6807\u68c0\u6d4b\u80fd\u529b\u3002\u9996\u5148\uff0c\u6211\u4eec\u589e\u5f3a\u5c0f\u7126\u70b9\u548c\u8fb9\u754c\u533a\u57df\u5185\u7684\u50cf\u7d20\u5c5e\u6027\uff0c\u968f\u540e\u5c06\u5176\u4e0e\u89c6\u89c9\u663e\u7740\u6027\u68c0\u6d4b\u76f8\u7ed3\u200b\u200b\u5408\uff0c\u4ee5\u83b7\u5f97\u7528\u4e8e\u533a\u5206\u805a\u7126\u50cf\u7d20\u5206\u5e03\u7684\u9884\u878d\u5408\u7ed3\u679c\u3002\u4e3a\u4e86\u51c6\u786e\u5730\u786e\u4fdd\u50cf\u7d20\u805a\u7126\uff0c\u6211\u4eec\u5c06\u6e90\u56fe\u50cf\u89c6\u4e3a\u805a\u7126\u3001\u6563\u7126\u548c\u4e0d\u786e\u5b9a\u533a\u57df\u7684\u7ec4\u5408\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u533a\u57df\u5206\u5272\u7b56\u7565\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u6709\u6548\u7684\u50cf\u7d20\u9009\u62e9\u89c4\u5219\u6765\u751f\u6210\u5206\u5272\u51b3\u7b56\u56fe\u5e76\u83b7\u5f97\u6700\u7ec8\u7684\u878d\u5408\u7ed3\u679c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u5c0f\u4e14\u5e73\u6ed1\u7684\u805a\u7126\u533a\u57df\uff0c\u540c\u65f6\u63d0\u9ad8\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/ixilai/SAMF \u83b7\u53d6\u3002|[2401.08357v1](http://arxiv.org/pdf/2401.08357v1)|null|\n", "2401.08345": "|**2024-01-16**|**Multi-view Distillation based on Multi-modal Fusion for Few-shot Action Recognition(CLIP-$\\mathrm{M^2}$DF)**|\u57fa\u4e8e\u591a\u6a21\u6001\u878d\u5408\u7684\u591a\u89c6\u56fe\u84b8\u998f\u8fdb\u884c\u5c0f\u6837\u672c\u52a8\u4f5c\u8bc6\u522b(CLIP-$\\mathrm{M^2}$DF)|Fei Guo, YiKang Wang, Han Qi, WenPing Jin, Li Zhu|In recent years, few-shot action recognition has attracted increasing attention. It generally adopts the paradigm of meta-learning. In this field, overcoming the overlapping distribution of classes and outliers is still a challenging problem based on limited samples. We believe the combination of Multi-modal and Multi-view can improve this issue depending on information complementarity. Therefore, we propose a method of Multi-view Distillation based on Multi-modal Fusion. Firstly, a Probability Prompt Selector for the query is constructed to generate probability prompt embedding based on the comparison score between the prompt embeddings of the support and the visual embedding of the query. Secondly, we establish a Multi-view. In each view, we fuse the prompt embedding as consistent information with visual and the global or local temporal context to overcome the overlapping distribution of classes and outliers. Thirdly, we perform the distance fusion for the Multi-view and the mutual distillation of matching ability from one to another, enabling the model to be more robust to the distribution bias. Our code is available at the URL: \\url{https://github.com/cofly2014/MDMF}.|\u8fd1\u5e74\u6765\uff0c\u5c11\u955c\u5934\u52a8\u4f5c\u8bc6\u522b\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u5b83\u4e00\u822c\u91c7\u7528\u5143\u5b66\u4e60\u7684\u8303\u5f0f\u3002\u5728\u8fd9\u4e00\u9886\u57df\uff0c\u57fa\u4e8e\u6709\u9650\u7684\u6837\u672c\uff0c\u514b\u670d\u7c7b\u548c\u5f02\u5e38\u503c\u7684\u91cd\u53e0\u5206\u5e03\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u6211\u4eec\u76f8\u4fe1\u591a\u6a21\u6001\u548c\u591a\u89c6\u56fe\u7684\u7ed3\u5408\u53ef\u4ee5\u901a\u8fc7\u4fe1\u606f\u4e92\u8865\u6765\u6539\u5584\u8fd9\u4e2a\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u878d\u5408\u7684\u591a\u89c6\u56fe\u84b8\u998f\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6784\u9020\u67e5\u8be2\u7684\u6982\u7387\u63d0\u793a\u9009\u62e9\u5668\uff0c\u4ee5\u57fa\u4e8e\u652f\u6301\u7684\u63d0\u793a\u5d4c\u5165\u4e0e\u67e5\u8be2\u7684\u89c6\u89c9\u5d4c\u5165\u4e4b\u95f4\u7684\u6bd4\u8f83\u5206\u6570\u6765\u751f\u6210\u6982\u7387\u63d0\u793a\u5d4c\u5165\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5efa\u7acb\u591a\u89c6\u89d2\u3002\u5728\u6bcf\u4e2a\u89c6\u56fe\u4e2d\uff0c\u6211\u4eec\u5c06\u63d0\u793a\u5d4c\u5165\u4f5c\u4e3a\u4e00\u81f4\u7684\u4fe1\u606f\u4e0e\u89c6\u89c9\u548c\u5168\u5c40\u6216\u5c40\u90e8\u65f6\u95f4\u4e0a\u4e0b\u6587\u878d\u5408\uff0c\u4ee5\u514b\u670d\u7c7b\u548c\u5f02\u5e38\u503c\u7684\u91cd\u53e0\u5206\u5e03\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u5bf9\u591a\u89c6\u56fe\u8fdb\u884c\u8ddd\u79bb\u878d\u5408\u4ee5\u53ca\u76f8\u4e92\u5339\u914d\u80fd\u529b\u7684\u76f8\u4e92\u5347\u534e\uff0c\u4f7f\u6a21\u578b\u5bf9\u5206\u5e03\u504f\u5dee\u66f4\u52a0\u9c81\u68d2\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u4ece\u4ee5\u4e0b URL \u83b7\u53d6\uff1a\\url{https://github.com/cofly2014/MDMF}\u3002|[2401.08345v1](http://arxiv.org/pdf/2401.08345v1)|null|\n", "2401.08332": "|**2024-01-16**|**Generative Denoise Distillation: Simple Stochastic Noises Induce Efficient Knowledge Transfer for Dense Prediction**|\u751f\u6210\u53bb\u566a\u84b8\u998f\uff1a\u7b80\u5355\u7684\u968f\u673a\u566a\u58f0\u8bf1\u5bfc\u9ad8\u6548\u7684\u77e5\u8bc6\u8f6c\u79fb\u4ee5\u5b9e\u73b0\u5bc6\u96c6\u9884\u6d4b|Zhaoge Liu, Xiaohao Xu, Yunkang Cao, Weiming Shen|Knowledge distillation is the process of transferring knowledge from a more powerful large model (teacher) to a simpler counterpart (student). Numerous current approaches involve the student imitating the knowledge of the teacher directly. However, redundancy still exists in the learned representations through these prevalent methods, which tend to learn each spatial location's features indiscriminately. To derive a more compact representation (concept feature) from the teacher, inspired by human cognition, we suggest an innovative method, termed Generative Denoise Distillation (GDD), where stochastic noises are added to the concept feature of the student to embed them into the generated instance feature from a shallow network. Then, the generated instance feature is aligned with the knowledge of the instance from the teacher. We extensively experiment with object detection, instance segmentation, and semantic segmentation to demonstrate the versatility and effectiveness of our method. Notably, GDD achieves new state-of-the-art performance in the tasks mentioned above. We have achieved substantial improvements in semantic segmentation by enhancing PspNet and DeepLabV3, both of which are based on ResNet-18, resulting in mIoU scores of 74.67 and 77.69, respectively, surpassing their previous scores of 69.85 and 73.20 on the Cityscapes dataset of 20 categories. The source code of GDD is available at https://github.com/ZhgLiu/GDD.|\u77e5\u8bc6\u84b8\u998f\u662f\u5c06\u77e5\u8bc6\u4ece\u66f4\u5f3a\u5927\u7684\u5927\u578b\u6a21\u578b\uff08\u6559\u5e08\uff09\u8f6c\u79fb\u5230\u66f4\u7b80\u5355\u7684\u5bf9\u5e94\u6a21\u578b\uff08\u5b66\u751f\uff09\u7684\u8fc7\u7a0b\u3002\u5f53\u524d\u7684\u8bb8\u591a\u65b9\u6cd5\u90fd\u6d89\u53ca\u5b66\u751f\u76f4\u63a5\u6a21\u4eff\u8001\u5e08\u7684\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u901a\u8fc7\u8fd9\u4e9b\u6d41\u884c\u7684\u65b9\u6cd5\u5b66\u4e60\u5230\u7684\u8868\u793a\u4ecd\u7136\u5b58\u5728\u5197\u4f59\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u4e0d\u52a0\u533a\u522b\u5730\u5b66\u4e60\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\u7684\u7279\u5f81\u3002\u4e3a\u4e86\u4ece\u6559\u5e08\u90a3\u91cc\u83b7\u5f97\u66f4\u7d27\u51d1\u7684\u8868\u793a\uff08\u6982\u5ff5\u7279\u5f81\uff09\uff0c\u53d7\u4eba\u7c7b\u8ba4\u77e5\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u751f\u6210\u964d\u566a\u84b8\u998f\uff08GDD\uff09\uff0c\u5176\u4e2d\u5c06\u968f\u673a\u566a\u58f0\u6dfb\u52a0\u5230\u5b66\u751f\u7684\u6982\u5ff5\u7279\u5f81\u4e2d\uff0c\u5c06\u5176\u5d4c\u5165\u5230\u5b66\u751f\u7684\u6982\u5ff5\u7279\u5f81\u4e2d\u3002\u4ece\u6d45\u5c42\u7f51\u7edc\u751f\u6210\u5b9e\u4f8b\u7279\u5f81\u3002\u7136\u540e\uff0c\u751f\u6210\u7684\u5b9e\u4f8b\u7279\u5f81\u4e0e\u6559\u5e08\u63d0\u4f9b\u7684\u5b9e\u4f8b\u77e5\u8bc6\u76f8\u5339\u914d\u3002\u6211\u4eec\u5bf9\u5bf9\u8c61\u68c0\u6d4b\u3001\u5b9e\u4f8b\u5206\u5272\u548c\u8bed\u4e49\u5206\u5272\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u8bc1\u660e\u6211\u4eec\u65b9\u6cd5\u7684\u591a\u529f\u80fd\u6027\u548c\u6709\u6548\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cGDD \u5728\u4e0a\u8ff0\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u901a\u8fc7\u589e\u5f3a PspNet \u548c DeepLabV3 \u5728\u8bed\u4e49\u5206\u5272\u65b9\u9762\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u7684\u6539\u8fdb\uff0c\u8fd9\u4e24\u4e2a\u6a21\u578b\u90fd\u57fa\u4e8e ResNet-18\uff0c\u5176 mIoU \u5206\u6570\u5206\u522b\u4e3a 74.67 \u548c 77.69\uff0c\u8d85\u8fc7\u4e86\u5b83\u4eec\u4e4b\u524d\u5728 20 \u4e2a\u7c7b\u522b\u7684 Cityscapes \u6570\u636e\u96c6\u4e0a\u7684\u5206\u6570 69.85 \u548c 73.20 \u3002 GDD\u7684\u6e90\u4ee3\u7801\u53ef\u4ee5\u5728https://github.com/ZhgLiu/GDD\u83b7\u53d6\u3002|[2401.08332v1](http://arxiv.org/pdf/2401.08332v1)|**[link](https://github.com/ZhgLiu/GDD)**|\n", "2401.08275": "|**2024-01-16**|**Modeling Spoof Noise by De-spoofing Diffusion and its Application in Face Anti-spoofing**|\u53cd\u6b3a\u9a97\u6269\u6563\u6a21\u62df\u6b3a\u9a97\u566a\u58f0\u53ca\u5176\u5728\u4eba\u8138\u53cd\u6b3a\u9a97\u4e2d\u7684\u5e94\u7528|Bin Zhang, Xiangyu Zhu, Xiaoyu Zhang, Zhen Lei|Face anti-spoofing is crucial for ensuring the security and reliability of face recognition systems. Several existing face anti-spoofing methods utilize GAN-like networks to detect presentation attacks by estimating the noise pattern of a spoof image and recovering the corresponding genuine image. But GAN's limited face appearance space results in the denoised faces cannot cover the full data distribution of genuine faces, thereby undermining the generalization performance of such methods. In this work, we present a pioneering attempt to employ diffusion models to denoise a spoof image and restore the genuine image. The difference between these two images is considered as the spoof noise, which can serve as a discriminative cue for face anti-spoofing. We evaluate our proposed method on several intra-testing and inter-testing protocols, where the experimental results showcase the effectiveness of our method in achieving competitive performance in terms of both accuracy and generalization.|\u4eba\u8138\u53cd\u6b3a\u9a97\u5bf9\u4e8e\u786e\u4fdd\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u51e0\u79cd\u73b0\u6709\u7684\u9762\u90e8\u53cd\u6b3a\u9a97\u65b9\u6cd5\u5229\u7528\u7c7b\u4f3c GAN \u7684\u7f51\u7edc\uff0c\u901a\u8fc7\u4f30\u8ba1\u6b3a\u9a97\u56fe\u50cf\u7684\u566a\u58f0\u6a21\u5f0f\u5e76\u6062\u590d\u76f8\u5e94\u7684\u771f\u5b9e\u56fe\u50cf\u6765\u68c0\u6d4b\u5448\u73b0\u653b\u51fb\u3002\u4f46GAN\u6709\u9650\u7684\u4eba\u8138\u5916\u89c2\u7a7a\u95f4\u5bfc\u81f4\u53bb\u566a\u540e\u7684\u4eba\u8138\u65e0\u6cd5\u8986\u76d6\u771f\u5b9e\u4eba\u8138\u7684\u5b8c\u6574\u6570\u636e\u5206\u5e03\uff0c\u4ece\u800c\u524a\u5f31\u4e86\u6b64\u7c7b\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5229\u7528\u6269\u6563\u6a21\u578b\u5bf9\u6076\u641e\u56fe\u50cf\u8fdb\u884c\u53bb\u566a\u5e76\u6062\u590d\u771f\u5b9e\u56fe\u50cf\u7684\u5f00\u521b\u6027\u5c1d\u8bd5\u3002\u8fd9\u4e24\u4e2a\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u88ab\u8ba4\u4e3a\u662f\u6b3a\u9a97\u566a\u58f0\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u9762\u90e8\u53cd\u6b3a\u9a97\u7684\u5224\u522b\u7ebf\u7d22\u3002\u6211\u4eec\u5728\u51e0\u4e2a\u5185\u90e8\u6d4b\u8bd5\u548c\u76f8\u4e92\u6d4b\u8bd5\u534f\u8bae\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u65b9\u9762\u5b9e\u73b0\u7ade\u4e89\u6027\u80fd\u7684\u6709\u6548\u6027\u3002|[2401.08275v1](http://arxiv.org/pdf/2401.08275v1)|null|\n", "2401.08263": "|**2024-01-16**|**Multi-Technique Sequential Information Consistency For Dynamic Visual Place Recognition In Changing Environments**|\u53d8\u5316\u73af\u5883\u4e2d\u52a8\u6001\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u7684\u591a\u6280\u672f\u5e8f\u5217\u4fe1\u606f\u4e00\u81f4\u6027|Bruno Arcanjo, Bruno Ferrarini, Michael Milford, Klaus D. McDonald-Maier, Shoaib Ehsan|Visual place recognition (VPR) is an essential component of robot navigation and localization systems that allows them to identify a place using only image data. VPR is challenging due to the significant changes in a place's appearance driven by different daily illumination, seasonal weather variations and diverse viewpoints. Currently, no single VPR technique excels in every environmental condition, each exhibiting unique benefits and shortcomings, and therefore combining multiple techniques can achieve more reliable VPR performance. Present multi-method approaches either rely on online ground-truth information, which is often not available, or on brute-force technique combination, potentially lowering performance with high variance technique sets. Addressing these shortcomings, we propose a VPR system dubbed Multi-Sequential Information Consistency (MuSIC) which leverages sequential information to select the most cohesive technique on an online per-frame basis. For each technique in a set, MuSIC computes their respective sequential consistencies by analysing the frame-to-frame continuity of their top match candidates, which are then directly compared to select the optimal technique for the current query image. The use of sequential information to select between VPR methods results in an overall VPR performance increase across different benchmark datasets, while avoiding the need for extra ground-truth of the runtime environment.|\u89c6\u89c9\u5730\u70b9\u8bc6\u522b (VPR) \u662f\u673a\u5668\u4eba\u5bfc\u822a\u548c\u5b9a\u4f4d\u7cfb\u7edf\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ec5\u4f7f\u7528\u56fe\u50cf\u6570\u636e\u6765\u8bc6\u522b\u5730\u70b9\u3002 VPR \u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e0d\u540c\u7684\u65e5\u5e38\u7167\u660e\u3001\u5b63\u8282\u6027\u5929\u6c14\u53d8\u5316\u548c\u4e0d\u540c\u7684\u89c2\u70b9\u4f1a\u5bfc\u81f4\u4e00\u4e2a\u5730\u65b9\u7684\u5916\u89c2\u53d1\u751f\u663e\u7740\u53d8\u5316\u3002\u76ee\u524d\uff0c\u6ca1\u6709\u4e00\u79cd VPR \u6280\u672f\u80fd\u591f\u9002\u5e94\u6240\u6709\u73af\u5883\u6761\u4ef6\uff0c\u6bcf\u79cd\u6280\u672f\u90fd\u5177\u6709\u72ec\u7279\u7684\u4f18\u70b9\u548c\u7f3a\u70b9\uff0c\u56e0\u6b64\u7ed3\u5408\u591a\u79cd\u6280\u672f\u53ef\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u7684 VPR \u6027\u80fd\u3002\u76ee\u524d\u7684\u591a\u65b9\u6cd5\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u901a\u5e38\u4e0d\u53ef\u7528\u7684\u5728\u7ebf\u771f\u5b9e\u4fe1\u606f\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u5f3a\u529b\u6280\u672f\u7ec4\u5408\uff0c\u53ef\u80fd\u4f1a\u964d\u4f4e\u9ad8\u65b9\u5dee\u6280\u672f\u96c6\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u7f3a\u70b9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u591a\u5e8f\u5217\u4fe1\u606f\u4e00\u81f4\u6027\uff08MuSIC\uff09\u7684 VPR \u7cfb\u7edf\uff0c\u5b83\u5229\u7528\u5e8f\u5217\u4fe1\u606f\u5728\u6bcf\u5e27\u7684\u5728\u7ebf\u57fa\u7840\u4e0a\u9009\u62e9\u6700\u5177\u51dd\u805a\u529b\u7684\u6280\u672f\u3002\u5bf9\u4e8e\u4e00\u7ec4\u4e2d\u7684\u6bcf\u79cd\u6280\u672f\uff0cMuSIC \u901a\u8fc7\u5206\u6790\u5176\u6700\u4f73\u5339\u914d\u5019\u9009\u8005\u7684\u5e27\u5230\u5e27\u8fde\u7eed\u6027\u6765\u8ba1\u7b97\u5176\u5404\u81ea\u7684\u987a\u5e8f\u4e00\u81f4\u6027\uff0c\u7136\u540e\u76f4\u63a5\u6bd4\u8f83\u8fd9\u4e9b\u6280\u672f\u4ee5\u9009\u62e9\u5f53\u524d\u67e5\u8be2\u56fe\u50cf\u7684\u6700\u4f73\u6280\u672f\u3002\u4f7f\u7528\u987a\u5e8f\u4fe1\u606f\u5728 VPR \u65b9\u6cd5\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\u53ef\u4ee5\u63d0\u9ad8\u4e0d\u540c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6574\u4f53 VPR \u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u9700\u8981\u8fd0\u884c\u65f6\u73af\u5883\u7684\u989d\u5916\u771f\u5b9e\u6570\u636e\u3002|[2401.08263v1](http://arxiv.org/pdf/2401.08263v1)|null|\n", "2401.08232": "|**2024-01-16**|**Multi-scale 2D Temporal Map Diffusion Models for Natural Language Video Localization**|\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u89c6\u9891\u5b9a\u4f4d\u7684\u591a\u5c3a\u5ea6 2D \u65f6\u95f4\u56fe\u6269\u6563\u6a21\u578b|Chongzhi Zhang, Mingyuan Zhang, Zhiyang Teng, Jiayi Li, Xizhou Zhu, Lewei Lu, Ziwei Liu, Aixin Sun|Natural Language Video Localization (NLVL), grounding phrases from natural language descriptions to corresponding video segments, is a complex yet critical task in video understanding. Despite ongoing advancements, many existing solutions lack the capability to globally capture temporal dynamics of the video data. In this study, we present a novel approach to NLVL that aims to address this issue. Our method involves the direct generation of a global 2D temporal map via a conditional denoising diffusion process, based on the input video and language query. The main challenges are the inherent sparsity and discontinuity of a 2D temporal map in devising the diffusion decoder. To address these challenges, we introduce a multi-scale technique and develop an innovative diffusion decoder. Our approach effectively encapsulates the interaction between the query and video data across various time scales. Experiments on the Charades and DiDeMo datasets underscore the potency of our design.|\u81ea\u7136\u8bed\u8a00\u89c6\u9891\u672c\u5730\u5316\uff08NLVL\uff09\u662f\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u77ed\u8bed\u57fa\u7840\u5230\u76f8\u5e94\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u662f\u89c6\u9891\u7406\u89e3\u4e2d\u4e00\u9879\u590d\u6742\u4f46\u5173\u952e\u7684\u4efb\u52a1\u3002\u5c3d\u7ba1\u4e0d\u65ad\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u8bb8\u591a\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u5168\u5c40\u6355\u83b7\u89c6\u9891\u6570\u636e\u7684\u65f6\u95f4\u52a8\u6001\u7684\u80fd\u529b\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 NLVL \u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d89\u53ca\u57fa\u4e8e\u8f93\u5165\u89c6\u9891\u548c\u8bed\u8a00\u67e5\u8be2\uff0c\u901a\u8fc7\u6761\u4ef6\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\u76f4\u63a5\u751f\u6210\u5168\u5c40 2D \u65f6\u95f4\u56fe\u3002\u4e3b\u8981\u6311\u6218\u662f\u8bbe\u8ba1\u6269\u6563\u89e3\u7801\u5668\u65f6\u4e8c\u7ef4\u65f6\u95f4\u56fe\u56fa\u6709\u7684\u7a00\u758f\u6027\u548c\u4e0d\u8fde\u7eed\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u591a\u5c3a\u5ea6\u6280\u672f\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6269\u6563\u89e3\u7801\u5668\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u5c01\u88c5\u4e86\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u67e5\u8be2\u548c\u89c6\u9891\u6570\u636e\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002 Charades \u548c DiDeMo \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u6211\u4eec\u8bbe\u8ba1\u7684\u6548\u529b\u3002|[2401.08232v1](http://arxiv.org/pdf/2401.08232v1)|null|\n", "2401.08210": "|**2024-01-16**|**ModelNet-O: A Large-Scale Synthetic Dataset for Occlusion-Aware Point Cloud Classification**|ModelNet-O\uff1a\u7528\u4e8e\u906e\u6321\u611f\u77e5\u70b9\u4e91\u5206\u7c7b\u7684\u5927\u89c4\u6a21\u7efc\u5408\u6570\u636e\u96c6|Zhongbin Fang, Xia Li, Xiangtai Li, Shen Zhao, Mengyuan Liu|Recently, 3D point cloud classification has made significant progress with the help of many datasets. However, these datasets do not reflect the incomplete nature of real-world point clouds caused by occlusion, which limits the practical application of current methods. To bridge this gap, we propose ModelNet-O, a large-scale synthetic dataset of 123,041 samples that emulate real-world point clouds with self-occlusion caused by scanning from monocular cameras. ModelNet-O is 10 times larger than existing datasets and offers more challenging cases to evaluate the robustness of existing methods. Our observation on ModelNet-O reveals that well-designed sparse structures can preserve structural information of point clouds under occlusion, motivating us to propose a robust point cloud processing method that leverages a critical point sampling (CPS) strategy in a multi-level manner. We term our method PointMLS. Through extensive experiments, we demonstrate that our PointMLS achieves state-of-the-art results on ModelNet-O and competitive results on regular datasets, and it is robust and effective. More experiments also demonstrate the robustness and effectiveness of PointMLS.|\u6700\u8fd1\uff0c3D \u70b9\u4e91\u5206\u7c7b\u5728\u8bb8\u591a\u6570\u636e\u96c6\u7684\u5e2e\u52a9\u4e0b\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u5e76\u4e0d\u80fd\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u70b9\u4e91\u56e0\u906e\u6321\u800c\u5bfc\u81f4\u7684\u4e0d\u5b8c\u6574\u6027\u8d28\uff0c\u8fd9\u9650\u5236\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ModelNet-O\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 123,041 \u4e2a\u6837\u672c\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff0c\u5b83\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u7684\u70b9\u4e91\uff0c\u5177\u6709\u7531\u5355\u76ee\u76f8\u673a\u626b\u63cf\u5f15\u8d77\u7684\u81ea\u906e\u6321\u3002 ModelNet-O \u6bd4\u73b0\u6709\u6570\u636e\u96c6\u5927 10 \u500d\uff0c\u5e76\u63d0\u4f9b\u66f4\u5177\u6311\u6218\u6027\u7684\u6848\u4f8b\u6765\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u3002\u6211\u4eec\u5bf9 ModelNet-O \u7684\u89c2\u5bdf\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7a00\u758f\u7ed3\u6784\u53ef\u4ee5\u5728\u906e\u6321\u4e0b\u4fdd\u7559\u70b9\u4e91\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u8fd9\u4fc3\u4f7f\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u9c81\u68d2\u7684\u70b9\u4e91\u5904\u7406\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ee5\u591a\u7ea7\u65b9\u5f0f\u5229\u7528\u4e34\u754c\u70b9\u91c7\u6837\uff08CPS\uff09\u7b56\u7565\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3a PointMLS\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684 PointMLS \u5728 ModelNet-O \u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5728\u5e38\u89c4\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5b83\u662f\u7a33\u5065\u4e14\u6709\u6548\u7684\u3002\u66f4\u591a\u7684\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u4e86 PointMLS \u7684\u7a33\u5065\u6027\u548c\u6709\u6548\u6027\u3002|[2401.08210v1](http://arxiv.org/pdf/2401.08210v1)|null|\n", "2401.08194": "|**2024-01-16**|**End-to-End Optimized Image Compression with the Frequency-Oriented Transform**|\u4f7f\u7528\u9762\u5411\u9891\u7387\u7684\u53d8\u6362\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u7684\u56fe\u50cf\u538b\u7f29|Yuefeng Zhang, Kai Lin|Image compression constitutes a significant challenge amidst the era of information explosion. Recent studies employing deep learning methods have demonstrated the superior performance of learning-based image compression methods over traditional codecs. However, an inherent challenge associated with these methods lies in their lack of interpretability. Following an analysis of the varying degrees of compression degradation across different frequency bands, we propose the end-to-end optimized image compression model facilitated by the frequency-oriented transform. The proposed end-to-end image compression model consists of four components: spatial sampling, frequency-oriented transform, entropy estimation, and frequency-aware fusion. The frequency-oriented transform separates the original image signal into distinct frequency bands, aligning with the human-interpretable concept. Leveraging the non-overlapping hypothesis, the model enables scalable coding through the selective transmission of arbitrary frequency components. Extensive experiments are conducted to demonstrate that our model outperforms all traditional codecs including next-generation standard H.266/VVC on MS-SSIM metric. Moreover, visual analysis tasks (i.e., object detection and semantic segmentation) are conducted to verify the proposed compression method could preserve semantic fidelity besides signal-level precision.|\u56fe\u50cf\u538b\u7f29\u662f\u4fe1\u606f\u7206\u70b8\u65f6\u4ee3\u7684\u91cd\u5927\u6311\u6218\u3002\u6700\u8fd1\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u6bd4\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u4e0e\u8fd9\u4e9b\u65b9\u6cd5\u76f8\u5173\u7684\u56fa\u6709\u6311\u6218\u5728\u4e8e\u5b83\u4eec\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u5728\u5206\u6790\u4e0d\u540c\u9891\u6bb5\u7684\u4e0d\u540c\u7a0b\u5ea6\u7684\u538b\u7f29\u9000\u5316\u4e4b\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7531\u9762\u5411\u9891\u7387\u7684\u53d8\u6362\u4fc3\u8fdb\u7684\u7aef\u5230\u7aef\u4f18\u5316\u7684\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u3002\u6240\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u7531\u56db\u4e2a\u90e8\u5206\u7ec4\u6210\uff1a\u7a7a\u95f4\u91c7\u6837\u3001\u9762\u5411\u9891\u7387\u7684\u53d8\u6362\u3001\u71b5\u4f30\u8ba1\u548c\u9891\u7387\u611f\u77e5\u878d\u5408\u3002\u9762\u5411\u9891\u7387\u7684\u53d8\u6362\u5c06\u539f\u59cb\u56fe\u50cf\u4fe1\u53f7\u5206\u6210\u4e0d\u540c\u7684\u9891\u5e26\uff0c\u4e0e\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u4fdd\u6301\u4e00\u81f4\u3002\u5229\u7528\u975e\u91cd\u53e0\u5047\u200b\u200b\u8bbe\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u9009\u62e9\u6027\u4f20\u8f93\u4efb\u610f\u9891\u7387\u5206\u91cf\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u7f16\u7801\u3002\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\u6765\u8bc1\u660e\u6211\u4eec\u7684\u6a21\u578b\u5728 MS-SSIM \u6307\u6807\u4e0a\u4f18\u4e8e\u6240\u6709\u4f20\u7edf\u7f16\u89e3\u7801\u5668\uff0c\u5305\u62ec\u4e0b\u4e00\u4ee3\u6807\u51c6 H.266/VVC\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u89c6\u89c9\u5206\u6790\u4efb\u52a1\uff08\u5373\u5bf9\u8c61\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\uff09\uff0c\u4ee5\u9a8c\u8bc1\u6240\u63d0\u51fa\u7684\u538b\u7f29\u65b9\u6cd5\u9664\u4e86\u4fe1\u53f7\u7ea7\u7cbe\u5ea6\u4e4b\u5916\u8fd8\u53ef\u4ee5\u4fdd\u7559\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002|[2401.08194v1](http://arxiv.org/pdf/2401.08194v1)|null|\n", "2401.08174": "|**2024-01-16**|**Completely Occluded and Dense Object Instance Segmentation Using Box Prompt-Based Segmentation Foundation Models**|\u4f7f\u7528\u57fa\u4e8e\u6846\u63d0\u793a\u7684\u5206\u5272\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5b8c\u5168\u906e\u6321\u548c\u5bc6\u96c6\u7684\u5bf9\u8c61\u5b9e\u4f8b\u5206\u5272|Zhen Zhou, Junfeng Fan, Yunkai Ma, Sihan Zhao, Fengshui Jing, Min Tan|Completely occluded and dense object instance segmentation (IS) is an important and challenging task. Although current amodal IS methods can predict invisible regions of occluded objects, they are difficult to directly predict completely occluded objects. For dense object IS, existing box-based methods are overly dependent on the performance of bounding box detection. In this paper, we propose CFNet, a coarse-to-fine IS framework for completely occluded and dense objects, which is based on box prompt-based segmentation foundation models (BSMs). Specifically, CFNet first detects oriented bounding boxes (OBBs) to distinguish instances and provide coarse localization information. Then, it predicts OBB prompt-related masks for fine segmentation. To predict completely occluded object instances, CFNet performs IS on occluders and utilizes prior geometric properties, which overcomes the difficulty of directly predicting completely occluded object instances. Furthermore, based on BSMs, CFNet reduces the dependence on bounding box detection performance, improving dense object IS performance. Moreover, we propose a novel OBB prompt encoder for BSMs. To make CFNet more lightweight, we perform knowledge distillation on it and introduce a Gaussian smoothing method for teacher targets. Experimental results demonstrate that CFNet achieves the best performance on both industrial and publicly available datasets.|\u5b8c\u5168\u906e\u6321\u548c\u5bc6\u96c6\u7684\u5bf9\u8c61\u5b9e\u4f8b\u5206\u5272\uff08IS\uff09\u662f\u4e00\u9879\u91cd\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u5c3d\u7ba1\u5f53\u524d\u7684\u975e\u6a21\u6001IS\u65b9\u6cd5\u53ef\u4ee5\u9884\u6d4b\u88ab\u906e\u6321\u7269\u4f53\u7684\u4e0d\u53ef\u89c1\u533a\u57df\uff0c\u4f46\u5b83\u4eec\u5f88\u96be\u76f4\u63a5\u9884\u6d4b\u5b8c\u5168\u88ab\u906e\u6321\u7684\u7269\u4f53\u3002\u5bf9\u4e8e\u5bc6\u96c6\u5bf9\u8c61 IS\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u6846\u7684\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u4e8e\u8fb9\u754c\u6846\u68c0\u6d4b\u7684\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 CFNet\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9\u5b8c\u5168\u906e\u6321\u548c\u5bc6\u96c6\u5bf9\u8c61\u7684\u4ece\u7c97\u5230\u7ec6\u7684 IS \u6846\u67b6\uff0c\u5b83\u57fa\u4e8e\u57fa\u4e8e\u6846\u63d0\u793a\u7684\u5206\u5272\u57fa\u7840\u6a21\u578b\uff08BSM\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0cCFNet \u9996\u5148\u68c0\u6d4b\u5b9a\u5411\u8fb9\u754c\u6846 (OBB) \u4ee5\u533a\u5206\u5b9e\u4f8b\u5e76\u63d0\u4f9b\u7c97\u7565\u7684\u5b9a\u4f4d\u4fe1\u606f\u3002\u7136\u540e\uff0c\u5b83\u9884\u6d4b OBB \u63d0\u793a\u76f8\u5173\u7684\u63a9\u6a21\u4ee5\u8fdb\u884c\u7cbe\u7ec6\u5206\u5272\u3002\u4e3a\u4e86\u9884\u6d4b\u5b8c\u5168\u906e\u6321\u7684\u5bf9\u8c61\u5b9e\u4f8b\uff0cCFNet \u5bf9\u906e\u6321\u5668\u6267\u884c IS \u5e76\u5229\u7528\u5148\u9a8c\u51e0\u4f55\u5c5e\u6027\uff0c\u514b\u670d\u4e86\u76f4\u63a5\u9884\u6d4b\u5b8c\u5168\u906e\u6321\u7684\u5bf9\u8c61\u5b9e\u4f8b\u7684\u56f0\u96be\u3002\u6b64\u5916\uff0c\u57fa\u4e8e BSM\uff0cCFNet \u51cf\u5c11\u4e86\u5bf9\u8fb9\u754c\u6846\u68c0\u6d4b\u6027\u80fd\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u4e86\u5bc6\u96c6\u5bf9\u8c61 IS \u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e3a BSM \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 OBB \u63d0\u793a\u7f16\u7801\u5668\u3002\u4e3a\u4e86\u4f7fCFNet\u66f4\u52a0\u8f7b\u91cf\u7ea7\uff0c\u6211\u4eec\u5bf9\u5176\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u5e76\u4e3a\u6559\u5e08\u76ee\u6807\u5f15\u5165\u9ad8\u65af\u5e73\u6ed1\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCFNet \u5728\u5de5\u4e1a\u6570\u636e\u96c6\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002|[2401.08174v1](http://arxiv.org/pdf/2401.08174v1)|null|\n", "2401.08111": "|**2024-01-16**|**Mobile Contactless Palmprint Recognition: Use of Multiscale, Multimodel Embeddings**|\u79fb\u52a8\u975e\u63a5\u89e6\u5f0f\u638c\u7eb9\u8bc6\u522b\uff1a\u4f7f\u7528\u591a\u5c3a\u5ea6\u3001\u591a\u6a21\u578b\u5d4c\u5165|Steven A. Grosz, Akash Godbole, Anil K. Jain|Contactless palmprints are comprised of both global and local discriminative features. Most prior work focuses on extracting global features or local features alone for palmprint matching, whereas this research introduces a novel framework that combines global and local features for enhanced palmprint matching accuracy. Leveraging recent advancements in deep learning, this study integrates a vision transformer (ViT) and a convolutional neural network (CNN) to extract complementary local and global features. Next, a mobile-based, end-to-end palmprint recognition system is developed, referred to as Palm-ID. On top of the ViT and CNN features, Palm-ID incorporates a palmprint enhancement module and efficient dimensionality reduction (for faster matching). Palm-ID balances the trade-off between accuracy and latency, requiring just 18ms to extract a template of size 516 bytes, which can be efficiently searched against a 10,000 palmprint gallery in 0.33ms on an AMD EPYC 7543 32-Core CPU utilizing 128-threads. Cross-database matching protocols and evaluations on large-scale operational datasets demonstrate the robustness of the proposed method, achieving a TAR of 98.06% at FAR=0.01% on a newly collected, time-separated dataset. To show a practical deployment of the end-to-end system, the entire recognition pipeline is embedded within a mobile device for enhanced user privacy and security.|\u975e\u63a5\u89e6\u5f0f\u638c\u7eb9\u7531\u5168\u5c40\u548c\u5c40\u90e8\u5224\u522b\u7279\u5f81\u7ec4\u6210\u3002\u5927\u591a\u6570\u5148\u524d\u7684\u5de5\u4f5c\u90fd\u96c6\u4e2d\u4e8e\u5355\u72ec\u63d0\u53d6\u5168\u5c40\u7279\u5f81\u6216\u5c40\u90e8\u7279\u5f81\u6765\u8fdb\u884c\u638c\u7eb9\u5339\u914d\uff0c\u800c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u7684\u65b0\u9896\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u638c\u7eb9\u5339\u914d\u7684\u51c6\u786e\u6027\u3002\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u96c6\u6210\u4e86\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6765\u63d0\u53d6\u4e92\u8865\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u3002\u63a5\u4e0b\u6765\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u79fb\u52a8\u8bbe\u5907\u7684\u7aef\u5230\u7aef\u638c\u7eb9\u8bc6\u522b\u7cfb\u7edf\uff0c\u79f0\u4e3aPalm-ID\u3002\u9664\u4e86 ViT \u548c CNN \u529f\u80fd\u4e4b\u5916\uff0cPalm-ID \u8fd8\u96c6\u6210\u4e86\u638c\u7eb9\u589e\u5f3a\u6a21\u5757\u548c\u9ad8\u6548\u7684\u964d\u7ef4\uff08\u4ee5\u5b9e\u73b0\u66f4\u5feb\u7684\u5339\u914d\uff09\u3002 Palm-ID \u5e73\u8861\u4e86\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u53ea\u9700 18 \u6beb\u79d2\u5373\u53ef\u63d0\u53d6\u5927\u5c0f\u4e3a 516 \u5b57\u8282\u7684\u6a21\u677f\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728 AMD EPYC 7543 32 \u6838 CPU \u4e0a\u5229\u7528 128 \u4e2a\u5904\u7406\u5668\u5728 0.33 \u6beb\u79d2\u5185\u6709\u6548\u5730\u641c\u7d22 10,000 \u4e2a\u638c\u7eb9\u56fe\u5e93\u3002\u7ebf\u7a0b\u3002\u8de8\u6570\u636e\u5e93\u5339\u914d\u534f\u8bae\u548c\u5bf9\u5927\u89c4\u6a21\u64cd\u4f5c\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5728\u65b0\u6536\u96c6\u7684\u65f6\u95f4\u5206\u79bb\u6570\u636e\u96c6\u4e0a\uff0cFAR=0.01% \u65f6\u5b9e\u73b0\u4e86 98.06% \u7684 TAR\u3002\u4e3a\u4e86\u5c55\u793a\u7aef\u5230\u7aef\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\uff0c\u6574\u4e2a\u8bc6\u522b\u7ba1\u9053\u5d4c\u5165\u5230\u79fb\u52a8\u8bbe\u5907\u4e2d\uff0c\u4ee5\u589e\u5f3a\u7528\u6237\u9690\u79c1\u548c\u5b89\u5168\u6027\u3002|[2401.08111v1](http://arxiv.org/pdf/2401.08111v1)|null|\n", "2401.08105": "|**2024-01-16**|**Hardware Acceleration for Real-Time Wildfire Detection Onboard Drone Networks**|\u7528\u4e8e\u5b9e\u65f6\u91ce\u706b\u68c0\u6d4b\u673a\u8f7d\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u786c\u4ef6\u52a0\u901f|Austin Briley, Fatemeh Afghah|Early wildfire detection in remote and forest areas is crucial for minimizing devastation and preserving ecosystems. Autonomous drones offer agile access to remote, challenging terrains, equipped with advanced imaging technology that delivers both high-temporal and detailed spatial resolution, making them valuable assets in the early detection and monitoring of wildfires. However, the limited computation and battery resources of Unmanned Aerial Vehicles (UAVs) pose significant challenges in implementing robust and efficient image classification models. Current works in this domain often operate offline, emphasizing the need for solutions that can perform inference in real time, given the constraints of UAVs. To address these challenges, this paper aims to develop a real-time image classification and fire segmentation model. It presents a comprehensive investigation into hardware acceleration using the Jetson Nano P3450 and the implications of TensorRT, NVIDIA's high-performance deep-learning inference library, on fire classification accuracy and speed. The study includes implementations of Quantization Aware Training (QAT), Automatic Mixed Precision (AMP), and post-training mechanisms, comparing them against the latest baselines for fire segmentation and classification. All experiments utilize the FLAME dataset - an image dataset collected by low-altitude drones during a prescribed forest fire. This work contributes to the ongoing efforts to enable real-time, on-board wildfire detection capabilities for UAVs, addressing speed and the computational and energy constraints of these crucial monitoring systems. The results show a 13% increase in classification speed compared to similar models without hardware optimization. Comparatively, loss and accuracy are within 1.225% of the original values.|\u504f\u8fdc\u5730\u533a\u548c\u68ee\u6797\u5730\u533a\u7684\u65e9\u671f\u91ce\u706b\u68c0\u6d4b\u5bf9\u4e8e\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u7834\u574f\u548c\u4fdd\u62a4\u751f\u6001\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u81ea\u4e3b\u65e0\u4eba\u673a\u53ef\u7075\u6d3b\u8fdb\u5165\u504f\u8fdc\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u5730\u5f62\uff0c\u914d\u5907\u5148\u8fdb\u7684\u6210\u50cf\u6280\u672f\uff0c\u53ef\u63d0\u4f9b\u9ad8\u65f6\u95f4\u548c\u8be6\u7ec6\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u4f7f\u5176\u6210\u4e3a\u65e9\u671f\u53d1\u73b0\u548c\u76d1\u6d4b\u91ce\u706b\u7684\u5b9d\u8d35\u8d44\u4ea7\u3002\u7136\u800c\uff0c\u65e0\u4eba\u673a (UAV) \u6709\u9650\u7684\u8ba1\u7b97\u548c\u7535\u6c60\u8d44\u6e90\u5bf9\u5b9e\u73b0\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u76ee\u524d\u8be5\u9886\u57df\u7684\u5de5\u4f5c\u901a\u5e38\u662f\u79bb\u7ebf\u8fd0\u884c\u7684\uff0c\u8003\u8651\u5230\u65e0\u4eba\u673a\u7684\u9650\u5236\uff0c\u5f3a\u8c03\u9700\u8981\u80fd\u591f\u5b9e\u65f6\u8fdb\u884c\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u5b9e\u65f6\u56fe\u50cf\u5206\u7c7b\u548c\u706b\u707e\u5206\u5272\u6a21\u578b\u3002\u5b83\u5bf9\u4f7f\u7528 Jetson Nano P3450 \u7684\u786c\u4ef6\u52a0\u901f\u4ee5\u53ca NVIDIA \u7684\u9ad8\u6027\u80fd\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u5e93 TensorRT \u5bf9\u706b\u707e\u5206\u7c7b\u51c6\u786e\u6027\u548c\u901f\u5ea6\u7684\u5f71\u54cd\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\u3002\u8be5\u7814\u7a76\u5305\u62ec\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u3001\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff08AMP\uff09\u548c\u8bad\u7ec3\u540e\u673a\u5236\u7684\u5b9e\u73b0\uff0c\u5e76\u5c06\u5b83\u4eec\u4e0e\u706b\u707e\u5206\u5272\u548c\u5206\u7c7b\u7684\u6700\u65b0\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002\u6240\u6709\u5b9e\u9a8c\u90fd\u5229\u7528 FLAME \u6570\u636e\u96c6\u2014\u2014\u4f4e\u7a7a\u65e0\u4eba\u673a\u5728\u89c4\u5b9a\u7684\u68ee\u6797\u706b\u707e\u671f\u95f4\u6536\u96c6\u7684\u56fe\u50cf\u6570\u636e\u96c6\u3002\u8fd9\u9879\u5de5\u4f5c\u6709\u52a9\u4e8e\u6301\u7eed\u52aa\u529b\uff0c\u4e3a\u65e0\u4eba\u673a\u63d0\u4f9b\u5b9e\u65f6\u673a\u8f7d\u91ce\u706b\u68c0\u6d4b\u529f\u80fd\uff0c\u89e3\u51b3\u8fd9\u4e9b\u5173\u952e\u76d1\u6d4b\u7cfb\u7edf\u7684\u901f\u5ea6\u3001\u8ba1\u7b97\u548c\u80fd\u6e90\u9650\u5236\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u672a\u7ecf\u786c\u4ef6\u4f18\u5316\u7684\u7c7b\u4f3c\u6a21\u578b\u76f8\u6bd4\uff0c\u5206\u7c7b\u901f\u5ea6\u63d0\u9ad8\u4e86 13%\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u635f\u5931\u548c\u51c6\u786e\u7387\u5747\u5728\u539f\u59cb\u503c\u7684 1.225% \u4ee5\u5185\u3002|[2401.08105v1](http://arxiv.org/pdf/2401.08105v1)|null|\n", "2401.08083": "|**2024-01-16**|**UV-SAM: Adapting Segment Anything Model for Urban Village Identification**|UV-SAM\uff1a\u91c7\u7528\u5206\u6bb5\u4efb\u610f\u6a21\u578b\u8fdb\u884c\u57ce\u4e2d\u6751\u8bc6\u522b|Xin Zhang, Yu Liu, Yuming Lin, Qingming Liao, Yong Li|Urban villages, defined as informal residential areas in or around urban centers, are characterized by inadequate infrastructures and poor living conditions, closely related to the Sustainable Development Goals (SDGs) on poverty, adequate housing, and sustainable cities. Traditionally, governments heavily depend on field survey methods to monitor the urban villages, which however are time-consuming, labor-intensive, and possibly delayed. Thanks to widely available and timely updated satellite images, recent studies develop computer vision techniques to detect urban villages efficiently. However, existing studies either focus on simple urban village image classification or fail to provide accurate boundary information. To accurately identify urban village boundaries from satellite images, we harness the power of the vision foundation model and adapt the Segment Anything Model (SAM) to urban village segmentation, named UV-SAM. Specifically, UV-SAM first leverages a small-sized semantic segmentation model to produce mixed prompts for urban villages, including mask, bounding box, and image representations, which are then fed into SAM for fine-grained boundary identification. Extensive experimental results on two datasets in China demonstrate that UV-SAM outperforms existing baselines, and identification results over multiple years show that both the number and area of urban villages are decreasing over time, providing deeper insights into the development trends of urban villages and sheds light on the vision foundation models for sustainable cities. The dataset and codes of this study are available at https://github.com/tsinghua-fib-lab/UV-SAM.|\u57ce\u4e2d\u6751\u662f\u6307\u57ce\u5e02\u4e2d\u5fc3\u53ca\u5176\u5468\u8fb9\u7684\u975e\u6b63\u89c4\u5c45\u4f4f\u533a\uff0c\u5176\u7279\u70b9\u662f\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u3001\u751f\u6d3b\u6761\u4ef6\u5dee\uff0c\u4e0e\u8d2b\u56f0\u3001\u9002\u8db3\u4f4f\u623f\u548c\u53ef\u6301\u7eed\u57ce\u5e02\u7b49\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u5bc6\u5207\u76f8\u5173\u3002\u4f20\u7edf\u4e0a\uff0c\u653f\u5e9c\u4e25\u91cd\u4f9d\u8d56\u5b9e\u5730\u8c03\u67e5\u65b9\u6cd5\u6765\u76d1\u6d4b\u57ce\u4e2d\u6751\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u8d39\u65f6\u3001\u8d39\u529b\uff0c\u800c\u4e14\u53ef\u80fd\u4f1a\u9020\u6210\u5ef6\u8bef\u3002\u7531\u4e8e\u536b\u661f\u56fe\u50cf\u5e7f\u6cdb\u53ef\u7528\u4e14\u53ca\u65f6\u66f4\u65b0\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u5f00\u53d1\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u6765\u6709\u6548\u5730\u68c0\u6d4b\u57ce\u4e2d\u6751\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u8981\u4e48\u4fa7\u91cd\u4e8e\u7b80\u5355\u7684\u57ce\u4e2d\u6751\u56fe\u50cf\u5206\u7c7b\uff0c\u8981\u4e48\u65e0\u6cd5\u63d0\u4f9b\u51c6\u786e\u7684\u8fb9\u754c\u4fe1\u606f\u3002\u4e3a\u4e86\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u51c6\u786e\u8bc6\u522b\u57ce\u4e2d\u6751\u8fb9\u754c\uff0c\u6211\u4eec\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u529f\u80fd\uff0c\u5c06\u5206\u6bb5\u4efb\u610f\u6a21\u578b\uff08SAM\uff09\u5e94\u7528\u4e8e\u57ce\u4e2d\u6751\u5206\u5272\uff0c\u79f0\u4e3aUV-SAM\u3002\u5177\u4f53\u6765\u8bf4\uff0cUV-SAM \u9996\u5148\u5229\u7528\u5c0f\u578b\u8bed\u4e49\u5206\u5272\u6a21\u578b\u4e3a\u57ce\u4e2d\u6751\u751f\u6210\u6df7\u5408\u63d0\u793a\uff0c\u5305\u62ec\u63a9\u6a21\u3001\u8fb9\u754c\u6846\u548c\u56fe\u50cf\u8868\u793a\uff0c\u7136\u540e\u5c06\u5176\u8f93\u5165 SAM \u8fdb\u884c\u7ec6\u7c92\u5ea6\u8fb9\u754c\u8bc6\u522b\u3002\u5728\u4e2d\u56fd\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUV-SAM\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u4e14\u591a\u5e74\u7684\u8bc6\u522b\u7ed3\u679c\u8868\u660e\uff0c\u57ce\u4e2d\u6751\u7684\u6570\u91cf\u548c\u9762\u79ef\u90fd\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u800c\u51cf\u5c11\uff0c\u4e3a\u57ce\u4e2d\u6751\u548c\u68da\u5c4b\u7684\u53d1\u5c55\u8d8b\u52bf\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u9610\u8ff0\u53ef\u6301\u7eed\u57ce\u5e02\u7684\u613f\u666f\u57fa\u7840\u6a21\u578b\u3002\u672c\u7814\u7a76\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u53ef\u5728https://github.com/tsinghua-fib-lab/UV-SAM\u83b7\u53d6\u3002|[2401.08083v1](http://arxiv.org/pdf/2401.08083v1)|null|\n", "2401.08079": "|**2024-01-16**|**Adversarial Masking Contrastive Learning for vein recognition**|\u7528\u4e8e\u9759\u8109\u8bc6\u522b\u7684\u5bf9\u6297\u6027\u63a9\u853d\u5bf9\u6bd4\u5b66\u4e60|Huafeng Qin, Yiquan Wu, Mounim A. El-Yacoubi, Jun Wang, Guangxiang Yang|Vein recognition has received increasing attention due to its high security and privacy. Recently, deep neural networks such as Convolutional neural networks (CNN) and Transformers have been introduced for vein recognition and achieved state-of-the-art performance. Despite the recent advances, however, existing solutions for finger-vein feature extraction are still not optimal due to scarce training image samples. To overcome this problem, in this paper, we propose an adversarial masking contrastive learning (AMCL) approach, that generates challenging samples to train a more robust contrastive learning model for the downstream palm-vein recognition task, by alternatively optimizing the encoder in the contrastive learning model and a set of latent variables. First, a huge number of masks are generated to train a robust generative adversarial network (GAN). The trained generator transforms a latent variable from the latent variable space into a mask space. Then, we combine the trained generator with a contrastive learning model to obtain our AMCL, where the generator produces challenging masking images to increase the contrastive loss and the contrastive learning model is trained based on the harder images to learn a more robust feature representation. After training, the trained encoder in the contrastive learning model is combined with a classification layer to build a classifier, which is further fine-tuned on labeled training data for vein recognition. The experimental results on three databases demonstrate that our approach outperforms existing contrastive learning approaches in terms of improving identification accuracy of vein classifiers and achieves state-of-the-art recognition results.|\u9759\u8109\u8bc6\u522b\u56e0\u5176\u8f83\u9ad8\u7684\u5b89\u5168\u6027\u548c\u9690\u79c1\u6027\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u6700\u8fd1\uff0c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c Transformers \u7b49\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u88ab\u5f15\u5165\u9759\u8109\u8bc6\u522b\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u6700\u8fd1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u8bad\u7ec3\u56fe\u50cf\u6837\u672c\u7a00\u7f3a\uff0c\u73b0\u6709\u7684\u624b\u6307\u9759\u8109\u7279\u5f81\u63d0\u53d6\u89e3\u51b3\u65b9\u6848\u4ecd\u7136\u4e0d\u662f\u6700\u4f73\u7684\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u63a9\u853d\u5bf9\u6bd4\u5b66\u4e60\uff08AMCL\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u5730\u4f18\u5316\u5bf9\u6bd4\u4e2d\u7684\u7f16\u7801\u5668\uff0c\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u6837\u672c\uff0c\u4e3a\u4e0b\u6e38\u624b\u638c\u9759\u8109\u8bc6\u522b\u4efb\u52a1\u8bad\u7ec3\u66f4\u9c81\u68d2\u7684\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u3002\u5b66\u4e60\u6a21\u578b\u548c\u4e00\u7ec4\u6f5c\u5728\u53d8\u91cf\u3002\u9996\u5148\uff0c\u751f\u6210\u5927\u91cf\u63a9\u6a21\u6765\u8bad\u7ec3\u5f3a\u5927\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u3002\u7ecf\u8fc7\u8bad\u7ec3\u7684\u751f\u6210\u5668\u5c06\u6f5c\u5728\u53d8\u91cf\u4ece\u6f5c\u5728\u53d8\u91cf\u7a7a\u95f4\u8f6c\u6362\u4e3a\u63a9\u7801\u7a7a\u95f4\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u7ecf\u8fc7\u8bad\u7ec3\u7684\u751f\u6210\u5668\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u76f8\u7ed3\u5408\u4ee5\u83b7\u5f97\u6211\u4eec\u7684 AMCL\uff0c\u5176\u4e2d\u751f\u6210\u5668\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u63a9\u853d\u56fe\u50cf\u4ee5\u589e\u52a0\u5bf9\u6bd4\u635f\u5931\uff0c\u5e76\u4e14\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u57fa\u4e8e\u66f4\u96be\u7684\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u4ee5\u5b66\u4e60\u66f4\u9c81\u68d2\u7684\u7279\u5f81\u8868\u793a\u3002\u8bad\u7ec3\u540e\uff0c\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u4e2d\u7ecf\u8fc7\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u4e0e\u5206\u7c7b\u5c42\u76f8\u7ed3\u5408\u6765\u6784\u5efa\u5206\u7c7b\u5668\uff0c\u8be5\u5206\u7c7b\u5668\u5728\u6807\u8bb0\u7684\u8bad\u7ec3\u6570\u636e\u4e0a\u8fdb\u4e00\u6b65\u5fae\u8c03\u4ee5\u8fdb\u884c\u9759\u8109\u8bc6\u522b\u3002\u4e09\u4e2a\u6570\u636e\u5e93\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63d0\u9ad8\u9759\u8109\u5206\u7c7b\u5668\u7684\u8bc6\u522b\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8bc6\u522b\u7ed3\u679c\u3002|[2401.08079v1](http://arxiv.org/pdf/2401.08079v1)|null|\n", "2401.08066": "|**2024-01-16**|**Achieve Fairness without Demographics for Dermatological Disease Diagnosis**|\u65e0\u9700\u4eba\u53e3\u7edf\u8ba1\u5373\u53ef\u5b9e\u73b0\u76ae\u80a4\u75c5\u8bca\u65ad\u7684\u516c\u5e73\u6027|Ching-Hao Chiu, Yu-Jen Chen, Yawen Wu, Yiyu Shi, Tsung-Yi Ho|In medical image diagnosis, fairness has become increasingly crucial. Without bias mitigation, deploying unfair AI would harm the interests of the underprivileged population and potentially tear society apart. Recent research addresses prediction biases in deep learning models concerning demographic groups (e.g., gender, age, and race) by utilizing demographic (sensitive attribute) information during training. However, many sensitive attributes naturally exist in dermatological disease images. If the trained model only targets fairness for a specific attribute, it remains unfair for other attributes. Moreover, training a model that can accommodate multiple sensitive attributes is impractical due to privacy concerns. To overcome this, we propose a method enabling fair predictions for sensitive attributes during the testing phase without using such information during training. Inspired by prior work highlighting the impact of feature entanglement on fairness, we enhance the model features by capturing the features related to the sensitive and target attributes and regularizing the feature entanglement between corresponding classes. This ensures that the model can only classify based on the features related to the target attribute without relying on features associated with sensitive attributes, thereby improving fairness and accuracy. Additionally, we use disease masks from the Segment Anything Model (SAM) to enhance the quality of the learned feature. Experimental results demonstrate that the proposed method can improve fairness in classification compared to state-of-the-art methods in two dermatological disease datasets.|\u5728\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u4e2d\uff0c\u516c\u5e73\u6027\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5982\u679c\u4e0d\u51cf\u5c11\u504f\u89c1\uff0c\u90e8\u7f72\u4e0d\u516c\u5e73\u7684\u4eba\u5de5\u667a\u80fd\u5c06\u635f\u5bb3\u5f31\u52bf\u7fa4\u4f53\u7684\u5229\u76ca\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u793e\u4f1a\u5206\u88c2\u3002\u6700\u8fd1\u7684\u7814\u7a76\u901a\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u5229\u7528\u4eba\u53e3\u7edf\u8ba1\uff08\u654f\u611f\u5c5e\u6027\uff09\u4fe1\u606f\u6765\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u6709\u5173\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\uff08\u4f8b\u5982\u6027\u522b\u3001\u5e74\u9f84\u548c\u79cd\u65cf\uff09\u7684\u9884\u6d4b\u504f\u5dee\u3002\u7136\u800c\uff0c\u76ae\u80a4\u75c5\u56fe\u50cf\u4e2d\u81ea\u7136\u5b58\u5728\u8bb8\u591a\u654f\u611f\u5c5e\u6027\u3002\u5982\u679c\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u4ec5\u9488\u5bf9\u7279\u5b9a\u5c5e\u6027\u7684\u516c\u5e73\u6027\uff0c\u90a3\u4e48\u5bf9\u4e8e\u5176\u4ed6\u5c5e\u6027\u6765\u8bf4\u4ecd\u7136\u4e0d\u516c\u5e73\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u9690\u79c1\u95ee\u9898\uff0c\u8bad\u7ec3\u53ef\u4ee5\u5bb9\u7eb3\u591a\u4e2a\u654f\u611f\u5c5e\u6027\u7684\u6a21\u578b\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u6d4b\u8bd5\u9636\u6bb5\u5bf9\u654f\u611f\u5c5e\u6027\u8fdb\u884c\u516c\u5e73\u9884\u6d4b\uff0c\u800c\u65e0\u9700\u5728\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u6b64\u7c7b\u4fe1\u606f\u3002\u53d7\u5230\u5148\u524d\u5f3a\u8c03\u7279\u5f81\u7ea0\u7f20\u5bf9\u516c\u5e73\u6027\u5f71\u54cd\u7684\u5de5\u4f5c\u7684\u542f\u53d1\uff0c\u6211\u4eec\u901a\u8fc7\u6355\u83b7\u4e0e\u654f\u611f\u5c5e\u6027\u548c\u76ee\u6807\u5c5e\u6027\u76f8\u5173\u7684\u7279\u5f81\u5e76\u89c4\u8303\u76f8\u5e94\u7c7b\u4e4b\u95f4\u7684\u7279\u5f81\u7ea0\u7f20\u6765\u589e\u5f3a\u6a21\u578b\u7279\u5f81\u3002\u8fd9\u4fdd\u8bc1\u4e86\u6a21\u578b\u53ea\u80fd\u57fa\u4e8e\u4e0e\u76ee\u6807\u5c5e\u6027\u76f8\u5173\u7684\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u4e0e\u654f\u611f\u5c5e\u6027\u76f8\u5173\u7684\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u5206\u6bb5\u4efb\u610f\u6a21\u578b (SAM) \u4e2d\u7684\u75be\u75c5\u63a9\u6a21\u6765\u63d0\u9ad8\u5b66\u4e60\u7279\u5f81\u7684\u8d28\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e24\u4e2a\u76ae\u80a4\u75c5\u6570\u636e\u96c6\u4e2d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u5206\u7c7b\u7684\u516c\u5e73\u6027\u3002|[2401.08066v1](http://arxiv.org/pdf/2401.08066v1)|null|\n", "2401.08058": "|**2024-01-16**|**Toward Clinically Trustworthy Deep Learning: Applying Conformal Prediction to Intracranial Hemorrhage Detection**|\u8fc8\u5411\u4e34\u5e8a\u503c\u5f97\u4fe1\u8d56\u7684\u6df1\u5ea6\u5b66\u4e60\uff1a\u5c06\u9002\u5f62\u9884\u6d4b\u5e94\u7528\u4e8e\u9885\u5185\u51fa\u8840\u68c0\u6d4b|Cooper Gamble, Shahriar Faghani, Bradley J. Erickson|As deep learning (DL) continues to demonstrate its ability in radiological tasks, it is critical that we optimize clinical DL solutions to include safety. One of the principal concerns in the clinical adoption of DL tools is trust. This study aims to apply conformal prediction as a step toward trustworthiness for DL in radiology. This is a retrospective study of 491 non-contrast head CTs from the CQ500 dataset, in which three senior radiologists annotated slices containing intracranial hemorrhage (ICH). The dataset was split into definite and challenging subsets, where challenging images were defined to those in which there was disagreement among readers. A DL model was trained on 146 patients (10,815 slices) from the definite data (training dataset) to perform ICH localization and classification for five classes of ICH. To develop an uncertainty-aware DL model, 1,546 cases of the definite data (calibration dataset) was used for Mondrian conformal prediction (MCP). The uncertainty-aware DL model was tested on 8,401 definite and challenging cases to assess its ability to identify challenging cases. After the MCP procedure, the model achieved an F1 score of 0.920 for ICH classification on the test dataset. Additionally, it correctly identified 6,837 of the 6,856 total challenging cases as challenging (99.7% accuracy). It did not incorrectly label any definite cases as challenging. The uncertainty-aware ICH detector performs on par with state-of-the-art models. MCP's performance in detecting challenging cases demonstrates that it is useful in automated ICH detection and promising for trustworthiness in radiological DL.|\u968f\u7740\u6df1\u5ea6\u5b66\u4e60 (DL) \u4e0d\u65ad\u8bc1\u660e\u5176\u5728\u653e\u5c04\u5b66\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u6211\u4eec\u4f18\u5316\u4e34\u5e8a DL \u89e3\u51b3\u65b9\u6848\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u4e34\u5e8a\u91c7\u7528\u7684\u4e3b\u8981\u95ee\u9898\u4e4b\u4e00\u662f\u4fe1\u4efb\u3002\u672c\u7814\u7a76\u65e8\u5728\u5e94\u7528\u4fdd\u5f62\u9884\u6d4b\u4f5c\u4e3a\u653e\u5c04\u5b66\u4e2d DL \u53ef\u4fe1\u5ea6\u7684\u4e00\u6b65\u3002\u8fd9\u662f\u5bf9 CQ500 \u6570\u636e\u96c6\u4e2d 491 \u4e2a\u975e\u9020\u5f71\u5934\u90e8 CT \u7684\u56de\u987e\u6027\u7814\u7a76\uff0c\u5176\u4e2d\u4e09\u540d\u9ad8\u7ea7\u653e\u5c04\u79d1\u533b\u751f\u5bf9\u5305\u542b\u9885\u5185\u51fa\u8840 (ICH) \u7684\u5207\u7247\u8fdb\u884c\u4e86\u6ce8\u91ca\u3002\u6570\u636e\u96c6\u88ab\u5206\u4e3a\u660e\u786e\u7684\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u5b50\u96c6\uff0c\u5176\u4e2d\u5177\u6709\u6311\u6218\u6027\u7684\u56fe\u50cf\u88ab\u5b9a\u4e49\u4e3a\u8bfb\u8005\u4e4b\u95f4\u5b58\u5728\u5206\u6b67\u7684\u56fe\u50cf\u3002\u6839\u636e\u786e\u5b9a\u6570\u636e\uff08\u8bad\u7ec3\u6570\u636e\u96c6\uff09\u5bf9 146 \u540d\u60a3\u8005\uff0810,815 \u4e2a\u5207\u7247\uff09\u8bad\u7ec3 DL \u6a21\u578b\uff0c\u4ee5\u5bf9\u4e94\u7c7b ICH \u8fdb\u884c ICH \u5b9a\u4f4d\u548c\u5206\u7c7b\u3002\u4e3a\u4e86\u5f00\u53d1\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528\u4e86 1,546 \u4f8b\u786e\u5b9a\u6570\u636e\uff08\u6821\u51c6\u6570\u636e\u96c6\uff09\u8fdb\u884c\u8499\u5fb7\u91cc\u5b89\u4fdd\u5f62\u9884\u6d4b (MCP)\u3002\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728 8,401 \u4e2a\u660e\u786e\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u6848\u4f8b\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5176\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\u7684\u6848\u4f8b\u7684\u80fd\u529b\u3002\u7ecf\u8fc7 MCP \u7a0b\u5e8f\u540e\uff0c\u8be5\u6a21\u578b\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u7684 ICH \u5206\u7c7b F1 \u5f97\u5206\u4e3a 0.920\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u6b63\u786e\u5730\u5c06 6,856 \u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6848\u4f8b\u4e2d\u7684 6,837 \u4e2a\u8bc6\u522b\u4e3a\u5177\u6709\u6311\u6218\u6027\uff08\u51c6\u786e\u5ea6\u4e3a 99.7%\uff09\u3002\u5b83\u5e76\u6ca1\u6709\u9519\u8bef\u5730\u5c06\u4efb\u4f55\u660e\u786e\u7684\u6848\u4f8b\u6807\u8bb0\u4e3a\u5177\u6709\u6311\u6218\u6027\u3002\u4e0d\u786e\u5b9a\u6027\u611f\u77e5 ICH \u68c0\u6d4b\u5668\u7684\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u76f8\u5f53\u3002 MCP \u5728\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\u7684\u75c5\u4f8b\u65b9\u9762\u7684\u8868\u73b0\u8868\u660e\uff0c\u5b83\u5728\u81ea\u52a8 ICH \u68c0\u6d4b\u4e2d\u975e\u5e38\u6709\u7528\uff0c\u5e76\u6709\u671b\u63d0\u9ad8\u653e\u5c04\u5b66 DL \u7684\u53ef\u4fe1\u5ea6\u3002|[2401.08058v1](http://arxiv.org/pdf/2401.08058v1)|null|\n", "2401.08056": "|**2024-01-16**|**Robust Tiny Object Detection in Aerial Images amidst Label Noise**|\u6807\u7b7e\u566a\u58f0\u4e2d\u822a\u7a7a\u56fe\u50cf\u4e2d\u7a33\u5065\u7684\u5fae\u5c0f\u7269\u4f53\u68c0\u6d4b|Haoran Zhu, Chang Xu, Wen Yang, Ruixiang Zhang, Yan Zhang, Gui-Song Xia|Precise detection of tiny objects in remote sensing imagery remains a significant challenge due to their limited visual information and frequent occurrence within scenes. This challenge is further exacerbated by the practical burden and inherent errors associated with manual annotation: annotating tiny objects is laborious and prone to errors (i.e., label noise). Training detectors for such objects using noisy labels often leads to suboptimal performance, with networks tending to overfit on noisy labels. In this study, we address the intricate issue of tiny object detection under noisy label supervision. We systematically investigate the impact of various types of noise on network training, revealing the vulnerability of object detectors to class shifts and inaccurate bounding boxes for tiny objects. To mitigate these challenges, we propose a DeNoising Tiny Object Detector (DN-TOD), which incorporates a Class-aware Label Correction (CLC) scheme to address class shifts and a Trend-guided Learning Strategy (TLS) to handle bounding box noise. CLC mitigates inaccurate class supervision by identifying and filtering out class-shifted positive samples, while TLS reduces noisy box-induced erroneous supervision through sample reweighting and bounding box regeneration. Additionally, Our method can be seamlessly integrated into both one-stage and two-stage object detection pipelines. Comprehensive experiments conducted on synthetic (i.e., noisy AI-TOD-v2.0 and DOTA-v2.0) and real-world (i.e., AI-TOD) noisy datasets demonstrate the robustness of DN-TOD under various types of label noise. Notably, when applied to the strong baseline RFLA, DN-TOD exhibits a noteworthy performance improvement of 4.9 points under 40% mixed noise. Datasets, codes, and models will be made publicly available.|\u7531\u4e8e\u89c6\u89c9\u4fe1\u606f\u6709\u9650\u4e14\u5728\u573a\u666f\u4e2d\u9891\u7e41\u51fa\u73b0\uff0c\u9065\u611f\u56fe\u50cf\u4e2d\u5fae\u5c0f\u7269\u4f53\u7684\u7cbe\u786e\u68c0\u6d4b\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4e0e\u624b\u52a8\u6ce8\u91ca\u76f8\u5173\u7684\u5b9e\u9645\u8d1f\u62c5\u548c\u56fa\u6709\u9519\u8bef\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u6311\u6218\uff1a\u6ce8\u91ca\u5fae\u5c0f\u5bf9\u8c61\u975e\u5e38\u8d39\u529b\u4e14\u5bb9\u6613\u51fa\u9519\uff08\u5373\u6807\u7b7e\u566a\u58f0\uff09\u3002\u4f7f\u7528\u566a\u58f0\u6807\u7b7e\u8bad\u7ec3\u6b64\u7c7b\u5bf9\u8c61\u7684\u68c0\u6d4b\u5668\u901a\u5e38\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\uff0c\u7f51\u7edc\u5f80\u5f80\u4f1a\u5728\u566a\u58f0\u6807\u7b7e\u4e0a\u8fc7\u5ea6\u62df\u5408\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u566a\u58f0\u6807\u7b7e\u76d1\u7763\u4e0b\u5fae\u5c0f\u7269\u4f53\u68c0\u6d4b\u7684\u590d\u6742\u95ee\u9898\u3002\u6211\u4eec\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5404\u79cd\u7c7b\u578b\u7684\u566a\u58f0\u5bf9\u7f51\u7edc\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u5bf9\u8c61\u68c0\u6d4b\u5668\u5bf9\u7c7b\u8f6c\u79fb\u548c\u5fae\u5c0f\u5bf9\u8c61\u4e0d\u51c6\u786e\u7684\u8fb9\u754c\u6846\u7684\u8106\u5f31\u6027\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u566a\u5fae\u578b\u5bf9\u8c61\u68c0\u6d4b\u5668\uff08DN-TOD\uff09\uff0c\u5b83\u7ed3\u5408\u4e86\u7c7b\u611f\u77e5\u6807\u7b7e\u6821\u6b63\uff08CLC\uff09\u65b9\u6848\u6765\u89e3\u51b3\u7c7b\u8f6c\u79fb\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u8d8b\u52bf\u5f15\u5bfc\u5b66\u4e60\u7b56\u7565\uff08TLS\uff09\u6765\u5904\u7406\u8fb9\u754c\u6846\u566a\u58f0\u3002 CLC \u901a\u8fc7\u8bc6\u522b\u548c\u8fc7\u6ee4\u6389\u7c7b\u8f6c\u79fb\u7684\u6b63\u6837\u672c\u6765\u51cf\u8f7b\u4e0d\u51c6\u786e\u7684\u7c7b\u76d1\u7763\uff0c\u800c TLS \u901a\u8fc7\u6837\u672c\u91cd\u65b0\u52a0\u6743\u548c\u8fb9\u754c\u6846\u91cd\u65b0\u751f\u6210\u6765\u51cf\u5c11\u566a\u58f0\u6846\u5f15\u8d77\u7684\u9519\u8bef\u76d1\u7763\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u4e00\u7ea7\u548c\u4e24\u7ea7\u76ee\u6807\u68c0\u6d4b\u7ba1\u9053\u4e2d\u3002\u5bf9\u5408\u6210\uff08\u5373\u566a\u58f0 AI-TOD-v2.0 \u548c DOTA-v2.0\uff09\u548c\u73b0\u5b9e\u4e16\u754c\uff08\u5373 AI-TOD\uff09\u566a\u58f0\u6570\u636e\u96c6\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86 DN-TOD \u5728\u5404\u79cd\u7c7b\u578b\u7684\u6807\u7b7e\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u5e94\u7528\u4e8e\u5f3a\u57fa\u7ebf RFLA \u65f6\uff0cDN-TOD \u5728 40% \u6df7\u5408\u566a\u58f0\u4e0b\u8868\u73b0\u51fa 4.9 \u4e2a\u767e\u5206\u70b9\u7684\u663e\u7740\u6027\u80fd\u6539\u8fdb\u3002\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002|[2401.08056v1](http://arxiv.org/pdf/2401.08056v1)|null|\n", "2401.08036": "|**2024-01-16**|**3D Lane Detection from Front or Surround-View using Joint-Modeling & Matching**|\u4f7f\u7528\u8054\u5408\u5efa\u6a21\u548c\u5339\u914d\u4ece\u524d\u89c6\u6216\u73af\u89c6\u8fdb\u884c 3D \u8f66\u9053\u68c0\u6d4b|Haibin Zhou, Jun Chang, Tao Lu, Huabing Zhou|3D lanes offer a more comprehensive understanding of the road surface geometry than 2D lanes, thereby providing crucial references for driving decisions and trajectory planning. While many efforts aim to improve prediction accuracy, we recognize that an efficient network can bring results closer to lane modeling. However, if the modeling data is imprecise, the results might not accurately capture the real-world scenario. Therefore, accurate lane modeling is essential to align prediction results closely with the environment. This study centers on efficient and accurate lane modeling, proposing a joint modeling approach that combines Bezier curves and interpolation methods. Furthermore, based on this lane modeling approach, we developed a Global2Local Lane Matching method with Bezier Control-Point and Key-Point, which serve as a comprehensive solution that leverages hierarchical features with two mathematical models to ensure a precise match. We also introduce a novel 3D Spatial Constructor, representing an exploration of 3D surround-view lane detection research. The framework is suitable for front-view or surround-view 3D lane detection. By directly outputting the key points of lanes in 3D space, it overcomes the limitations of anchor-based methods, enabling accurate prediction of closed-loop or U-shaped lanes and effective adaptation to complex road conditions. This innovative method establishes a new benchmark in front-view 3D lane detection on the Openlane dataset and achieves competitive performance in surround-view 2D lane detection on the Argoverse2 dataset.|\u4e0e 2D \u8f66\u9053\u76f8\u6bd4\uff0c3D \u8f66\u9053\u53ef\u4ee5\u66f4\u5168\u9762\u5730\u4e86\u89e3\u8def\u9762\u51e0\u4f55\u5f62\u72b6\uff0c\u4ece\u800c\u4e3a\u9a7e\u9a76\u51b3\u7b56\u548c\u8f68\u8ff9\u89c4\u5212\u63d0\u4f9b\u91cd\u8981\u53c2\u8003\u3002\u867d\u7136\u8bb8\u591a\u52aa\u529b\u65e8\u5728\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4f46\u6211\u4eec\u8ba4\u8bc6\u5230\u9ad8\u6548\u7684\u7f51\u7edc\u53ef\u4ee5\u4f7f\u7ed3\u679c\u66f4\u63a5\u8fd1\u8f66\u9053\u5efa\u6a21\u3002\u7136\u800c\uff0c\u5982\u679c\u5efa\u6a21\u6570\u636e\u4e0d\u7cbe\u786e\uff0c\u7ed3\u679c\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u7684\u573a\u666f\u3002\u56e0\u6b64\uff0c\u51c6\u786e\u7684\u8f66\u9053\u5efa\u6a21\u5bf9\u4e8e\u4f7f\u9884\u6d4b\u7ed3\u679c\u4e0e\u73af\u5883\u7d27\u5bc6\u7ed3\u5408\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u4ee5\u9ad8\u6548\u3001\u51c6\u786e\u7684\u8f66\u9053\u5efa\u6a21\u4e3a\u4e2d\u5fc3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8d1d\u585e\u5c14\u66f2\u7ebf\u548c\u63d2\u503c\u65b9\u6cd5\u7684\u8054\u5408\u5efa\u6a21\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u8fd9\u79cd\u8f66\u9053\u5efa\u6a21\u65b9\u6cd5\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u5177\u6709\u8d1d\u585e\u5c14\u66f2\u7ebf\u63a7\u5236\u70b9\u548c\u5173\u952e\u70b9\u7684\u5168\u5c40\u5230\u5c40\u90e8\u8f66\u9053\u5339\u914d\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u7efc\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u4e24\u4e2a\u6570\u5b66\u6a21\u578b\u7684\u5c42\u6b21\u7279\u5f81\u6765\u786e\u4fdd\u7cbe\u786e\u5339\u914d\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684 3D \u7a7a\u95f4\u6784\u9020\u5668\uff0c\u4ee3\u8868\u4e86\u5bf9 3D \u73af\u89c6\u8f66\u9053\u68c0\u6d4b\u7814\u7a76\u7684\u63a2\u7d22\u3002\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u524d\u89c6\u6216\u73af\u89c6 3D \u8f66\u9053\u68c0\u6d4b\u3002\u901a\u8fc7\u76f4\u63a5\u8f93\u51fa3D\u7a7a\u95f4\u4e2d\u7684\u8f66\u9053\u5173\u952e\u70b9\uff0c\u514b\u670d\u4e86\u57fa\u4e8eanchor\u7684\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u95ed\u73af\u6216U\u5f62\u8f66\u9053\uff0c\u6709\u6548\u9002\u5e94\u590d\u6742\u8def\u51b5\u3002\u8fd9\u79cd\u521b\u65b0\u65b9\u6cd5\u5728 Openlane \u6570\u636e\u96c6\u4e0a\u5efa\u7acb\u4e86\u524d\u89c6 3D \u8f66\u9053\u68c0\u6d4b\u7684\u65b0\u57fa\u51c6\uff0c\u5e76\u5728 Argoverse2 \u6570\u636e\u96c6\u4e0a\u7684\u73af\u89c6 2D \u8f66\u9053\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002|[2401.08036v1](http://arxiv.org/pdf/2401.08036v1)|null|\n", "2401.08035": "|**2024-01-16**|**BanglaNet: Bangla Handwritten Character Recognition using Ensembling of Convolutional Neural Network**|BanglaNet\uff1a\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u8fdb\u884c\u5b5f\u52a0\u62c9\u8bed\u624b\u5199\u5b57\u7b26\u8bc6\u522b|Chandrika Saha, Md. Mostafijur Rahman|Handwritten character recognition is a crucial task because of its abundant applications. The recognition task of Bangla handwritten characters is especially challenging because of the cursive nature of Bangla characters and the presence of compound characters with more than one way of writing. In this paper, a classification model based on the ensembling of several Convolutional Neural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic characters, compound characters, numerals, and modifiers. Three different models based on the idea of state-of-the-art CNN models like Inception, ResNet, and DenseNet have been trained with both augmented and non-augmented inputs. Finally, all these models are averaged or ensembled to get the finishing model. Rigorous experimentation on three benchmark Bangla handwritten characters datasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited significant recognition accuracies compared to some recent CNN-based research. The top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and the top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb, BanglaLekha-Isolated, and Ekush datasets respectively.|\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u56e0\u5176\u4e30\u5bcc\u7684\u5e94\u7528\u800c\u6210\u4e3a\u4e00\u9879\u81f3\u5173\u91cd\u8981\u7684\u4efb\u52a1\u3002\u7531\u4e8e\u5b5f\u52a0\u62c9\u8bed\u5b57\u7b26\u7684\u8349\u4e66\u6027\u8d28\u4ee5\u53ca\u5b58\u5728\u591a\u79cd\u4e66\u5199\u65b9\u5f0f\u7684\u590d\u5408\u5b57\u7b26\uff0c\u5b5f\u52a0\u62c9\u8bed\u624b\u5199\u5b57\u7b26\u7684\u8bc6\u522b\u4efb\u52a1\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u96c6\u6210\u7684\u5206\u7c7b\u6a21\u578b\uff0c\u5373 BanglaNet\uff0c\u5bf9\u5b5f\u52a0\u62c9\u8bed\u57fa\u672c\u5b57\u7b26\u3001\u590d\u5408\u5b57\u7b26\u3001\u6570\u5b57\u548c\u4fee\u9970\u8bed\u8fdb\u884c\u5206\u7c7b\u3002\u57fa\u4e8e\u6700\u5148\u8fdb\u7684 CNN \u6a21\u578b\uff08\u5982 Inception\u3001ResNet \u548c DenseNet\uff09\u7406\u5ff5\u7684\u4e09\u79cd\u4e0d\u540c\u6a21\u578b\u5df2\u7ecf\u4f7f\u7528\u589e\u5f3a\u548c\u975e\u589e\u5f3a\u8f93\u5165\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002\u6700\u540e\uff0c\u5bf9\u6240\u6709\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u5e73\u5747\u6216\u96c6\u6210\u4ee5\u83b7\u5f97\u6700\u7ec8\u6a21\u578b\u3002\u4e0e\u6700\u8fd1\u4e00\u4e9b\u57fa\u4e8e CNN \u7684\u7814\u7a76\u76f8\u6bd4\uff0c\u5bf9\u4e09\u4e2a\u57fa\u51c6\u5b5f\u52a0\u62c9\u624b\u5199\u5b57\u7b26\u6570\u636e\u96c6\uff08CMATERdb\u3001BanglaLekha-Isolated \u548c Ekush\uff09\u8fdb\u884c\u7684\u4e25\u683c\u5b9e\u9a8c\u663e\u793a\u51fa\u663e\u7740\u7684\u8bc6\u522b\u7cbe\u5ea6\u3002 CMATERdb\u3001BanglaLekha-Isolated \u548c Ekush \u6570\u636e\u96c6\u7684 top-1 \u8bc6\u522b\u51c6\u786e\u7387\u5206\u522b\u4e3a 98.40%\u300197.65% \u548c 97.32%\uff0ctop-3 \u51c6\u786e\u7387\u5206\u522b\u4e3a 99.79%\u300199.74% \u548c 99.56%\u3002|[2401.08035v1](http://arxiv.org/pdf/2401.08035v1)|null|\n", "2401.08017": "|**2024-01-16**|**Small Object Detection by DETR via Information Augmentation and Adaptive Feature Fusion**|DETR \u901a\u8fc7\u4fe1\u606f\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u8fdb\u884c\u5c0f\u7269\u4f53\u68c0\u6d4b|Ji Huang, Hui Wang|The main challenge for small object detection algorithms is to ensure accuracy while pursuing real-time performance. The RT-DETR model performs well in real-time object detection, but performs poorly in small object detection accuracy. In order to compensate for the shortcomings of the RT-DETR model in small object detection, two key improvements are proposed in this study. Firstly, The RT-DETR utilises a Transformer that receives input solely from the final layer of Backbone features. This means that the Transformer's input only receives semantic information from the highest level of abstraction in the Deep Network, and ignores detailed information such as edges, texture or color gradients that are critical to the location of small objects at lower levels of abstraction. Including only deep features can introduce additional background noise. This can have a negative impact on the accuracy of small object detection. To address this issue, we propose the fine-grained path augmentation method. This method helps to locate small objects more accurately by providing detailed information to the deep network. So, the input to the transformer contains both semantic and detailed information. Secondly, In RT-DETR, the decoder takes feature maps of different levels as input after concatenating them with equal weight. However, this operation is not effective in dealing with the complex relationship of multi-scale information captured by feature maps of different sizes. Therefore, we propose an adaptive feature fusion algorithm that assigns learnable parameters to each feature map from different levels. This allows the model to adaptively fuse feature maps from different levels and effectively integrate feature information from different scales. This enhances the model's ability to capture object features at different scales, thereby improving the accuracy of detecting small objects.|\u5c0f\u7269\u4f53\u68c0\u6d4b\u7b97\u6cd5\u7684\u4e3b\u8981\u6311\u6218\u662f\u5728\u8ffd\u6c42\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u4fdd\u8bc1\u7cbe\u5ea6\u3002 RT-DETR\u6a21\u578b\u5728\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5c0f\u76ee\u6807\u68c0\u6d4b\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u8f83\u5dee\u3002\u4e3a\u4e86\u5f25\u8865RT-DETR\u6a21\u578b\u5728\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u7684\u7f3a\u70b9\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u4e2a\u5173\u952e\u6539\u8fdb\u3002\u9996\u5148\uff0cRT-DETR \u4f7f\u7528\u4ec5\u4ece Backbone \u529f\u80fd\u7684\u6700\u540e\u4e00\u5c42\u63a5\u6536\u8f93\u5165\u7684 Transformer\u3002\u8fd9\u610f\u5473\u7740 Transformer \u7684\u8f93\u5165\u4ec5\u63a5\u6536\u6765\u81ea\u6df1\u5ea6\u7f51\u7edc\u4e2d\u6700\u9ad8\u62bd\u8c61\u7ea7\u522b\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u800c\u5ffd\u7565\u5bf9\u8f83\u4f4e\u62bd\u8c61\u7ea7\u522b\u7684\u5c0f\u7269\u4f53\u7684\u4f4d\u7f6e\u81f3\u5173\u91cd\u8981\u7684\u8fb9\u7f18\u3001\u7eb9\u7406\u6216\u989c\u8272\u6e10\u53d8\u7b49\u8be6\u7ec6\u4fe1\u606f\u3002\u4ec5\u5305\u542b\u6df1\u5c42\u7279\u5f81\u4f1a\u5f15\u5165\u989d\u5916\u7684\u80cc\u666f\u566a\u58f0\u3002\u8fd9\u4f1a\u5bf9\u5c0f\u7269\u4f53\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u8def\u5f84\u589e\u5f3a\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5411\u6df1\u5c42\u7f51\u7edc\u63d0\u4f9b\u8be6\u7ec6\u4fe1\u606f\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u5b9a\u4f4d\u5c0f\u7269\u4f53\u3002\u56e0\u6b64\uff0c\u53d8\u538b\u5668\u7684\u8f93\u5165\u5305\u542b\u8bed\u4e49\u548c\u8be6\u7ec6\u4fe1\u606f\u3002\u5176\u6b21\uff0c\u5728RT-DETR\u4e2d\uff0c\u89e3\u7801\u5668\u5c06\u4e0d\u540c\u7ea7\u522b\u7684\u7279\u5f81\u56fe\u7b49\u6743\u91cd\u8fde\u63a5\u540e\u4f5c\u4e3a\u8f93\u5165\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u64cd\u4f5c\u5728\u5904\u7406\u4e0d\u540c\u5c3a\u5bf8\u7684\u7279\u5f81\u56fe\u6355\u83b7\u7684\u591a\u5c3a\u5ea6\u4fe1\u606f\u7684\u590d\u6742\u5173\u7cfb\u65f6\u5e76\u4e0d\u6709\u6548\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u7b97\u6cd5\uff0c\u5c06\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u5206\u914d\u7ed9\u4e0d\u540c\u7ea7\u522b\u7684\u6bcf\u4e2a\u7279\u5f81\u56fe\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u5730\u878d\u5408\u4e0d\u540c\u7ea7\u522b\u7684\u7279\u5f81\u56fe\uff0c\u5e76\u6709\u6548\u5730\u96c6\u6210\u4e0d\u540c\u5c3a\u5ea6\u7684\u7279\u5f81\u4fe1\u606f\u3002\u8fd9\u589e\u5f3a\u4e86\u6a21\u578b\u6355\u83b7\u4e0d\u540c\u5c3a\u5ea6\u7269\u4f53\u7279\u5f81\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u6d4b\u5c0f\u7269\u4f53\u7684\u51c6\u786e\u6027\u3002|[2401.08017v1](http://arxiv.org/pdf/2401.08017v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2401.08178": "|**2024-01-16**|**Key-point Guided Deformable Image Manipulation Using Diffusion Model**|\u4f7f\u7528\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u70b9\u5f15\u5bfc\u53ef\u53d8\u5f62\u56fe\u50cf\u5904\u7406|Seok-Hwan Oh, Guil Jung, Myeong-Gee Kim, Sang-Yun Kim, Young-Min Kim, Hyeon-Jik Lee, Hyuk-Sool Kwon, Hyeon-Min Bae|In this paper, we introduce a Key-point-guided Diffusion probabilistic Model (KDM) that gains precise control over images by manipulating the object's key-point. We propose a two-stage generative model incorporating an optical flow map as an intermediate output. By doing so, a dense pixel-wise understanding of the semantic relation between the image and sparse key point is configured, leading to more realistic image generation. Additionally, the integration of optical flow helps regulate the inter-frame variance of sequential images, demonstrating an authentic sequential image generation. The KDM is evaluated with diverse key-point conditioned image synthesis tasks, including facial image generation, human pose synthesis, and echocardiography video prediction, demonstrating the KDM is proving consistency enhanced and photo-realistic images compared with state-of-the-art models.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5173\u952e\u70b9\u5f15\u5bfc\u7684\u6269\u6563\u6982\u7387\u6a21\u578b\uff08KDM\uff09\uff0c\u5b83\u901a\u8fc7\u64cd\u7eb5\u5bf9\u8c61\u7684\u5173\u952e\u70b9\u6765\u83b7\u5f97\u5bf9\u56fe\u50cf\u7684\u7cbe\u786e\u63a7\u5236\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u751f\u6210\u6a21\u578b\uff0c\u5176\u4e2d\u5305\u542b\u5149\u6d41\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u8f93\u51fa\u3002\u901a\u8fc7\u8fd9\u6837\u505a\uff0c\u53ef\u4ee5\u914d\u7f6e\u5bf9\u56fe\u50cf\u548c\u7a00\u758f\u5173\u952e\u70b9\u4e4b\u95f4\u8bed\u4e49\u5173\u7cfb\u7684\u5bc6\u96c6\u50cf\u7d20\u7ea7\u7406\u89e3\uff0c\u4ece\u800c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u5149\u6d41\u7684\u96c6\u6210\u6709\u52a9\u4e8e\u8c03\u8282\u987a\u5e8f\u56fe\u50cf\u7684\u5e27\u95f4\u65b9\u5dee\uff0c\u5c55\u793a\u771f\u5b9e\u7684\u987a\u5e8f\u56fe\u50cf\u751f\u6210\u3002 KDM \u901a\u8fc7\u5404\u79cd\u5173\u952e\u70b9\u6761\u4ef6\u56fe\u50cf\u5408\u6210\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u62ec\u9762\u90e8\u56fe\u50cf\u751f\u6210\u3001\u4eba\u4f53\u59ff\u52bf\u5408\u6210\u548c\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u9884\u6d4b\uff0c\u8bc1\u660e\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u76f8\u6bd4\uff0cKDM \u5177\u6709\u4e00\u81f4\u6027\u589e\u5f3a\u548c\u903c\u771f\u7684\u56fe\u50cf\u3002|[2401.08178v1](http://arxiv.org/pdf/2401.08178v1)|null|\n", "2401.08099": "|**2024-01-16**|**Inpainting Normal Maps for Lightstage data**|\u4fee\u590d Lightstage \u6570\u636e\u7684\u6cd5\u7ebf\u8d34\u56fe|Hancheng Zuo, Bernard Tiddeman|This study introduces a novel method for inpainting normal maps using a generative adversarial network (GAN). Normal maps, often derived from a lightstage, are crucial in performance capture but can have obscured areas due to movement (e.g., by arms, hair, or props). Inpainting fills these missing areas with plausible data. Our approach extends previous general image inpainting techniques, employing a bow tie-like generator network and a discriminator network, with alternating training phases. The generator aims to synthesize images aligning with the ground truth and deceive the discriminator, which differentiates between real and processed images. Periodically, the discriminator undergoes retraining to enhance its ability to identify processed images. Importantly, our method adapts to the unique characteristics of normal map data, necessitating modifications to the loss function. We utilize a cosine loss instead of mean squared error loss for generator training. Limited training data availability, even with synthetic datasets, demands significant augmentation, considering the specific nature of the input data. This includes appropriate image flipping and in-plane rotations to accurately alter normal vectors. Throughout training, we monitored key metrics such as average loss, Structural Similarity Index Measure (SSIM), and Peak Signal-to-Noise Ratio (PSNR) for the generator, along with average loss and accuracy for the discriminator. Our findings suggest that the proposed model effectively generates high-quality, realistic inpainted normal maps, suitable for performance capture applications. These results establish a foundation for future research, potentially involving more advanced networks and comparisons with inpainting of source images used to create the normal maps.|\u8fd9\u9879\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u4fee\u590d\u6cd5\u7ebf\u8d34\u56fe\u7684\u65b0\u65b9\u6cd5\u3002\u6cd5\u7ebf\u8d34\u56fe\u901a\u5e38\u6e90\u81ea\u5149\u573a\uff0c\u5bf9\u4e8e\u8868\u6f14\u6355\u6349\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53ef\u80fd\u4f1a\u56e0\u8fd0\u52a8\uff08\u4f8b\u5982\uff0c\u624b\u81c2\u3001\u5934\u53d1\u6216\u9053\u5177\uff09\u800c\u906e\u6321\u533a\u57df\u3002\u4fee\u590d\u7528\u53ef\u4fe1\u7684\u6570\u636e\u586b\u5145\u4e86\u8fd9\u4e9b\u7f3a\u5931\u7684\u533a\u57df\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6269\u5c55\u4e86\u4ee5\u524d\u7684\u901a\u7528\u56fe\u50cf\u4fee\u590d\u6280\u672f\uff0c\u91c7\u7528\u7c7b\u4f3c\u9886\u7ed3\u7684\u751f\u6210\u5668\u7f51\u7edc\u548c\u9274\u522b\u5668\u7f51\u7edc\uff0c\u5e76\u5177\u6709\u4ea4\u66ff\u7684\u8bad\u7ec3\u9636\u6bb5\u3002\u751f\u6210\u5668\u7684\u76ee\u7684\u662f\u5408\u6210\u4e0e\u771f\u5b9e\u60c5\u51b5\u4e00\u81f4\u7684\u56fe\u50cf\u5e76\u6b3a\u9a97\u9274\u522b\u5668\uff0c\u9274\u522b\u5668\u533a\u5206\u771f\u5b9e\u56fe\u50cf\u548c\u5904\u7406\u540e\u7684\u56fe\u50cf\u3002\u9274\u522b\u5668\u5b9a\u671f\u63a5\u53d7\u518d\u8bad\u7ec3\uff0c\u4ee5\u589e\u5f3a\u5176\u8bc6\u522b\u5904\u7406\u56fe\u50cf\u7684\u80fd\u529b\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u9002\u5e94\u4e86\u6cd5\u7ebf\u8d34\u56fe\u6570\u636e\u7684\u72ec\u7279\u7279\u5f81\uff0c\u9700\u8981\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4fee\u6539\u3002\u6211\u4eec\u4f7f\u7528\u4f59\u5f26\u635f\u5931\u800c\u4e0d\u662f\u5747\u65b9\u8bef\u5dee\u635f\u5931\u6765\u8fdb\u884c\u751f\u6210\u5668\u8bad\u7ec3\u3002\u8003\u8651\u5230\u8f93\u5165\u6570\u636e\u7684\u5177\u4f53\u6027\u8d28\uff0c\u5373\u4f7f\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u7528\u6027\u4e5f\u6709\u9650\uff0c\u9700\u8981\u5927\u91cf\u589e\u5f3a\u3002\u8fd9\u5305\u62ec\u9002\u5f53\u7684\u56fe\u50cf\u7ffb\u8f6c\u548c\u5e73\u9762\u5185\u65cb\u8f6c\u4ee5\u51c6\u786e\u5730\u6539\u53d8\u6cd5\u5411\u77e2\u91cf\u3002\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u76d1\u63a7\u4e86\u751f\u6210\u5668\u7684\u5e73\u5747\u635f\u5931\u3001\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u6d4b\u91cf\uff08SSIM\uff09\u548c\u5cf0\u503c\u4fe1\u566a\u6bd4\uff08PSNR\uff09\u7b49\u5173\u952e\u6307\u6807\uff0c\u4ee5\u53ca\u9274\u522b\u5668\u7684\u5e73\u5747\u635f\u5931\u548c\u51c6\u786e\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u903c\u771f\u7684\u4fee\u590d\u6cd5\u7ebf\u8d34\u56fe\uff0c\u9002\u5408\u8868\u6f14\u6355\u6349\u5e94\u7528\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u53ef\u80fd\u6d89\u53ca\u66f4\u5148\u8fdb\u7684\u7f51\u7edc\u4ee5\u53ca\u4e0e\u7528\u4e8e\u521b\u5efa\u6cd5\u7ebf\u8d34\u56fe\u7684\u6e90\u56fe\u50cf\u4fee\u590d\u7684\u6bd4\u8f83\u3002|[2401.08099v1](http://arxiv.org/pdf/2401.08099v1)|null|\n", "2401.08049": "|**2024-01-16**|**EmoTalker: Emotionally Editable Talking Face Generation via Diffusion Model**|EmoTalker\uff1a\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u60c5\u611f\u53ef\u7f16\u8f91\u7684\u8bf4\u8bdd\u9762\u5b54|Bingyuan Zhang, Xulong Zhang, Ning Cheng, Jun Yu, Jing Xiao, Jianzong Wang|In recent years, the field of talking faces generation has attracted considerable attention, with certain methods adept at generating virtual faces that convincingly imitate human expressions. However, existing methods face challenges related to limited generalization, particularly when dealing with challenging identities. Furthermore, methods for editing expressions are often confined to a singular emotion, failing to adapt to intricate emotions. To overcome these challenges, this paper proposes EmoTalker, an emotionally editable portraits animation approach based on the diffusion model. EmoTalker modifies the denoising process to ensure preservation of the original portrait's identity during inference. To enhance emotion comprehension from text input, Emotion Intensity Block is introduced to analyze fine-grained emotions and strengths derived from prompts. Additionally, a crafted dataset is harnessed to enhance emotion comprehension within prompts. Experiments show the effectiveness of EmoTalker in generating high-quality, emotionally customizable facial expressions.|\u8fd1\u5e74\u6765\uff0c\u8bf4\u8bdd\u9762\u5b54\u751f\u6210\u9886\u57df\u5f15\u8d77\u4e86\u76f8\u5f53\u5927\u7684\u5173\u6ce8\uff0c\u67d0\u4e9b\u65b9\u6cd5\u64c5\u957f\u751f\u6210\u4ee4\u4eba\u4fe1\u670d\u5730\u6a21\u4eff\u4eba\u7c7b\u8868\u60c5\u7684\u865a\u62df\u9762\u5b54\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7740\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u8eab\u4efd\u65f6\u3002\u6b64\u5916\uff0c\u7f16\u8f91\u8868\u60c5\u7684\u65b9\u6cd5\u5f80\u5f80\u5c40\u9650\u4e8e\u5355\u4e00\u7684\u60c5\u611f\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u7684\u60c5\u611f\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86 EmoTalker\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u60c5\u611f\u53ef\u7f16\u8f91\u8096\u50cf\u52a8\u753b\u65b9\u6cd5\u3002 EmoTalker \u4fee\u6539\u4e86\u53bb\u566a\u8fc7\u7a0b\u200b\u200b\uff0c\u4ee5\u786e\u4fdd\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u539f\u59cb\u8096\u50cf\u7684\u8eab\u4efd\u3002\u4e3a\u4e86\u589e\u5f3a\u5bf9\u6587\u672c\u8f93\u5165\u7684\u60c5\u611f\u7406\u89e3\uff0c\u5f15\u5165\u4e86\u60c5\u611f\u5f3a\u5ea6\u6a21\u5757\u6765\u5206\u6790\u6765\u81ea\u63d0\u793a\u7684\u7ec6\u7c92\u5ea6\u60c5\u611f\u548c\u5f3a\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\u6765\u589e\u5f3a\u63d0\u793a\u4e2d\u7684\u60c5\u611f\u7406\u89e3\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86 EmoTalker \u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u6839\u636e\u60c5\u7eea\u5b9a\u5236\u7684\u9762\u90e8\u8868\u60c5\u65b9\u9762\u7684\u6709\u6548\u6027\u3002|[2401.08049v1](http://arxiv.org/pdf/2401.08049v1)|null|\n", "2401.08045": "|**2024-01-16**|**Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities**|\u6253\u9020\u81ea\u52a8\u9a7e\u9a76\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff1a\u6311\u6218\u3001\u65b9\u6cd5\u548c\u673a\u9047|Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, et.al.|The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving.|\u5728\u5927\u91cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u5174\u8d77\u6b63\u5728\u5f7b\u5e95\u6539\u53d8\u4eba\u5de5\u667a\u80fd\u9886\u57df\u3002 SAM\u3001DALL-E2 \u548c GPT-4 \u7b49\u6a21\u578b\u901a\u8fc7\u63d0\u53d6\u590d\u6742\u7684\u6a21\u5f0f\u5e76\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e2d\u6709\u6548\u6267\u884c\u6765\u5c55\u793a\u5176\u9002\u5e94\u6027\uff0c\u4ece\u800c\u6210\u4e3a\u5e7f\u6cdb\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u7a0b\u5e8f\u7684\u6709\u6548\u6784\u5efa\u5757\u3002\u81ea\u52a8\u9a7e\u9a76\u662f\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u9886\u57df\u7684\u4e00\u4e2a\u5145\u6ee1\u6d3b\u529b\u7684\u524d\u6cbf\u9886\u57df\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u4e13\u7528\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\uff0c\u4ecd\u7136\u9762\u4e34\u7740\u6311\u6218\u3002\u7efc\u5408\u8bad\u7ec3\u6570\u636e\u7684\u7f3a\u4e4f\u3001\u591a\u4f20\u611f\u5668\u96c6\u6210\u7684\u9700\u6c42\u4ee5\u53ca\u591a\u6837\u5316\u7684\u7279\u5b9a\u4efb\u52a1\u67b6\u6784\u5bf9\u8be5\u9886\u57df\u7684 VFM \u53d1\u5c55\u6784\u6210\u4e86\u91cd\u5927\u969c\u788d\u3002\u672c\u6587\u6df1\u5165\u63a2\u8ba8\u4e86\u4e13\u95e8\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6253\u9020 VFM \u7684\u5173\u952e\u6311\u6218\uff0c\u540c\u65f6\u4e5f\u6982\u8ff0\u4e86\u672a\u6765\u7684\u65b9\u5411\u3002\u901a\u8fc7\u5bf9 250 \u591a\u7bc7\u8bba\u6587\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u6211\u4eec\u5256\u6790\u4e86 VFM \u5f00\u53d1\u7684\u57fa\u672c\u6280\u672f\uff0c\u5305\u62ec\u6570\u636e\u51c6\u5907\u3001\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u7d22\u4e86 NeRF\u3001\u6269\u6563\u6a21\u578b\u30013D \u9ad8\u65af\u5206\u5e03\u548c\u4e16\u754c\u6a21\u578b\u7b49\u5173\u952e\u8fdb\u5c55\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8def\u7ebf\u56fe\u3002\u4e3a\u4e86\u589e\u5f3a\u7814\u7a76\u4eba\u5458\u7684\u80fd\u529b\uff0c\u6211\u4eec\u6784\u5efa\u5e76\u7ef4\u62a4\u4e86 https://github.com/zhanghm1995/Forge_VFM4AD\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u653e\u8bbf\u95ee\u5b58\u50a8\u5e93\uff0c\u4e0d\u65ad\u66f4\u65b0\u81ea\u52a8\u9a7e\u9a76 VFM \u7684\u6700\u65b0\u8fdb\u5c55\u3002|[2401.08045v1](http://arxiv.org/pdf/2401.08045v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.08276": "|**2024-01-16**|**AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception**|AesBench\uff1a\u56fe\u50cf\u7f8e\u5b66\u611f\u77e5\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e13\u5bb6\u57fa\u51c6|Yipo Huang, Quan Yuan, Xiangfei Sheng, Zhichao Yang, Haoning Wu, Pengfei Chen, Yuzhe Yang, Leida Li, Weisi Lin|With collective endeavors, multimodal large language models (MLLMs) are undergoing a flourishing development. However, their performances on image aesthetics perception remain indeterminate, which is highly desired in real-world applications. An obvious obstacle lies in the absence of a specific benchmark to evaluate the effectiveness of MLLMs on aesthetic perception. This blind groping may impede the further development of more advanced MLLMs with aesthetic perception capacity. To address this dilemma, we propose AesBench, an expert benchmark aiming to comprehensively evaluate the aesthetic perception capacities of MLLMs through elaborate design across dual facets. (1) We construct an Expert-labeled Aesthetics Perception Database (EAPD), which features diversified image contents and high-quality annotations provided by professional aesthetic experts. (2) We propose a set of integrative criteria to measure the aesthetic perception abilities of MLLMs from four perspectives, including Perception (AesP), Empathy (AesE), Assessment (AesA) and Interpretation (AesI). Extensive experimental results underscore that the current MLLMs only possess rudimentary aesthetic perception ability, and there is still a significant gap between MLLMs and humans. We hope this work can inspire the community to engage in deeper explorations on the aesthetic potentials of MLLMs. Source data will be available at https://github.com/yipoh/AesBench.|\u5728\u5927\u5bb6\u7684\u5171\u540c\u52aa\u529b\u4e0b\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6b63\u5728\u84ec\u52c3\u53d1\u5c55\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u56fe\u50cf\u7f8e\u5b66\u611f\u77e5\u4e0a\u7684\u8868\u73b0\u4ecd\u7136\u4e0d\u786e\u5b9a\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\u662f\u975e\u5e38\u9700\u8981\u7684\u3002\u4e00\u4e2a\u660e\u663e\u7684\u969c\u788d\u5728\u4e8e\u7f3a\u4e4f\u5177\u4f53\u7684\u57fa\u51c6\u6765\u8bc4\u4f30 MLLM \u5bf9\u5ba1\u7f8e\u611f\u77e5\u7684\u6709\u6548\u6027\u3002\u8fd9\u79cd\u76f2\u76ee\u7684\u6478\u7d22\u53ef\u80fd\u4f1a\u963b\u788d\u66f4\u5148\u8fdb\u7684\u5177\u6709\u5ba1\u7f8e\u611f\u77e5\u80fd\u529b\u7684MLLM\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u56f0\u5883\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AesBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u5bb6\u57fa\u51c6\uff0c\u65e8\u5728\u901a\u8fc7\u8de8\u53cc\u9762\u7684\u7cbe\u5fc3\u8bbe\u8ba1\u6765\u7efc\u5408\u8bc4\u4f30 MLLM \u7684\u7f8e\u611f\u80fd\u529b\u3002 \uff081\uff09\u6211\u4eec\u6784\u5efa\u4e86\u4e13\u5bb6\u6807\u8bb0\u7684\u7f8e\u5b66\u611f\u77e5\u6570\u636e\u5e93\uff08EAPD\uff09\uff0c\u8be5\u6570\u636e\u5e93\u5177\u6709\u591a\u6837\u5316\u7684\u56fe\u50cf\u5185\u5bb9\u548c\u7531\u4e13\u4e1a\u7f8e\u5b66\u4e13\u5bb6\u63d0\u4f9b\u7684\u9ad8\u8d28\u91cf\u6ce8\u91ca\u3002 \uff082\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u5957\u7efc\u5408\u6807\u51c6\uff0c\u4ece\u611f\u77e5\uff08AesP\uff09\u3001\u79fb\u60c5\uff08AesE\uff09\u3001\u8bc4\u4f30\uff08AesA\uff09\u548c\u89e3\u91ca\uff08AesI\uff09\u56db\u4e2a\u89d2\u5ea6\u8861\u91cfMLLM\u7684\u5ba1\u7f8e\u611f\u77e5\u80fd\u529b\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76ee\u524d\u7684MLLM\u4ec5\u5177\u5907\u521d\u7ea7\u7684\u5ba1\u7f8e\u611f\u77e5\u80fd\u529b\uff0c\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u4ecd\u5b58\u5728\u663e\u7740\u5dee\u8ddd\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u591f\u6fc0\u53d1\u793e\u533a\u5bf9 MLLM \u7684\u7f8e\u5b66\u6f5c\u529b\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u63a2\u7d22\u3002\u6e90\u6570\u636e\u53ef\u5728 https://github.com/yipoh/AesBench \u83b7\u53d6\u3002|[2401.08276v1](http://arxiv.org/pdf/2401.08276v1)|null|\n", "2401.08212": "|**2024-01-16**|**Human vs. LMMs: Exploring the Discrepancy in Emoji Interpretation and Usage in Digital Communication**|\u4eba\u7c7b\u4e0e LMM\uff1a\u63a2\u7d22\u6570\u5b57\u901a\u4fe1\u4e2d\u8868\u60c5\u7b26\u53f7\u89e3\u91ca\u548c\u4f7f\u7528\u7684\u5dee\u5f02|Hanjia Lyu, Weihong Qi, Zhongyu Wei, Jiebo Luo|Leveraging Large Multimodal Models (LMMs) to simulate human behaviors when processing multimodal information, especially in the context of social media, has garnered immense interest due to its broad potential and far-reaching implications. Emojis, as one of the most unique aspects of digital communication, are pivotal in enriching and often clarifying the emotional and tonal dimensions. Yet, there is a notable gap in understanding how these advanced models, such as GPT-4V, interpret and employ emojis in the nuanced context of online interaction. This study intends to bridge this gap by examining the behavior of GPT-4V in replicating human-like use of emojis. The findings reveal a discernible discrepancy between human and GPT-4V behaviors, likely due to the subjective nature of human interpretation and the limitations of GPT-4V's English-centric training, suggesting cultural biases and inadequate representation of non-English cultures.|\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u6765\u6a21\u62df\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u65f6\u7684\u4eba\u7c7b\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u793e\u4ea4\u5a92\u4f53\u80cc\u666f\u4e0b\uff0c\u7531\u4e8e\u5176\u5e7f\u6cdb\u7684\u6f5c\u529b\u548c\u6df1\u8fdc\u7684\u5f71\u54cd\u800c\u5f15\u8d77\u4e86\u4eba\u4eec\u7684\u6781\u5927\u5174\u8da3\u3002\u8868\u60c5\u7b26\u53f7\u4f5c\u4e3a\u6570\u5b57\u901a\u4fe1\u6700\u72ec\u7279\u7684\u65b9\u9762\u4e4b\u4e00\uff0c\u5bf9\u4e8e\u4e30\u5bcc\u5e76\u5e38\u5e38\u6f84\u6e05\u60c5\u611f\u548c\u8bed\u6c14\u7ef4\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728\u7406\u89e3\u8fd9\u4e9b\u5148\u8fdb\u6a21\u578b\uff08\u4f8b\u5982 GPT-4V\uff09\u5982\u4f55\u5728\u5fae\u5999\u7684\u5728\u7ebf\u4ea4\u4e92\u73af\u5883\u4e2d\u89e3\u91ca\u548c\u4f7f\u7528\u8868\u60c5\u7b26\u53f7\u65b9\u9762\u5b58\u5728\u663e\u7740\u5dee\u8ddd\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u68c0\u67e5 GPT-4V \u5728\u590d\u5236\u7c7b\u4eba\u8868\u60c5\u7b26\u53f7\u4f7f\u7528\u65b9\u9762\u7684\u884c\u4e3a\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u4eba\u7c7b\u548c GPT-4V \u884c\u4e3a\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u5dee\u5f02\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u4eba\u7c7b\u89e3\u91ca\u7684\u4e3b\u89c2\u6027\u548c GPT-4V \u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u8868\u660e\u6587\u5316\u504f\u89c1\u548c\u5bf9\u975e\u82f1\u8bed\u6587\u5316\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u3002|[2401.08212v1](http://arxiv.org/pdf/2401.08212v1)|null|\n", "2401.08123": "|**2024-01-16**|**The Devil is in the Details: Boosting Guided Depth Super-Resolution via Rethinking Cross-Modal Alignment and Aggregation**|\u7ec6\u8282\u51b3\u5b9a\u6210\u8d25\uff1a\u901a\u8fc7\u91cd\u65b0\u601d\u8003\u8de8\u6a21\u5f0f\u5bf9\u9f50\u548c\u805a\u5408\u6765\u63d0\u9ad8\u5f15\u5bfc\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387|Xinni Jiang, Zengsheng Kuang, Chunle Guo, Ruixun Zhang, Lei Cai, Xiao Fan, Chongyi Li|Guided depth super-resolution (GDSR) involves restoring missing depth details using the high-resolution RGB image of the same scene. Previous approaches have struggled with the heterogeneity and complementarity of the multi-modal inputs, and neglected the issues of modal misalignment, geometrical misalignment, and feature selection. In this study, we rethink some essential components in GDSR networks and propose a simple yet effective Dynamic Dual Alignment and Aggregation network (D2A2). D2A2 mainly consists of 1) a dynamic dual alignment module that adapts to alleviate the modal misalignment via a learnable domain alignment block and geometrically align cross-modal features by learning the offset; and 2) a mask-to-pixel feature aggregate module that uses the gated mechanism and pixel attention to filter out irrelevant texture noise from RGB features and combine the useful features with depth features. By combining the strengths of RGB and depth features while minimizing disturbance introduced by the RGB image, our method with simple reuse and redesign of basic components achieves state-of-the-art performance on multiple benchmark datasets. The code is available at https://github.com/JiangXinni/D2A2.|\u5f15\u5bfc\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387 (GDSR) \u6d89\u53ca\u4f7f\u7528\u540c\u4e00\u573a\u666f\u7684\u9ad8\u5206\u8fa8\u7387 RGB \u56fe\u50cf\u6765\u6062\u590d\u4e22\u5931\u7684\u6df1\u5ea6\u7ec6\u8282\u3002\u4ee5\u524d\u7684\u65b9\u6cd5\u4e00\u76f4\u5728\u52aa\u529b\u89e3\u51b3\u591a\u6a21\u6001\u8f93\u5165\u7684\u5f02\u8d28\u6027\u548c\u4e92\u8865\u6027\uff0c\u5e76\u5ffd\u7565\u4e86\u6a21\u6001\u9519\u4f4d\u3001\u51e0\u4f55\u9519\u4f4d\u548c\u7279\u5f81\u9009\u62e9\u7684\u95ee\u9898\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u91cd\u65b0\u601d\u8003\u4e86 GDSR \u7f51\u7edc\u4e2d\u7684\u4e00\u4e9b\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u52a8\u6001\u53cc\u91cd\u5bf9\u9f50\u548c\u805a\u5408\u7f51\u7edc\uff08D2A2\uff09\u3002 D2A2\u4e3b\u8981\u75311\uff09\u4e00\u4e2a\u52a8\u6001\u53cc\u5bf9\u9f50\u6a21\u5757\u7ec4\u6210\uff0c\u8be5\u6a21\u5757\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u57df\u5bf9\u9f50\u5757\u6765\u7f13\u89e3\u6a21\u6001\u672a\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u504f\u79fb\u91cf\u6765\u51e0\u4f55\u5bf9\u9f50\u8de8\u6a21\u6001\u7279\u5f81\uff1b 2\uff09\u63a9\u6a21\u5230\u50cf\u7d20\u7684\u7279\u5f81\u805a\u5408\u6a21\u5757\uff0c\u4f7f\u7528\u95e8\u63a7\u673a\u5236\u548c\u50cf\u7d20\u6ce8\u610f\u529b\u4eceRGB\u7279\u5f81\u4e2d\u8fc7\u6ee4\u6389\u4e0d\u76f8\u5173\u7684\u7eb9\u7406\u566a\u58f0\uff0c\u5e76\u5c06\u6709\u7528\u7684\u7279\u5f81\u4e0e\u6df1\u5ea6\u7279\u5f81\u7ed3\u5408\u8d77\u6765\u3002\u901a\u8fc7\u7ed3\u5408 RGB \u548c\u6df1\u5ea6\u7279\u5f81\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11 RGB \u56fe\u50cf\u5f15\u5165\u7684\u5e72\u6270\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u7b80\u5355\u5730\u91cd\u7528\u548c\u91cd\u65b0\u8bbe\u8ba1\u57fa\u672c\u7ec4\u200b\u200b\u4ef6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/JiangXinni/D2A2 \u83b7\u53d6\u3002|[2401.08123v1](http://arxiv.org/pdf/2401.08123v1)|null|\n"}, "LLM": {}, "Transformer": {"2401.08209": "|**2024-01-16**|**Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary**|\u8d85\u8d8a\u672c\u5730\u7a97\u53e3\u7684\u9650\u5236\uff1a\u5177\u6709\u81ea\u9002\u5e94\u4ee4\u724c\u5b57\u5178\u7684\u9ad8\u7ea7\u8d85\u5206\u8fa8\u7387\u53d8\u538b\u5668|Leheng Zhang, Yawei Li, Xingyu Zhou, Xiaorui Zhao, Shuhang Gu|Single Image Super-Resolution is a classic computer vision problem that involves estimating high-resolution (HR) images from low-resolution (LR) ones. Although deep neural networks (DNNs), especially Transformers for super-resolution, have seen significant advancements in recent years, challenges still remain, particularly in limited receptive field caused by window-based self-attention. To address these issues, we introduce a group of auxiliary Adapeive Token Dictionary to SR Transformer and establish an ATD-SR method. The introduced token dictionary could learn prior information from training data and adapt the learned prior to specific testing image through an adaptive refinement step. The refinement strategy could not only provide global information to all input tokens but also group image tokens into categories. Based on category partitions, we further propose a category-based self-attention mechanism designed to leverage distant but similar tokens for enhancing input features. The experimental results show that our method achieves the best performance on various single image super-resolution benchmarks.|\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u662f\u4e00\u4e2a\u7ecf\u5178\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898\uff0c\u6d89\u53ca\u4ece\u4f4e\u5206\u8fa8\u7387 (LR) \u56fe\u50cf\u4f30\u8ba1\u9ad8\u5206\u8fa8\u7387 (HR) \u56fe\u50cf\u3002\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\uff0c\u7279\u522b\u662f\u7528\u4e8e\u8d85\u5206\u8fa8\u7387\u7684 Transformer\uff0c\u8fd1\u5e74\u6765\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u6b65\uff0c\u4f46\u6311\u6218\u4ecd\u7136\u5b58\u5728\uff0c\u7279\u522b\u662f\u5728\u57fa\u4e8e\u7a97\u53e3\u7684\u81ea\u6ce8\u610f\u529b\u5f15\u8d77\u7684\u6709\u9650\u611f\u53d7\u91ce\u65b9\u9762\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5411 SR Transformer \u5f15\u5165\u4e00\u7ec4\u8f85\u52a9\u7684 Adapeive Token Dictionary\uff0c\u5e76\u5efa\u7acb\u4e86 ATD-SR \u65b9\u6cd5\u3002\u5f15\u5165\u7684\u6807\u8bb0\u5b57\u5178\u53ef\u4ee5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u5b66\u4e60\u5148\u9a8c\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u7ec6\u5316\u6b65\u9aa4\u5c06\u5b66\u4e60\u5230\u7684\u5148\u9a8c\u4fe1\u606f\u9002\u5e94\u7279\u5b9a\u7684\u6d4b\u8bd5\u56fe\u50cf\u3002\u7ec6\u5316\u7b56\u7565\u4e0d\u4ec5\u53ef\u4ee5\u4e3a\u6240\u6709\u8f93\u5165\u6807\u8bb0\u63d0\u4f9b\u5168\u5c40\u4fe1\u606f\uff0c\u8fd8\u53ef\u4ee5\u5c06\u56fe\u50cf\u6807\u8bb0\u5206\u7ec4\u3002\u57fa\u4e8e\u7c7b\u522b\u5212\u5206\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c7b\u522b\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u65e8\u5728\u5229\u7528\u9065\u8fdc\u4f46\u76f8\u4f3c\u7684\u6807\u8bb0\u6765\u589e\u5f3a\u8f93\u5165\u7279\u5f81\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u3002|[2401.08209v1](http://arxiv.org/pdf/2401.08209v1)|null|\n", "2401.08185": "|**2024-01-16**|**DPAFNet:Dual Path Attention Fusion Network for Single Image Deraining**|DPAFNet\uff1a\u7528\u4e8e\u5355\u56fe\u50cf\u53bb\u96e8\u7684\u53cc\u8def\u5f84\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edc|Bingcai Wei|Rainy weather will have a significant impact on the regular operation of the imaging system. Based on this premise, image rain removal has always been a popular branch of low-level visual tasks, especially methods using deep neural networks. However, most neural networks are but-branched, such as only using convolutional neural networks or Transformers, which is unfavourable for the multidimensional fusion of image features. In order to solve this problem, this paper proposes a dual-branch attention fusion network. Firstly, a two-branch network structure is proposed. Secondly, an attention fusion module is proposed to selectively fuse the features extracted by the two branches rather than simply adding them. Finally, complete ablation experiments and sufficient comparison experiments prove the rationality and effectiveness of the proposed method.|\u9634\u96e8\u5929\u6c14\u4f1a\u5bf9\u6210\u50cf\u7cfb\u7edf\u7684\u6b63\u5e38\u8fd0\u884c\u4ea7\u751f\u8f83\u5927\u5f71\u54cd\u3002\u57fa\u4e8e\u8fd9\u4e2a\u524d\u63d0\uff0c\u56fe\u50cf\u53bb\u96e8\u4e00\u76f4\u662f\u4f4e\u7ea7\u89c6\u89c9\u4efb\u52a1\u7684\u4e00\u4e2a\u6d41\u884c\u5206\u652f\uff0c\u7279\u522b\u662f\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u90fd\u662fbut-branched\uff0c\u4f8b\u5982\u4ec5\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6216Transformers\uff0c\u8fd9\u4e0d\u5229\u4e8e\u56fe\u50cf\u7279\u5f81\u7684\u591a\u7ef4\u878d\u5408\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u53cc\u5206\u652f\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edc\u3002\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652f\u7f51\u7edc\u7ed3\u6784\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u6765\u9009\u62e9\u6027\u5730\u878d\u5408\u4e24\u4e2a\u5206\u652f\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u5c06\u5b83\u4eec\u76f8\u52a0\u3002\u6700\u540e\uff0c\u5b8c\u6574\u7684\u6d88\u878d\u5b9e\u9a8c\u548c\u5145\u5206\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u5408\u7406\u6027\u548c\u6709\u6548\u6027\u3002|[2401.08185v1](http://arxiv.org/pdf/2401.08185v1)|null|\n", "2401.08171": "|**2024-01-16**|**Deep Linear Array Pushbroom Image Restoration: A Degradation Pipeline and Jitter-Aware Restoration Network**|\u6df1\u5ea6\u7ebf\u6027\u9635\u5217\u63a8\u626b\u5f0f\u56fe\u50cf\u6062\u590d\uff1a\u9000\u5316\u7ba1\u9053\u548c\u6296\u52a8\u611f\u77e5\u6062\u590d\u7f51\u7edc|Zida Chen, Ziran Zhang, Haoying Li, Menghao Li, Yueting Chen, Qi Li, Huajun Feng, Zhihai Xu, Shiqi Chen|Linear Array Pushbroom (LAP) imaging technology is widely used in the realm of remote sensing. However, images acquired through LAP always suffer from distortion and blur because of camera jitter. Traditional methods for restoring LAP images, such as algorithms estimating the point spread function (PSF), exhibit limited performance. To tackle this issue, we propose a Jitter-Aware Restoration Network (JARNet), to remove the distortion and blur in two stages. In the first stage, we formulate an Optical Flow Correction (OFC) block to refine the optical flow of the degraded LAP images, resulting in pre-corrected images where most of the distortions are alleviated. In the second stage, for further enhancement of the pre-corrected images, we integrate two jitter-aware techniques within the Spatial and Frequency Residual (SFRes) block: 1) introducing Coordinate Attention (CoA) to the SFRes block in order to capture the jitter state in orthogonal direction; 2) manipulating image features in both spatial and frequency domains to leverage local and global priors. Additionally, we develop a data synthesis pipeline, which applies Continue Dynamic Shooting Model (CDSM) to simulate realistic degradation in LAP images. Both the proposed JARNet and LAP image synthesis pipeline establish a foundation for addressing this intricate challenge. Extensive experiments demonstrate that the proposed two-stage method outperforms state-of-the-art image restoration models. Code is available at https://github.com/JHW2000/JARNet.|\u7ebf\u6027\u9635\u5217\u63a8\u626b\u5f0f\uff08LAP\uff09\u6210\u50cf\u6280\u672f\u5e7f\u6cdb\u5e94\u7528\u4e8e\u9065\u611f\u9886\u57df\u3002\u7136\u800c\uff0c\u7531\u4e8e\u76f8\u673a\u6296\u52a8\uff0c\u901a\u8fc7 LAP \u83b7\u53d6\u7684\u56fe\u50cf\u603b\u662f\u4f1a\u51fa\u73b0\u5931\u771f\u548c\u6a21\u7cca\u3002\u7528\u4e8e\u6062\u590d LAP \u56fe\u50cf\u7684\u4f20\u7edf\u65b9\u6cd5\uff08\u4f8b\u5982\u4f30\u8ba1\u70b9\u6269\u6563\u51fd\u6570 (PSF) \u7684\u7b97\u6cd5\uff09\u8868\u73b0\u51fa\u6709\u9650\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6296\u52a8\u611f\u77e5\u6062\u590d\u7f51\u7edc\uff08JARNet\uff09\uff0c\u4ee5\u5206\u4e24\u4e2a\u9636\u6bb5\u6d88\u9664\u5931\u771f\u548c\u6a21\u7cca\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u5236\u5b9a\u5149\u6d41\u6821\u6b63\uff08OFC\uff09\u5757\u6765\u7ec6\u5316\u9000\u5316\u7684 LAP \u56fe\u50cf\u7684\u5149\u6d41\uff0c\u4ece\u800c\u4ea7\u751f\u9884\u6821\u6b63\u56fe\u50cf\uff0c\u5176\u4e2d\u5927\u90e8\u5206\u5931\u771f\u5f97\u5230\u7f13\u89e3\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u9884\u6821\u6b63\u56fe\u50cf\uff0c\u6211\u4eec\u5728\u7a7a\u95f4\u548c\u9891\u7387\u6b8b\u5dee\uff08SFRes\uff09\u5757\u4e2d\u96c6\u6210\u4e86\u4e24\u79cd\u6296\u52a8\u611f\u77e5\u6280\u672f\uff1a1\uff09\u5411 SFRes \u5757\u5f15\u5165\u5750\u6807\u6ce8\u610f\uff08CoA\uff09\u4ee5\u6355\u83b7\u6b63\u4ea4\u65b9\u5411\u7684\u6296\u52a8\u72b6\u6001\uff1b 2\uff09\u5728\u7a7a\u95f4\u548c\u9891\u7387\u57df\u4e2d\u64cd\u7eb5\u56fe\u50cf\u7279\u5f81\u4ee5\u5229\u7528\u5c40\u90e8\u548c\u5168\u5c40\u5148\u9a8c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u5b83\u5e94\u7528\u8fde\u7eed\u52a8\u6001\u62cd\u6444\u6a21\u578b\uff08CDSM\uff09\u6765\u6a21\u62df LAP \u56fe\u50cf\u4e2d\u7684\u771f\u5b9e\u9000\u5316\u3002\u6240\u63d0\u51fa\u7684 JARNet \u548c LAP \u56fe\u50cf\u5408\u6210\u7ba1\u9053\u90fd\u4e3a\u89e3\u51b3\u8fd9\u4e00\u590d\u6742\u7684\u6311\u6218\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u6062\u590d\u6a21\u578b\u3002\u4ee3\u7801\u53ef\u4ece https://github.com/JHW2000/JARNet \u83b7\u53d6\u3002|[2401.08171v1](http://arxiv.org/pdf/2401.08171v1)|null|\n", "2401.08086": "|**2024-01-16**|**Spatial-Semantic Collaborative Cropping for User Generated Content**|\u7528\u6237\u751f\u6210\u5185\u5bb9\u7684\u7a7a\u95f4\u8bed\u4e49\u534f\u4f5c\u88c1\u526a|Yukun Su, Yiwen Cao, Jingliang Deng, Fengyun Rao, Qingyao Wu|A large amount of User Generated Content (UGC) is uploaded to the Internet daily and displayed to people world-widely through the client side (e.g., mobile and PC). This requires the cropping algorithms to produce the aesthetic thumbnail within a specific aspect ratio on different devices. However, existing image cropping works mainly focus on landmark or landscape images, which fail to model the relations among the multi-objects with the complex background in UGC. Besides, previous methods merely consider the aesthetics of the cropped images while ignoring the content integrity, which is crucial for UGC cropping. In this paper, we propose a Spatial-Semantic Collaborative cropping network (S2CNet) for arbitrary user generated content accompanied by a new cropping benchmark. Specifically, we first mine the visual genes of the potential objects. Then, the suggested adaptive attention graph recasts this task as a procedure of information association over visual nodes. The underlying spatial and semantic relations are ultimately centralized to the crop candidate through differentiable message passing, which helps our network efficiently to preserve both the aesthetics and the content integrity. Extensive experiments on the proposed UGCrop5K and other public datasets demonstrate the superiority of our approach over state-of-the-art counterparts. Our project is available at https://github.com/suyukun666/S2CNet.|\u6bcf\u5929\u90fd\u6709\u5927\u91cf\u7684\u7528\u6237\u751f\u6210\u5185\u5bb9\uff08UGC\uff09\u4e0a\u4f20\u5230\u4e92\u8054\u7f51\uff0c\u5e76\u901a\u8fc7\u5ba2\u6237\u7aef\uff08\u4f8b\u5982\u79fb\u52a8\u548cPC\uff09\u5411\u5168\u4e16\u754c\u7684\u4eba\u4eec\u5c55\u793a\u3002\u8fd9\u9700\u8981\u88c1\u526a\u7b97\u6cd5\u5728\u4e0d\u540c\u8bbe\u5907\u4e0a\u4ee5\u7279\u5b9a\u7684\u5bbd\u9ad8\u6bd4\u751f\u6210\u7f8e\u89c2\u7684\u7f29\u7565\u56fe\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u56fe\u50cf\u88c1\u526a\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u5730\u6807\u6216\u98ce\u666f\u56fe\u50cf\uff0c\u672a\u80fd\u5bf9UGC\u4e2d\u590d\u6742\u80cc\u666f\u7684\u591a\u5bf9\u8c61\u4e4b\u95f4\u7684\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002\u6b64\u5916\uff0c\u4ee5\u524d\u7684\u65b9\u6cd5\u4ec5\u8003\u8651\u88c1\u526a\u56fe\u50cf\u7684\u7f8e\u89c2\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5185\u5bb9\u5b8c\u6574\u6027\uff0c\u800c\u5185\u5bb9\u5b8c\u6574\u6027\u5bf9\u4e8e UGC \u88c1\u526a\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u8bed\u4e49\u534f\u4f5c\u88c1\u526a\u7f51\u7edc\uff08S2CNet\uff09\uff0c\u7528\u4e8e\u4efb\u610f\u7528\u6237\u751f\u6210\u7684\u5185\u5bb9\uff0c\u5e76\u9644\u5e26\u65b0\u7684\u88c1\u526a\u57fa\u51c6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u6316\u6398\u6f5c\u5728\u7269\u4f53\u7684\u89c6\u89c9\u57fa\u56e0\u3002\u7136\u540e\uff0c\u5efa\u8bae\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u56fe\u5c06\u8be5\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u89c9\u8282\u70b9\u4e0a\u7684\u4fe1\u606f\u5173\u8054\u8fc7\u7a0b\u3002\u5e95\u5c42\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u5173\u7cfb\u6700\u7ec8\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u6d88\u606f\u4f20\u9012\u96c6\u4e2d\u5230\u5019\u9009\u4f5c\u7269\u4e0a\uff0c\u8fd9\u6709\u52a9\u4e8e\u6211\u4eec\u7684\u7f51\u7edc\u6709\u6548\u5730\u4fdd\u6301\u7f8e\u89c2\u548c\u5185\u5bb9\u5b8c\u6574\u6027\u3002\u5bf9\u6240\u63d0\u51fa\u7684 UGCrop5K \u548c\u5176\u4ed6\u516c\u5171\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u540c\u884c\u7684\u4f18\u8d8a\u6027\u3002\u6211\u4eec\u7684\u9879\u76ee\u4f4d\u4e8e https://github.com/suyukun666/S2CNet\u3002|[2401.08086v1](http://arxiv.org/pdf/2401.08086v1)|null|\n"}, "Nerf": {"2401.08140": "|**2024-01-16**|**ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process**|ProvNeRF\uff1aNeRF \u4e2d\u6bcf\u70b9\u6765\u6e90\u7684\u5efa\u6a21\u4f5c\u4e3a\u968f\u673a\u8fc7\u7a0b|Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas Guibas|Neural radiance fields (NeRFs) have gained popularity across various applications. However, they face challenges in the sparse view setting, lacking sufficient constraints from volume rendering. Reconstructing and understanding a 3D scene from sparse and unconstrained cameras is a long-standing problem in classical computer vision with diverse applications. While recent works have explored NeRFs in sparse, unconstrained view scenarios, their focus has been primarily on enhancing reconstruction and novel view synthesis. Our approach takes a broader perspective by posing the question: \"from where has each point been seen?\" -- which gates how well we can understand and reconstruct it. In other words, we aim to determine the origin or provenance of each 3D point and its associated information under sparse, unconstrained views. We introduce ProvNeRF, a model that enriches a traditional NeRF representation by incorporating per-point provenance, modeling likely source locations for each point. We achieve this by extending implicit maximum likelihood estimation (IMLE) for stochastic processes. Notably, our method is compatible with any pre-trained NeRF model and the associated training camera poses. We demonstrate that modeling per-point provenance offers several advantages, including uncertainty estimation, criteria-based view selection, and improved novel view synthesis, compared to state-of-the-art methods.|\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u5728\u5404\u79cd\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u7a00\u758f\u89c6\u56fe\u8bbe\u7f6e\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u7f3a\u4e4f\u4f53\u6e32\u67d3\u7684\u8db3\u591f\u7ea6\u675f\u3002\u4ece\u7a00\u758f\u4e14\u4e0d\u53d7\u7ea6\u675f\u7684\u76f8\u673a\u4e2d\u91cd\u5efa\u548c\u7406\u89e3 3D \u573a\u666f\u662f\u5177\u6709\u591a\u79cd\u5e94\u7528\u7684\u7ecf\u5178\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\u3002\u867d\u7136\u6700\u8fd1\u7684\u5de5\u4f5c\u5728\u7a00\u758f\u3001\u65e0\u7ea6\u675f\u7684\u89c6\u56fe\u573a\u666f\u4e2d\u63a2\u7d22\u4e86 NeRF\uff0c\u4f46\u4ed6\u4eec\u7684\u91cd\u70b9\u4e3b\u8981\u96c6\u4e2d\u5728\u589e\u5f3a\u91cd\u5efa\u548c\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u4e0a\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\u6765\u91c7\u53d6\u66f4\u5e7f\u6cdb\u7684\u89c6\u89d2\uff1a\u201c\u4ece\u54ea\u91cc\u770b\u5230\u6bcf\u4e2a\u70b9\uff1f\u201d \u2014\u2014\u8fd9\u51b3\u5b9a\u4e86\u6211\u4eec\u7406\u89e3\u548c\u91cd\u5efa\u5b83\u7684\u7a0b\u5ea6\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u5728\u7a00\u758f\u3001\u65e0\u7ea6\u675f\u7684\u89c6\u56fe\u4e0b\u786e\u5b9a\u6bcf\u4e2a 3D \u70b9\u53ca\u5176\u76f8\u5173\u4fe1\u606f\u7684\u8d77\u6e90\u6216\u51fa\u5904\u3002\u6211\u4eec\u5f15\u5165\u4e86 ProvNeRF\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5408\u5e76\u6bcf\u4e2a\u70b9\u7684\u51fa\u5904\u3001\u5bf9\u6bcf\u4e2a\u70b9\u53ef\u80fd\u7684\u6e90\u4f4d\u7f6e\u8fdb\u884c\u5efa\u6a21\u6765\u4e30\u5bcc\u4f20\u7edf\u7684 NeRF \u8868\u793a\u3002\u6211\u4eec\u901a\u8fc7\u6269\u5c55\u968f\u673a\u8fc7\u7a0b\u7684\u9690\u5f0f\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff08IMLE\uff09\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u4efb\u4f55\u9884\u5148\u8bad\u7ec3\u7684 NeRF \u6a21\u578b\u548c\u76f8\u5173\u7684\u8bad\u7ec3\u76f8\u673a\u59ff\u52bf\u517c\u5bb9\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6bcf\u70b9\u8d77\u6e90\u5efa\u6a21\u5177\u6709\u591a\u79cd\u4f18\u52bf\uff0c\u5305\u62ec\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3001\u57fa\u4e8e\u6807\u51c6\u7684\u89c6\u56fe\u9009\u62e9\u548c\u6539\u8fdb\u7684\u65b0\u9896\u89c6\u56fe\u5408\u6210\u3002|[2401.08140v1](http://arxiv.org/pdf/2401.08140v1)|null|\n"}, "3DGS": {}, "3D/CG": {"2401.08115": "|**2024-01-16**|**No-Clean-Reference Image Super-Resolution: Application to Electron Microscopy**|\u514d\u6e05\u6d01\u53c2\u8003\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff1a\u5728\u7535\u5b50\u663e\u5fae\u955c\u4e2d\u7684\u5e94\u7528|Mohammad Khateri, Morteza Ghahremani, Alejandra Sierra, Jussi Tohka|The inability to acquire clean high-resolution (HR) electron microscopy (EM) images over a large brain tissue volume hampers many neuroscience studies. To address this challenge, we propose a deep-learning-based image super-resolution (SR) approach to computationally reconstruct clean HR 3D-EM with a large field of view (FoV) from noisy low-resolution (LR) acquisition. Our contributions are I) Investigating training with no-clean references for $\\ell_2$ and $\\ell_1$ loss functions; II) Introducing a novel network architecture, named EMSR, for enhancing the resolution of LR EM images while reducing inherent noise; and, III) Comparing different training strategies including using acquired LR and HR image pairs, i.e., real pairs with no-clean references contaminated with real corruptions, the pairs of synthetic LR and acquired HR, as well as acquired LR and denoised HR pairs. Experiments with nine brain datasets showed that training with real pairs can produce high-quality super-resolved results, demonstrating the feasibility of training with non-clean references for both loss functions. Additionally, comparable results were observed, both visually and numerically, when employing denoised and noisy references for training. Moreover, utilizing the network trained with synthetically generated LR images from HR counterparts proved effective in yielding satisfactory SR results, even in certain cases, outperforming training with real pairs. The proposed SR network was compared quantitatively and qualitatively with several established SR techniques, showcasing either the superiority or competitiveness of the proposed method in mitigating noise while recovering fine details.|\u65e0\u6cd5\u5728\u8f83\u5927\u7684\u8111\u7ec4\u7ec7\u4f53\u79ef\u4e0a\u83b7\u53d6\u6e05\u6670\u7684\u9ad8\u5206\u8fa8\u7387 (HR) \u7535\u5b50\u663e\u5fae\u955c (EM) \u56fe\u50cf\u963b\u788d\u4e86\u8bb8\u591a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387 (SR) \u65b9\u6cd5\uff0c\u53ef\u4ee5\u4ece\u5608\u6742\u7684\u4f4e\u5206\u8fa8\u7387 (LR) \u91c7\u96c6\u4e2d\u8ba1\u7b97\u91cd\u5efa\u5177\u6709\u5927\u89c6\u573a (FoV) \u7684\u5e72\u51c0 HR 3D-EM\u3002\u6211\u4eec\u7684\u8d21\u732e\u662f I) \u7814\u7a76 $\\ell_2$ \u548c $\\ell_1$ \u635f\u5931\u51fd\u6570\u7684 no-clean \u53c2\u8003\u7684\u8bad\u7ec3\uff1b II) \u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u79f0\u4e3a EMSR\uff0c\u7528\u4e8e\u589e\u5f3a LR EM \u56fe\u50cf\u7684\u5206\u8fa8\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u56fa\u6709\u566a\u58f0\uff1b III) \u6bd4\u8f83\u4e0d\u540c\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u4f7f\u7528\u83b7\u53d6\u7684 LR \u548c HR \u56fe\u50cf\u5bf9\uff0c\u5373\u5177\u6709\u53d7\u771f\u5b9e\u635f\u574f\u6c61\u67d3\u7684\u514d\u6d17\u53c2\u8003\u7684\u771f\u5b9e\u5bf9\u3001\u5408\u6210 LR \u548c\u83b7\u53d6\u7684 HR \u5bf9\uff0c\u4ee5\u53ca\u83b7\u53d6\u7684 LR \u548c\u53bb\u566a HR \u5bf9\u3002\u5bf9\u4e5d\u4e2a\u5927\u8111\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u771f\u5b9e\u914d\u5bf9\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u8d85\u5206\u8fa8\u7387\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u4f7f\u7528\u975e\u5e72\u51c0\u53c2\u8003\u5bf9\u4e24\u79cd\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\u7684\u53ef\u884c\u6027\u3002\u6b64\u5916\uff0c\u5f53\u4f7f\u7528\u53bb\u566a\u548c\u566a\u58f0\u53c2\u8003\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u5728\u89c6\u89c9\u548c\u6570\u5b57\u4e0a\u90fd\u89c2\u5bdf\u5230\u4e86\u53ef\u6bd4\u8f83\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u4e8b\u5b9e\u8bc1\u660e\uff0c\u5229\u7528\u7531 HR \u5bf9\u5e94\u5bf9\u8c61\u5408\u6210\u751f\u6210\u7684 LR \u56fe\u50cf\u8bad\u7ec3\u7684\u7f51\u7edc\u53ef\u4ee5\u6709\u6548\u5730\u4ea7\u751f\u4ee4\u4eba\u6ee1\u610f\u7684 SR \u7ed3\u679c\uff0c\u5373\u4f7f\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5176\u6027\u80fd\u4e5f\u4f18\u4e8e\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u5bf9\u8fdb\u884c\u7684\u8bad\u7ec3\u3002\u6240\u63d0\u51fa\u7684 SR \u7f51\u7edc\u4e0e\u51e0\u79cd\u5df2\u5efa\u7acb\u7684 SR \u6280\u672f\u8fdb\u884c\u4e86\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51cf\u8f7b\u566a\u58f0\u540c\u65f6\u6062\u590d\u7cbe\u7ec6\u7ec6\u8282\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u6216\u7ade\u4e89\u529b\u3002|[2401.08115v1](http://arxiv.org/pdf/2401.08115v1)|null|\n", "2401.08061": "|**2024-01-16**|**Augmenting Ground-Level PM2.5 Prediction via Kriging-Based Pseudo-Label Generation**|\u901a\u8fc7\u57fa\u4e8e\u514b\u91cc\u91d1\u6cd5\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u589e\u5f3a\u5730\u9762 PM2.5 \u9884\u6d4b|Lei Duan, Ziyang Jiang, David Carlson|Fusing abundant satellite data with sparse ground measurements constitutes a major challenge in climate modeling. To address this, we propose a strategy to augment the training dataset by introducing unlabeled satellite images paired with pseudo-labels generated through a spatial interpolation technique known as ordinary kriging, thereby making full use of the available satellite data resources. We show that the proposed data augmentation strategy helps enhance the performance of the state-of-the-art convolutional neural network-random forest (CNN-RF) model by a reasonable amount, resulting in a noteworthy improvement in spatial correlation and a reduction in prediction error.|\u5c06\u4e30\u5bcc\u7684\u536b\u661f\u6570\u636e\u4e0e\u7a00\u758f\u7684\u5730\u9762\u6d4b\u91cf\u6570\u636e\u878d\u5408\u662f\u6c14\u5019\u5efa\u6a21\u7684\u4e3b\u8981\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165\u672a\u6807\u8bb0\u7684\u536b\u661f\u56fe\u50cf\u4e0e\u901a\u8fc7\u79f0\u4e3a\u666e\u901a\u514b\u91cc\u91d1\u6cd5\u7684\u7a7a\u95f4\u63d2\u503c\u6280\u672f\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u914d\u5bf9\u6765\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5145\u5206\u5229\u7528\u53ef\u7528\u7684\u536b\u661f\u6570\u636e\u8d44\u6e90\u3002\u6211\u4eec\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6709\u52a9\u4e8e\u4ee5\u5408\u7406\u7684\u91cf\u63d0\u9ad8\u6700\u5148\u8fdb\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u968f\u673a\u68ee\u6797\uff08CNN-RF\uff09\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ece\u800c\u663e\u7740\u6539\u5584\u7a7a\u95f4\u76f8\u5173\u6027\u5e76\u51cf\u5c11\u9884\u6d4b\u9519\u8bef\u3002|[2401.08061v1](http://arxiv.org/pdf/2401.08061v1)|null|\n", "2401.08043": "|**2024-01-16**|**Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions**|\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u4e8b\u4ef6\u76f8\u673a\u7684\u8de8\u6a21\u6001\u534a\u5bc6\u96c6 6 \u81ea\u7531\u5ea6\u8ddf\u8e2a|Yi-Fan Zuo, Wanting Xu, Xia Wang, Yifu Wang, Laurent Kneip|Vision-based localization is a cost-effective and thus attractive solution for many intelligent mobile platforms. However, its accuracy and especially robustness still suffer from low illumination conditions, illumination changes, and aggressive motion. Event-based cameras are bio-inspired visual sensors that perform well in HDR conditions and have high temporal resolution, and thus provide an interesting alternative in such challenging scenarios. While purely event-based solutions currently do not yet produce satisfying mapping results, the present work demonstrates the feasibility of purely event-based tracking if an alternative sensor is permitted for mapping. The method relies on geometric 3D-2D registration of semi-dense maps and events, and achieves highly reliable and accurate cross-modal tracking results. Practically relevant scenarios are given by depth camera-supported tracking or map-based localization with a semi-dense map prior created by a regular image-based visual SLAM or structure-from-motion system. Conventional edge-based 3D-2D alignment is extended by a novel polarity-aware registration that makes use of signed time-surface maps (STSM) obtained from event streams. We furthermore introduce a novel culling strategy for occluded points. Both modifications increase the speed of the tracker and its robustness against occlusions or large view-point variations. The approach is validated on many real datasets covering the above-mentioned challenging conditions, and compared against similar solutions realised with regular cameras.|\u57fa\u4e8e\u89c6\u89c9\u7684\u672c\u5730\u5316\u662f\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u6b64\u5bf9\u8bb8\u591a\u667a\u80fd\u79fb\u52a8\u5e73\u53f0\u6765\u8bf4\u90fd\u5f88\u6709\u5438\u5f15\u529b\u3002\u7136\u800c\uff0c\u5176\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u9c81\u68d2\u6027\u4ecd\u7136\u53d7\u5230\u4f4e\u5149\u7167\u6761\u4ef6\u3001\u5149\u7167\u53d8\u5316\u548c\u5267\u70c8\u8fd0\u52a8\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u4e8b\u4ef6\u7684\u76f8\u673a\u662f\u53d7\u751f\u7269\u542f\u53d1\u7684\u89c6\u89c9\u4f20\u611f\u5668\uff0c\u5728 HDR \u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\u5e76\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u56e0\u6b64\u5728\u8fd9\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u63d0\u4f9b\u4e86\u6709\u8da3\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u867d\u7136\u7eaf\u7cb9\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89e3\u51b3\u65b9\u6848\u76ee\u524d\u5c1a\u672a\u4ea7\u751f\u4ee4\u4eba\u6ee1\u610f\u7684\u6620\u5c04\u7ed3\u679c\uff0c\u4f46\u76ee\u524d\u7684\u5de5\u4f5c\u8bc1\u660e\u4e86\u5982\u679c\u5141\u8bb8\u4f7f\u7528\u66ff\u4ee3\u4f20\u611f\u5668\u8fdb\u884c\u6620\u5c04\uff0c\u5219\u7eaf\u7cb9\u57fa\u4e8e\u4e8b\u4ef6\u7684\u8ddf\u8e2a\u7684\u53ef\u884c\u6027\u3002\u8be5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u534a\u5bc6\u96c6\u5730\u56fe\u548c\u4e8b\u4ef6\u7684\u51e0\u4f553D-2D\u914d\u51c6\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u53ef\u9760\u548c\u51c6\u786e\u7684\u8de8\u6a21\u6001\u8ddf\u8e2a\u7ed3\u679c\u3002\u5b9e\u9645\u76f8\u5173\u7684\u573a\u666f\u662f\u7531\u6df1\u5ea6\u76f8\u673a\u652f\u6301\u7684\u8ddf\u8e2a\u6216\u57fa\u4e8e\u5730\u56fe\u7684\u5b9a\u4f4d\u4ee5\u53ca\u7531\u5e38\u89c4\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u89c6\u89c9 SLAM \u6216\u8fd0\u52a8\u7ed3\u6784\u7cfb\u7edf\u4e8b\u5148\u521b\u5efa\u7684\u534a\u5bc6\u96c6\u5730\u56fe\u7ed9\u51fa\u7684\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u8fb9\u7f18\u7684 3D-2D \u5bf9\u9f50\u901a\u8fc7\u65b0\u9896\u7684\u6781\u6027\u611f\u77e5\u914d\u51c6\u8fdb\u884c\u4e86\u6269\u5c55\uff0c\u8be5\u914d\u51c6\u5229\u7528\u4ece\u4e8b\u4ef6\u6d41\u83b7\u5f97\u7684\u5e26\u7b26\u53f7\u65f6\u95f4\u8868\u9762\u56fe (STSM)\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u906e\u6321\u70b9\u5254\u9664\u7b56\u7565\u3002\u8fd9\u4e24\u79cd\u4fee\u6539\u90fd\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u5668\u7684\u901f\u5ea6\u53ca\u5176\u9488\u5bf9\u906e\u6321\u6216\u5927\u89c6\u70b9\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u6db5\u76d6\u4e0a\u8ff0\u6311\u6218\u6027\u6761\u4ef6\u7684\u8bb8\u591a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u4f7f\u7528\u5e38\u89c4\u76f8\u673a\u5b9e\u73b0\u7684\u7c7b\u4f3c\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002|[2401.08043v1](http://arxiv.org/pdf/2401.08043v1)|null|\n", "2401.08023": "|**2024-01-16**|**Spatial Channel State Information Prediction with Generative AI: Towards Holographic Communication and Digital Radio Twin**|\u5229\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u8fdb\u884c\u7a7a\u95f4\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u9884\u6d4b\uff1a\u8fc8\u5411\u5168\u606f\u901a\u4fe1\u548c\u6570\u5b57\u65e0\u7ebf\u7535\u5b6a\u751f|Lihao Zhang, Haijian Sun, Yong Zeng, Rose Qingyang Hu|As 5G technology becomes increasingly established, the anticipation for 6G is growing, which promises to deliver faster and more reliable wireless connections via cutting-edge radio technologies. However, efficient management method of the large-scale antenna arrays deployed by those radio technologies is crucial. Traditional management methods are mainly reactive, usually based on feedback from users to adapt to the dynamic wireless channel. However, a more promising approach lies in the prediction of spatial channel state information (spatial-CSI), which is an all-inclusive channel characterization and consists of all the feasible line-of-sight (LoS) and non-line-of-sight (NLoS) paths between the transmitter (Tx) and receiver (Rx), with the three-dimension (3D) trajectory, attenuation, phase shift, delay, and polarization of each path. Advances in hardware and neural networks make it possible to predict such spatial-CSI using precise environmental information, and further look into the possibility of holographic communication, which implies complete control over every aspect of the radio waves emitted. Based on the integration of holographic communication and digital twin, we proposed a new framework, digital radio twin, which takes advantages from both the digital world and deterministic control over radio waves, supporting a wide range of high-level applications. As a preliminary attempt towards this visionary direction, in this paper, we explore the use of generative artificial intelligence (AI) to pinpoint the valid paths in a given environment, demonstrating promising results, and highlighting the potential of this approach in driving forward the evolution of 6G wireless communication technologies.|\u968f\u7740 5G \u6280\u672f\u7684\u65e5\u76ca\u6210\u719f\uff0c\u4eba\u4eec\u5bf9 6G \u7684\u671f\u671b\u4e5f\u8d8a\u6765\u8d8a\u9ad8\uff0c\u5b83\u6709\u671b\u901a\u8fc7\u5c16\u7aef\u7684\u65e0\u7ebf\u7535\u6280\u672f\u63d0\u4f9b\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u7684\u65e0\u7ebf\u8fde\u63a5\u3002\u7136\u800c\uff0c\u5bf9\u8fd9\u4e9b\u65e0\u7ebf\u7535\u6280\u672f\u90e8\u7f72\u7684\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u7684\u6709\u6548\u7ba1\u7406\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u7ba1\u7406\u65b9\u6cd5\u4e3b\u8981\u662f\u53cd\u5e94\u5f0f\u7684\uff0c\u901a\u5e38\u6839\u636e\u7528\u6237\u7684\u53cd\u9988\u6765\u9002\u5e94\u52a8\u6001\u7684\u65e0\u7ebf\u4fe1\u9053\u3002\u7136\u800c\uff0c\u66f4\u6709\u524d\u9014\u7684\u65b9\u6cd5\u5728\u4e8e\u7a7a\u95f4\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08spatial-CSI\uff09\u7684\u9884\u6d4b\uff0c\u5b83\u662f\u4e00\u79cd\u5305\u7f57\u4e07\u8c61\u7684\u4fe1\u9053\u8868\u5f81\uff0c\u7531\u6240\u6709\u53ef\u884c\u7684\u89c6\u8ddd\uff08LoS\uff09\u548c\u975e\u89c6\u8ddd\u7ec4\u6210\u3002\u53d1\u5c04\u5668 (Tx) \u548c\u63a5\u6536\u5668 (Rx) \u4e4b\u95f4\u7684\u89c6\u7ebf (NLoS) \u8def\u5f84\uff0c\u4ee5\u53ca\u6bcf\u6761\u8def\u5f84\u7684\u4e09\u7ef4 (3D) \u8f68\u8ff9\u3001\u8870\u51cf\u3001\u76f8\u79fb\u3001\u5ef6\u8fdf\u548c\u504f\u632f\u3002\u786c\u4ef6\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u8fdb\u6b65\u4f7f\u5f97\u4f7f\u7528\u7cbe\u786e\u7684\u73af\u5883\u4fe1\u606f\u6765\u9884\u6d4b\u6b64\u7c7b\u7a7a\u95f4CSI\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u8fdb\u4e00\u6b65\u7814\u7a76\u5168\u606f\u901a\u4fe1\u7684\u53ef\u80fd\u6027\uff0c\u8fd9\u610f\u5473\u7740\u5bf9\u6240\u53d1\u5c04\u7684\u65e0\u7ebf\u7535\u6ce2\u7684\u5404\u4e2a\u65b9\u9762\u8fdb\u884c\u5b8c\u5168\u63a7\u5236\u3002\u57fa\u4e8e\u5168\u606f\u901a\u4fe1\u548c\u6570\u5b57\u5b6a\u751f\u7684\u878d\u5408\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u2014\u2014\u6570\u5b57\u65e0\u7ebf\u7535\u5b6a\u751f\uff0c\u5b83\u7ed3\u5408\u4e86\u6570\u5b57\u4e16\u754c\u548c\u65e0\u7ebf\u7535\u6ce2\u786e\u5b9a\u6027\u63a7\u5236\u7684\u4f18\u52bf\uff0c\u652f\u6301\u5e7f\u6cdb\u7684\u9ad8\u7ea7\u5e94\u7528\u3002\u4f5c\u4e3a\u671d\u7740\u8fd9\u4e00\u613f\u666f\u65b9\u5411\u7684\u521d\u6b65\u5c1d\u8bd5\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4f7f\u7528\u751f\u6210\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6765\u67e5\u660e\u7ed9\u5b9a\u73af\u5883\u4e2d\u7684\u6709\u6548\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u5e76\u5f3a\u8c03\u4e86\u8fd9\u79cd\u65b9\u6cd5\u5728\u63a8\u52a8\u8fdb\u5316\u65b9\u9762\u7684\u6f5c\u529b6G\u65e0\u7ebf\u901a\u4fe1\u6280\u672f\u3002|[2401.08023v1](http://arxiv.org/pdf/2401.08023v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2401.08256": "|**2024-01-16**|**Multitask Learning in Minimally Invasive Surgical Vision: A Review**|\u5fae\u521b\u624b\u672f\u89c6\u89c9\u4e2d\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff1a\u7efc\u8ff0|Oluwatosin Alabi, Tom Vercauteren, Miaojing Shi|Minimally invasive surgery (MIS) has revolutionized many procedures and led to reduced recovery time and risk of patient injury. However, MIS poses additional complexity and burden on surgical teams. Data-driven surgical vision algorithms are thought to be key building blocks in the development of future MIS systems with improved autonomy. Recent advancements in machine learning and computer vision have led to successful applications in analyzing videos obtained from MIS with the promise of alleviating challenges in MIS videos. Surgical scene and action understanding encompasses multiple related tasks that, when solved individually, can be memory-intensive, inefficient, and fail to capture task relationships. Multitask learning (MTL), a learning paradigm that leverages information from multiple related tasks to improve performance and aid generalization, is wellsuited for fine-grained and high-level understanding of MIS data. This review provides an overview of the current state-of-the-art MTL systems that leverage videos obtained from MIS. Beyond listing published approaches, we discuss the benefits and limitations of these MTL systems. Moreover, this manuscript presents an analysis of the literature for various application fields of MTL in MIS, including those with large models, highlighting notable trends, new directions of research, and developments.|\u5fae\u521b\u624b\u672f (MIS) \u5f7b\u5e95\u6539\u53d8\u4e86\u8bb8\u591a\u624b\u672f\u65b9\u5f0f\uff0c\u7f29\u77ed\u4e86\u6062\u590d\u65f6\u95f4\u5e76\u964d\u4f4e\u4e86\u60a3\u8005\u53d7\u4f24\u7684\u98ce\u9669\u3002\u7136\u800c\uff0cMIS \u7ed9\u624b\u672f\u56e2\u961f\u5e26\u6765\u4e86\u989d\u5916\u7684\u590d\u6742\u6027\u548c\u8d1f\u62c5\u3002\u6570\u636e\u9a71\u52a8\u7684\u624b\u672f\u89c6\u89c9\u7b97\u6cd5\u88ab\u8ba4\u4e3a\u662f\u5f00\u53d1\u672a\u6765\u5177\u6709\u66f4\u9ad8\u81ea\u4e3b\u6027\u7684 MIS \u7cfb\u7edf\u7684\u5173\u952e\u6784\u5efa\u6a21\u5757\u3002\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6700\u65b0\u8fdb\u5c55\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5206\u6790\u4ece MIS \u83b7\u5f97\u7684\u89c6\u9891\uff0c\u6709\u671b\u7f13\u89e3 MIS \u89c6\u9891\u4e2d\u7684\u6311\u6218\u3002\u624b\u672f\u573a\u666f\u548c\u52a8\u4f5c\u7406\u89e3\u5305\u542b\u591a\u4e2a\u76f8\u5173\u4efb\u52a1\uff0c\u5f53\u5355\u72ec\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\u65f6\uff0c\u8fd9\u4e9b\u4efb\u52a1\u53ef\u80fd\u4f1a\u5360\u7528\u5927\u91cf\u5185\u5b58\u3001\u6548\u7387\u4f4e\u4e0b\uff0c\u5e76\u4e14\u65e0\u6cd5\u6355\u83b7\u4efb\u52a1\u5173\u7cfb\u3002\u591a\u4efb\u52a1\u5b66\u4e60 (MTL) \u662f\u4e00\u79cd\u5229\u7528\u591a\u4e2a\u76f8\u5173\u4efb\u52a1\u7684\u4fe1\u606f\u6765\u63d0\u9ad8\u6027\u80fd\u5e76\u5e2e\u52a9\u6cdb\u5316\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u975e\u5e38\u9002\u5408\u5bf9 MIS \u6570\u636e\u8fdb\u884c\u7ec6\u7c92\u5ea6\u548c\u9ad8\u5c42\u6b21\u7684\u7406\u89e3\u3002\u8fd9\u7bc7\u8bc4\u8bba\u6982\u8ff0\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684 MTL \u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5229\u7528\u4e86\u4ece MIS \u83b7\u5f97\u7684\u89c6\u9891\u3002\u9664\u4e86\u5217\u51fa\u5df2\u53d1\u5e03\u7684\u65b9\u6cd5\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86\u8fd9\u4e9b MTL \u7cfb\u7edf\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u672c\u624b\u7a3f\u8fd8\u5bf9 MIS \u4e2d MTL \u7684\u5404\u4e2a\u5e94\u7528\u9886\u57df\uff08\u5305\u62ec\u5927\u578b\u6a21\u578b\uff09\u7684\u6587\u732e\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u7a81\u51fa\u4e86\u663e\u7740\u7684\u8d8b\u52bf\u3001\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u53d1\u5c55\u3002|[2401.08256v1](http://arxiv.org/pdf/2401.08256v1)|null|\n"}, "\u5176\u4ed6": {"2401.08328": "|**2024-01-16**|**Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation**|\u53d6\u6d88\u6df7\u5408\u6d4b\u8bd5\u65f6\u95f4\u5f52\u4e00\u5316\u7edf\u8ba1\uff1a\u5bf9\u6297\u6807\u7b7e\u65f6\u95f4\u76f8\u5173\u6027|Devavrat Tomar, Guillaume Vray, Jean-Philippe Thiran, Behzad Bozorgtabar|In an era where test-time adaptation methods increasingly rely on the nuanced manipulation of batch normalization (BN) parameters, one critical assumption often goes overlooked: that of independently and identically distributed (i.i.d.) test batches with respect to unknown labels. This assumption culminates in biased estimates of BN statistics and jeopardizes system stability under non-i.i.d. conditions. This paper pioneers a departure from the i.i.d. paradigm by introducing a groundbreaking strategy termed \"Un-Mixing Test-Time Normalization Statistics\" (UnMix-TNS). UnMix-TNS re-calibrates the instance-wise statistics used to normalize each instance in a batch by mixing it with multiple unmixed statistics components, thus inherently simulating the i.i.d. environment. The key lies in our innovative online unmixing procedure, which persistently refines these statistics components by drawing upon the closest instances from an incoming test batch. Remarkably generic in its design, UnMix-TNS seamlessly integrates with an array of state-of-the-art test-time adaptation methods and pre-trained architectures equipped with BN layers. Empirical evaluations corroborate the robustness of UnMix-TNS under varied scenarios ranging from single to continual and mixed domain shifts. UnMix-TNS stands out when handling test data streams with temporal correlation, including those with corrupted real-world non-i.i.d. streams, sustaining its efficacy even with minimal batch sizes and individual samples. Our results set a new standard for test-time adaptation, demonstrating significant improvements in both stability and performance across multiple benchmarks.|\u5728\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u6279\u91cf\u5f52\u4e00\u5316 (BN) \u53c2\u6570\u7684\u7ec6\u5fae\u64cd\u4f5c\u7684\u65f6\u4ee3\uff0c\u4e00\u4e2a\u5173\u952e\u5047\u8bbe\u7ecf\u5e38\u88ab\u5ffd\u89c6\uff1a\u76f8\u5bf9\u4e8e\u672a\u77e5\u6807\u7b7e\u7684\u72ec\u7acb\u540c\u5206\u5e03 (i.i.d.) \u6d4b\u8bd5\u6279\u6b21\u3002\u8fd9\u79cd\u5047\u8bbe\u6700\u7ec8\u4f1a\u5bfc\u81f4 BN \u7edf\u8ba1\u6570\u636e\u7684\u4f30\u8ba1\u51fa\u73b0\u504f\u5dee\uff0c\u5e76\u5371\u53ca\u975e\u72ec\u7acb\u540c\u5206\u5e03\u4e0b\u7684\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002\u72b6\u51b5\u3002\u8fd9\u7bc7\u8bba\u6587\u5f00\u521b\u4e86\u5bf9\u72ec\u7acb\u540c\u5206\u5e03\u7684\u80cc\u79bb\u3002\u8303\u5f0f\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u975e\u6df7\u5408\u6d4b\u8bd5\u65f6\u95f4\u6807\u51c6\u5316\u7edf\u8ba1\u201d\uff08UnMix-TNS\uff09\u7684\u7a81\u7834\u6027\u7b56\u7565\u3002 UnMix-TNS \u901a\u8fc7\u5c06\u5b9e\u4f8b\u4e0e\u591a\u4e2a\u672a\u6df7\u5408\u7684\u7edf\u8ba1\u7ec4\u4ef6\u6df7\u5408\u6765\u91cd\u65b0\u6821\u51c6\u7528\u4e8e\u6807\u51c6\u5316\u6279\u6b21\u4e2d\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u5b9e\u4f8b\u7edf\u8ba1\u6570\u636e\uff0c\u4ece\u800c\u672c\u8d28\u4e0a\u6a21\u62df\u72ec\u7acb\u540c\u5206\u5e03\u3002\u73af\u5883\u3002\u5173\u952e\u5728\u4e8e\u6211\u4eec\u521b\u65b0\u7684\u5728\u7ebf\u5206\u89e3\u7a0b\u5e8f\uff0c\u8be5\u7a0b\u5e8f\u901a\u8fc7\u5229\u7528\u4f20\u5165\u6d4b\u8bd5\u6279\u6b21\u4e2d\u6700\u63a5\u8fd1\u7684\u5b9e\u4f8b\u6765\u6301\u7eed\u5b8c\u5584\u8fd9\u4e9b\u7edf\u8ba1\u7ec4\u4ef6\u3002 UnMix-TNS \u7684\u8bbe\u8ba1\u975e\u5e38\u901a\u7528\uff0c\u5b83\u4e0e\u4e00\u7cfb\u5217\u6700\u5148\u8fdb\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5\u548c\u914d\u5907 BN \u5c42\u7684\u9884\u8bad\u7ec3\u67b6\u6784\u65e0\u7f1d\u96c6\u6210\u3002\u5b9e\u8bc1\u8bc4\u4f30\u8bc1\u5b9e\u4e86 UnMix-TNS \u5728\u4ece\u5355\u4e00\u57df\u8f6c\u6362\u5230\u8fde\u7eed\u57df\u8f6c\u6362\u548c\u6df7\u5408\u57df\u8f6c\u6362\u7684\u5404\u79cd\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002 UnMix-TNS \u5728\u5904\u7406\u5177\u6709\u65f6\u95f4\u76f8\u5173\u6027\u7684\u6d4b\u8bd5\u6570\u636e\u6d41\uff08\u5305\u62ec\u90a3\u4e9b\u5177\u6709\u635f\u574f\u7684\u73b0\u5b9e\u4e16\u754c\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u6570\u636e\u6d41\uff09\u65f6\u8131\u9896\u800c\u51fa\u3002\u5373\u4f7f\u5728\u6700\u5c0f\u6279\u91cf\u548c\u5355\u4e2a\u6837\u54c1\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u7ef4\u6301\u5176\u529f\u6548\u3002\u6211\u4eec\u7684\u7ed3\u679c\u4e3a\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u8bc1\u660e\u4e86\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u7684\u663e\u7740\u6539\u8fdb\u3002|[2401.08328v1](http://arxiv.org/pdf/2401.08328v1)|null|\n", "2401.08281": "|**2024-01-16**|**The Faiss library**|\u8d39\u65af\u56fe\u4e66\u9986|Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar\u00e9, Maria Lomeli, Lucas Hosseini, Herv\u00e9 J\u00e9gou|Vector databases manage large collections of embedding vectors. As AI applications are growing rapidly, so are the number of embeddings that need to be stored and indexed. The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors. This paper first describes the tradeoff space of vector search, then the design principles of Faiss in terms of structure, approach to optimization and interfacing. We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability.|\u77e2\u91cf\u6570\u636e\u5e93\u7ba1\u7406\u5927\u91cf\u5d4c\u5165\u77e2\u91cf\u3002\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u7a0b\u5e8f\u7684\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u5b58\u50a8\u548c\u7d22\u5f15\u7684\u5d4c\u5165\u6570\u91cf\u4e5f\u5728\u5feb\u901f\u589e\u957f\u3002 Faiss \u5e93\u81f4\u529b\u4e8e\u77e2\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\uff0c\u8fd9\u662f\u77e2\u91cf\u6570\u636e\u5e93\u7684\u6838\u5fc3\u529f\u80fd\u3002 Faiss \u662f\u4e00\u4e2a\u5305\u542b\u7d22\u5f15\u65b9\u6cd5\u548c\u76f8\u5173\u539f\u8bed\u7684\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u641c\u7d22\u3001\u805a\u7c7b\u3001\u538b\u7f29\u548c\u8f6c\u6362\u5411\u91cf\u3002\u672c\u6587\u9996\u5148\u63cf\u8ff0\u4e86\u5411\u91cf\u641c\u7d22\u7684\u6743\u8861\u7a7a\u95f4\uff0c\u7136\u540e\u63cf\u8ff0\u4e86Faiss\u5728\u7ed3\u6784\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u63a5\u53e3\u65b9\u9762\u7684\u8bbe\u8ba1\u539f\u5219\u3002\u6211\u4eec\u5bf9\u5e93\u7684\u4e3b\u8981\u529f\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e00\u4e9b\u9009\u5b9a\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ee5\u5f3a\u8c03\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002|[2401.08281v1](http://arxiv.org/pdf/2401.08281v1)|null|\n", "2401.08272": "|**2024-01-16**|**Siamese Content-based Search Engine for a More Transparent Skin and Breast Cancer Diagnosis through Histological Imaging**|\u57fa\u4e8e\u8fde\u4f53\u5185\u5bb9\u7684\u641c\u7d22\u5f15\u64ce\uff0c\u901a\u8fc7\u7ec4\u7ec7\u5b66\u6210\u50cf\u5b9e\u73b0\u66f4\u900f\u660e\u7684\u76ae\u80a4\u548c\u4e73\u817a\u764c\u8bca\u65ad|Zahra Tabatabaei, Adri\u00e1n Colomer, JAvier Oliver Moll, Valery Naranjo|Computer Aid Diagnosis (CAD) has developed digital pathology with Deep Learning (DL)-based tools to assist pathologists in decision-making. Content-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek highly correlated patches in terms of similarity in histopathological features. In this work, we proposed two CBHIR approaches on breast (Breast-twins) and skin cancer (Skin-twins) data sets for robust and accurate patch-level retrieval, integrating a custom-built Siamese network as a feature extractor. The proposed Siamese network is able to generalize for unseen images by focusing on the similar histopathological features of the input pairs. The proposed CBHIR approaches are evaluated on the Breast (public) and Skin (private) data sets with top K accuracy. Finding the optimum amount of K is challenging, but also, as much as K increases, the dissimilarity between the query and the returned images increases which might mislead the pathologists. To the best of the author's belief, this paper is tackling this issue for the first time on histopathological images by evaluating the top first retrieved images. The Breast-twins model achieves 70% of the F1score at the top first, which exceeds the other state-of-the-art methods at a higher amount of K such as 5 and 400. Skin-twins overpasses the recently proposed Convolutional Auto Encoder (CAE) by 67%, increasing the precision. Besides, the Skin-twins model tackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential (STUMP) to assist pathologists with retrieving top K images and their corresponding labels. So, this approach can offer a more explainable CAD tool to pathologists in terms of transparency, trustworthiness, or reliability among other characteristics.|\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad (CAD) \u5229\u7528\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60 (DL) \u7684\u5de5\u5177\u5f00\u53d1\u4e86\u6570\u5b57\u75c5\u7406\u5b66\uff0c\u4ee5\u534f\u52a9\u75c5\u7406\u5b66\u5bb6\u505a\u51fa\u51b3\u7b56\u3002\u57fa\u4e8e\u5185\u5bb9\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u68c0\u7d22\uff08CBHIR\uff09\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5de5\u5177\uff0c\u53ef\u6839\u636e\u7ec4\u7ec7\u75c5\u7406\u5b66\u7279\u5f81\u7684\u76f8\u4f3c\u6027\u5bfb\u627e\u9ad8\u5ea6\u76f8\u5173\u7684\u6591\u5757\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u9488\u5bf9\u4e73\u817a\u764c\uff08Breast-twins\uff09\u548c\u76ae\u80a4\u764c\uff08Skin-twins\uff09\u6570\u636e\u96c6\u63d0\u51fa\u4e86\u4e24\u79cd CBHIR \u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5065\u4e14\u51c6\u786e\u7684\u8865\u4e01\u7ea7\u68c0\u7d22\uff0c\u5e76\u5c06\u5b9a\u5236\u7684\u8fde\u4f53\u7f51\u7edc\u96c6\u6210\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\u3002\u6240\u63d0\u51fa\u7684 Siamese \u7f51\u7edc\u80fd\u591f\u901a\u8fc7\u5173\u6ce8\u8f93\u5165\u5bf9\u7684\u76f8\u4f3c\u7ec4\u7ec7\u75c5\u7406\u5b66\u7279\u5f81\u6765\u6982\u62ec\u672a\u89c1\u8fc7\u7684\u56fe\u50cf\u3002\u6240\u63d0\u51fa\u7684 CBHIR \u65b9\u6cd5\u5728\u4e73\u623f\uff08\u516c\u5171\uff09\u548c\u76ae\u80a4\uff08\u79c1\u4eba\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5177\u6709\u6700\u9ad8 K \u7cbe\u5ea6\u3002\u627e\u5230 K \u7684\u6700\u4f73\u91cf\u5177\u6709\u6311\u6218\u6027\uff0c\u800c\u4e14\u968f\u7740 K \u7684\u589e\u52a0\uff0c\u67e5\u8be2\u548c\u8fd4\u56de\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u4e5f\u4f1a\u589e\u52a0\uff0c\u8fd9\u53ef\u80fd\u4f1a\u8bef\u5bfc\u75c5\u7406\u5b66\u5bb6\u3002\u636e\u4f5c\u8005\u6240\u77e5\uff0c\u672c\u6587\u901a\u8fc7\u8bc4\u4f30\u6700\u5148\u68c0\u7d22\u5230\u7684\u56fe\u50cf\uff0c\u9996\u6b21\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e0a\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\u3002 Breast-twins \u6a21\u578b\u9996\u5148\u8fbe\u5230\u4e86 F1score \u7684 70%\uff0c\u8fd9\u5728\u66f4\u9ad8\u7684 K \u503c\uff08\u4f8b\u5982 5 \u548c 400\uff09\u4e0b\u8d85\u8fc7\u4e86\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002Skin-twins \u8d85\u8d8a\u4e86\u6700\u8fd1\u63d0\u51fa\u7684\u5377\u79ef\u81ea\u52a8\u7f16\u7801\u5668(CAE) \u63d0\u9ad8\u4e86 67%\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u76ae\u80a4\u53cc\u80de\u80ce\u6a21\u578b\u89e3\u51b3\u4e86\u4e0d\u786e\u5b9a\u6076\u6027\u6f5c\u80fd\u7684 Spitzoid \u80bf\u7624 (STUMP) \u7684\u6311\u6218\uff0c\u4ee5\u5e2e\u52a9\u75c5\u7406\u5b66\u5bb6\u68c0\u7d22\u524d K \u4e2a\u56fe\u50cf\u53ca\u5176\u76f8\u5e94\u7684\u6807\u7b7e\u3002\u56e0\u6b64\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u4e3a\u75c5\u7406\u5b66\u5bb6\u63d0\u4f9b\u4e00\u79cd\u5728\u900f\u660e\u5ea6\u3001\u53ef\u4fe1\u5ea6\u6216\u53ef\u9760\u6027\u7b49\u7279\u5f81\u65b9\u9762\u66f4\u6613\u4e8e\u89e3\u91ca\u7684 CAD \u5de5\u5177\u3002|[2401.08272v1](http://arxiv.org/pdf/2401.08272v1)|null|\n", "2401.08154": "|**2024-01-16**|**Learned Image Compression with ROI-Weighted Distortion and Bit Allocation**|\u901a\u8fc7 ROI \u52a0\u6743\u5931\u771f\u548c\u4f4d\u5206\u914d\u5b66\u4e60\u56fe\u50cf\u538b\u7f29|Wei Jiang, Yongqi Zhai, Hangyu Li, Ronggang Wang|This one page paper describes our method for the track of image compression. To achieve better perceptual quality, we use the adversarial loss to generate realistic textures, use region of interest (ROI) mask to guide the bit allocation for different regions. Our Team name is TLIC.|\u8fd9\u7bc7\u4e00\u9875\u7eb8\u63cf\u8ff0\u4e86\u6211\u4eec\u8ddf\u8e2a\u56fe\u50cf\u538b\u7f29\u7684\u65b9\u6cd5\u3002\u4e3a\u4e86\u83b7\u5f97\u66f4\u597d\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u6211\u4eec\u4f7f\u7528\u5bf9\u6297\u6027\u635f\u5931\u6765\u751f\u6210\u903c\u771f\u7684\u7eb9\u7406\uff0c\u4f7f\u7528\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u63a9\u6a21\u6765\u6307\u5bfc\u4e0d\u540c\u533a\u57df\u7684\u4f4d\u5206\u914d\u3002\u6211\u4eec\u7684\u56e2\u961f\u540d\u79f0\u662f TLIC\u3002|[2401.08154v1](http://arxiv.org/pdf/2401.08154v1)|null|\n", "2401.08117": "|**2024-01-16**|**E2HQV: High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning**|E2HQV\uff1a\u901a\u8fc7\u7406\u8bba\u542f\u53d1\u7684\u6a21\u578b\u8f85\u52a9\u6df1\u5ea6\u5b66\u4e60\u4ece\u4e8b\u4ef6\u6444\u50cf\u673a\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891|Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, Tongliang Liu|The bio-inspired event cameras or dynamic vision sensors are capable of asynchronously capturing per-pixel brightness changes (called event-streams) in high temporal resolution and high dynamic range. However, the non-structural spatial-temporal event-streams make it challenging for providing intuitive visualization with rich semantic information for human vision. It calls for events-to-video (E2V) solutions which take event-streams as input and generate high quality video frames for intuitive visualization. However, current solutions are predominantly data-driven without considering the prior knowledge of the underlying statistics relating event-streams and video frames. It highly relies on the non-linearity and generalization capability of the deep neural networks, thus, is struggling on reconstructing detailed textures when the scenes are complex. In this work, we propose \\textbf{E2HQV}, a novel E2V paradigm designed to produce high-quality video frames from events. This approach leverages a model-aided deep learning framework, underpinned by a theory-inspired E2V model, which is meticulously derived from the fundamental imaging principles of event cameras. To deal with the issue of state-reset in the recurrent components of E2HQV, we also design a temporal shift embedding module to further improve the quality of the video frames. Comprehensive evaluations on the real world event camera datasets validate our approach, with E2HQV, notably outperforming state-of-the-art approaches, e.g., surpassing the second best by over 40\\% for some evaluation metrics.|\u4eff\u751f\u4e8b\u4ef6\u76f8\u673a\u6216\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u80fd\u591f\u4ee5\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u5f02\u6b65\u6355\u83b7\u6bcf\u4e2a\u50cf\u7d20\u7684\u4eae\u5ea6\u53d8\u5316\uff08\u79f0\u4e3a\u4e8b\u4ef6\u6d41\uff09\u3002\u7136\u800c\uff0c\u975e\u7ed3\u6784\u6027\u7684\u65f6\u7a7a\u4e8b\u4ef6\u6d41\u4f7f\u5f97\u4e3a\u4eba\u7c7b\u89c6\u89c9\u63d0\u4f9b\u5177\u6709\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\u7684\u76f4\u89c2\u53ef\u89c6\u5316\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\u3002\u5b83\u9700\u8981\u4e8b\u4ef6\u5230\u89c6\u9891 (E2V) \u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u5c06\u4e8b\u4ef6\u6d41\u4f5c\u4e3a\u8f93\u5165\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u5e27\u4ee5\u5b9e\u73b0\u76f4\u89c2\u7684\u53ef\u89c6\u5316\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u662f\u6570\u636e\u9a71\u52a8\u7684\uff0c\u6ca1\u6709\u8003\u8651\u4e0e\u4e8b\u4ef6\u6d41\u548c\u89c6\u9891\u5e27\u76f8\u5173\u7684\u5e95\u5c42\u7edf\u8ba1\u6570\u636e\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u5b83\u9ad8\u5ea6\u4f9d\u8d56\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\uff0c\u5f53\u573a\u666f\u590d\u6742\u65f6\uff0c\u5f88\u96be\u91cd\u5efa\u8be6\u7ec6\u7684\u7eb9\u7406\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 \\textbf{E2HQV}\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684 E2V \u8303\u4f8b\uff0c\u65e8\u5728\u4ece\u4e8b\u4ef6\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u5e27\u3002\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528\u4e86\u6a21\u578b\u8f85\u52a9\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u7406\u8bba\u542f\u53d1\u7684 E2V \u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u8be5\u6a21\u578b\u662f\u4ece\u4e8b\u4ef6\u76f8\u673a\u7684\u57fa\u672c\u6210\u50cf\u539f\u7406\u4e2d\u7cbe\u5fc3\u63a8\u5bfc\u51fa\u6765\u7684\u3002\u4e3a\u4e86\u89e3\u51b3 E2HQV \u5faa\u73af\u7ec4\u4ef6\u4e2d\u7684\u72b6\u6001\u91cd\u7f6e\u95ee\u9898\uff0c\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u65f6\u95f4\u79fb\u4f4d\u5d4c\u5165\u6a21\u5757\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u89c6\u9891\u5e27\u7684\u8d28\u91cf\u3002\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e8b\u4ef6\u6444\u50cf\u673a\u6570\u636e\u96c6\u7684\u7efc\u5408\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0cE2HQV \u7684\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\uff0c\u5728\u67d0\u4e9b\u8bc4\u4f30\u6307\u6807\u4e0a\u8d85\u8fc7\u7b2c\u4e8c\u597d\u7684\u65b9\u6cd5 40% \u4ee5\u4e0a\u3002|[2401.08117v1](http://arxiv.org/pdf/2401.08117v1)|null|\n", "2401.08107": "|**2024-01-16**|**Deep Shape-Texture Statistics for Completely Blind Image Quality Evaluation**|\u7528\u4e8e\u5b8c\u5168\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u6df1\u5c42\u5f62\u72b6\u7eb9\u7406\u7edf\u8ba1|Yixuan Li, Peilin Chen, Hanwei Zhu, Keyan Ding, Leida Li, Shiqi Wang|Opinion-Unaware Blind Image Quality Assessment (OU-BIQA) models aim to predict image quality without training on reference images and subjective quality scores. Thereinto, image statistical comparison is a classic paradigm, while the performance is limited by the representation ability of visual descriptors. Deep features as visual descriptors have advanced IQA in recent research, but they are discovered to be highly texture-biased and lack of shape-bias. On this basis, we find out that image shape and texture cues respond differently towards distortions, and the absence of either one results in an incomplete image representation. Therefore, to formulate a well-round statistical description for images, we utilize the shapebiased and texture-biased deep features produced by Deep Neural Networks (DNNs) simultaneously. More specifically, we design a Shape-Texture Adaptive Fusion (STAF) module to merge shape and texture information, based on which we formulate qualityrelevant image statistics. The perceptual quality is quantified by the variant Mahalanobis Distance between the inner and outer Shape-Texture Statistics (DSTS), wherein the inner and outer statistics respectively describe the quality fingerprints of the distorted image and natural images. The proposed DSTS delicately utilizes shape-texture statistical relations between different data scales in the deep domain, and achieves state-of-the-art (SOTA) quality prediction performance on images with artificial and authentic distortions.|Opinion-Unaware \u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30 (OU-BIQA) \u6a21\u578b\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u8d28\u91cf\uff0c\u800c\u65e0\u9700\u5bf9\u53c2\u8003\u56fe\u50cf\u548c\u4e3b\u89c2\u8d28\u91cf\u5206\u6570\u8fdb\u884c\u8bad\u7ec3\u3002\u5176\u4e2d\uff0c\u56fe\u50cf\u7edf\u8ba1\u6bd4\u8f83\u662f\u7ecf\u5178\u8303\u4f8b\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u5230\u89c6\u89c9\u63cf\u8ff0\u7b26\u7684\u8868\u793a\u80fd\u529b\u7684\u9650\u5236\u3002\u5728\u6700\u8fd1\u7684\u7814\u7a76\u4e2d\uff0c\u4f5c\u4e3a\u89c6\u89c9\u63cf\u8ff0\u7b26\u7684\u6df1\u5c42\u7279\u5f81\u63d0\u9ad8\u4e86 IQA\uff0c\u4f46\u4eba\u4eec\u53d1\u73b0\u5b83\u4eec\u5177\u6709\u9ad8\u5ea6\u7eb9\u7406\u504f\u5dee\u4e14\u7f3a\u4e4f\u5f62\u72b6\u504f\u5dee\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u53d1\u73b0\u56fe\u50cf\u5f62\u72b6\u548c\u7eb9\u7406\u7ebf\u7d22\u5bf9\u626d\u66f2\u7684\u53cd\u5e94\u4e0d\u540c\uff0c\u5e76\u4e14\u5176\u4e2d\u4efb\u4f55\u4e00\u4e2a\u7684\u7f3a\u5931\u90fd\u4f1a\u5bfc\u81f4\u56fe\u50cf\u8868\u793a\u4e0d\u5b8c\u6574\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u5bf9\u56fe\u50cf\u5236\u5b9a\u5168\u9762\u7684\u7edf\u8ba1\u63cf\u8ff0\uff0c\u6211\u4eec\u540c\u65f6\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4ea7\u751f\u7684\u5f62\u72b6\u504f\u5dee\u548c\u7eb9\u7406\u504f\u5dee\u7684\u6df1\u5c42\u7279\u5f81\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5f62\u72b6-\u7eb9\u7406\u81ea\u9002\u5e94\u878d\u5408\uff08STAF\uff09\u6a21\u5757\u6765\u5408\u5e76\u5f62\u72b6\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u6211\u4eec\u5236\u5b9a\u4e0e\u8d28\u91cf\u76f8\u5173\u7684\u56fe\u50cf\u7edf\u8ba1\u6570\u636e\u3002\u611f\u77e5\u8d28\u91cf\u901a\u8fc7\u5185\u90e8\u548c\u5916\u90e8\u5f62\u72b6\u7eb9\u7406\u7edf\u8ba1\uff08DSTS\uff09\u4e4b\u95f4\u7684\u53d8\u4f53\u9a6c\u54c8\u62c9\u8bfa\u6bd4\u65af\u8ddd\u79bb\u6765\u91cf\u5316\uff0c\u5176\u4e2d\u5185\u90e8\u548c\u5916\u90e8\u7edf\u8ba1\u5206\u522b\u63cf\u8ff0\u5931\u771f\u56fe\u50cf\u548c\u81ea\u7136\u56fe\u50cf\u7684\u8d28\u91cf\u6307\u7eb9\u3002\u6240\u63d0\u51fa\u7684 DSTS \u5de7\u5999\u5730\u5229\u7528\u4e86\u6df1\u57df\u4e2d\u4e0d\u540c\u6570\u636e\u5c3a\u5ea6\u4e4b\u95f4\u7684\u5f62\u72b6\u7eb9\u7406\u7edf\u8ba1\u5173\u7cfb\uff0c\u5e76\u5728\u5177\u6709\u4eba\u5de5\u548c\u771f\u5b9e\u5931\u771f\u7684\u56fe\u50cf\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u8d28\u91cf\u9884\u6d4b\u6027\u80fd\u3002|[2401.08107v1](http://arxiv.org/pdf/2401.08107v1)|null|\n", "2401.08100": "|**2024-01-16**|**KTVIC: A Vietnamese Image Captioning Dataset on the Life Domain**|KTVIC\uff1a\u751f\u547d\u9886\u57df\u7684\u8d8a\u5357\u56fe\u50cf\u5b57\u5e55\u6570\u636e\u96c6|Anh-Cuong Pham, Van-Quang Nguyen, Thi-Hong Vuong, Quang-Thuy Ha|Image captioning is a crucial task with applications in a wide range of domains, including healthcare and education. Despite extensive research on English image captioning datasets, the availability of such datasets for Vietnamese remains limited, with only two existing datasets. In this study, we introduce KTVIC, a comprehensive Vietnamese Image Captioning dataset focused on the life domain, covering a wide range of daily activities. This dataset comprises 4,327 images and 21,635 Vietnamese captions, serving as a valuable resource for advancing image captioning in the Vietnamese language. We conduct experiments using various deep neural networks as the baselines on our dataset, evaluating them using the standard image captioning metrics, including BLEU, METEOR, CIDEr, and ROUGE. Our findings underscore the effectiveness of the proposed dataset and its potential contributions to the field of image captioning in the Vietnamese context.|\u56fe\u50cf\u5b57\u5e55\u662f\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u5176\u5e94\u7528\u8303\u56f4\u5e7f\u6cdb\uff0c\u5305\u62ec\u533b\u7597\u4fdd\u5065\u548c\u6559\u80b2\u3002\u5c3d\u7ba1\u5bf9\u82f1\u8bed\u56fe\u50cf\u5b57\u5e55\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\uff0c\u4f46\u6b64\u7c7b\u8d8a\u5357\u8bed\u6570\u636e\u96c6\u7684\u53ef\u7528\u6027\u4ecd\u7136\u6709\u9650\uff0c\u73b0\u6709\u7684\u6570\u636e\u96c6\u53ea\u6709\u4e24\u4e2a\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 KTVIC\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u751f\u6d3b\u9886\u57df\u7684\u7efc\u5408\u8d8a\u5357\u56fe\u50cf\u5b57\u5e55\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u5e7f\u6cdb\u7684\u65e5\u5e38\u6d3b\u52a8\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b 4,327 \u5f20\u56fe\u50cf\u548c 21,635 \u4e2a\u8d8a\u5357\u8bed\u5b57\u5e55\uff0c\u662f\u63a8\u8fdb\u8d8a\u5357\u8bed\u56fe\u50cf\u5b57\u5e55\u7684\u5b9d\u8d35\u8d44\u6e90\u3002\u6211\u4eec\u4f7f\u7528\u5404\u79cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u6570\u636e\u96c6\u7684\u57fa\u7ebf\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u56fe\u50cf\u5b57\u5e55\u6307\u6807\uff08\u5305\u62ec BLEU\u3001METEOR\u3001CIDEr \u548c ROUGE\uff09\u5bf9\u5176\u8fdb\u884c\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u53ca\u5176\u5bf9\u8d8a\u5357\u80cc\u666f\u4e0b\u56fe\u50cf\u5b57\u5e55\u9886\u57df\u7684\u6f5c\u5728\u8d21\u732e\u3002|[2401.08100v1](http://arxiv.org/pdf/2401.08100v1)|null|\n", "2401.08068": "|**2024-01-16**|**Representation Learning on Event Stream via an Elastic Net-incorporated Tensor Network**|\u901a\u8fc7\u5f39\u6027\u7f51\u7edc\u7ed3\u5408\u7684\u5f20\u91cf\u7f51\u7edc\u5bf9\u4e8b\u4ef6\u6d41\u8fdb\u884c\u8868\u793a\u5b66\u4e60|Beibei Yang, Weiling Li, Yan Fang|Event cameras are neuromorphic sensors that capture asynchronous and sparse event stream when per-pixel brightness changes. The state-of-the-art processing methods for event signals typically aggregate events into a frame or a grid. However, events are dense in time, these works are limited to local information of events due to the stacking. In this paper, we present a novel spatiotemporal representation learning method which can capture the global correlations of all events in the event stream simultaneously by tensor decomposition. In addition, with the events are sparse in space, we propose an Elastic Net-incorporated tensor network (ENTN) model to obtain more spatial and temporal details about event stream. Empirically, the results indicate that our method can represent the spatiotemporal correlation of events with high quality, and can achieve effective results in applications like filtering noise compared with the state-of-the-art methods.|\u4e8b\u4ef6\u76f8\u673a\u662f\u795e\u7ecf\u5f62\u6001\u4f20\u611f\u5668\uff0c\u53ef\u5728\u6bcf\u50cf\u7d20\u4eae\u5ea6\u53d1\u751f\u53d8\u5316\u65f6\u6355\u83b7\u5f02\u6b65\u4e14\u7a00\u758f\u7684\u4e8b\u4ef6\u6d41\u3002\u6700\u5148\u8fdb\u7684\u4e8b\u4ef6\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u901a\u5e38\u5c06\u4e8b\u4ef6\u805a\u5408\u5230\u5e27\u6216\u7f51\u683c\u4e2d\u3002\u7136\u800c\uff0c\u4e8b\u4ef6\u5728\u65f6\u95f4\u4e0a\u662f\u5bc6\u96c6\u7684\uff0c\u7531\u4e8e\u5806\u53e0\uff0c\u8fd9\u4e9b\u5de5\u4f5c\u4ec5\u9650\u4e8e\u4e8b\u4ef6\u7684\u5c40\u90e8\u4fe1\u606f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u7a7a\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u540c\u65f6\u6355\u83b7\u4e8b\u4ef6\u6d41\u4e2d\u6240\u6709\u4e8b\u4ef6\u7684\u5168\u5c40\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u4e8b\u4ef6\u5728\u7a7a\u95f4\u4e0a\u7a00\u758f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5f39\u6027\u7f51\u7edc\u5408\u5e76\u5f20\u91cf\u7f51\u7edc\uff08ENTN\uff09\u6a21\u578b\u6765\u83b7\u53d6\u6709\u5173\u4e8b\u4ef6\u6d41\u7684\u66f4\u591a\u7a7a\u95f4\u548c\u65f6\u95f4\u7ec6\u8282\u3002\u6839\u636e\u7ecf\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u9ad8\u8d28\u91cf\u5730\u8868\u793a\u4e8b\u4ef6\u7684\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53ef\u4ee5\u5728\u8fc7\u6ee4\u566a\u58f0\u7b49\u5e94\u7528\u4e2d\u53d6\u5f97\u6709\u6548\u7684\u7ed3\u679c\u3002|[2401.08068v1](http://arxiv.org/pdf/2401.08068v1)|null|\n", "2401.08053": "|**2024-01-16**|**SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation**|SCoFT\uff1a\u81ea\u6211\u5bf9\u6bd4\u5fae\u8c03\u4ee5\u5b9e\u73b0\u516c\u5e73\u7684\u56fe\u50cf\u751f\u6210|Zhixuan Liu, Peter Schaldenbrand, Beverley-Claire Okogwu, Wenxuan Peng, Youngsik Yun, Andrew Hundt, Jihie Kim, Jean Oh|Accurate representation in media is known to improve the well-being of the people who consume it. Generative image models trained on large web-crawled datasets such as LAION are known to produce images with harmful stereotypes and misrepresentations of cultures. We improve inclusive representation in generated images by (1) engaging with communities to collect a culturally representative dataset that we call the Cross-Cultural Understanding Benchmark (CCUB) and (2) proposing a novel Self-Contrastive Fine-Tuning (SCoFT) method that leverages the model's known biases to self-improve. SCoFT is designed to prevent overfitting on small datasets, encode only high-level information from the data, and shift the generated distribution away from misrepresentations encoded in a pretrained model. Our user study conducted on 51 participants from 5 different countries based on their self-selected national cultural affiliation shows that fine-tuning on CCUB consistently generates images with higher cultural relevance and fewer stereotypes when compared to the Stable Diffusion baseline, which is further improved with our SCoFT technique.|\u4f17\u6240\u5468\u77e5\uff0c\u5a92\u4f53\u7684\u51c6\u786e\u62a5\u9053\u53ef\u4ee5\u6539\u5584\u5a92\u4f53\u53d7\u4f17\u7684\u798f\u7949\u3002\u4f17\u6240\u5468\u77e5\uff0c\u5728 LAION \u7b49\u5927\u578b\u7f51\u7edc\u722c\u53d6\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u751f\u6210\u56fe\u50cf\u6a21\u578b\u4f1a\u751f\u6210\u5e26\u6709\u6709\u5bb3\u523b\u677f\u5370\u8c61\u548c\u6b6a\u66f2\u6587\u5316\u7684\u56fe\u50cf\u3002\u6211\u4eec\u901a\u8fc7\uff081\uff09\u4e0e\u793e\u533a\u5408\u4f5c\u6536\u96c6\u5177\u6709\u6587\u5316\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\uff08\u6211\u4eec\u79f0\u4e4b\u4e3a\u8de8\u6587\u5316\u7406\u89e3\u57fa\u51c6\uff08CCUB\uff09\uff09\u548c\uff082\uff09\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u6211\u5bf9\u6bd4\u5fae\u8c03\uff08SCoFT\uff09\u65b9\u6cd5\u6765\u6539\u5584\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u5305\u5bb9\u6027\u8868\u793a\u5229\u7528\u6a21\u578b\u7684\u5df2\u77e5\u504f\u5dee\u6765\u81ea\u6211\u6539\u8fdb\u3002 SCoFT \u65e8\u5728\u9632\u6b62\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u8fc7\u5ea6\u62df\u5408\uff0c\u4ec5\u5bf9\u6570\u636e\u4e2d\u7684\u9ad8\u7ea7\u4fe1\u606f\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u4f7f\u751f\u6210\u7684\u5206\u5e03\u8fdc\u79bb\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7f16\u7801\u7684\u9519\u8bef\u8868\u793a\u3002\u6211\u4eec\u5bf9\u6765\u81ea 5 \u4e2a\u4e0d\u540c\u56fd\u5bb6\u7684 51 \u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e86\u57fa\u4e8e\u4ed6\u4eec\u81ea\u884c\u9009\u62e9\u7684\u56fd\u5bb6\u6587\u5316\u5f52\u5c5e\u7684\u7528\u6237\u7814\u7a76\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7a33\u5b9a\u6269\u6563\u57fa\u7ebf\u76f8\u6bd4\uff0cCCUB \u4e0a\u7684\u5fae\u8c03\u59cb\u7ec8\u80fd\u591f\u751f\u6210\u5177\u6709\u66f4\u9ad8\u6587\u5316\u76f8\u5173\u6027\u548c\u66f4\u5c11\u523b\u677f\u5370\u8c61\u7684\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6539\u8fdb\uff1a\u6211\u4eec\u7684 SCoFT \u6280\u672f\u3002|[2401.08053v1](http://arxiv.org/pdf/2401.08053v1)|null|\n"}}