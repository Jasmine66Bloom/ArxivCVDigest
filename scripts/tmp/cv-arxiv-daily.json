{"\u751f\u6210\u6a21\u578b": {"2402.12376": "|**2024-02-19**|**FiT: Flexible Vision Transformer for Diffusion Model**|FiT\uff1a\u7528\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7075\u6d3b\u89c6\u89c9\u53d8\u538b\u5668|Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, Lei Bai|Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at https://github.com/whlzy/FiT.|\u81ea\u7136\u662f\u65e0\u9650\u65e0\u5206\u8fa8\u7387\u7684\u3002\u5728\u8fd9\u79cd\u73b0\u5b9e\u80cc\u666f\u4e0b\uff0c\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\uff08\u4f8b\u5982\u6269\u6563\u53d8\u538b\u5668\uff09\u5728\u5904\u7406\u8bad\u7ec3\u57df\u4e4b\u5916\u7684\u56fe\u50cf\u5206\u8fa8\u7387\u65f6\u7ecf\u5e38\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7075\u6d3b\u89c6\u89c9\u53d8\u538b\u5668\uff08FiT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a\u751f\u6210\u5206\u8fa8\u7387\u548c\u7eb5\u6a2a\u6bd4\u4e0d\u53d7\u9650\u5236\u7684\u56fe\u50cf\u800c\u8bbe\u8ba1\u7684\u53d8\u538b\u5668\u67b6\u6784\u3002\u4e0e\u5c06\u56fe\u50cf\u89c6\u4e3a\u9759\u6001\u5206\u8fa8\u7387\u7f51\u683c\u7684\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0cFiT \u5c06\u56fe\u50cf\u6982\u5ff5\u5316\u4e3a\u52a8\u6001\u5927\u5c0f\u7684\u6807\u8bb0\u5e8f\u5217\u3002\u8fd9\u79cd\u89c6\u89d2\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u8f7b\u677e\u9002\u5e94\u4e0d\u540c\u7684\u7eb5\u6a2a\u6bd4\uff0c\u4ece\u800c\u4fc3\u8fdb\u5206\u8fa8\u7387\u6cdb\u5316\u5e76\u6d88\u9664\u56fe\u50cf\u88c1\u526a\u5f15\u8d77\u7684\u504f\u5dee\u3002\u901a\u8fc7\u7cbe\u5fc3\u8c03\u6574\u7684\u7f51\u7edc\u7ed3\u6784\u548c\u514d\u8bad\u7ec3\u5916\u63a8\u6280\u672f\u7684\u96c6\u6210\uff0cFiT \u5728\u5206\u8fa8\u7387\u5916\u63a8\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u7075\u6d3b\u6027\u3002\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86 FiT \u5728\u5e7f\u6cdb\u7684\u5206\u8fa8\u7387\u8303\u56f4\u5185\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bad\u7ec3\u5206\u8fa8\u7387\u5206\u5e03\u5185\u5916\u7684\u6709\u6548\u6027\u3002\u5b58\u50a8\u5e93\u4f4d\u4e8e https://github.com/whlzy/FiT\u3002|[2402.12376v1](http://arxiv.org/pdf/2402.12376v1)|null|\n", "2402.12238": "|**2024-02-19**|**Mixed Gaussian Flow for Diverse Trajectory Prediction**|\u7528\u4e8e\u591a\u79cd\u8f68\u8ff9\u9884\u6d4b\u7684\u6df7\u5408\u9ad8\u65af\u6d41|Jiahe Chen, Jinkun Cao, Dahua Lin, Kris Kitani, Jiangmiao Pang|Existing trajectory prediction studies intensively leverage generative models. Normalizing flow is one of the genres with the advantage of being invertible to derive the probability density of predicted trajectories. However, mapping from a standard Gaussian by a flow-based model hurts the capacity to capture complicated patterns of trajectories, ignoring the under-represented motion intentions in the training data. To solve the problem, we propose a flow-based model to transform a mixed Gaussian prior into the future trajectory manifold. The model shows a better capacity for generating diverse trajectory patterns. Also, by associating each sub-Gaussian with a certain subspace of trajectories, we can generate future trajectories with controllable motion intentions. In such a fashion, the flow-based model is not encouraged to simply seek the most likelihood of the intended manifold anymore but a family of controlled manifolds with explicit interpretability. Our proposed method is demonstrated to show state-of-the-art performance in the quantitative evaluation of sampling well-aligned trajectories in top-M generated candidates. We also demonstrate that it can generate diverse, controllable, and out-of-distribution trajectories. Code is available at https://github.com/mulplue/MGF.|\u73b0\u6709\u7684\u8f68\u8ff9\u9884\u6d4b\u7814\u7a76\u96c6\u4e2d\u5229\u7528\u751f\u6210\u6a21\u578b\u3002\u5f52\u4e00\u5316\u6d41\u662f\u4e00\u79cd\u5177\u6709\u53ef\u9006\u6027\u7684\u4f18\u70b9\uff0c\u53ef\u4ee5\u5bfc\u51fa\u9884\u6d4b\u8f68\u8ff9\u7684\u6982\u7387\u5bc6\u5ea6\u3002\u7136\u800c\uff0c\u901a\u8fc7\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u4ece\u6807\u51c6\u9ad8\u65af\u6620\u5c04\u4f1a\u635f\u5bb3\u6355\u83b7\u590d\u6742\u8f68\u8ff9\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u5ffd\u7565\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u88ab\u5145\u5206\u4ee3\u8868\u7684\u8fd0\u52a8\u610f\u56fe\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\uff0c\u5c06\u6df7\u5408\u9ad8\u65af\u5148\u9a8c\u8f6c\u6362\u4e3a\u672a\u6765\u7684\u8f68\u8ff9\u6d41\u5f62\u3002\u8be5\u6a21\u578b\u663e\u793a\u51fa\u66f4\u597d\u7684\u751f\u6210\u4e0d\u540c\u8f68\u8ff9\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u4e9a\u9ad8\u65af\u4e0e\u7279\u5b9a\u7684\u8f68\u8ff9\u5b50\u7a7a\u95f4\u76f8\u5173\u8054\uff0c\u6211\u4eec\u53ef\u4ee5\u751f\u6210\u5177\u6709\u53ef\u63a7\u8fd0\u52a8\u610f\u56fe\u7684\u672a\u6765\u8f68\u8ff9\u3002\u4ee5\u8fd9\u79cd\u65b9\u5f0f\uff0c\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u4e0d\u518d\u88ab\u9f13\u52b1\u7b80\u5355\u5730\u5bfb\u627e\u9884\u671f\u6d41\u5f62\u7684\u6700\u5927\u53ef\u80fd\u6027\uff0c\u800c\u662f\u5bfb\u627e\u5177\u6709\u660e\u786e\u53ef\u89e3\u91ca\u6027\u7684\u53d7\u63a7\u6d41\u5f62\u65cf\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u88ab\u8bc1\u660e\u5728\u5bf9 top-M \u751f\u6210\u7684\u5019\u9009\u4e2d\u7684\u91c7\u6837\u5bf9\u9f50\u8f68\u8ff9\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\u65b9\u9762\u663e\u793a\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u8bc1\u660e\u5b83\u53ef\u4ee5\u751f\u6210\u591a\u6837\u5316\u3001\u53ef\u63a7\u4e14\u4e0d\u5206\u5e03\u7684\u8f68\u8ff9\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/mulplue/MGF \u83b7\u53d6\u3002|[2402.12238v1](http://arxiv.org/pdf/2402.12238v1)|null|\n", "2402.12226": "|**2024-02-19**|**AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling**|AnyGPT\uff1a\u5177\u6709\u79bb\u6563\u5e8f\u5217\u5efa\u6a21\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6cd5\u5b66\u7855\u58eb|Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et.al.|We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/|\u6211\u4eec\u4ecb\u7ecd AnyGPT\uff0c\u8fd9\u662f\u4e00\u79cd\u4efb\u610f\u5bf9\u4efb\u610f\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u5229\u7528\u79bb\u6563\u8868\u793a\u6765\u7edf\u4e00\u5904\u7406\u5404\u79cd\u6a21\u6001\uff0c\u5305\u62ec\u8bed\u97f3\u3001\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u4e50\u3002 AnyGPT \u53ef\u4ee5\u7a33\u5b9a\u5730\u8bad\u7ec3\uff0c\u65e0\u9700\u5bf9\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u67b6\u6784\u6216\u8bad\u7ec3\u8303\u5f0f\u8fdb\u884c\u4efb\u4f55\u6539\u53d8\u3002\u76f8\u53cd\uff0c\u5b83\u5b8c\u5168\u4f9d\u8d56\u4e8e\u6570\u636e\u7ea7\u9884\u5904\u7406\uff0c\u4fc3\u8fdb\u65b0\u6a21\u5f0f\u65e0\u7f1d\u96c6\u6210\u5230\u6cd5\u5b66\u7855\u58eb\u4e2d\uff0c\u7c7b\u4f3c\u4e8e\u65b0\u8bed\u8a00\u7684\u5408\u5e76\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u4ee5\u6587\u672c\u4e3a\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5bf9\u9f50\u9884\u8bad\u7ec3\u3002\u5229\u7528\u751f\u6210\u6a21\u578b\uff0c\u6211\u4eec\u5408\u6210\u4e86\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21\u4efb\u610f\u5bf9\u4efb\u610f\u591a\u6a21\u5f0f\u6307\u4ee4\u6570\u636e\u96c6\u3002\u5b83\u7531 108k \u4e2a\u591a\u8f6e\u5bf9\u8bdd\u6837\u672c\u7ec4\u6210\uff0c\u8fd9\u4e9b\u5bf9\u8bdd\u9519\u7efc\u590d\u6742\u5730\u4ea4\u7ec7\u7740\u5404\u79cd\u6a21\u6001\uff0c\u4ece\u800c\u4f7f\u6a21\u578b\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u548c\u8f93\u51fa\u7684\u4efb\u610f\u7ec4\u5408\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAnyGPT \u80fd\u591f\u4fc3\u8fdb\u4efb\u610f\u5bf9\u4efb\u610f\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\uff0c\u540c\u65f6\u5728\u6240\u6709\u6a21\u6001\u4e2d\u5b9e\u73b0\u4e0e\u4e13\u7528\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u79bb\u6563\u8868\u793a\u53ef\u4ee5\u6709\u6548\u4e14\u65b9\u4fbf\u5730\u7edf\u4e00\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u591a\u79cd\u6a21\u6001\u3002\u6f14\u793a\u89c1 https://junzhan2000.github.io/AnyGPT.github.io/|[2402.12226v1](http://arxiv.org/pdf/2402.12226v1)|null|\n", "2402.12187": "|**2024-02-19**|**Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training**|\u5bf9\u6297\u6027\u7279\u5f81\u5bf9\u9f50\uff1a\u901a\u8fc7\u5bf9\u6297\u6027\u8bad\u7ec3\u5e73\u8861\u6df1\u5ea6\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027|Leo Hyun Park, Jaeuk Kim, Myung Gyo Oh, Jaewoo Park, Taekyoung Kwon|Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples. Adversarial training is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algorithm based on contrastive learning to alleviate potential feature misalignment. Through our evaluations, we demonstrate the superior performance of AFA. The baseline AFA delivers higher robust accuracy than previous adversarial contrastive learning methods while minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and CIFAR100, respectively, in comparison to cross-entropy. We also show that joint optimization of AFA and TRADES, accompanied by data augmentation using a recent diffusion model, achieves state-of-the-art accuracy and robustness.|\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u51c6\u786e\u6027\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u9519\u8bef\u5206\u7c7b\u3002\u5bf9\u6297\u6027\u8bad\u7ec3\u7528\u4e8e\u901a\u8fc7\u63d0\u9ad8\u9488\u5bf9\u8fd9\u4e9b\u653b\u51fb\u7684\u9c81\u68d2\u6027\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u901a\u5e38\u4f1a\u964d\u4f4e\u6a21\u578b\u5728\u5e72\u51c0\u3001\u975e\u5bf9\u6297\u6027\u6837\u672c\u4e0a\u7684\u6807\u51c6\u7cbe\u5ea6\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e73\u8861\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u4ee5\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u5fc5\u8981\u6027\u662f\u663e\u800c\u6613\u89c1\u7684\uff0c\u4f46\u5b9e\u73b0\u8fd9\u79cd\u5e73\u8861\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5176\u6839\u672c\u539f\u56e0\u5c1a\u5f85\u9610\u660e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u5bf9\u6297\u7279\u5f81\u5bf9\u9f50\uff08AFA\uff09\u7684\u65b0\u578b\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u89c1\u89e3\uff1a\u7279\u5f81\u7a7a\u95f4\u5185\u7684\u9519\u4f4d\u901a\u5e38\u4f1a\u5bfc\u81f4\u9519\u8bef\u5206\u7c7b\uff0c\u65e0\u8bba\u6837\u672c\u662f\u826f\u6027\u7684\u8fd8\u662f\u5bf9\u6297\u6027\u7684\u3002 AFA \u901a\u8fc7\u91c7\u7528\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b0\u9896\u4f18\u5316\u7b97\u6cd5\u6765\u51cf\u8f7b\u6f5c\u5728\u7684\u7279\u5f81\u9519\u4f4d\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u8fd9\u79cd\u98ce\u9669\u3002\u901a\u8fc7\u6211\u4eec\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u5c55\u793a\u4e86 AFA \u7684\u5353\u8d8a\u6027\u80fd\u3002\u4e0e\u4ea4\u53c9\u71b5\u76f8\u6bd4\uff0c\u57fa\u7ebf AFA \u63d0\u4f9b\u4e86\u6bd4\u4e4b\u524d\u7684\u5bf9\u6297\u6027\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u66f4\u9ad8\u7684\u9c81\u68d2\u7cbe\u5ea6\uff0c\u540c\u65f6\u5c06 CIFAR10 \u548c CIFAR100 \u4e0a\u7684\u5e72\u51c0\u7cbe\u5ea6\u4e0b\u964d\u5e45\u5ea6\u964d\u81f3\u6700\u4f4e\uff0c\u5206\u522b\u4e3a 1.86% \u548c 8.91%\u3002\u6211\u4eec\u8fd8\u8868\u660e\uff0cAFA \u548c TRADES \u7684\u8054\u5408\u4f18\u5316\uff0c\u52a0\u4e0a\u4f7f\u7528\u6700\u65b0\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002|[2402.12187v1](http://arxiv.org/pdf/2402.12187v1)|null|\n", "2402.12128": "|**2024-02-19**|**3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection**|\u7531\u6700\u5927\u5f3a\u5ea6\u6295\u5f71\u7684 2D \u6ce8\u91ca\u76d1\u7763\u7684 3D \u8840\u7ba1\u5206\u5272|Zhanqiang Guo, Zimeng Tan, Jianjiang Feng, Jie Zhou|Vascular structure segmentation plays a crucial role in medical analysis and clinical applications. The practical adoption of fully supervised segmentation models is impeded by the intricacy and time-consuming nature of annotating vessels in the 3D space. This has spurred the exploration of weakly-supervised approaches that reduce reliance on expensive segmentation annotations. Despite this, existing weakly supervised methods employed in organ segmentation, which encompass points, bounding boxes, or graffiti, have exhibited suboptimal performance when handling sparse vascular structure. To alleviate this issue, we employ maximum intensity projection (MIP) to decrease the dimensionality of 3D volume to 2D image for efficient annotation, and the 2D labels are utilized to provide guidance and oversight for training 3D vessel segmentation model. Initially, we generate pseudo-labels for 3D blood vessels using the annotations of 2D projections. Subsequently, taking into account the acquisition method of the 2D labels, we introduce a weakly-supervised network that fuses 2D-3D deep features via MIP to further improve segmentation performance. Furthermore, we integrate confidence learning and uncertainty estimation to refine the generated pseudo-labels, followed by fine-tuning the segmentation network. Our method is validated on five datasets (including cerebral vessel, aorta and coronary artery), demonstrating highly competitive performance in segmenting vessels and the potential to significantly reduce the time and effort required for vessel annotation. Our code is available at: https://github.com/gzq17/Weakly-Supervised-by-MIP.|\u8840\u7ba1\u7ed3\u6784\u5206\u5272\u5728\u533b\u5b66\u5206\u6790\u548c\u4e34\u5e8a\u5e94\u7528\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002 3D \u7a7a\u95f4\u4e2d\u6ce8\u91ca\u8840\u7ba1\u7684\u590d\u6742\u6027\u548c\u8017\u65f6\u6027\u963b\u788d\u4e86\u5b8c\u5168\u76d1\u7763\u5206\u5272\u6a21\u578b\u7684\u5b9e\u9645\u91c7\u7528\u3002\u8fd9\u523a\u6fc0\u4e86\u5bf9\u5f31\u76d1\u7763\u65b9\u6cd5\u7684\u63a2\u7d22\uff0c\u4ee5\u51cf\u5c11\u5bf9\u6602\u8d35\u7684\u5206\u5272\u6ce8\u91ca\u7684\u4f9d\u8d56\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u5668\u5b98\u5206\u5272\u4e2d\u4f7f\u7528\u7684\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\uff08\u5305\u62ec\u70b9\u3001\u8fb9\u754c\u6846\u6216\u6d82\u9e26\uff09\u5728\u5904\u7406\u7a00\u758f\u8840\u7ba1\u7ed3\u6784\u65f6\u8868\u73b0\u51fa\u6b21\u4f18\u6027\u80fd\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u91c7\u7528\u6700\u5927\u5f3a\u5ea6\u6295\u5f71\uff08MIP\uff09\u5c063D\u4f53\u79ef\u964d\u7ef4\u4e3a2D\u56fe\u50cf\u4ee5\u8fdb\u884c\u6709\u6548\u6ce8\u91ca\uff0c\u5e76\u5229\u75282D\u6807\u7b7e\u4e3a\u8bad\u7ec33D\u8840\u7ba1\u5206\u5272\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc\u548c\u76d1\u7763\u3002\u6700\u521d\uff0c\u6211\u4eec\u4f7f\u7528 2D \u6295\u5f71\u7684\u6ce8\u91ca\u751f\u6210 3D \u8840\u7ba1\u7684\u4f2a\u6807\u7b7e\u3002\u968f\u540e\uff0c\u8003\u8651\u5230 2D \u6807\u7b7e\u7684\u83b7\u53d6\u65b9\u6cd5\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u7f51\u7edc\uff0c\u901a\u8fc7 MIP \u878d\u5408 2D-3D \u6df1\u5ea6\u7279\u5f81\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5206\u5272\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u96c6\u6210\u7f6e\u4fe1\u5ea6\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u7ec6\u5316\u751f\u6210\u7684\u4f2a\u6807\u7b7e\uff0c\u7136\u540e\u5fae\u8c03\u5206\u5272\u7f51\u7edc\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e94\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u8111\u8840\u7ba1\u3001\u4e3b\u52a8\u8109\u548c\u51a0\u72b6\u52a8\u8109\uff09\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5728\u5206\u5272\u8840\u7ba1\u65b9\u9762\u5177\u6709\u9ad8\u5ea6\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6709\u53ef\u80fd\u663e\u7740\u51cf\u5c11\u8840\u7ba1\u6ce8\u91ca\u6240\u9700\u7684\u65f6\u95f4\u548c\u7cbe\u529b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/gzq17/Weakly-Supervised-by-MIP\u3002|[2402.12128v1](http://arxiv.org/pdf/2402.12128v1)|null|\n", "2402.12099": "|**2024-02-19**|**Human Video Translation via Query Warping**|\u901a\u8fc7\u67e5\u8be2\u53d8\u5f62\u8fdb\u884c\u4eba\u7c7b\u89c6\u9891\u7ffb\u8bd1|Haiming Zhu, Yangyang Xu, Shengfeng He|In this paper, we present QueryWarp, a novel framework for temporally coherent human motion video translation. Existing diffusion-based video editing approaches that rely solely on key and value tokens to ensure temporal consistency, which scarifies the preservation of local and structural regions. In contrast, we aim to consider complementary query priors by constructing the temporal correlations among query tokens from different frames. Initially, we extract appearance flows from source poses to capture continuous human foreground motion. Subsequently, during the denoising process of the diffusion model, we employ appearance flows to warp the previous frame's query token, aligning it with the current frame's query. This query warping imposes explicit constraints on the outputs of self-attention layers, effectively guaranteeing temporally coherent translation. We perform experiments on various human motion video translation tasks, and the results demonstrate that our QueryWarp framework surpasses state-of-the-art methods both qualitatively and quantitatively.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 QueryWarp\uff0c\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u8fde\u8d2f\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u7ffb\u8bd1\u7684\u65b0\u9896\u6846\u67b6\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u952e\u548c\u503c\u6807\u8bb0\u6765\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u8fd9\u4f1a\u7834\u574f\u5c40\u90e8\u548c\u7ed3\u6784\u533a\u57df\u7684\u4fdd\u5b58\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u6784\u5efa\u6765\u81ea\u4e0d\u540c\u5e27\u7684\u67e5\u8be2\u6807\u8bb0\u4e4b\u95f4\u7684\u65f6\u95f4\u76f8\u5173\u6027\u6765\u8003\u8651\u4e92\u8865\u67e5\u8be2\u5148\u9a8c\u3002\u6700\u521d\uff0c\u6211\u4eec\u4ece\u6e90\u59ff\u52bf\u4e2d\u63d0\u53d6\u5916\u89c2\u6d41\u4ee5\u6355\u83b7\u8fde\u7eed\u7684\u4eba\u4f53\u524d\u666f\u8fd0\u52a8\u3002\u968f\u540e\uff0c\u5728\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u5916\u89c2\u6d41\u6765\u626d\u66f2\u524d\u4e00\u5e27\u7684\u67e5\u8be2\u6807\u8bb0\uff0c\u5c06\u5176\u4e0e\u5f53\u524d\u5e27\u7684\u67e5\u8be2\u5bf9\u9f50\u3002\u8fd9\u79cd\u67e5\u8be2\u626d\u66f2\u5bf9\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u8f93\u51fa\u65bd\u52a0\u4e86\u660e\u786e\u7684\u7ea6\u675f\uff0c\u6709\u6548\u5730\u4fdd\u8bc1\u4e86\u65f6\u95f4\u8fde\u8d2f\u7684\u7ffb\u8bd1\u3002\u6211\u4eec\u5bf9\u5404\u79cd\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u7ffb\u8bd1\u4efb\u52a1\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684 QueryWarp \u6846\u67b6\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u90fd\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2402.12099v1](http://arxiv.org/pdf/2402.12099v1)|null|\n", "2402.12004": "|**2024-02-19**|**Direct Consistency Optimization for Compositional Text-to-Image Personalization**|\u7ec4\u5408\u6587\u672c\u5230\u56fe\u50cf\u4e2a\u6027\u5316\u7684\u76f4\u63a5\u4e00\u81f4\u6027\u4f18\u5316|Kyungmin Lee, Sangkyung Kwak, Kihyuk Sohn, Jinwoo Shin|Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, are able to generate visuals with a high degree of consistency. However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models. To address this, we propose to fine-tune the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model. We devise a novel training objective for T2I diffusion models that minimally fine-tunes the pretrained model to achieve consistency. Our method, dubbed \\emph{Direct Consistency Optimization}, is as simple as regular diffusion loss, while significantly enhancing the compositionality of personalized T2I models. Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and prompt fidelity. Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the image-text alignment. We show the efficacy of the proposed method on the T2I personalization for subject, style, or both. In particular, our method results in a superior Pareto frontier to the baselines. Generated examples and codes are in our project page( https://dco-t2i.github.io/).|\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u5728\u5bf9\u4e00\u4e9b\u4e2a\u4eba\u56fe\u50cf\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u4e00\u81f4\u6027\u7684\u89c6\u89c9\u6548\u679c\u3002\u7136\u800c\uff0c\u4ed6\u4eec\u4ecd\u7136\u7f3a\u4e4f\u5408\u6210\u539f\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u53ef\u80fd\u7684\u4e0d\u540c\u573a\u666f\u6216\u98ce\u683c\u7684\u56fe\u50cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u6700\u5927\u5316\u53c2\u8003\u56fe\u50cf\u7684\u4e00\u81f4\u6027\u6765\u5fae\u8c03 T2I \u6a21\u578b\uff0c\u540c\u65f6\u60e9\u7f5a\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u504f\u5dee\u3002\u6211\u4eec\u4e3a T2I \u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u5b83\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u6700\u5c0f\u7a0b\u5ea6\u7684\u5fae\u8c03\u4ee5\u5b9e\u73b0\u4e00\u81f4\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u88ab\u79f0\u4e3a \\emph{\u76f4\u63a5\u4e00\u81f4\u6027\u4f18\u5316}\uff0c\u5c31\u50cf\u5e38\u89c4\u6269\u6563\u635f\u5931\u4e00\u6837\u7b80\u5355\uff0c\u540c\u65f6\u663e\u7740\u589e\u5f3a\u4e86\u4e2a\u6027\u5316 T2I \u6a21\u578b\u7684\u7ec4\u5408\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63a7\u5236\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f3a\u8c03\u5bf9\u53c2\u8003\u56fe\u50cf\u4f7f\u7528\u7efc\u5408\u6807\u9898\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u56fe\u50cf\u6587\u672c\u5bf9\u9f50\u7684\u5fc5\u8981\u6027\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5bf9\u4e3b\u9898\u3001\u98ce\u683c\u6216\u4e24\u8005\u7684 T2I \u4e2a\u6027\u5316\u7684\u6709\u6548\u6027\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ea7\u751f\u4e86\u4f18\u4e8e\u57fa\u7ebf\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u751f\u6210\u7684\u793a\u4f8b\u548c\u4ee3\u7801\u4f4d\u4e8e\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\uff08https://dco-t2i.github.io/\uff09\u4e2d\u3002|[2402.12004v1](http://arxiv.org/pdf/2402.12004v1)|null|\n", "2402.11989": "|**2024-02-19**|**Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models**|\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u4f4e\u9636\u9002\u5e94|Zihao Luo, Xilie Xu, Feng Liu, Yun Sing Koh, Di Wang, Jingfeng Zhang|Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization. Our comprehensive empirical results corroborate that adapted LDMs via Stable PrivateLoRA can effectively defend against MI attacks while generating high-quality images. Our code is available at https://github.com/WilliamLUO0/StablePrivateLoRA.|\u4f4e\u79e9\u9002\u5e94 (LoRA) \u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8c03\u6574\u6f5c\u5728\u6269\u6563\u6a21\u578b (LDM)\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9002\u5e94\u635f\u5931\u6765\u751f\u6210\u7279\u5b9a\u5bf9\u8c61\u3002\u7136\u800c\uff0c\u901a\u8fc7 LoRA \u9002\u914d\u7684 LDM \u5f88\u5bb9\u6613\u53d7\u5230\u6210\u5458\u63a8\u7406\uff08MI\uff09\u653b\u51fb\uff0c\u53ef\u4ee5\u5224\u65ad\u7279\u5b9a\u6570\u636e\u70b9\u662f\u5426\u5c5e\u4e8e\u79c1\u4eba\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ece\u800c\u9762\u4e34\u4e25\u91cd\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u4e3a\u4e86\u9632\u5fa1 MI \u653b\u51fb\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848\uff1a\u9690\u79c1\u4fdd\u62a4 LoRA\uff08PrivateLoRA\uff09\u3002 PrivateLoRA \u88ab\u8868\u8ff0\u4e3a\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d\u901a\u8fc7\u6700\u5927\u5316\u5176 MI \u589e\u76ca\u6765\u8bad\u7ec3\u4ee3\u7406\u653b\u51fb\u6a21\u578b\uff0c\u540c\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u9002\u5e94\u635f\u5931\u548c\u4ee3\u7406\u653b\u51fb\u6a21\u578b\u7684 MI \u589e\u76ca\u4e4b\u548c\u6765\u9002\u5e94 LDM\u3002\u7136\u800c\uff0c\u6211\u4eec\u6839\u636e\u7ecf\u9a8c\u53d1\u73b0\uff0cPrivateLoRA \u5b58\u5728\u4f18\u5316\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u68af\u5ea6\u5c3a\u5ea6\u6ce2\u52a8\u8f83\u5927\uff0c\u963b\u788d\u4e86\u81ea\u9002\u5e94\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Stable PrivateLoRA\uff0c\u5b83\u901a\u8fc7\u6700\u5c0f\u5316\u9002\u5e94\u635f\u5931\u4e0e MI \u589e\u76ca\u7684\u6bd4\u7387\u6765\u9002\u5e94 LDM\uff0c\u8fd9\u9690\u5f0f\u5730\u91cd\u65b0\u8c03\u6574\u4e86\u68af\u5ea6\uff0c\u4ece\u800c\u7a33\u5b9a\u4e86\u4f18\u5316\u3002\u6211\u4eec\u5168\u9762\u7684\u5b9e\u8bc1\u7ed3\u679c\u8bc1\u5b9e\uff0c\u901a\u8fc7 Stable PrivateLoRA \u6539\u9020\u7684 LDM \u53ef\u4ee5\u6709\u6548\u9632\u5fa1 MI \u653b\u51fb\uff0c\u540c\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/WilliamLUO0/StablePrivateLoRA \u83b7\u53d6\u3002|[2402.11989v1](http://arxiv.org/pdf/2402.11989v1)|null|\n", "2402.11929": "|**2024-02-19**|**DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation**|DiLightNet\uff1a\u7528\u4e8e\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u751f\u6210\u7684\u7ec6\u7c92\u5ea6\u7167\u660e\u63a7\u5236|Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, Xin Tong|This paper presents a novel method for exerting fine-grained lighting control during text-driven diffusion-based image generation. While existing diffusion models already have the ability to generate images under any lighting condition, without additional guidance these models tend to correlate image content and lighting. Moreover, text prompts lack the necessary expressional power to describe detailed lighting setups. To provide the content creator with fine-grained control over the lighting during image generation, we augment the text-prompt with detailed lighting information in the form of radiance hints, i.e., visualizations of the scene geometry with a homogeneous canonical material under the target lighting. However, the scene geometry needed to produce the radiance hints is unknown. Our key observation is that we only need to guide the diffusion process, hence exact radiance hints are not necessary; we only need to point the diffusion model in the right direction. Based on this observation, we introduce a three stage method for controlling the lighting during image generation. In the first stage, we leverage a standard pretrained diffusion model to generate a provisional image under uncontrolled lighting. Next, in the second stage, we resynthesize and refine the foreground object in the generated image by passing the target lighting to a refined diffusion model, named DiLightNet, using radiance hints computed on a coarse shape of the foreground object inferred from the provisional image. To retain the texture details, we multiply the radiance hints with a neural encoding of the provisional synthesized image before passing it to DiLightNet. Finally, in the third stage, we resynthesize the background to be consistent with the lighting on the foreground object. We demonstrate and validate our lighting controlled diffusion model on a variety of text prompts and lighting conditions.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6587\u672c\u9a71\u52a8\u7684\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b9e\u65bd\u7ec6\u7c92\u5ea6\u7167\u660e\u63a7\u5236\u7684\u65b0\u65b9\u6cd5\u3002\u867d\u7136\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u5df2\u7ecf\u80fd\u591f\u5728\u4efb\u4f55\u7167\u660e\u6761\u4ef6\u4e0b\u751f\u6210\u56fe\u50cf\uff0c\u4f46\u5982\u679c\u6ca1\u6709\u989d\u5916\u7684\u6307\u5bfc\uff0c\u8fd9\u4e9b\u6a21\u578b\u5f80\u5f80\u4f1a\u5c06\u56fe\u50cf\u5185\u5bb9\u548c\u7167\u660e\u76f8\u5173\u8054\u3002\u6b64\u5916\uff0c\u6587\u672c\u63d0\u793a\u7f3a\u4e4f\u5fc5\u8981\u7684\u8868\u8fbe\u80fd\u529b\u6765\u63cf\u8ff0\u8be6\u7ec6\u7684\u7167\u660e\u8bbe\u7f6e\u3002\u4e3a\u4e86\u5728\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e3a\u5185\u5bb9\u521b\u5efa\u8005\u63d0\u4f9b\u5bf9\u7167\u660e\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u6211\u4eec\u4ee5\u8f90\u5c04\u63d0\u793a\u7684\u5f62\u5f0f\u4f7f\u7528\u8be6\u7ec6\u7684\u7167\u660e\u4fe1\u606f\u6765\u589e\u5f3a\u6587\u672c\u63d0\u793a\uff0c\u5373\u5728\u76ee\u6807\u7167\u660e\u4e0b\u4f7f\u7528\u5747\u5300\u89c4\u8303\u6750\u8d28\u5bf9\u573a\u666f\u51e0\u4f55\u8fdb\u884c\u53ef\u89c6\u5316\u3002\u7136\u800c\uff0c\u4ea7\u751f\u8f90\u5c04\u63d0\u793a\u6240\u9700\u7684\u573a\u666f\u51e0\u4f55\u5f62\u72b6\u672a\u77e5\u3002\u6211\u4eec\u7684\u4e3b\u8981\u89c2\u5bdf\u662f\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u7cbe\u786e\u7684\u8f90\u5c04\u63d0\u793a\uff1b\u6211\u4eec\u53ea\u9700\u8981\u5c06\u6269\u6563\u6a21\u578b\u6307\u5411\u6b63\u786e\u7684\u65b9\u5411\u5373\u53ef\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5728\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u63a7\u5236\u7167\u660e\u7684\u4e09\u9636\u6bb5\u65b9\u6cd5\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u5229\u7528\u6807\u51c6\u7684\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u4e0d\u53d7\u63a7\u5236\u7684\u7167\u660e\u4e0b\u751f\u6210\u4e34\u65f6\u56fe\u50cf\u3002\u63a5\u4e0b\u6765\uff0c\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u76ee\u6807\u5149\u7167\u4f20\u9012\u5230\u540d\u4e3a DiLightNet \u7684\u7ec6\u5316\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u6839\u636e\u4e34\u65f6\u56fe\u50cf\u63a8\u65ad\u7684\u524d\u666f\u5bf9\u8c61\u7684\u7c97\u7565\u5f62\u72b6\u8ba1\u7b97\u51fa\u7684\u8f90\u5c04\u5ea6\u63d0\u793a\uff0c\u91cd\u65b0\u5408\u6210\u548c\u7ec6\u5316\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u524d\u666f\u5bf9\u8c61\u3002\u4e3a\u4e86\u4fdd\u7559\u7eb9\u7406\u7ec6\u8282\uff0c\u6211\u4eec\u5c06\u8f90\u5c04\u63d0\u793a\u4e0e\u4e34\u65f6\u5408\u6210\u56fe\u50cf\u7684\u795e\u7ecf\u7f16\u7801\u76f8\u4e58\uff0c\u7136\u540e\u5c06\u5176\u4f20\u9012\u7ed9 DiLightNet\u3002\u6700\u540e\uff0c\u5728\u7b2c\u4e09\u9636\u6bb5\uff0c\u6211\u4eec\u91cd\u65b0\u5408\u6210\u80cc\u666f\uff0c\u4f7f\u5176\u4e0e\u524d\u666f\u7269\u4f53\u4e0a\u7684\u5149\u7167\u4fdd\u6301\u4e00\u81f4\u3002\u6211\u4eec\u5728\u5404\u79cd\u6587\u672c\u63d0\u793a\u548c\u7167\u660e\u6761\u4ef6\u4e0b\u6f14\u793a\u5e76\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u7167\u660e\u63a7\u5236\u6269\u6563\u6a21\u578b\u3002|[2402.11929v1](http://arxiv.org/pdf/2402.11929v1)|null|\n", "2402.11909": "|**2024-02-19**|**One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation**|One2Avatar\uff1a\u7528\u4e8e\u5c0f\u6837\u672c\u7528\u6237\u9002\u5e94\u7684\u751f\u6210\u9690\u5f0f\u5934\u90e8\u5934\u50cf|Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang|Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation.|\u4ece\u5355\u773c\u89c6\u9891\u6784\u5efa\u9ad8\u8d28\u91cf\u3001\u4e2a\u6027\u5316\u5934\u90e8\u5934\u50cf\u7684\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u9762\u90e8\u6355\u6349\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u8fd9\u5bf9\u53ef\u6269\u5c55\u6027\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u5934\u50cf\uff0c\u6bcf\u4e2a\u7528\u6237\u4ec5\u4f7f\u7528\u4e00\u5f20\u6216\u51e0\u5f20\u56fe\u50cf\u3002\u6211\u4eec\u4ece 2407 \u540d\u53d7\u8bd5\u8005\u7684\u8868\u60c5\u591a\u89c6\u56fe\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u4e86 3D \u53ef\u52a8\u753b\u3001\u7167\u7247\u822c\u771f\u5b9e\u5934\u90e8\u5934\u50cf\u7684\u751f\u6210\u6a21\u578b\uff0c\u5e76\u5229\u7528\u5b83\u4f5c\u4e3a\u4ece\u5c11\u91cf\u56fe\u50cf\u521b\u5efa\u4e2a\u6027\u5316\u5934\u50cf\u7684\u5148\u9a8c\u3002\u4e0e\u4e4b\u524d\u7684 3D \u611f\u77e5\u9762\u90e8\u751f\u6210\u6a21\u578b\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u5148\u9a8c\u6a21\u578b\u662f\u7528 3DMM \u951a\u5b9a\u7684\u795e\u7ecf\u8f90\u5c04\u573a\u4e3b\u5e72\u6784\u5efa\u7684\uff0c\u6211\u4eec\u8bc1\u660e\u901a\u8fc7\u57fa\u4e8e\u5c11\u91cf\u8f93\u5165\u7684\u81ea\u52a8\u89e3\u7801\uff0c\u5b83\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u521b\u5efa\u5934\u50cf\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u8054\u5408\u4f18\u5316 3DMM \u62df\u5408\u548c\u76f8\u673a\u6821\u51c6\u6765\u5904\u7406\u4e0d\u7a33\u5b9a\u7684 3DMM \u62df\u5408\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u597d\u7684\u5c11\u955c\u5934\u81ea\u9002\u5e94\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u5c11\u955c\u5934\u5316\u8eab\u9002\u5e94\u65b9\u6cd5\uff0c\u4e3a\u66f4\u9ad8\u6548\u548c\u4e2a\u6027\u5316\u7684\u5316\u8eab\u521b\u5efa\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2402.11909v1](http://arxiv.org/pdf/2402.11909v1)|null|\n", "2402.11882": "|**2024-02-19**|**NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization**|\u6ce8\uff1a\u901a\u8fc7\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u6709\u6548\u65b9\u6cd5\u751f\u6210\u663e\u7740\u7684\u60a3\u8005\u6587\u672c\u6458\u8981|Imjin Ahn, Hansle Gwon, Young-Hak Kim, Tae Joon Jun, Sanghyun Park|The discharge summary is a one of critical documents in the patient journey, encompassing all events experienced during hospitalization, including multiple visits, medications, tests, surgery/procedures, and admissions/discharge. Providing a summary of the patient's progress is crucial, as it significantly influences future care and planning. Consequently, clinicians face the laborious and resource-intensive task of manually collecting, organizing, and combining all the necessary data for a discharge summary. Therefore, we propose \"NOTE\", which stands for \"Notable generation Of patient Text summaries through an Efficient approach based on direct preference optimization\". NOTE is based on Medical Information Mart for Intensive Care- III dataset and summarizes a single hospitalization of a patient. Patient events are sequentially combined and used to generate a discharge summary for each hospitalization. In the present circumstances, large language models' application programming interfaces (LLMs' APIs) are widely available, but importing and exporting medical data presents significant challenges due to privacy protection policies in healthcare institutions. Moreover, to ensure optimal performance, it is essential to implement a lightweight model for internal server or program within the hospital. Therefore, we utilized DPO and parameter efficient fine tuning (PEFT) techniques to apply a fine-tuning method that guarantees superior performance. To demonstrate the practical application of the developed NOTE, we provide a webpage-based demonstration software. In the future, we will aim to deploy the software available for actual use by clinicians in hospital. NOTE can be utilized to generate various summaries not only discharge summaries but also throughout a patient's journey, thereby alleviating the labor-intensive workload of clinicians and aiming for increased efficiency.|\u51fa\u9662\u603b\u7ed3\u662f\u60a3\u8005\u65c5\u7a0b\u4e2d\u7684\u91cd\u8981\u6587\u4ef6\u4e4b\u4e00\uff0c\u6db5\u76d6\u4f4f\u9662\u671f\u95f4\u7ecf\u5386\u7684\u6240\u6709\u4e8b\u4ef6\uff0c\u5305\u62ec\u591a\u6b21\u5c31\u8bca\u3001\u836f\u7269\u3001\u68c0\u67e5\u3001\u624b\u672f/\u7a0b\u5e8f\u4ee5\u53ca\u5165\u9662/\u51fa\u9662\u3002\u63d0\u4f9b\u60a3\u8005\u8fdb\u5c55\u7684\u603b\u7ed3\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4f1a\u663e\u7740\u5f71\u54cd\u672a\u6765\u7684\u62a4\u7406\u548c\u8ba1\u5212\u3002\u56e0\u6b64\uff0c\u4e34\u5e8a\u533b\u751f\u9762\u4e34\u7740\u624b\u52a8\u6536\u96c6\u3001\u7ec4\u7ec7\u548c\u7ec4\u5408\u51fa\u9662\u603b\u7ed3\u6240\u9700\u7684\u6240\u6709\u6570\u636e\u7684\u8d39\u529b\u4e14\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u201cNOTE\u201d\uff0c\u5b83\u4ee3\u8868\u201c\u901a\u8fc7\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u6709\u6548\u65b9\u6cd5\u663e\u7740\u751f\u6210\u60a3\u8005\u6587\u672c\u6458\u8981\u201d\u3002 NOTE \u57fa\u4e8e\u91cd\u75c7\u76d1\u62a4\u533b\u7597\u4fe1\u606f\u96c6\u5e02 III \u6570\u636e\u96c6\uff0c\u603b\u7ed3\u4e86\u60a3\u8005\u7684\u5355\u6b21\u4f4f\u9662\u60c5\u51b5\u3002\u60a3\u8005\u4e8b\u4ef6\u6309\u987a\u5e8f\u7ec4\u5408\u5e76\u7528\u4e8e\u751f\u6210\u6bcf\u6b21\u4f4f\u9662\u7684\u51fa\u9662\u6458\u8981\u3002\u5728\u76ee\u524d\u60c5\u51b5\u4e0b\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\u7a0b\u5e8f\u7f16\u7a0b\u63a5\u53e3\uff08LLM\u7684API\uff09\u5df2\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u7531\u4e8e\u533b\u7597\u673a\u6784\u7684\u9690\u79c1\u4fdd\u62a4\u653f\u7b56\uff0c\u5bfc\u5165\u548c\u5bfc\u51fa\u533b\u7597\u6570\u636e\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u786e\u4fdd\u6700\u4f73\u6027\u80fd\uff0c\u5fc5\u987b\u4e3a\u533b\u9662\u5185\u90e8\u670d\u52a1\u5668\u6216\u7a0b\u5e8f\u5b9e\u65bd\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5229\u7528DPO\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\u6765\u5e94\u7528\u4fdd\u8bc1\u5353\u8d8a\u6027\u80fd\u7684\u5fae\u8c03\u65b9\u6cd5\u3002\u4e3a\u4e86\u6f14\u793a\u6240\u5f00\u53d1\u7684NOTE\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7f51\u9875\u7684\u6f14\u793a\u8f6f\u4ef6\u3002\u672a\u6765\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u90e8\u7f72\u53ef\u4f9b\u533b\u9662\u4e34\u5e8a\u533b\u751f\u5b9e\u9645\u4f7f\u7528\u7684\u8f6f\u4ef6\u3002 NOT\u53ef\u4ee5\u7528\u6765\u751f\u6210\u5404\u79cd\u6458\u8981\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u751f\u6210\u51fa\u9662\u6458\u8981\uff0c\u8fd8\u53ef\u4ee5\u751f\u6210\u6574\u4e2a\u60a3\u8005\u65c5\u7a0b\u7684\u6458\u8981\uff0c\u4ece\u800c\u51cf\u8f7b\u4e34\u5e8a\u533b\u751f\u7684\u52b3\u52a8\u5bc6\u96c6\u578b\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u6548\u7387\u3002|[2402.11882v1](http://arxiv.org/pdf/2402.11882v1)|null|\n", "2402.11849": "|**2024-02-19**|**ComFusion: Personalized Subject Generation in Multiple Specific Scenes From Single Image**|ComFusion\uff1a\u4ece\u5355\u4e2a\u56fe\u50cf\u5728\u591a\u4e2a\u7279\u5b9a\u573a\u666f\u4e2d\u751f\u6210\u4e2a\u6027\u5316\u4e3b\u9898|Yan Hong, Jianfu Zhang|Recent advancements in personalizing text-to-image (T2I) diffusion models have shown the capability to generate images based on personalized visual concepts using a limited number of user-provided examples. However, these models often struggle with maintaining high visual fidelity, particularly in manipulating scenes as defined by textual inputs. Addressing this, we introduce ComFusion, a novel approach that leverages pretrained models generating composition of a few user-provided subject images and predefined-text scenes, effectively fusing visual-subject instances with textual-specific scenes, resulting in the generation of high-fidelity instances within diverse scenes. ComFusion integrates a class-scene prior preservation regularization, which leverages composites the subject class and scene-specific knowledge from pretrained models to enhance generation fidelity. Additionally, ComFusion uses coarse generated images, ensuring they align effectively with both the instance image and scene texts. Consequently, ComFusion maintains a delicate balance between capturing the essence of the subject and maintaining scene fidelity.Extensive evaluations of ComFusion against various baselines in T2I personalization have demonstrated its qualitative and quantitative superiority.|\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf (T2I) \u6269\u6563\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5df2\u7ecf\u663e\u793a\u51fa\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u7528\u6237\u63d0\u4f9b\u7684\u793a\u4f8b\u57fa\u4e8e\u4e2a\u6027\u5316\u89c6\u89c9\u6982\u5ff5\u751f\u6210\u56fe\u50cf\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u7279\u522b\u662f\u5728\u64cd\u7eb5\u6587\u672c\u8f93\u5165\u5b9a\u4e49\u7684\u573a\u666f\u65f6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ComFusion\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u4e00\u4e9b\u7528\u6237\u63d0\u4f9b\u7684\u4e3b\u9898\u56fe\u50cf\u548c\u9884\u5b9a\u4e49\u6587\u672c\u573a\u666f\u7684\u7ec4\u5408\uff0c\u6709\u6548\u5730\u5c06\u89c6\u89c9\u4e3b\u9898\u5b9e\u4f8b\u4e0e\u6587\u672c\u7279\u5b9a\u573a\u666f\u878d\u5408\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u5b9e\u4f8b\u3002 ComFusion \u96c6\u6210\u4e86\u7c7b\u573a\u666f\u5148\u9a8c\u4fdd\u7559\u6b63\u5219\u5316\uff0c\u5b83\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u4e3b\u9898\u7c7b\u548c\u573a\u666f\u7279\u5b9a\u77e5\u8bc6\u6765\u589e\u5f3a\u751f\u6210\u4fdd\u771f\u5ea6\u3002\u6b64\u5916\uff0cComFusion \u4f7f\u7528\u7c97\u7565\u751f\u6210\u7684\u56fe\u50cf\uff0c\u786e\u4fdd\u5b83\u4eec\u4e0e\u5b9e\u4f8b\u56fe\u50cf\u548c\u573a\u666f\u6587\u672c\u6709\u6548\u5bf9\u9f50\u3002\u56e0\u6b64\uff0cComFusion \u5728\u6355\u6349\u4e3b\u9898\u672c\u8d28\u548c\u4fdd\u6301\u573a\u666f\u4fdd\u771f\u5ea6\u4e4b\u95f4\u4fdd\u6301\u7740\u5fae\u5999\u7684\u5e73\u8861\u3002\u6839\u636e T2I \u4e2a\u6027\u5316\u4e2d\u7684\u5404\u79cd\u57fa\u7ebf\u5bf9 ComFusion \u8fdb\u884c\u7684\u5e7f\u6cdb\u8bc4\u4f30\u5df2\u7ecf\u8bc1\u660e\u4e86\u5176\u5b9a\u6027\u548c\u5b9a\u91cf\u7684\u4f18\u52bf\u3002|[2402.11849v1](http://arxiv.org/pdf/2402.11849v1)|null|\n", "2402.11846": "|**2024-02-19**|**UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models**|UnlearnCanvas\uff1a\u7528\u4e8e\u5bf9\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u9057\u5fd8\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a0b\u5f0f\u5316\u56fe\u50cf\u6570\u636e\u96c6|Yihua Zhang, Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jiancheng Liu, Xiaoming Liu, Sijia Liu|The rapid advancement of diffusion models (DMs) has not only transformed various real-world industries but has also introduced negative societal concerns, including the generation of harmful content, copyright disputes, and the rise of stereotypes and biases. To mitigate these issues, machine unlearning (MU) has emerged as a potential solution, demonstrating its ability to remove undesired generative capabilities of DMs in various applications. However, by examining existing MU evaluation methods, we uncover several key challenges that can result in incomplete, inaccurate, or biased evaluations for MU in DMs. To address them, we enhance the evaluation metrics for MU, including the introduction of an often-overlooked retainability measurement for DMs post-unlearning. Additionally, we introduce UnlearnCanvas, a comprehensive high-resolution stylized image dataset that facilitates us to evaluate the unlearning of artistic painting styles in conjunction with associated image objects. We show that this dataset plays a pivotal role in establishing a standardized and automated evaluation framework for MU techniques on DMs, featuring 7 quantitative metrics to address various aspects of unlearning effectiveness. Through extensive experiments, we benchmark 5 state-of-the-art MU methods, revealing novel insights into their pros and cons, and the underlying unlearning mechanisms. Furthermore, we demonstrate the potential of UnlearnCanvas to benchmark other generative modeling tasks, such as style transfer. The UnlearnCanvas dataset, benchmark, and the codes to reproduce all the results in this work can be found at https://github.com/OPTML-Group/UnlearnCanvas.|\u6269\u6563\u6a21\u578b\uff08DM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u4e0d\u4ec5\u6539\u53d8\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u5404\u79cd\u884c\u4e1a\uff0c\u800c\u4e14\u8fd8\u5e26\u6765\u4e86\u8d1f\u9762\u7684\u793e\u4f1a\u95ee\u9898\uff0c\u5305\u62ec\u6709\u5bb3\u5185\u5bb9\u7684\u4ea7\u751f\u3001\u7248\u6743\u7ea0\u7eb7\u4ee5\u53ca\u523b\u677f\u5370\u8c61\u548c\u504f\u89c1\u7684\u5174\u8d77\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u673a\u5668\u5b66\u4e60 (MU) \u5df2\u6210\u4e3a\u4e00\u79cd\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u6d88\u9664 DM \u4e0d\u9700\u8981\u7684\u751f\u6210\u80fd\u529b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u901a\u8fc7\u68c0\u67e5\u73b0\u6709\u7684 MU \u8bc4\u4f30\u65b9\u6cd5\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u51e0\u4e2a\u53ef\u80fd\u5bfc\u81f4 DM \u4e2d MU \u8bc4\u4f30\u4e0d\u5b8c\u6574\u3001\u4e0d\u51c6\u786e\u6216\u6709\u504f\u5dee\u7684\u5173\u952e\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u589e\u5f3a\u4e86 MU \u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u62ec\u4e3a DM \u9057\u5fd8\u540e\u5f15\u5165\u7ecf\u5e38\u88ab\u5ffd\u89c6\u7684\u4fdd\u7559\u6027\u6d4b\u91cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86 UnlearnCanvas\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u9ad8\u5206\u8fa8\u7387\u98ce\u683c\u5316\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u6709\u52a9\u4e8e\u6211\u4eec\u7ed3\u5408\u76f8\u5173\u56fe\u50cf\u5bf9\u8c61\u6765\u8bc4\u4f30\u827a\u672f\u7ed8\u753b\u98ce\u683c\u7684\u9057\u5fd8\u3002\u6211\u4eec\u8868\u660e\uff0c\u8be5\u6570\u636e\u96c6\u5728\u4e3a DM \u4e0a\u7684 MU \u6280\u672f\u5efa\u7acb\u6807\u51c6\u5316\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u65b9\u9762\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\uff0c\u5177\u6709 7 \u4e2a\u5b9a\u91cf\u6307\u6807\u6765\u89e3\u51b3\u9057\u5fd8\u6709\u6548\u6027\u7684\u5404\u4e2a\u65b9\u9762\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u5bf9 5 \u79cd\u6700\u5148\u8fdb\u7684 MU \u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5bf9\u5176\u4f18\u7f3a\u70b9\u4ee5\u53ca\u6f5c\u5728\u7684\u9057\u5fd8\u673a\u5236\u7684\u65b0\u9896\u89c1\u89e3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86 UnlearnCanvas \u5bf9\u5176\u4ed6\u751f\u6210\u5efa\u6a21\u4efb\u52a1\uff08\u4f8b\u5982\u98ce\u683c\u8fc1\u79fb\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u7684\u6f5c\u529b\u3002 UnlearnCanvas \u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u91cd\u73b0\u672c\u5de5\u4f5c\u4e2d\u6240\u6709\u7ed3\u679c\u7684\u4ee3\u7801\u53ef\u4ee5\u5728 https://github.com/OPTML-Group/UnlearnCanvas \u4e2d\u627e\u5230\u3002|[2402.11846v1](http://arxiv.org/pdf/2402.11846v1)|null|\n", "2402.11843": "|**2024-02-19**|**WildFake: A Large-scale Challenging Dataset for AI-Generated Images Detection**|WildFake\uff1a\u7528\u4e8e\u4eba\u5de5\u667a\u80fd\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u5927\u89c4\u6a21\u6311\u6218\u6027\u6570\u636e\u96c6|Yan Hong, Jianfu Zhang|The extraordinary ability of generative models enabled the generation of images with such high quality that human beings cannot distinguish Artificial Intelligence (AI) generated images from real-life photographs. The development of generation techniques opened up new opportunities but concurrently introduced potential risks to privacy, authenticity, and security. Therefore, the task of detecting AI-generated imagery is of paramount importance to prevent illegal activities. To assess the generalizability and robustness of AI-generated image detection, we present a large-scale dataset, referred to as WildFake, comprising state-of-the-art generators, diverse object categories, and real-world applications. WildFake dataset has the following advantages: 1) Rich Content with Wild collection: WildFake collects fake images from the open-source community, enriching its diversity with a broad range of image classes and image styles. 2) Hierarchical structure: WildFake contains fake images synthesized by different types of generators from GANs, diffusion models, to other generative models. These key strengths enhance the generalization and robustness of detectors trained on WildFake, thereby demonstrating WildFake's considerable relevance and effectiveness for AI-generated detectors in real-world scenarios. Moreover, our extensive evaluation experiments are tailored to yield profound insights into the capabilities of different levels of generative models, a distinctive advantage afforded by WildFake's unique hierarchical structure.|\u751f\u6210\u6a21\u578b\u7684\u975e\u51e1\u80fd\u529b\u4f7f\u5f97\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u5982\u6b64\u4e4b\u9ad8\uff0c\u4ee5\u81f3\u4e8e\u4eba\u7c7b\u65e0\u6cd5\u533a\u5206\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u751f\u6210\u7684\u56fe\u50cf\u548c\u73b0\u5b9e\u751f\u6d3b\u4e2d\u7684\u7167\u7247\u3002\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u5f00\u8f9f\u4e86\u65b0\u7684\u673a\u9047\uff0c\u4f46\u540c\u65f6\u4e5f\u7ed9\u9690\u79c1\u3001\u771f\u5b9e\u6027\u548c\u5b89\u5168\u6027\u5e26\u6765\u4e86\u6f5c\u5728\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u68c0\u6d4b\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u56fe\u50cf\u5bf9\u4e8e\u9632\u6b62\u975e\u6cd5\u6d3b\u52a8\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u56fe\u50cf\u68c0\u6d4b\u7684\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a WildFake \u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u6700\u5148\u8fdb\u7684\u751f\u6210\u5668\u3001\u4e0d\u540c\u7684\u5bf9\u8c61\u7c7b\u522b\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u7a0b\u5e8f\u3002 WildFake\u6570\u636e\u96c6\u5177\u6709\u4ee5\u4e0b\u4f18\u70b9\uff1a 1\uff09Wild\u96c6\u5408\u5185\u5bb9\u4e30\u5bcc\uff1aWildFake\u4ece\u5f00\u6e90\u793e\u533a\u6536\u96c6\u5047\u56fe\u50cf\uff0c\u901a\u8fc7\u5e7f\u6cdb\u7684\u56fe\u50cf\u7c7b\u522b\u548c\u56fe\u50cf\u98ce\u683c\u4e30\u5bcc\u4e86\u5176\u591a\u6837\u6027\u3002 2\uff09\u5206\u5c42\u7ed3\u6784\uff1aWildFake \u5305\u542b\u7531\u4e0d\u540c\u7c7b\u578b\u7684\u751f\u6210\u5668\u5408\u6210\u7684\u5047\u56fe\u50cf\uff0c\u4ece GAN\u3001\u6269\u6563\u6a21\u578b\u5230\u5176\u4ed6\u751f\u6210\u6a21\u578b\u3002\u8fd9\u4e9b\u5173\u952e\u4f18\u52bf\u589e\u5f3a\u4e86\u5728 WildFake \u4e0a\u8bad\u7ec3\u7684\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u8bc1\u660e\u4e86 WildFake \u5bf9\u4e8e\u73b0\u5b9e\u573a\u666f\u4e2d\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u68c0\u6d4b\u5668\u5177\u6709\u76f8\u5f53\u5927\u7684\u76f8\u5173\u6027\u548c\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5e7f\u6cdb\u7684\u8bc4\u4f30\u5b9e\u9a8c\u65e8\u5728\u6df1\u5165\u4e86\u89e3\u4e0d\u540c\u7ea7\u522b\u7684\u751f\u6210\u6a21\u578b\u7684\u80fd\u529b\uff0c\u8fd9\u662f WildFake \u72ec\u7279\u7684\u5c42\u6b21\u7ed3\u6784\u6240\u63d0\u4f9b\u7684\u72ec\u7279\u4f18\u52bf\u3002|[2402.11843v1](http://arxiv.org/pdf/2402.11843v1)|null|\n", "2402.11789": "|**2024-02-19**|**Statistical Test for Generated Hypotheses by Diffusion Models**|\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5047\u8bbe\u7684\u7edf\u8ba1\u68c0\u9a8c|Teruyuki Katsuoka, Tomohiro Shiraishi, Daiki Miwa, Vo Nguyen Le Duy, Ichiro Takeuchi|The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. We show the theoretical validity of the proposed statistical test and its effectiveness through numerical experiments on synthetic and brain image datasets.|\u4eba\u5de5\u667a\u80fd\u6027\u80fd\u7684\u589e\u5f3a\u52a0\u901f\u4e86\u5176\u4e0e\u79d1\u5b66\u7814\u7a76\u7684\u878d\u5408\u3002\u7279\u522b\u662f\uff0c\u4f7f\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6765\u521b\u5efa\u79d1\u5b66\u5047\u8bbe\u662f\u6709\u524d\u666f\u7684\uff0c\u5e76\u4e14\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u5404\u4e2a\u9886\u57df\u3002\u7136\u800c\uff0c\u5f53\u91c7\u7528\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u5047\u8bbe\u8fdb\u884c\u5173\u952e\u51b3\u7b56\uff08\u4f8b\u5982\u533b\u7597\u8bca\u65ad\uff09\u65f6\uff0c\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u8003\u8651\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u8fdb\u884c\u533b\u5b66\u8bca\u65ad\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7edf\u8ba1\u6d4b\u8bd5\u6765\u91cf\u5316\u5176\u53ef\u9760\u6027\u3002\u6240\u63d0\u51fa\u7684\u7edf\u8ba1\u6d4b\u8bd5\u80cc\u540e\u7684\u57fa\u672c\u601d\u60f3\u662f\u91c7\u7528\u9009\u62e9\u6027\u63a8\u7406\u6846\u67b6\uff0c\u5176\u4e2d\u6211\u4eec\u8003\u8651\u7edf\u8ba1\u6d4b\u8bd5\u7684\u6761\u4ef6\u662f\u751f\u6210\u7684\u56fe\u50cf\u662f\u7531\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u3002\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4ee5 p \u503c\u7684\u5f62\u5f0f\u91cf\u5316\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u7ed3\u679c\u7684\u7edf\u8ba1\u53ef\u9760\u6027\uff0c\u4ece\u800c\u53ef\u4ee5\u5728\u9519\u8bef\u7387\u53d7\u63a7\u7684\u60c5\u51b5\u4e0b\u505a\u51fa\u51b3\u7b56\u3002\u6211\u4eec\u901a\u8fc7\u5bf9\u5408\u6210\u6570\u636e\u96c6\u548c\u5927\u8111\u56fe\u50cf\u6570\u636e\u96c6\u7684\u6570\u503c\u5b9e\u9a8c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u7edf\u8ba1\u6d4b\u8bd5\u7684\u7406\u8bba\u6709\u6548\u6027\u53ca\u5176\u6709\u6548\u6027\u3002|[2402.11789v1](http://arxiv.org/pdf/2402.11789v1)|null|\n"}, "\u591a\u6a21\u6001": {"2402.12336": "|**2024-02-19**|**Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models**|\u9c81\u68d2 CLIP\uff1a\u9c81\u68d2\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5d4c\u5165\u7684\u65e0\u76d1\u7763\u5bf9\u6297\u6027\u5fae\u8c03|Christian Schlarmann, Naman Deep Singh, Francesco Croce, Matthias Hein|Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the VLM is required. The code and robust models are available at https://github.com/chs20/RobustVLM|OpenFlamingo\u3001LLaVA \u548c GPT-4 \u7b49\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u5404\u79cd\u5b9e\u9645\u4efb\u52a1\u3002\u5148\u524d\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u975e\u5e38\u5bb9\u6613\u53d7\u5230\u89c6\u89c9\u6a21\u5f0f\u7684\u5bf9\u6297\u6027\u653b\u51fb\u3002\u8fd9\u4e9b\u653b\u51fb\u53ef\u88ab\u7528\u6765\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u6216\u6b3a\u9a97\u7528\u6237\uff0c\u4ece\u800c\u5e26\u6765\u5de8\u5927\u7684\u98ce\u9669\uff0c\u8fd9\u4f7f\u5f97\u5927\u578b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u9c81\u68d2\u6027\u6210\u4e3a\u4e00\u4e2a\u7d27\u8feb\u7684\u95ee\u9898\u3002 CLIP \u6a21\u578b\u6216\u5176\u53d8\u4f53\u4e4b\u4e00\u5728\u8bb8\u591a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u4e2d\u7528\u4f5c\u51bb\u7ed3\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u4f8b\u5982LLaVA \u548c OpenFlamingo\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u5bf9\u6297\u6027\u5fae\u8c03\u65b9\u6848\uff0c\u4ee5\u83b7\u5f97\u9c81\u68d2\u7684 CLIP \u89c6\u89c9\u7f16\u7801\u5668\uff0c\u8be5\u7f16\u7801\u5668\u5bf9\u4f9d\u8d56\u4e8e CLIP \u7684\u6240\u6709\u89c6\u89c9\u4e0b\u6e38\u4efb\u52a1\uff08VLM\u3001\u96f6\u6837\u672c\u5206\u7c7b\uff09\u4ea7\u751f\u9c81\u68d2\u6027\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u8868\u660e\uff0c\u4e00\u65e6\u7528\u6211\u4eec\u5f3a\u5927\u7684\u6a21\u578b\u66ff\u6362\u4e86\u539f\u59cb\u7684 CLIP \u6a21\u578b\uff0c\u6076\u610f\u7b2c\u4e09\u65b9\u63d0\u4f9b\u7ecf\u8fc7\u64cd\u7eb5\u7684\u56fe\u50cf\u5bf9 VLM \u7528\u6237\u8fdb\u884c\u79d8\u5bc6\u653b\u51fb\u5c31\u4e0d\u518d\u53ef\u80fd\u4e86\u3002\u65e0\u9700\u5bf9 VLM \u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u3002\u4ee3\u7801\u548c\u7a33\u5065\u6a21\u578b\u53ef\u5728 https://github.com/chs20/RobustVLM \u83b7\u53d6|[2402.12336v1](http://arxiv.org/pdf/2402.12336v1)|null|\n", "2402.12185": "|**2024-02-19**|**ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning**|ChartX \u548c ChartVLM\uff1a\u590d\u6742\u56fe\u8868\u63a8\u7406\u7684\u591a\u529f\u80fd\u57fa\u51c6\u548c\u57fa\u7840\u6a21\u578b|Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et.al.|Recently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously. However, their capacity to query information depicted in visual charts and engage in reasoning based on the queried contents remains under-explored. In this paper, to comprehensively and rigorously benchmark the ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data. Besides, we develop ChartVLM to offer a new perspective on handling multi-modal tasks that strongly depend on interpretable patterns, such as reasoning tasks in the field of charts or geometric images. We evaluate the chart-related ability of mainstream MLLMs and our ChartVLM on the proposed ChartX evaluation set. Extensive experiments demonstrate that ChartVLM surpasses both versatile and chart-related large models, achieving results comparable to GPT-4V. We believe that our study can pave the way for further exploration in creating a more comprehensive chart evaluation set and developing more interpretable multi-modal models. Both ChartX and ChartVLM are available at: https://github.com/UniModal4Reasoning/ChartVLM|\u8fd1\u5e74\u6765\uff0c\u8bb8\u591a\u591a\u529f\u80fd\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4e0d\u65ad\u51fa\u73b0\u3002\u7136\u800c\uff0c\u5b83\u4eec\u67e5\u8be2\u53ef\u89c6\u5316\u56fe\u8868\u4e2d\u63cf\u8ff0\u7684\u4fe1\u606f\u5e76\u6839\u636e\u67e5\u8be2\u5185\u5bb9\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u4e3a\u4e86\u5168\u9762\u3001\u4e25\u683c\u5730\u5bf9\u56fe\u8868\u9886\u57df\u73b0\u6210\u7684 MLLM \u7684\u80fd\u529b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6211\u4eec\u6784\u5efa\u4e86 ChartX\uff0c\u8fd9\u662f\u4e00\u4e2a\u6db5\u76d6 18 \u4e2a\u56fe\u8868\u7c7b\u578b\u30017 \u4e2a\u56fe\u8868\u4efb\u52a1\u300122 \u4e2a\u5b66\u79d1\u4e3b\u9898\u548c\u9ad8\u5c42\u6b21\u6a21\u578b\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u96c6\u3002\u8d28\u91cf\u56fe\u8868\u6570\u636e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1 ChartVLM \u4e3a\u5904\u7406\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u53ef\u89e3\u91ca\u6a21\u5f0f\u7684\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u4f8b\u5982\u56fe\u8868\u6216\u51e0\u4f55\u56fe\u50cf\u9886\u57df\u7684\u63a8\u7406\u4efb\u52a1\u3002\u6211\u4eec\u5728\u63d0\u8bae\u7684 ChartX \u8bc4\u4f30\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4e3b\u6d41 MLLM \u548c\u6211\u4eec\u7684 ChartVLM \u7684\u56fe\u8868\u76f8\u5173\u80fd\u529b\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cChartVLM \u8d85\u8d8a\u4e86\u901a\u7528\u578b\u548c\u56fe\u8868\u76f8\u5173\u7684\u5927\u578b\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u4e0e GPT-4V \u76f8\u5f53\u7684\u7ed3\u679c\u3002\u6211\u4eec\u76f8\u4fe1\uff0c\u6211\u4eec\u7684\u7814\u7a76\u53ef\u4ee5\u4e3a\u8fdb\u4e00\u6b65\u63a2\u7d22\u521b\u5efa\u66f4\u5168\u9762\u7684\u56fe\u8868\u8bc4\u4f30\u96c6\u548c\u5f00\u53d1\u66f4\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u6a21\u578b\u94fa\u5e73\u9053\u8def\u3002 ChartX \u548c ChartVLM \u5747\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u83b7\u53d6\uff1ahttps://github.com/UniModal4Reasoning/ChartVLM|[2402.12185v1](http://arxiv.org/pdf/2402.12185v1)|null|\n", "2402.12079": "|**2024-02-19**|**LVCHAT: Facilitating Long Video Comprehension**|LVCHAT\uff1a\u4fc3\u8fdb\u957f\u89c6\u9891\u7406\u89e3|Yu Wang, Zeyuan Zhang, Julian McAuley, Zexue He|Enabling large language models (LLMs) to read videos is vital for multimodal LLMs. Existing works show promise on short videos whereas long video (longer than e.g.~1 minute) comprehension remains challenging. The major problem lies in the over-compression of videos, i.e., the encoded video representations are not enough to represent the whole video. To address this issue, we propose Long Video Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to dynamically adjust the number of embeddings in alignment with the duration of the video to ensure long videos are not overly compressed into a few embeddings. To deal with long videos whose length is beyond videos seen during training, we propose Interleaved Frame Encoding (IFE), repeating positional embedding and interleaving multiple groups of videos to enable long video input, avoiding performance degradation due to overly long videos. Experimental results show that LVChat significantly outperforms existing methods by up to 27\\% in accuracy on long-video QA datasets and long-video captioning benchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.|\u542f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u9605\u8bfb\u89c6\u9891\u5bf9\u4e8e\u591a\u6a21\u5f0f LLM \u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u4f5c\u54c1\u5728\u77ed\u89c6\u9891\u4e0a\u8868\u73b0\u51fa\u4e86\u5e0c\u671b\uff0c\u800c\u957f\u89c6\u9891\uff08\u4f8b\u5982\u957f\u4e8e\u7ea6 1 \u5206\u949f\uff09\u7684\u7406\u89e3\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u89c6\u9891\u7684\u8fc7\u5ea6\u538b\u7f29\uff0c\u5373\u7f16\u7801\u7684\u89c6\u9891\u8868\u793a\u4e0d\u8db3\u4ee5\u8868\u793a\u6574\u4e2a\u89c6\u9891\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u957f\u89c6\u9891\u804a\u5929\uff08LVChat\uff09\uff0c\u5176\u4e2d\u5f15\u5165\u4e86\u5e27\u53ef\u6269\u5c55\u7f16\u7801\uff08FSE\uff09\u6765\u6839\u636e\u89c6\u9891\u7684\u6301\u7eed\u65f6\u95f4\u52a8\u6001\u8c03\u6574\u5d4c\u5165\u7684\u6570\u91cf\uff0c\u4ee5\u786e\u4fdd\u957f\u89c6\u9891\u4e0d\u4f1a\u88ab\u8fc7\u5ea6\u538b\u7f29\u4e3a\u51e0\u4e2a\u5d4c\u5165\u3002\u4e3a\u4e86\u5904\u7406\u957f\u5ea6\u8d85\u51fa\u8bad\u7ec3\u671f\u95f4\u770b\u5230\u7684\u89c6\u9891\u7684\u957f\u89c6\u9891\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4ea4\u9519\u5e27\u7f16\u7801\uff08IFE\uff09\uff0c\u91cd\u590d\u4f4d\u7f6e\u5d4c\u5165\u5e76\u4ea4\u9519\u591a\u7ec4\u89c6\u9891\u4ee5\u5b9e\u73b0\u957f\u89c6\u9891\u8f93\u5165\uff0c\u907f\u514d\u56e0\u89c6\u9891\u8fc7\u957f\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLVChat \u5728\u957f\u89c6\u9891 QA \u6570\u636e\u96c6\u548c\u957f\u89c6\u9891\u5b57\u5e55\u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u7387\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe 27%\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53d1\u5e03\u5728https://github.com/wangyu-ustc/LVChat\u3002|[2402.12079v1](http://arxiv.org/pdf/2402.12079v1)|null|\n", "2402.12058": "|**2024-02-19**|**Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models**|\u811a\u624b\u67b6\u5750\u6807\u4fc3\u8fdb\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u534f\u8c03|Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, Yang Liu|State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information. Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to promote vision-language coordination. Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of Scaffold over GPT-4V with the textual CoT prompting. Our code is released in https://github.com/leixy20/Scaffold.|\u6700\u5148\u8fdb\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\u3002\u5c3d\u7ba1 LMM \u5177\u6709\u5148\u8fdb\u7684\u529f\u80fd\uff0c\u4f46\u5728\u9700\u8981\u4f7f\u7528\u591a\u7ea7\u89c6\u89c9\u4fe1\u606f\u8fdb\u884c\u590d\u6742\u63a8\u7406\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\uff0c\u5176\u6027\u80fd\u4ecd\u7136\u53d7\u5230\u9650\u5236\u3002\u73b0\u6709\u7684 LMM \u63d0\u793a\u6280\u672f\u4fa7\u91cd\u4e8e\u6539\u8fdb\u6587\u672c\u63a8\u7406\u6216\u5229\u7528\u56fe\u50cf\u9884\u5904\u7406\u5de5\u5177\uff0c\u7f3a\u4e4f\u7b80\u5355\u901a\u7528\u7684\u89c6\u89c9\u63d0\u793a\u65b9\u6848\u6765\u4fc3\u8fdb LMM \u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u534f\u8c03\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u652f\u67b6\u63d0\u793a\u652f\u67b6\u534f\u8c03\u4ee5\u4fc3\u8fdb\u89c6\u89c9\u8bed\u8a00\u534f\u8c03\u3002\u5177\u4f53\u6765\u8bf4\uff0cScaffold \u5728\u56fe\u50cf\u5185\u8986\u76d6\u70b9\u77e9\u9635\u4f5c\u4e3a\u89c6\u89c9\u4fe1\u606f\u951a\u70b9\uff0c\u5e76\u5229\u7528\u591a\u7ef4\u5750\u6807\u4f5c\u4e3a\u6587\u672c\u4f4d\u7f6e\u53c2\u8003\u3002\u5bf9\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86 Scaffold \u76f8\u5bf9\u4e8e GPT-4V \u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5177\u6709\u6587\u672c CoT \u63d0\u793a\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53d1\u5e03\u5728 https://github.com/leixy20/Scaffold \u4e2d\u3002|[2402.12058v1](http://arxiv.org/pdf/2402.12058v1)|null|\n", "2402.11908": "|**2024-02-19**|**Semantic Textual Similarity Assessment in Chest X-ray Reports Using a Domain-Specific Cosine-Based Metric**|\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u7684\u57fa\u4e8e\u4f59\u5f26\u7684\u5ea6\u91cf\u5bf9\u80f8\u90e8 X \u5c04\u7ebf\u62a5\u544a\u8fdb\u884c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u8bc4\u4f30|Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier|Medical language processing and deep learning techniques have emerged as critical tools for improving healthcare, particularly in the analysis of medical imaging and medical text data. These multimodal data fusion techniques help to improve the interpretation of medical imaging and lead to increased diagnostic accuracy, informed clinical decisions, and improved patient outcomes. The success of these models relies on the ability to extract and consolidate semantic information from clinical text. This paper addresses the need for more robust methods to evaluate the semantic content of medical reports. Conventional natural language processing approaches and metrics are initially designed for considering the semantic context in the natural language domain and machine translation, often failing to capture the complex semantic meanings inherent in medical content. In this study, we introduce a novel approach designed specifically for assessing the semantic similarity between generated medical reports and the ground truth. Our approach is validated, demonstrating its efficiency in assessing domain-specific semantic similarity within medical contexts. By applying our metric to state-of-the-art Chest X-ray report generation models, we obtain results that not only align with conventional metrics but also provide more contextually meaningful scores in the considered medical domain.|\u533b\u5b66\u8bed\u8a00\u5904\u7406\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5df2\u6210\u4e3a\u6539\u5584\u533b\u7597\u4fdd\u5065\u7684\u5173\u952e\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u6210\u50cf\u548c\u533b\u5b66\u6587\u672c\u6570\u636e\u7684\u5206\u6790\u65b9\u9762\u3002\u8fd9\u4e9b\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u6280\u672f\u6709\u52a9\u4e8e\u6539\u5584\u533b\u5b66\u6210\u50cf\u7684\u89e3\u91ca\uff0c\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3001\u660e\u667a\u7684\u4e34\u5e8a\u51b3\u7b56\u5e76\u6539\u5584\u60a3\u8005\u7684\u6cbb\u7597\u7ed3\u679c\u3002\u8fd9\u4e9b\u6a21\u578b\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u4ece\u4e34\u5e8a\u6587\u672c\u4e2d\u63d0\u53d6\u548c\u6574\u5408\u8bed\u4e49\u4fe1\u606f\u7684\u80fd\u529b\u3002\u672c\u6587\u89e3\u51b3\u4e86\u5bf9\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u533b\u7597\u62a5\u544a\u8bed\u4e49\u5185\u5bb9\u7684\u9700\u6c42\u3002\u4f20\u7edf\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u6cd5\u548c\u6307\u6807\u6700\u521d\u662f\u4e3a\u4e86\u8003\u8651\u81ea\u7136\u8bed\u8a00\u9886\u57df\u548c\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u800c\u8bbe\u8ba1\u7684\uff0c\u901a\u5e38\u65e0\u6cd5\u6355\u83b7\u533b\u5b66\u5185\u5bb9\u56fa\u6709\u7684\u590d\u6742\u8bed\u4e49\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4e13\u95e8\u4e3a\u8bc4\u4f30\u751f\u6210\u7684\u533b\u7597\u62a5\u544a\u4e0e\u771f\u5b9e\u60c5\u51b5\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u800c\u8bbe\u8ba1\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7ecf\u8fc7\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8bc4\u4f30\u533b\u5b66\u4e0a\u4e0b\u6587\u4e2d\u7279\u5b9a\u9886\u57df\u8bed\u4e49\u76f8\u4f3c\u6027\u65b9\u9762\u7684\u6548\u7387\u3002\u901a\u8fc7\u5c06\u6211\u4eec\u7684\u6307\u6807\u5e94\u7528\u4e8e\u6700\u5148\u8fdb\u7684\u80f8\u90e8 X \u5c04\u7ebf\u62a5\u544a\u751f\u6210\u6a21\u578b\uff0c\u6211\u4eec\u83b7\u5f97\u7684\u7ed3\u679c\u4e0d\u4ec5\u4e0e\u4f20\u7edf\u6307\u6807\u4e00\u81f4\uff0c\u800c\u4e14\u8fd8\u5728\u6240\u8003\u8651\u7684\u533b\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u5177\u4e0a\u4e0b\u6587\u610f\u4e49\u7684\u5206\u6570\u3002|[2402.11908v1](http://arxiv.org/pdf/2402.11908v1)|null|\n", "2402.11826": "|**2024-02-19**|**Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging Scenarios**|\u63ed\u5f00\u6df1\u5ea6\uff1a\u5e94\u5bf9\u6311\u6218\u6027\u573a\u666f\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6|Jialei Xu, Xianming Liu, Junjun Jiang, Kui Jiang, Rui Li, Kai Cheng, Xiangyang Ji|Monocular depth estimation from RGB images plays a pivotal role in 3D vision. However, its accuracy can deteriorate in challenging environments such as nighttime or adverse weather conditions. While long-wave infrared cameras offer stable imaging in such challenging conditions, they are inherently low-resolution, lacking rich texture and semantics as delivered by the RGB image. Current methods focus solely on a single modality due to the difficulties to identify and integrate faithful depth cues from both sources. To address these issues, this paper presents a novel approach that identifies and integrates dominant cross-modality depth features with a learning-based framework. Concretely, we independently compute the coarse depth maps with separate networks by fully utilizing the individual depth cues from each modality. As the advantageous depth spreads across both modalities, we propose a novel confidence loss steering a confidence predictor network to yield a confidence map specifying latent potential depth areas. With the resulting confidence map, we propose a multi-modal fusion network that fuses the final depth in an end-to-end manner. Harnessing the proposed pipeline, our method demonstrates the ability of robust depth estimation in a variety of difficult scenarios. Experimental results on the challenging MS$^2$ and ViViD++ datasets demonstrate the effectiveness and robustness of our method.|RGB \u56fe\u50cf\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5728 3D \u89c6\u89c9\u4e2d\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u5728\u591c\u95f4\u6216\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\uff0c\u5176\u51c6\u786e\u6027\u53ef\u80fd\u4f1a\u4e0b\u964d\u3002\u867d\u7136\u957f\u6ce2\u7ea2\u5916\u76f8\u673a\u53ef\u4ee5\u5728\u5982\u6b64\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u63d0\u4f9b\u7a33\u5b9a\u7684\u6210\u50cf\uff0c\u4f46\u5b83\u4eec\u672c\u8d28\u4e0a\u5206\u8fa8\u7387\u8f83\u4f4e\uff0c\u7f3a\u4e4f RGB \u56fe\u50cf\u6240\u63d0\u4f9b\u7684\u4e30\u5bcc\u7eb9\u7406\u548c\u8bed\u4e49\u3002\u7531\u4e8e\u96be\u4ee5\u8bc6\u522b\u548c\u6574\u5408\u6765\u81ea\u4e24\u4e2a\u6765\u6e90\u7684\u5fe0\u5b9e\u6df1\u5ea6\u7ebf\u7d22\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5355\u4e00\u6a21\u5f0f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u8bc6\u522b\u4e3b\u8981\u7684\u8de8\u6a21\u6001\u6df1\u5ea6\u7279\u5f81\u5e76\u5c06\u5176\u4e0e\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\u76f8\u96c6\u6210\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u901a\u8fc7\u5145\u5206\u5229\u7528\u6bcf\u79cd\u6a21\u6001\u7684\u5355\u72ec\u6df1\u5ea6\u7ebf\u7d22\uff0c\u4f7f\u7528\u5355\u72ec\u7684\u7f51\u7edc\u72ec\u7acb\u8ba1\u7b97\u7c97\u7565\u6df1\u5ea6\u56fe\u3002\u7531\u4e8e\u6709\u5229\u7684\u6df1\u5ea6\u5206\u5e03\u5728\u4e24\u79cd\u6a21\u5f0f\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7f6e\u4fe1\u635f\u5931\uff0c\u5f15\u5bfc\u7f6e\u4fe1\u9884\u6d4b\u5668\u7f51\u7edc\u4ea7\u751f\u6307\u5b9a\u6f5c\u5728\u6df1\u5ea6\u533a\u57df\u7684\u7f6e\u4fe1\u56fe\u3002\u6839\u636e\u5f97\u5230\u7684\u7f6e\u4fe1\u56fe\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u878d\u5408\u7f51\u7edc\uff0c\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u878d\u5408\u6700\u7ec8\u6df1\u5ea6\u3002\u5229\u7528\u6240\u63d0\u51fa\u7684\u7ba1\u9053\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u5404\u79cd\u56f0\u96be\u573a\u666f\u4e2d\u7a33\u5065\u7684\u6df1\u5ea6\u4f30\u8ba1\u7684\u80fd\u529b\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684 MS$^2$ \u548c ViViD++ \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002|[2402.11826v1](http://arxiv.org/pdf/2402.11826v1)|null|\n", "2402.11788": "|**2024-02-19**|**MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion**|MM-SurvNet\uff1a\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u8fdb\u884c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e73\u817a\u764c\u751f\u5b58\u98ce\u9669\u5206\u5c42|Raktim Kumar Mondol, Ewan K. A. Millar, Arcot Sowmya, Erik Meijering|Survival risk stratification is an important step in clinical decision making for breast cancer management. We propose a novel deep learning approach for this purpose by integrating histopathological imaging, genetic and clinical data. It employs vision transformers, specifically the MaxViT model, for image feature extraction, and self-attention to capture intricate image relationships at the patient level. A dual cross-attention mechanism fuses these features with genetic data, while clinical data is incorporated at the final layer to enhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show that our model, trained using the negative log likelihood loss function, can achieve superior performance with a mean C-index of 0.64, surpassing existing methods. This advancement facilitates tailored treatment strategies, potentially leading to improved patient outcomes.|\u751f\u5b58\u98ce\u9669\u5206\u5c42\u662f\u4e73\u817a\u764c\u7ba1\u7406\u4e34\u5e8a\u51b3\u7b56\u7684\u91cd\u8981\u4e00\u6b65\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u901a\u8fc7\u6574\u5408\u7ec4\u7ec7\u75c5\u7406\u5b66\u6210\u50cf\u3001\u9057\u4f20\u548c\u4e34\u5e8a\u6570\u636e\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u5b83\u91c7\u7528\u89c6\u89c9\u8f6c\u6362\u5668\uff08\u7279\u522b\u662f MaxViT \u6a21\u578b\uff09\u8fdb\u884c\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u4f7f\u7528\u81ea\u6211\u6ce8\u610f\u529b\u6765\u6355\u83b7\u60a3\u8005\u5c42\u9762\u7684\u590d\u6742\u56fe\u50cf\u5173\u7cfb\u3002\u53cc\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5c06\u8fd9\u4e9b\u7279\u5f81\u4e0e\u9057\u4f20\u6570\u636e\u878d\u5408\uff0c\u800c\u4e34\u5e8a\u6570\u636e\u5219\u88ab\u7eb3\u5165\u6700\u540e\u4e00\u5c42\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002\u5728\u516c\u5171 TCGA-BRCA \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4f7f\u7528\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5e73\u5747 C \u6307\u6570\u4e3a 0.64 \u7684\u5353\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002\u8fd9\u4e00\u8fdb\u6b65\u6709\u52a9\u4e8e\u5236\u5b9a\u91cf\u8eab\u5b9a\u5236\u7684\u6cbb\u7597\u7b56\u7565\uff0c\u6709\u53ef\u80fd\u6539\u5584\u60a3\u8005\u7684\u6cbb\u7597\u7ed3\u679c\u3002|[2402.11788v1](http://arxiv.org/pdf/2402.11788v1)|null|\n"}, "Nerf": {"2402.12377": "|**2024-02-19**|**Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis**|\u4e8c\u5143\u4e0d\u900f\u660e\u5ea6\u7f51\u683c\uff1a\u6355\u83b7\u7cbe\u7ec6\u7684\u51e0\u4f55\u7ec6\u8282\u4ee5\u8fdb\u884c\u57fa\u4e8e\u7f51\u683c\u7684\u89c6\u56fe\u5408\u6210|Christian Reiser, Stephan Garbin, Pratul P. Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan T. Barron, Peter Hedman, Andreas Geiger|While surface-based view synthesis algorithms are appealing due to their low computational requirements, they often struggle to reproduce thin structures. In contrast, more expensive methods that model the scene's geometry as a volumetric density field (e.g. NeRF) excel at reconstructing fine geometric detail. However, density fields often represent geometry in a \"fuzzy\" manner, which hinders exact localization of the surface. In this work, we modify density fields to encourage them to converge towards surfaces, without compromising their ability to reconstruct thin structures. First, we employ a discrete opacity grid representation instead of a continuous density field, which allows opacity values to discontinuously transition from zero to one at the surface. Second, we anti-alias by casting multiple rays per pixel, which allows occlusion boundaries and subpixel structures to be modelled without using semi-transparent voxels. Third, we minimize the binary entropy of the opacity values, which facilitates the extraction of surface geometry by encouraging opacity values to binarize towards the end of training. Lastly, we develop a fusion-based meshing strategy followed by mesh simplification and appearance model fitting. The compact meshes produced by our model can be rendered in real-time on mobile devices and achieve significantly higher view synthesis quality compared to existing mesh-based approaches.|\u867d\u7136\u57fa\u4e8e\u8868\u9762\u7684\u89c6\u56fe\u5408\u6210\u7b97\u6cd5\u7531\u4e8e\u8ba1\u7b97\u8981\u6c42\u4f4e\u800c\u9887\u5177\u5438\u5f15\u529b\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u96be\u4ee5\u518d\u73b0\u8584\u7ed3\u6784\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5c06\u573a\u666f\u51e0\u4f55\u6a21\u578b\u5efa\u6a21\u4e3a\u4f53\u79ef\u5bc6\u5ea6\u573a\u7684\u66f4\u6602\u8d35\u7684\u65b9\u6cd5\uff08\u4f8b\u5982 NeRF\uff09\u64c5\u957f\u91cd\u5efa\u7cbe\u7ec6\u7684\u51e0\u4f55\u7ec6\u8282\u3002\u7136\u800c\uff0c\u5bc6\u5ea6\u573a\u901a\u5e38\u4ee5\u201c\u6a21\u7cca\u201d\u65b9\u5f0f\u8868\u793a\u51e0\u4f55\u5f62\u72b6\uff0c\u8fd9\u963b\u788d\u4e86\u8868\u9762\u7684\u7cbe\u786e\u5b9a\u4f4d\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4fee\u6539\u5bc6\u5ea6\u573a\u4ee5\u9f13\u52b1\u5b83\u4eec\u5411\u8868\u9762\u6c47\u805a\uff0c\u800c\u4e0d\u635f\u5bb3\u5b83\u4eec\u91cd\u5efa\u8584\u7ed3\u6784\u7684\u80fd\u529b\u3002\u9996\u5148\uff0c\u6211\u4eec\u91c7\u7528\u79bb\u6563\u4e0d\u900f\u660e\u5ea6\u7f51\u683c\u8868\u793a\u800c\u4e0d\u662f\u8fde\u7eed\u5bc6\u5ea6\u573a\uff0c\u8fd9\u5141\u8bb8\u4e0d\u900f\u660e\u5ea6\u503c\u5728\u8868\u9762\u4ece\u96f6\u4e0d\u8fde\u7eed\u5730\u8fc7\u6e21\u5230\u4e00\u3002\u5176\u6b21\uff0c\u6211\u4eec\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u50cf\u7d20\u6295\u5c04\u591a\u6761\u5149\u7ebf\u6765\u6d88\u9664\u952f\u9f7f\uff0c\u8fd9\u5141\u8bb8\u5728\u4e0d\u4f7f\u7528\u534a\u900f\u660e\u4f53\u7d20\u7684\u60c5\u51b5\u4e0b\u5bf9\u906e\u6321\u8fb9\u754c\u548c\u5b50\u50cf\u7d20\u7ed3\u6784\u8fdb\u884c\u5efa\u6a21\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u6700\u5c0f\u5316\u4e0d\u900f\u660e\u5ea6\u503c\u7684\u4e8c\u5143\u71b5\uff0c\u8fd9\u901a\u8fc7\u9f13\u52b1\u4e0d\u900f\u660e\u5ea6\u503c\u5728\u8bad\u7ec3\u7ed3\u675f\u65f6\u4e8c\u503c\u5316\u6765\u4fc3\u8fdb\u8868\u9762\u51e0\u4f55\u5f62\u72b6\u7684\u63d0\u53d6\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u878d\u5408\u7684\u7f51\u683c\u5212\u5206\u7b56\u7565\uff0c\u7136\u540e\u8fdb\u884c\u7f51\u683c\u7b80\u5316\u548c\u5916\u89c2\u6a21\u578b\u62df\u5408\u3002\u6211\u4eec\u7684\u6a21\u578b\u751f\u6210\u7684\u7d27\u51d1\u7f51\u683c\u53ef\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u65f6\u6e32\u67d3\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53ef\u4ee5\u5b9e\u73b0\u663e\u7740\u66f4\u9ad8\u7684\u89c6\u56fe\u5408\u6210\u8d28\u91cf\u3002|[2402.12377v1](http://arxiv.org/pdf/2402.12377v1)|null|\n", "2402.12184": "|**2024-02-19**|**Colorizing Monochromatic Radiance Fields**|\u5bf9\u5355\u8272\u8f90\u5c04\u573a\u8fdb\u884c\u7740\u8272|Yean Cheng, Renjie Wan, Shuchen Weng, Chengxuan Zhu, Yakun Chang, Boxin Shi|Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided. Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial. To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space. By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module. We then reproduce a colorful implicit model through the representation of luminance, density, and color. Extensive experiments have been conducted to validate the effectiveness of our approaches. Our project page: https://liquidammonia.github.io/color-nerf.|\u5c3d\u7ba1\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u4e00\u7ec4 2D \u56fe\u50cf\u6765\u751f\u6210\u4e16\u754c\u7684\u5f69\u8272 3D \u8868\u793a\uff0c\u4f46\u5f53\u4ec5\u63d0\u4f9b\u5355\u8272\u56fe\u50cf\u65f6\uff0c\u8fd9\u79cd\u80fd\u529b\u5c31\u53d8\u5f97\u4e0d\u5b58\u5728\u3002\u7531\u4e8e\u989c\u8272\u5bf9\u4e8e\u8868\u793a\u4e16\u754c\u662f\u5fc5\u8981\u7684\uff0c\u56e0\u6b64\u4ece\u5355\u8272\u8f90\u5c04\u573a\u518d\u73b0\u989c\u8272\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u4e0d\u76f4\u63a5\u64cd\u4f5c\u5355\u8272\u8f90\u5c04\u573a\uff0c\u800c\u662f\u5c06\u5176\u89c6\u4e3a Lab \u989c\u8272\u7a7a\u95f4\u4e2d\u7684\u8868\u793a\u9884\u6d4b\u4efb\u52a1\u3002\u901a\u8fc7\u9996\u5148\u4f7f\u7528\u5355\u8272\u56fe\u50cf\u6784\u5efa\u4eae\u5ea6\u548c\u5bc6\u5ea6\u8868\u793a\uff0c\u6211\u4eec\u7684\u9884\u6d4b\u9636\u6bb5\u53ef\u4ee5\u57fa\u4e8e\u56fe\u50cf\u7740\u8272\u6a21\u5757\u91cd\u65b0\u521b\u5efa\u989c\u8272\u8868\u793a\u3002\u7136\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4eae\u5ea6\u3001\u5bc6\u5ea6\u548c\u989c\u8272\u7684\u8868\u793a\u6765\u91cd\u73b0\u5f69\u8272\u9690\u5f0f\u6a21\u578b\u3002\u5df2\u7ecf\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\uff1ahttps://liquidammonia.github.io/color-nerf\u3002|[2402.12184v1](http://arxiv.org/pdf/2402.12184v1)|null|\n"}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.11812": "|**2024-02-19**|**Interpretable Embedding for Ad-hoc Video Search**|\u7528\u4e8e\u4e34\u65f6\u89c6\u9891\u641c\u7d22\u7684\u53ef\u89e3\u91ca\u5d4c\u5165|Jiaxin Wu, Chong-Wah Ngo|Answering query with semantic concepts has long been the mainstream approach for video search. Until recently, its performance is surpassed by concept-free approach, which embeds queries in a joint space as videos. Nevertheless, the embedded features as well as search results are not interpretable, hindering subsequent steps in video browsing and query reformulation. This paper integrates feature embedding and concept interpretation into a neural network for unified dual-task learning. In this way, an embedding is associated with a list of semantic concepts as an interpretation of video content. This paper empirically demonstrates that, by using either the embedding features or concepts, considerable search improvement is attainable on TRECVid benchmarked datasets. Concepts are not only effective in pruning false positive videos, but also highly complementary to concept-free search, leading to large margin of improvement compared to state-of-the-art approaches.|\u7528\u8bed\u4e49\u6982\u5ff5\u56de\u7b54\u67e5\u8be2\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u662f\u89c6\u9891\u641c\u7d22\u7684\u4e3b\u6d41\u65b9\u6cd5\u3002\u76f4\u5230\u6700\u8fd1\uff0c\u5b83\u7684\u6027\u80fd\u8fd8\u88ab\u65e0\u6982\u5ff5\u65b9\u6cd5\u8d85\u8d8a\uff0c\u8be5\u65b9\u6cd5\u5c06\u67e5\u8be2\u4f5c\u4e3a\u89c6\u9891\u5d4c\u5165\u5230\u8054\u5408\u7a7a\u95f4\u4e2d\u3002\u7136\u800c\uff0c\u5d4c\u5165\u7684\u529f\u80fd\u4ee5\u53ca\u641c\u7d22\u7ed3\u679c\u662f\u4e0d\u53ef\u89e3\u91ca\u7684\uff0c\u963b\u788d\u4e86\u89c6\u9891\u6d4f\u89c8\u548c\u67e5\u8be2\u91cd\u65b0\u5236\u5b9a\u7684\u540e\u7eed\u6b65\u9aa4\u3002\u672c\u6587\u5c06\u7279\u5f81\u5d4c\u5165\u548c\u6982\u5ff5\u89e3\u91ca\u96c6\u6210\u5230\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u7edf\u4e00\u7684\u53cc\u4efb\u52a1\u5b66\u4e60\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u5d4c\u5165\u4e0e\u8bed\u4e49\u6982\u5ff5\u5217\u8868\u76f8\u5173\u8054\uff0c\u4f5c\u4e3a\u89c6\u9891\u5185\u5bb9\u7684\u89e3\u91ca\u3002\u672c\u6587\u51ed\u7ecf\u9a8c\u8bc1\u660e\uff0c\u901a\u8fc7\u4f7f\u7528\u5d4c\u5165\u7279\u5f81\u6216\u6982\u5ff5\uff0c\u53ef\u4ee5\u5728 TRECVid \u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u76f8\u5f53\u5927\u7684\u641c\u7d22\u6539\u8fdb\u3002\u6982\u5ff5\u4e0d\u4ec5\u53ef\u4ee5\u6709\u6548\u5730\u4fee\u526a\u8bef\u62a5\u89c6\u9891\uff0c\u800c\u4e14\u4e0e\u65e0\u6982\u5ff5\u641c\u7d22\u9ad8\u5ea6\u4e92\u8865\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002|[2402.11812v1](http://arxiv.org/pdf/2402.11812v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.12320": "|**2024-02-19**|**Landmark Stereo Dataset for Landmark Recognition and Moving Node Localization in a Non-GPS Battlefield Environment**|\u7528\u4e8e\u975e GPS \u6218\u573a\u73af\u5883\u4e2d\u5730\u6807\u8bc6\u522b\u548c\u79fb\u52a8\u8282\u70b9\u5b9a\u4f4d\u7684\u5730\u6807\u7acb\u4f53\u6570\u636e\u96c6|Ganesh Sapkota, Sanjay Madria|In this paper, we have proposed a new strategy of using the landmark anchor node instead of a radio-based anchor node to obtain the virtual coordinates (landmarkID, DISTANCE) of moving troops or defense forces that will help in tracking and maneuvering the troops along a safe path within a GPS-denied battlefield environment. The proposed strategy implements landmark recognition using the Yolov5 model and landmark distance estimation using an efficient Stereo Matching Algorithm. We consider that a moving node carrying a low-power mobile device facilitated with a calibrated stereo vision camera that captures stereo images of a scene containing landmarks within the battlefield region whose locations are stored in an offline server residing within the device itself. We created a custom landmark image dataset called MSTLandmarkv1 with 34 landmark classes and another landmark stereo dataset of those 34 landmark instances called MSTLandmarkStereov1. We trained the YOLOv5 model with MSTLandmarkv1 dataset and achieved 0.95 mAP @ 0.5 IoU and 0.767 mAP @ [0.5: 0.95] IoU. We calculated the distance from a node to the landmark utilizing the bounding box coordinates and the depth map generated by the improved SGM algorithm using MSTLandmarkStereov1. The tuple of landmark IDs obtained from the detection result and the distances calculated by the SGM algorithm are stored as the virtual coordinates of a node. In future work, we will use these virtual coordinates to obtain the location of a node using an efficient trilateration algorithm and optimize the node position using the appropriate optimization method.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b56\u7565\uff0c\u4f7f\u7528\u5730\u6807\u951a\u8282\u70b9\u4ee3\u66ff\u57fa\u4e8e\u65e0\u7ebf\u7535\u7684\u951a\u8282\u70b9\u6765\u83b7\u53d6\u79fb\u52a8\u90e8\u961f\u6216\u9632\u5fa1\u90e8\u961f\u7684\u865a\u62df\u5750\u6807\uff08landmarkID\uff0cDISTANCE\uff09\uff0c\u8fd9\u5c06\u6709\u52a9\u4e8e\u6cbf\u7ebf\u8ddf\u8e2a\u548c\u673a\u52a8\u90e8\u961f\u65e0\u6cd5\u4f7f\u7528 GPS \u7684\u6218\u573a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u8def\u5f84\u3002\u6240\u63d0\u51fa\u7684\u7b56\u7565\u4f7f\u7528 Yolov5 \u6a21\u578b\u5b9e\u73b0\u5730\u6807\u8bc6\u522b\uff0c\u5e76\u4f7f\u7528\u9ad8\u6548\u7684\u7acb\u4f53\u5339\u914d\u7b97\u6cd5\u5b9e\u73b0\u5730\u6807\u8ddd\u79bb\u4f30\u8ba1\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u643a\u5e26\u4f4e\u529f\u8017\u79fb\u52a8\u8bbe\u5907\u7684\u79fb\u52a8\u8282\u70b9\u501f\u52a9\u6821\u51c6\u7684\u7acb\u4f53\u89c6\u89c9\u76f8\u673a\u6765\u6355\u83b7\u6218\u573a\u533a\u57df\u5185\u5305\u542b\u5730\u6807\u7684\u573a\u666f\u7684\u7acb\u4f53\u56fe\u50cf\uff0c\u8fd9\u4e9b\u5730\u6807\u7684\u4f4d\u7f6e\u5b58\u50a8\u5728\u9a7b\u7559\u5728\u8bbe\u5907\u672c\u8eab\u5185\u7684\u79bb\u7ebf\u670d\u52a1\u5668\u4e2d\u3002\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a MSTLandmarkv1 \u7684\u81ea\u5b9a\u4e49\u5730\u6807\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 34 \u4e2a\u5730\u6807\u7c7b\u522b\uff0c\u4ee5\u53ca\u8fd9 34 \u4e2a\u5730\u6807\u5b9e\u4f8b\u7684\u53e6\u4e00\u4e2a\u5730\u6807\u7acb\u4f53\u6570\u636e\u96c6\uff0c\u79f0\u4e3a MSTLandmarkStereov1\u3002\u6211\u4eec\u4f7f\u7528 MSTLandmarkv1 \u6570\u636e\u96c6\u8bad\u7ec3 YOLOv5 \u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86 0.95 mAP @ 0.5 IoU \u548c 0.767 mAP @ [0.5: 0.95] IoU\u3002\u6211\u4eec\u5229\u7528\u8fb9\u754c\u6846\u5750\u6807\u548c\u4f7f\u7528 MSTLandmarkStereov1 \u7684\u6539\u8fdb SGM \u7b97\u6cd5\u751f\u6210\u7684\u6df1\u5ea6\u56fe\u6765\u8ba1\u7b97\u4ece\u8282\u70b9\u5230\u5730\u6807\u7684\u8ddd\u79bb\u3002\u5c06\u68c0\u6d4b\u7ed3\u679c\u5f97\u5230\u7684\u5730\u6807ID\u5143\u7ec4\u548cSGM\u7b97\u6cd5\u8ba1\u7b97\u51fa\u7684\u8ddd\u79bb\u5b58\u50a8\u4e3a\u8282\u70b9\u7684\u865a\u62df\u5750\u6807\u3002\u5728\u672a\u6765\u7684\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06\u5229\u7528\u8fd9\u4e9b\u865a\u62df\u5750\u6807\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u4e09\u8fb9\u6d4b\u91cf\u7b97\u6cd5\u6765\u83b7\u53d6\u8282\u70b9\u7684\u4f4d\u7f6e\uff0c\u5e76\u4f7f\u7528\u9002\u5f53\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u4f18\u5316\u8282\u70b9\u4f4d\u7f6e\u3002|[2402.12320v1](http://arxiv.org/pdf/2402.12320v1)|null|\n", "2402.12303": "|**2024-02-19**|**UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking**|UncertaintyTrack\uff1a\u5229\u7528\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027|Chang Won Lee, Steven L. Waslander|Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods. The majority of tracking methods follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking. We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19\\% and improves mMOTA by 2-3%. The source code is available at https://github.com/TRAILab/UncertaintyTrack|\u7531\u4e8e\u7814\u7a76\u754c\u7684\u5f3a\u70c8\u5174\u8da3\u548c\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u7684\u7a33\u6b65\u6539\u8fdb\uff0c\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u65b9\u6cd5\u6700\u8fd1\u6027\u80fd\u663e\u7740\u63d0\u5347\u3002\u5927\u591a\u6570\u8ddf\u8e2a\u65b9\u6cd5\u9075\u5faa\u68c0\u6d4b\u8ddf\u8e2a\uff08TBD\uff09\u8303\u4f8b\uff0c\u76f2\u76ee\u5730\u4fe1\u4efb\u4f20\u5165\u7684\u68c0\u6d4b\uff0c\u800c\u4e0d\u4e86\u89e3\u5176\u76f8\u5173\u7684\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\u3002\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u610f\u8bc6\u4f1a\u7ed9\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u4efb\u52a1\u5e26\u6765\u95ee\u9898\uff0c\u4e58\u5ba2\u53ef\u80fd\u4f1a\u56e0\u4e3a\u9519\u8bef\u68c0\u6d4b\u4f20\u64ad\u5230\u4e0b\u6e38\u4efb\u52a1\uff08\u5305\u62ec MOT\uff09\u800c\u9762\u4e34\u98ce\u9669\u3002\u867d\u7136\u6982\u7387\u5bf9\u8c61\u68c0\u6d4b\u65b9\u9762\u7684\u73b0\u6709\u5de5\u4f5c\u53ef\u4ee5\u9884\u6d4b\u76d2\u5b50\u5468\u56f4\u7684\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u81ea\u52a8\u9a7e\u9a76\u7684 2D MOT \u65b9\u9762\u8fd8\u6ca1\u6709\u7814\u7a76\u8fd9\u4e9b\u4f30\u8ba1\u662f\u5426\u8db3\u591f\u6709\u610f\u4e49\uff0c\u53ef\u4ee5\u5728\u5bf9\u8c61\u8ddf\u8e2a\u4e2d\u6709\u6548\u5229\u7528\u3002\u6211\u4eec\u5f15\u5165\u4e86 UncertaintyTrack\uff0c\u8fd9\u662f\u4e00\u4e2a\u6269\u5c55\u96c6\u5408\uff0c\u53ef\u5e94\u7528\u4e8e\u591a\u4e2a TBD \u8ddf\u8e2a\u5668\uff0c\u4ee5\u89e3\u91ca\u6982\u7387\u5bf9\u8c61\u68c0\u6d4b\u5668\u7684\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002 Berkeley Deep Drive MOT \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u548c\u4fe1\u606f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u7ed3\u5408\u5c06 ID \u5207\u6362\u7684\u6570\u91cf\u51cf\u5c11\u4e86\u7ea6 19%\uff0c\u5e76\u5c06 mMOTA \u63d0\u9ad8\u4e86 2-3%\u3002\u6e90\u4ee3\u7801\u4f4d\u4e8e https://github.com/TRAILab/UncertaintyTrack|[2402.12303v1](http://arxiv.org/pdf/2402.12303v1)|null|\n", "2402.12198": "|**2024-02-19**|**Zero shot VLMs for hate meme detection: Are we there yet?**|\u7528\u4e8e\u4ec7\u6068\u6a21\u56e0\u68c0\u6d4b\u7684\u96f6\u6837\u672c VLM\uff1a\u6211\u4eec\u5230\u4e86\u5417\uff1f|Naquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, Animesh Mukherjee|Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we observe that large VLMs are still vulnerable for zero-shot hate meme detection.|\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u591a\u5a92\u4f53\u5185\u5bb9\u6b63\u5728\u8fc5\u901f\u53d1\u5c55\uff0c\u6a21\u56e0\u4f5c\u4e3a\u4e00\u79cd\u72ec\u7279\u7684\u5f62\u5f0f\u65e5\u76ca\u53d7\u5230\u91cd\u89c6\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u4e00\u4e9b\u6076\u610f\u7528\u6237\u5229\u7528\u6a21\u56e0\u6765\u9488\u5bf9\u4e2a\u4eba\u6216\u5f31\u52bf\u793e\u533a\uff0c\u56e0\u6b64\u5fc5\u987b\u8bc6\u522b\u548c\u89e3\u51b3\u6b64\u7c7b\u4ec7\u6068\u6a21\u56e0\u7684\u5b9e\u4f8b\u3002\u4e3a\u4e86\u901a\u8fc7\u5f00\u53d1\u4ec7\u6068\u6a21\u56e0\u68c0\u6d4b\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4eba\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\u3002\u7136\u800c\uff0c\u4f20\u7edf\u673a\u5668/\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4e00\u4e2a\u663e\u7740\u9650\u5236\u662f\u9700\u8981\u6807\u8bb0\u6570\u636e\u96c6\u624d\u80fd\u8fdb\u884c\u51c6\u786e\u5206\u7c7b\u3002\u6700\u8fd1\uff0c\u7814\u7a76\u754c\u89c1\u8bc1\u4e86\u51e0\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u7814\u7a76\u8fd9\u4e9b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\uff08\u4f8b\u5982\u4ec7\u6068\u6a21\u56e0\u68c0\u6d4b\uff09\u65b9\u9762\u7684\u529f\u6548\u3002\u6211\u4eec\u4f7f\u7528\u5404\u79cd\u63d0\u793a\u8bbe\u7f6e\u6765\u4e13\u6ce8\u4e8e\u4ec7\u6068/\u6709\u5bb3\u6a21\u56e0\u7684\u96f6\u6837\u672c\u5206\u7c7b\u3002\u901a\u8fc7\u6211\u4eec\u7684\u5206\u6790\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5927\u578b VLM \u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u96f6\u6837\u672c\u4ec7\u6068\u6a21\u56e0\u68c0\u6d4b\u7684\u5f71\u54cd\u3002|[2402.12198v1](http://arxiv.org/pdf/2402.12198v1)|null|\n", "2402.12138": "|**2024-02-19**|**Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers**|\u4f7f\u7528\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u53d8\u538b\u5668\u611f\u77e5\u66f4\u957f\u7684\u5e8f\u5217|Markus Hiller, Krista A. Ehinger, Tom Drummond|We present a novel bi-directional Transformer architecture (BiXT) which scales linearly with input size in terms of computational cost and memory consumption, but does not suffer the drop in performance or limitation to only one input modality seen with other efficient Transformer-based approaches. BiXT is inspired by the Perceiver architectures but replaces iterative attention with an efficient bi-directional cross-attention module in which input tokens and latent variables attend to each other simultaneously, leveraging a naturally emerging attention-symmetry between the two. This approach unlocks a key bottleneck experienced by Perceiver-like architectures and enables the processing and interpretation of both semantics (`what') and location (`where') to develop alongside each other over multiple layers -- allowing its direct application to dense and instance-based tasks alike. By combining efficiency with the generality and performance of a full Transformer architecture, BiXT can process longer sequences like point clouds or images at higher feature resolutions and achieves competitive performance across a range of tasks like point cloud part segmentation, semantic image segmentation and image classification.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5411 Transformer \u67b6\u6784\uff08BiXT\uff09\uff0c\u5b83\u5728\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u6d88\u8017\u65b9\u9762\u968f\u8f93\u5165\u5927\u5c0f\u7ebf\u6027\u6269\u5c55\uff0c\u4f46\u4e0d\u4f1a\u906d\u53d7\u5176\u4ed6\u9ad8\u6548\u7684\u57fa\u4e8e Transformer \u7684\u65b9\u6cd5\u6240\u89c1\u7684\u6027\u80fd\u4e0b\u964d\u6216\u4ec5\u9650\u4e8e\u4e00\u79cd\u8f93\u5165\u6a21\u5f0f\u7684\u5f71\u54cd\u3002 BiXT \u53d7\u5230\u611f\u77e5\u5668\u67b6\u6784\u7684\u542f\u53d1\uff0c\u4f46\u7528\u9ad8\u6548\u7684\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u53d6\u4ee3\u4e86\u8fed\u4ee3\u6ce8\u610f\u529b\uff0c\u5176\u4e2d\u8f93\u5165\u6807\u8bb0\u548c\u6f5c\u5728\u53d8\u91cf\u540c\u65f6\u76f8\u4e92\u5173\u6ce8\uff0c\u5229\u7528\u4e24\u8005\u4e4b\u95f4\u81ea\u7136\u51fa\u73b0\u7684\u6ce8\u610f\u529b\u5bf9\u79f0\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u89e3\u9501\u4e86\u7c7b\u611f\u77e5\u5668\u67b6\u6784\u6240\u7ecf\u5386\u7684\u5173\u952e\u74f6\u9888\uff0c\u5e76\u4f7f\u8bed\u4e49\uff08\u201c\u4ec0\u4e48\u201d\uff09\u548c\u4f4d\u7f6e\uff08\u201c\u54ea\u91cc\u201d\uff09\u7684\u5904\u7406\u548c\u89e3\u91ca\u80fd\u591f\u5728\u591a\u4e2a\u5c42\u4e0a\u5e76\u884c\u5f00\u53d1\u2014\u2014\u5141\u8bb8\u5176\u76f4\u63a5\u5e94\u7528\u4e8e\u5bc6\u96c6\u548c\u57fa\u4e8e\u5b9e\u4f8b\u7684\u4efb\u52a1\u7c7b\u4f3c\u3002\u901a\u8fc7\u5c06\u6548\u7387\u4e0e\u5b8c\u6574 Transformer \u67b6\u6784\u7684\u901a\u7528\u6027\u548c\u6027\u80fd\u76f8\u7ed3\u5408\uff0cBiXT \u53ef\u4ee5\u5904\u7406\u66f4\u957f\u7684\u5e8f\u5217\uff0c\u5982\u70b9\u4e91\u6216\u66f4\u9ad8\u7279\u5f81\u5206\u8fa8\u7387\u7684\u56fe\u50cf\uff0c\u5e76\u5728\u70b9\u4e91\u90e8\u5206\u5206\u5272\u3001\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u548c\u56fe\u50cf\u5206\u7c7b\u7b49\u4e00\u7cfb\u5217\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002|[2402.12138v1](http://arxiv.org/pdf/2402.12138v1)|null|\n", "2402.12098": "|**2024-02-19**|**Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient Based Target Localization**|\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u76ee\u6807\u5b9a\u4f4d\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u8bed\u4e49\u5206\u5272|Abhishek Kuriyal, Vaibhav Kumar|Semantic Segmentation (SS) of LiDAR point clouds is essential for many applications, such as urban planning and autonomous driving. While much progress has been made in interpreting SS predictions for images, interpreting point cloud SS predictions remains a challenge. This paper introduces pGS-CAM, a novel gradient-based method for generating saliency maps in neural network activation layers. Inspired by Grad-CAM, which uses gradients to highlight local importance, pGS-CAM is robust and effective on a variety of datasets (SemanticKITTI, Paris-Lille3D, DALES) and 3D deep learning architectures (KPConv, RandLANet). Our experiments show that pGS-CAM effectively accentuates the feature learning in intermediate activations of SS architectures by highlighting the contribution of each point. This allows us to better understand how SS models make their predictions and identify potential areas for improvement. Relevant codes are available at https://github.com/geoai4cities/pGS-CAM.|LiDAR \u70b9\u4e91\u7684\u8bed\u4e49\u5206\u5272 (SS) \u5bf9\u4e8e\u57ce\u5e02\u89c4\u5212\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u8bb8\u591a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5728\u89e3\u91ca\u56fe\u50cf\u7684 SS \u9884\u6d4b\u65b9\u9762\u5df2\u7ecf\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u5c55\uff0c\u4f46\u89e3\u91ca\u70b9\u4e91 SS \u9884\u6d4b\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 pGS-CAM\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u795e\u7ecf\u7f51\u7edc\u6fc0\u6d3b\u5c42\u4e2d\u751f\u6210\u663e\u7740\u56fe\u3002\u53d7 Grad-CAM\uff08\u4f7f\u7528\u68af\u5ea6\u6765\u7a81\u51fa\u5c40\u90e8\u91cd\u8981\u6027\uff09\u7684\u542f\u53d1\uff0cpGS-CAM \u5728\u5404\u79cd\u6570\u636e\u96c6\uff08SemanticKITTI\u3001Paris-Lille3D\u3001DALES\uff09\u548c 3D \u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08KPConv\u3001RandLANet\uff09\u4e0a\u7a33\u5065\u4e14\u6709\u6548\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cpGS-CAM \u901a\u8fc7\u7a81\u51fa\u6bcf\u4e2a\u70b9\u7684\u8d21\u732e\uff0c\u6709\u6548\u5730\u5f3a\u8c03\u4e86 SS \u67b6\u6784\u4e2d\u95f4\u6fc0\u6d3b\u4e2d\u7684\u7279\u5f81\u5b66\u4e60\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u66f4\u597d\u5730\u4e86\u89e3 SS \u6a21\u578b\u5982\u4f55\u8fdb\u884c\u9884\u6d4b\u5e76\u786e\u5b9a\u6f5c\u5728\u7684\u6539\u8fdb\u9886\u57df\u3002\u76f8\u5173\u4ee3\u7801\u53ef\u5728https://github.com/geoai4cities/pGS-CAM\u83b7\u53d6\u3002|[2402.12098v1](http://arxiv.org/pdf/2402.12098v1)|null|\n", "2402.11996": "|**2024-02-19**|**ISCUTE: Instance Segmentation of Cables Using Text Embedding**|ISCUTE\uff1a\u4f7f\u7528\u6587\u672c\u5d4c\u5165\u5bf9\u7535\u7f06\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272|Shir Kozlovsky, Omkar Joglekar, Dotan Di Castro|In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation.|\u5728\u673a\u5668\u4eba\u548c\u81ea\u52a8\u5316\u9886\u57df\uff0c\u4f20\u7edf\u7684\u5bf9\u8c61\u8bc6\u522b\u548c\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\u5728\u611f\u77e5\u7535\u7ebf\u3001\u7535\u7f06\u548c\u8f6f\u7ba1\u7b49\u53ef\u53d8\u5f62\u7ebf\u6027\u5bf9\u8c61\uff08DLO\uff09\u65f6\u9762\u4e34\u7740\u5de8\u5927\u7684\u6311\u6218\u3002\u8fd9\u4e00\u6311\u6218\u4e3b\u8981\u6e90\u4e8e\u7f3a\u4e4f\u5f62\u72b6\u3001\u989c\u8272\u548c\u7eb9\u7406\u7b49\u72ec\u7279\u5c5e\u6027\uff0c\u8fd9\u5c31\u9700\u8981\u91cf\u8eab\u5b9a\u5236\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5b9e\u73b0\u7cbe\u786e\u8bc6\u522b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684 DLO \u5b9e\u4f8b\u5206\u5272\u6280\u672f\uff0c\u8be5\u6280\u672f\u662f\u6587\u672c\u63d0\u793a\u4e14\u7528\u6237\u53cb\u597d\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86 CLIPSeg \u6a21\u578b\u7684\u6587\u672c\u6761\u4ef6\u8bed\u4e49\u5206\u5272\u529f\u80fd\u548c\u5206\u6bb5\u4efb\u610f\u6a21\u578b (SAM) \u7684\u96f6\u6837\u672c\u6cdb\u5316\u529f\u80fd\u3002\u6211\u4eec\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u5728 DLO \u5b9e\u4f8b\u5206\u5272\u4e0a\u8d85\u8fc7\u4e86 SOTA \u6027\u80fd\uff0c\u8fbe\u5230\u4e86 $91.21\\%$ \u7684 mIoU\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e30\u5bcc\u591a\u6837\u7684 DLO \u7279\u5b9a\u6570\u636e\u96c6\u7528\u4e8e\u5b9e\u4f8b\u5206\u5272\u3002|[2402.11996v1](http://arxiv.org/pdf/2402.11996v1)|null|\n", "2402.11985": "|**2024-02-19**|**Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling**|\u5177\u6709\u53ef\u5fae\u5206 ROI \u5efa\u8bae\u7f51\u7edc\u548c\u8f6f ROI \u6c60\u5316\u7684\u80f8\u90e8 X \u5149\u5f31\u76d1\u7763\u5bf9\u8c61\u68c0\u6d4b|Philip M\u00fcller, Felix Meissen, Georgios Kaissis, Daniel Rueckert|Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision. The successes of multiple instance learning in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their objects (i.e. pathologies). In this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module. WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision. We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images. Code: https://github.com/philip-mueller/wsrpn|\u5f31\u76d1\u7763\u5bf9\u8c61\u68c0\u6d4b (WSup-OD) \u63d0\u9ad8\u4e86\u56fe\u50cf\u5206\u7c7b\u7b97\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u76d1\u7763\u3002\u7136\u800c\uff0c\u7531\u4e8e\u81ea\u7136\u56fe\u50cf\u7684\u5bf9\u8c61\u7279\u5f81\uff08\u5373\u75c5\u7406\uff09\u975e\u5e38\u4e0d\u540c\uff0c\u591a\u5b9e\u4f8b\u5b66\u4e60\u5728\u81ea\u7136\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u5e76\u4e0d\u80fd\u5f88\u597d\u5730\u8f6c\u5316\u4e3a\u533b\u5b66\u56fe\u50cf\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5f31\u76d1\u7763 ROI \u5efa\u8bae\u7f51\u7edc\uff08WSRPN\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4f7f\u7528\u4e13\u95e8\u7684\u5174\u8da3\u533a\u57df\uff08ROI-\u6ce8\u610f\uff09\u6a21\u5757\u52a8\u6001\u751f\u6210\u8fb9\u754c\u6846\u5efa\u8bae\u7684\u65b0\u65b9\u6cd5\u3002 WSRPN \u4e0e\u7ecf\u5178\u7684\u9aa8\u5e72\u5934\u90e8\u5206\u7c7b\u7b97\u6cd5\u5f88\u597d\u5730\u96c6\u6210\uff0c\u5e76\u4e14\u4ec5\u901a\u8fc7\u56fe\u50cf\u6807\u7b7e\u76d1\u7763\u5373\u53ef\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u65b0\u65b9\u6cd5\u5728\u80f8\u90e8 X \u5c04\u7ebf\u56fe\u50cf\u4e2d\u75be\u75c5\u5b9a\u4f4d\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4ee3\u7801\uff1ahttps://github.com/philip-mueller/wsrpn|[2402.11985v1](http://arxiv.org/pdf/2402.11985v1)|null|\n", "2402.11957": "|**2024-02-19**|**Event-Based Motion Magnification**|\u57fa\u4e8e\u4e8b\u4ef6\u7684\u8fd0\u52a8\u653e\u5927|Yutian Chen, Shi Guo, Fangzheng Yu, Feng Zhang, Jinwei Gu, Tianfan Xue|Detecting and magnifying imperceptible high-frequency motions in real-world scenarios has substantial implications for industrial and medical applications. These motions are characterized by small amplitudes and high frequencies. Traditional motion magnification methods rely on costly high-speed cameras or active light sources, which limit the scope of their applications. In this work, we propose a dual-camera system consisting of an event camera and a conventional RGB camera for video motion magnification, containing temporally-dense information from the event stream and spatially-dense data from the RGB images. This innovative combination enables a broad and cost-effective amplification of high-frequency motions. By revisiting the physical camera model, we observe that estimating motion direction and magnitude necessitates the integration of event streams with additional image features. On this basis, we propose a novel deep network for event-based video motion magnification that addresses two primary challenges: firstly, the high frequency of motion induces a large number of interpolated frames (up to 80), which our network mitigates with a Second-order Recurrent Propagation module for better handling of long-term frame interpolations; and secondly, magnifying subtle motions is sensitive to noise, which we address by utilizing a temporal filter to amplify motion at specific frequencies and reduce noise impact. We demonstrate the effectiveness and accuracy of our dual-camera system and network through extensive experiments in magnifying small-amplitude, high-frequency motions, offering a cost-effective and flexible solution for motion detection and magnification.|\u68c0\u6d4b\u548c\u653e\u5927\u73b0\u5b9e\u573a\u666f\u4e2d\u96be\u4ee5\u5bdf\u89c9\u7684\u9ad8\u9891\u8fd0\u52a8\u5bf9\u5de5\u4e1a\u548c\u533b\u7597\u5e94\u7528\u5177\u6709\u91cd\u5927\u5f71\u54cd\u3002\u8fd9\u4e9b\u8fd0\u52a8\u7684\u7279\u70b9\u662f\u632f\u5e45\u5c0f\u3001\u9891\u7387\u9ad8\u3002\u4f20\u7edf\u7684\u8fd0\u52a8\u653e\u5927\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u9ad8\u901f\u76f8\u673a\u6216\u4e3b\u52a8\u5149\u6e90\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6444\u50cf\u5934\u7cfb\u7edf\uff0c\u7531\u4e8b\u4ef6\u6444\u50cf\u5934\u548c\u4f20\u7edf RGB \u6444\u50cf\u5934\u7ec4\u6210\uff0c\u7528\u4e8e\u89c6\u9891\u8fd0\u52a8\u653e\u5927\uff0c\u5305\u542b\u6765\u81ea\u4e8b\u4ef6\u6d41\u7684\u65f6\u95f4\u5bc6\u96c6\u4fe1\u606f\u548c\u6765\u81ea RGB \u56fe\u50cf\u7684\u7a7a\u95f4\u5bc6\u96c6\u6570\u636e\u3002\u8fd9\u79cd\u521b\u65b0\u7ec4\u5408\u80fd\u591f\u5e7f\u6cdb\u4e14\u7ecf\u6d4e\u9ad8\u6548\u5730\u653e\u5927\u9ad8\u9891\u8fd0\u52a8\u3002\u901a\u8fc7\u91cd\u65b0\u5ba1\u89c6\u7269\u7406\u76f8\u673a\u6a21\u578b\uff0c\u6211\u4eec\u53d1\u73b0\u4f30\u8ba1\u8fd0\u52a8\u65b9\u5411\u548c\u5e45\u5ea6\u9700\u8981\u5c06\u4e8b\u4ef6\u6d41\u4e0e\u9644\u52a0\u56fe\u50cf\u7279\u5f81\u96c6\u6210\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u9891\u8fd0\u52a8\u653e\u5927\u7684\u65b0\u578b\u6df1\u5ea6\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u89e3\u51b3\u4e86\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u9996\u5148\uff0c\u9ad8\u9891\u7387\u7684\u8fd0\u52a8\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u63d2\u503c\u5e27\uff08\u6700\u591a 80 \u4e2a\uff09\uff0c\u6211\u4eec\u7684\u7f51\u7edc\u901a\u8fc7\u7b2c\u4e8c\u4e2a\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u79cd\u60c5\u51b5-\u987a\u5e8f\u5faa\u73af\u4f20\u64ad\u6a21\u5757\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u5904\u7406\u957f\u671f\u5e27\u63d2\u503c\uff1b\u5176\u6b21\uff0c\u653e\u5927\u7ec6\u5fae\u7684\u8fd0\u52a8\u5bf9\u566a\u58f0\u5f88\u654f\u611f\uff0c\u6211\u4eec\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u6ee4\u6ce2\u5668\u6765\u653e\u5927\u7279\u5b9a\u9891\u7387\u7684\u8fd0\u52a8\u5e76\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u901a\u8fc7\u653e\u5927\u5c0f\u5e45\u5ea6\u3001\u9ad8\u9891\u8fd0\u52a8\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u53cc\u6444\u50cf\u5934\u7cfb\u7edf\u548c\u7f51\u7edc\u7684\u6709\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u8fd0\u52a8\u68c0\u6d4b\u548c\u653e\u5927\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002|[2402.11957v1](http://arxiv.org/pdf/2402.11957v1)|null|\n", "2402.11928": "|**2024-02-19**|**Separating common from salient patterns with Contrastive Representation Learning**|\u901a\u8fc7\u5bf9\u6bd4\u8868\u5f81\u5b66\u4e60\u533a\u5206\u5e38\u89c1\u6a21\u5f0f\u548c\u663e\u7740\u6a21\u5f0f|Robin Louiset, Edouard Duchesnay, Antoine Grigis, Pietro Gori|Contrastive Analysis is a sub-field of Representation Learning that aims at separating common factors of variation between two datasets, a background (i.e., healthy subjects) and a target (i.e., diseased subjects), from the salient factors of variation, only present in the target dataset. Despite their relevance, current models based on Variational Auto-Encoders have shown poor performance in learning semantically-expressive representations. On the other hand, Contrastive Representation Learning has shown tremendous performance leaps in various applications (classification, clustering, etc.). In this work, we propose to leverage the ability of Contrastive Learning to learn semantically expressive representations well adapted for Contrastive Analysis. We reformulate it under the lens of the InfoMax Principle and identify two Mutual Information terms to maximize and one to minimize. We decompose the first two terms into an Alignment and a Uniformity term, as commonly done in Contrastive Learning. Then, we motivate a novel Mutual Information minimization strategy to prevent information leakage between common and salient distributions. We validate our method, called SepCLR, on three visual datasets and three medical datasets, specifically conceived to assess the pattern separation capability in Contrastive Analysis. Code available at https://github.com/neurospin-projects/2024_rlouiset_sep_clr.|\u5bf9\u6bd4\u5206\u6790\u662f\u8868\u5f81\u5b66\u4e60\u7684\u4e00\u4e2a\u5b50\u9886\u57df\uff0c\u65e8\u5728\u5c06\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u80cc\u666f\uff08\u5373\u5065\u5eb7\u53d7\u8bd5\u8005\uff09\u548c\u76ee\u6807\uff08\u5373\u60a3\u75c5\u53d7\u8bd5\u8005\uff09\uff09\u4e4b\u95f4\u7684\u5171\u540c\u53d8\u5f02\u56e0\u7d20\u4e0e\u4ec5\u5b58\u5728\u4e8e\u4e2d\u7684\u663e\u7740\u53d8\u5f02\u56e0\u7d20\u5206\u5f00\u3002\u76ee\u6807\u6570\u636e\u96c6\u3002\u5c3d\u7ba1\u5b83\u4eec\u5177\u6709\u76f8\u5173\u6027\uff0c\u4f46\u5f53\u524d\u57fa\u4e8e\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u7684\u6a21\u578b\u5728\u5b66\u4e60\u8bed\u4e49\u8868\u8fbe\u8868\u793a\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u5728\u5404\u79cd\u5e94\u7528\uff08\u5206\u7c7b\u3001\u805a\u7c7b\u7b49\uff09\u4e2d\u8868\u73b0\u51fa\u4e86\u5de8\u5927\u7684\u6027\u80fd\u98de\u8dc3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u7684\u80fd\u529b\u6765\u5b66\u4e60\u975e\u5e38\u9002\u5408\u5bf9\u6bd4\u5206\u6790\u7684\u8bed\u4e49\u8868\u8fbe\u8868\u793a\u3002\u6211\u4eec\u6839\u636e InfoMax \u539f\u5219\u91cd\u65b0\u8868\u8ff0\u5b83\uff0c\u5e76\u786e\u5b9a\u4e24\u4e2a\u4e92\u4fe1\u606f\u9879\u6700\u5927\u5316\u548c\u4e00\u4e2a\u6700\u5c0f\u5316\u3002\u6211\u4eec\u5c06\u524d\u4e24\u9879\u5206\u89e3\u4e3a\u5bf9\u9f50\u9879\u548c\u5747\u5300\u6027\u9879\uff0c\u5c31\u50cf\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u901a\u5e38\u6240\u505a\u7684\u90a3\u6837\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e92\u4fe1\u606f\u6700\u5c0f\u5316\u7b56\u7565\uff0c\u4ee5\u9632\u6b62\u5e38\u89c1\u5206\u5e03\u548c\u663e\u7740\u5206\u5e03\u4e4b\u95f4\u7684\u4fe1\u606f\u6cc4\u6f0f\u3002\u6211\u4eec\u5728\u4e09\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5 SepCLR\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5bf9\u6bd4\u5206\u6790\u4e2d\u7684\u6a21\u5f0f\u5206\u79bb\u80fd\u529b\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/neurospin-projects/2024_rlouiset_sep_clr \u83b7\u53d6\u3002|[2402.11928v1](http://arxiv.org/pdf/2402.11928v1)|null|\n", "2402.11845": "|**2024-02-19**|**Modularized Networks for Few-shot Hateful Meme Detection**|\u7528\u4e8e\u5c11\u91cf\u4ec7\u6068\u6a21\u56e0\u68c0\u6d4b\u7684\u6a21\u5757\u5316\u7f51\u7edc|Rui Cao, Roy Ka-Wei Lee, Jing Jiang|In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available. Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique. We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules. These modules are capable of essential reasoning skills for hateful meme detection. We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u5728\u8d44\u6e90\u532e\u4e4f\u7684\u73af\u5883\u4e2d\u68c0\u6d4b\u4ec7\u6068\u6a21\u56e0\u7684\u6311\u6218\uff0c\u5176\u4e2d\u53ea\u6709\u5c11\u6570\u6807\u8bb0\u7684\u793a\u4f8b\u53ef\u7528\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4e86\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u7684\u7ec4\u5408\u6027\uff0c\u8fd9\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u53c2\u6570\u9ad8\u6548\u8c03\u6574\u6280\u672f\u3002\u6211\u4eec\u9996\u5148\u4f7f\u7528 LoRA \u5bf9\u4e0e\u4ec7\u6068\u6a21\u56e0\u68c0\u6d4b\u76f8\u5173\u7684\u9009\u5b9a\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u751f\u6210\u4e00\u5957 LoRA \u6a21\u5757\u3002\u8fd9\u4e9b\u6a21\u5757\u5177\u6709\u7528\u4e8e\u68c0\u6d4b\u4ec7\u6068\u6a21\u56e0\u7684\u57fa\u672c\u63a8\u7406\u6280\u80fd\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5c11\u6570\u53ef\u7528\u7684\u5e26\u6ce8\u91ca\u6837\u672c\u6765\u8bad\u7ec3\u6a21\u5757\u7f16\u8f91\u5668\uff0c\u8be5\u6a21\u5757\u6839\u636e LoRA \u6a21\u5757\u7684\u76f8\u5173\u6027\u4e3a\u5176\u5206\u914d\u6743\u91cd\u3002\u8be5\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u4e0e LoRA \u6a21\u5757\u7684\u6570\u91cf\u6210\u6b63\u6bd4\u3002\u8fd9\u79cd\u6a21\u5757\u5316\u7f51\u7edc\u4ee5\u6cd5\u5b66\u7855\u58eb\u4e3a\u57fa\u7840\uff0c\u5e76\u901a\u8fc7 LoRA \u6a21\u5757\u8fdb\u884c\u589e\u5f3a\uff0c\u5728\u4ec7\u6068\u6a21\u56e0\u68c0\u6d4b\u7684\u80cc\u666f\u4e0b\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u6db5\u76d6\u4e86\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u4e13\u4e3a\u5728\u51e0\u6b21\u5b66\u4e60\u73af\u5883\u4e2d\u68c0\u6d4b\u4ec7\u6068\u6a21\u56e0\u800c\u8bbe\u8ba1\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u4e0a\u4e0b\u6587\u5b66\u4e60\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u800c\u4f20\u7edf\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u91cf\u4e5f\u66f4\u5927\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5c11\u6570\u53ef\u7528\u7684\u5e26\u6ce8\u91ca\u6837\u672c\u6765\u8bad\u7ec3\u6a21\u5757\u7f16\u8f91\u5668\uff0c\u8be5\u6a21\u5757\u6839\u636e LoRA \u6a21\u5757\u7684\u76f8\u5173\u6027\u4e3a\u5b83\u4eec\u5206\u914d\u6743\u91cd\u3002\u8be5\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u4e0e LoRA \u6a21\u5757\u7684\u6570\u91cf\u6210\u6b63\u6bd4\u3002\u8fd9\u79cd\u6a21\u5757\u5316\u7f51\u7edc\u4ee5\u6cd5\u5b66\u7855\u58eb\u4e3a\u57fa\u7840\uff0c\u5e76\u901a\u8fc7 LoRA \u6a21\u5757\u8fdb\u884c\u589e\u5f3a\uff0c\u5728\u4ec7\u6068\u6a21\u56e0\u68c0\u6d4b\u7684\u80cc\u666f\u4e0b\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u6db5\u76d6\u4e86\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u4e13\u4e3a\u5728\u51e0\u6b21\u5b66\u4e60\u73af\u5883\u4e2d\u68c0\u6d4b\u4ec7\u6068\u6a21\u56e0\u800c\u8bbe\u8ba1\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u800c\u4f20\u7edf\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u91cf\u4e5f\u66f4\u5927\u3002|[2402.11845v1](http://arxiv.org/pdf/2402.11845v1)|null|\n", "2402.11831": "|**2024-02-19**|**Rock Classification Based on Residual Networks**|\u57fa\u4e8e\u6b8b\u5dee\u7f51\u7edc\u7684\u5ca9\u77f3\u5206\u7c7b|Sining Zhoubian, Yuyang Wang, Zhihuan Jiang|Rock Classification is an essential geological problem since it provides important formation information. However, exploration on this problem using convolutional neural networks is not sufficient. To tackle this problem, we propose two approaches using residual neural networks. We first adopt data augmentation methods to enlarge our dataset. By modifying kernel sizes, normalization methods and composition based on ResNet34, we achieve an accuracy of 70.1% on the test dataset, with an increase of 3.5% compared to regular Resnet34. Furthermore, using a similar backbone like BoTNet that incorporates multihead self attention, we additionally use internal residual connections in our model. This boosts the model's performance, achieving an accuracy of 73.7% on the test dataset. We also explore how the number of bottleneck transformer blocks may influence model performance. We discover that models with more than one bottleneck transformer block may not further improve performance. Finally, we believe that our approach can inspire future work related to this problem and our model design can facilitate the development of new residual model architectures.|\u5ca9\u77f3\u5206\u7c7b\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u5730\u8d28\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5730\u5c42\u4fe1\u606f\u3002\u7136\u800c\uff0c\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5bf9\u8fd9\u4e2a\u95ee\u9898\u7684\u63a2\u7d22\u8fd8\u4e0d\u591f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u4f7f\u7528\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u9996\u5148\u91c7\u7528\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u6269\u5927\u6211\u4eec\u7684\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5728 ResNet34 \u7684\u57fa\u7840\u4e0a\u4fee\u6539\u6838\u5927\u5c0f\u3001\u5f52\u4e00\u5316\u65b9\u6cd5\u548c\u7ec4\u5408\uff0c\u6211\u4eec\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 70.1% \u7684\u51c6\u786e\u7387\uff0c\u4e0e\u5e38\u89c4 Resnet34 \u76f8\u6bd4\u63d0\u9ad8\u4e86 3.5%\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u50cf BotNet \u8fd9\u6837\u7684\u7c7b\u4f3c\u4e3b\u5e72\u7f51\uff0c\u5b83\u5305\u542b\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff0c\u6211\u4eec\u8fd8\u5728\u6a21\u578b\u4e2d\u4f7f\u7528\u5185\u90e8\u6b8b\u5dee\u8fde\u63a5\u3002\u8fd9\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 73.7% \u7684\u51c6\u786e\u7387\u3002\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86\u74f6\u9888\u53d8\u538b\u5668\u5757\u7684\u6570\u91cf\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u6211\u4eec\u53d1\u73b0\u5177\u6709\u591a\u4e2a\u74f6\u9888\u53d8\u538b\u5668\u5757\u7684\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002\u6700\u540e\uff0c\u6211\u4eec\u76f8\u4fe1\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6fc0\u53d1\u4e0e\u8be5\u95ee\u9898\u76f8\u5173\u7684\u672a\u6765\u5de5\u4f5c\uff0c\u5e76\u4e14\u6211\u4eec\u7684\u6a21\u578b\u8bbe\u8ba1\u53ef\u4ee5\u4fc3\u8fdb\u65b0\u6b8b\u5dee\u6a21\u578b\u67b6\u6784\u7684\u5f00\u53d1\u3002|[2402.11831v1](http://arxiv.org/pdf/2402.11831v1)|null|\n", "2402.11791": "|**2024-02-19**|**SDGE: Stereo Guided Depth Estimation for 360\u00b0 Camera Sets**|SDGE\uff1a360\u00b0 \u76f8\u673a\u7ec4\u7684\u7acb\u4f53\u5f15\u5bfc\u6df1\u5ea6\u4f30\u8ba1|Jialei Xu, Xianming Liu, Junjun Jiang, Xiangyang Ji|Depth estimation is a critical technology in autonomous driving, and multi-camera systems are often used to achieve a 360{\\deg} perception. These 360{\\deg} camera sets often have limited or low-quality overlap regions, making multi-view stereo methods infeasible for the entire image. Alternatively, monocular methods may not produce consistent cross-view predictions. To address these issues, we propose the Stereo Guided Depth Estimation (SGDE) method, which enhances depth estimation of the full image by explicitly utilizing multi-view stereo results on the overlap. We suggest building virtual pinhole cameras to resolve the distortion problem of fisheye cameras and unify the processing for the two types of 360{\\deg} cameras. For handling the varying noise on camera poses caused by unstable movement, the approach employs a self-calibration method to obtain highly accurate relative poses of the adjacent cameras with minor overlap. These enable the use of robust stereo methods to obtain high-quality depth prior in the overlap region. This prior serves not only as an additional input but also as pseudo-labels that enhance the accuracy of depth estimation methods and improve cross-view prediction consistency. The effectiveness of SGDE is evaluated on one fisheye camera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes. Our experiments demonstrate that SGDE is effective for both supervised and self-supervised depth estimation, and highlight the potential of our method for advancing downstream autonomous driving technologies, such as 3D object detection and occupancy prediction.|\u6df1\u5ea6\u4f30\u8ba1\u662f\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4e00\u9879\u5173\u952e\u6280\u672f\uff0c\u901a\u5e38\u4f7f\u7528\u591a\u6444\u50cf\u5934\u7cfb\u7edf\u6765\u5b9e\u73b0 360{\\deg} \u611f\u77e5\u3002\u8fd9\u4e9b 360{\\deg} \u76f8\u673a\u7ec4\u901a\u5e38\u5177\u6709\u6709\u9650\u6216\u4f4e\u8d28\u91cf\u7684\u91cd\u53e0\u533a\u57df\uff0c\u4f7f\u5f97\u591a\u89c6\u56fe\u7acb\u4f53\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u6574\u4e2a\u56fe\u50cf\u3002\u6216\u8005\uff0c\u5355\u76ee\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u4ea7\u751f\u4e00\u81f4\u7684\u8de8\u89c6\u56fe\u9884\u6d4b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7acb\u4f53\u5f15\u5bfc\u6df1\u5ea6\u4f30\u8ba1\uff08SGDE\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u660e\u786e\u5229\u7528\u91cd\u53e0\u4e0a\u7684\u591a\u89c6\u56fe\u7acb\u4f53\u7ed3\u679c\u6765\u589e\u5f3a\u6574\u4e2a\u56fe\u50cf\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002\u6211\u4eec\u5efa\u8bae\u6784\u5efa\u865a\u62df\u9488\u5b54\u76f8\u673a\u6765\u89e3\u51b3\u9c7c\u773c\u76f8\u673a\u7684\u7578\u53d8\u95ee\u9898\uff0c\u5e76\u7edf\u4e00\u4e24\u79cd\u7c7b\u578b\u7684360{\\deg}\u76f8\u673a\u7684\u5904\u7406\u3002\u4e3a\u4e86\u5904\u7406\u4e0d\u7a33\u5b9a\u8fd0\u52a8\u5f15\u8d77\u7684\u76f8\u673a\u4f4d\u59ff\u53d8\u5316\u566a\u58f0\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u81ea\u6821\u51c6\u65b9\u6cd5\u6765\u83b7\u5f97\u5177\u6709\u8f83\u5c0f\u91cd\u53e0\u7684\u76f8\u90bb\u76f8\u673a\u7684\u9ad8\u7cbe\u5ea6\u76f8\u5bf9\u4f4d\u59ff\u3002\u8fd9\u4e9b\u4f7f\u5f97\u80fd\u591f\u4f7f\u7528\u9c81\u68d2\u7684\u7acb\u4f53\u65b9\u6cd5\u6765\u5728\u91cd\u53e0\u533a\u57df\u4e2d\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u5148\u9a8c\u6df1\u5ea6\u3002\u8be5\u5148\u9a8c\u4e0d\u4ec5\u7528\u4f5c\u9644\u52a0\u8f93\u5165\uff0c\u8fd8\u7528\u4f5c\u4f2a\u6807\u7b7e\uff0c\u589e\u5f3a\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u5e76\u63d0\u9ad8\u8de8\u89c6\u56fe\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u3002 SGDE \u7684\u6709\u6548\u6027\u5728\u4e00\u4e2a\u9c7c\u773c\u76f8\u673a\u6570\u636e\u96c6 Synthetic Urban \u548c\u4e24\u4e2a\u9488\u5b54\u76f8\u673a\u6570\u636e\u96c6 DDAD \u548c nuScenes \u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSGDE \u5bf9\u4e8e\u76d1\u7763\u548c\u81ea\u76d1\u7763\u6df1\u5ea6\u4f30\u8ba1\u90fd\u662f\u6709\u6548\u7684\uff0c\u5e76\u51f8\u663e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63a8\u8fdb\u4e0b\u6e38\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\uff08\u4f8b\u5982 3D \u5bf9\u8c61\u68c0\u6d4b\u548c\u5360\u7528\u9884\u6d4b\uff09\u65b9\u9762\u7684\u6f5c\u529b\u3002|[2402.11791v1](http://arxiv.org/pdf/2402.11791v1)|null|\n", "2402.11775": "|**2024-02-19**|**FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model**|FOD-Swin-Net\uff1a\u4f7f\u7528\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u6df1\u5ea6\u6a21\u578b\u7684\u7ea4\u7ef4\u53d6\u5411\u5206\u5e03\u7684\u89d2\u5ea6\u8d85\u5206\u8fa8\u7387|Mateus Oliveira da Silva, Caio Pinheiro Santana, Diedre Santos do Carmo, Let\u00edcia Rittner|Identifying and characterizing brain fiber bundles can help to understand many diseases and conditions. An important step in this process is the estimation of fiber orientations using Diffusion-Weighted Magnetic Resonance Imaging (DW-MRI). However, obtaining robust orientation estimates demands high-resolution data, leading to lengthy acquisitions that are not always clinically available. In this work, we explore the use of automated angular super resolution from faster acquisitions to overcome this challenge. Using the publicly available Human Connectome Project (HCP) DW-MRI data, we trained a transformer-based deep learning architecture to achieve angular super resolution in fiber orientation distribution (FOD). Our patch-based methodology, FOD-Swin-Net, is able to bring a single-shell reconstruction driven from 32 directions to be comparable to a multi-shell 288 direction FOD reconstruction, greatly reducing the number of required directions on initial acquisition. Evaluations of the reconstructed FOD with Angular Correlation Coefficient and qualitative visualizations reveal superior performance than the state-of-the-art in HCP testing data. Open source code for reproducibility is available at https://github.com/MICLab-Unicamp/FOD-Swin-Net.|\u8bc6\u522b\u548c\u8868\u5f81\u8111\u7ea4\u7ef4\u675f\u53ef\u4ee5\u5e2e\u52a9\u4e86\u89e3\u8bb8\u591a\u75be\u75c5\u548c\u75c5\u75c7\u3002\u6b64\u8fc7\u7a0b\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u6b65\u9aa4\u662f\u4f7f\u7528\u6269\u6563\u52a0\u6743\u78c1\u5171\u632f\u6210\u50cf (DW-MRI) \u4f30\u8ba1\u7ea4\u7ef4\u65b9\u5411\u3002\u7136\u800c\uff0c\u83b7\u5f97\u53ef\u9760\u7684\u65b9\u5411\u4f30\u8ba1\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u6570\u636e\uff0c\u5bfc\u81f4\u91c7\u96c6\u65f6\u95f4\u8f83\u957f\uff0c\u800c\u4e34\u5e8a\u4e0a\u5e76\u4e0d\u603b\u662f\u53ef\u7528\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4f7f\u7528\u6765\u81ea\u66f4\u5feb\u91c7\u96c6\u7684\u81ea\u52a8\u89d2\u5ea6\u8d85\u5206\u8fa8\u7387\u6765\u514b\u670d\u8fd9\u4e00\u6311\u6218\u3002\u4f7f\u7528\u516c\u5f00\u7684\u4eba\u7c7b\u8fde\u63a5\u7ec4\u8ba1\u5212 (HCP) DW-MRI \u6570\u636e\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4ee5\u5b9e\u73b0\u7ea4\u7ef4\u53d6\u5411\u5206\u5e03 (FOD) \u7684\u89d2\u5ea6\u8d85\u5206\u8fa8\u7387\u3002\u6211\u4eec\u57fa\u4e8e\u8865\u4e01\u7684\u65b9\u6cd5 FOD-Swin-Net \u80fd\u591f\u4f7f\u4ece 32 \u4e2a\u65b9\u5411\u9a71\u52a8\u7684\u5355\u58f3\u91cd\u5efa\u4e0e\u591a\u58f3 288 \u65b9\u5411 FOD \u91cd\u5efa\u76f8\u5f53\uff0c\u4ece\u800c\u5927\u5927\u51cf\u5c11\u521d\u59cb\u91c7\u96c6\u65f6\u6240\u9700\u65b9\u5411\u7684\u6570\u91cf\u3002\u4f7f\u7528\u89d2\u5ea6\u76f8\u5173\u7cfb\u6570\u548c\u5b9a\u6027\u53ef\u89c6\u5316\u5bf9\u91cd\u5efa\u7684 FOD \u8fdb\u884c\u8bc4\u4f30\uff0c\u663e\u793a\u51fa\u6bd4\u6700\u5148\u8fdb\u7684 HCP \u6d4b\u8bd5\u6570\u636e\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u53ef\u91cd\u590d\u6027\u7684\u5f00\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/MICLab-Unicamp/FOD-Swin-Net \u4e0a\u83b7\u53d6\u3002|[2402.11775v1](http://arxiv.org/pdf/2402.11775v1)|null|\n", "2402.11760": "|**2024-02-19**|**Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation**|\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u9884\u6d4b\u7ea7\u8054\u7684\u7b80\u7ea6\u66ff\u4ee3\u65b9\u6848\uff1a\u56fe\u50cf\u5206\u5272\u7684\u6848\u4f8b\u7814\u7a76|Bharat Srikishan, Anika Tabassum, Srikanth Allu, Ramakrishnan Kannan, Nikhil Muralidhar|Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in low-resource settings. In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermediate computations. To address this, we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a non-cascading, cost-aware learning pipeline as an alternative to cascaded architectures. Through experimental evaluation on real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models. Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance. On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines. We also demonstrate PaSeR's adaptability to complementary models trained on a noisy MNIST dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4% over SOTA models. Code and data are available at https://github.com/scailab/paser .|\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u5272\u7b49\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684 (SOTA) \u6027\u80fd\u3002\u8fd9\u53ef\u80fd\u5f52\u56e0\u4e8e\u4f7f\u7528\u4e86\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u6267\u884c\u7684\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u6574\u4f53\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002\u5c3d\u7ba1\u8fd9\u79cd\u67b6\u6784\u53ef\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u8fd9\u901a\u5e38\u4f34\u968f\u7740\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u7684\u5927\u5e45\u589e\u52a0\u3002\u867d\u7136\u8fd9\u5728\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u4e2d\u4e0d\u662f\u95ee\u9898\uff0c\u4f46\u6700\u8fd1\u673a\u5668\u5b66\u4e60\u548c\u7269\u8054\u7f51\u7b49\u9886\u57df\u7684\u878d\u5408\u4f7f\u5f97\u5982\u6b64\u5927\u578b\u7684\u67b6\u6784\u65e0\u6cd5\u5728\u8d44\u6e90\u532e\u4e4f\u7684\u73af\u5883\u4e2d\u6267\u884c\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e4b\u524d\u7684\u5de5\u4f5c\u63d0\u51fa\u4e86\u51b3\u7b56\u7ea7\u8054\uff0c\u5176\u4e2d\u8f93\u5165\u901a\u8fc7\u590d\u6742\u6027\u4e0d\u65ad\u589e\u52a0\u7684\u6a21\u578b\u4f20\u9012\uff0c\u76f4\u5230\u5b9e\u73b0\u6240\u9700\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u6211\u4eec\u8ba4\u4e3a\u7ea7\u8054\u9884\u6d4b\u7531\u4e8e\u6d6a\u8d39\u4e2d\u95f4\u8ba1\u7b97\u800c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa PaSeR\uff08\u5f3a\u5316\u5b66\u4e60\u7b80\u7ea6\u5206\u5272\uff09\uff0c\u4e00\u79cd\u975e\u7ea7\u8054\u3001\u6210\u672c\u611f\u77e5\u7684\u5b66\u4e60\u7ba1\u9053\uff0c\u4f5c\u4e3a\u7ea7\u8054\u67b6\u6784\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u901a\u8fc7\u5bf9\u73b0\u5b9e\u4e16\u754c\u548c\u6807\u51c6\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6211\u4eec\u8bc1\u660e PaSeR \u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u76f8\u5bf9\u4e8e\u7ea7\u8054\u6a21\u578b\u6700\u5c0f\u5316\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6307\u6807 IoU/GigaFlop \u6765\u8bc4\u4f30\u6210\u672c\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u5728\u7535\u6c60\u6750\u6599\u76f8\u5206\u5272\u7684\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0cPaSeR \u5728 IoU/GigaFlop \u6307\u6807\u4e0a\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u7684\u6027\u80fd\u81f3\u5c11\u63d0\u9ad8\u4e86 174%\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86 PaSeR \u5bf9\u5728\u566a\u58f0 MNIST \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u4e92\u8865\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u4e0e SOTA \u6a21\u578b\u76f8\u6bd4\uff0c\u5b83\u5728 IoU/GigaFlop \u4e0a\u5b9e\u73b0\u4e86 13.4% \u7684\u6700\u4f4e\u6027\u80fd\u6539\u8fdb\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728 https://github.com/scailab/paser \u83b7\u53d6\u3002|[2402.11760v1](http://arxiv.org/pdf/2402.11760v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2402.12121": "|**2024-02-19**|**Evaluating Image Review Ability of Vision Language Models**|\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u50cf\u5ba1\u67e5\u80fd\u529b|Shigeki Saito, Kazuki Hayashi, Yusuke Ide, Yusuke Sakai, Kazuma Onishi, Toma Suzuki, Seiji Gobara, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe|Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset reveal that LVLMs, particularly those with proven superiority in other evaluative contexts, excel at distinguishing between high-quality and substandard image reviews.|\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u662f\u80fd\u591f\u901a\u8fc7\u5355\u4e2a\u6a21\u578b\u5904\u7406\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\u7684\u8bed\u8a00\u6a21\u578b\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528 LVLM \u751f\u6210\u56fe\u50cf\u8bc4\u8bba\u6587\u672c\u3002 LVLM \u5ba1\u67e5\u56fe\u50cf\u7684\u80fd\u529b\u5c1a\u672a\u5b8c\u5168\u4e86\u89e3\uff0c\u8fd9\u51f8\u663e\u4e86\u5bf9\u5176\u5ba1\u67e5\u80fd\u529b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002\u4e0e\u56fe\u50cf\u8bf4\u660e\u4e0d\u540c\uff0c\u8bc4\u8bba\u6587\u672c\u53ef\u4ee5\u4ece\u56fe\u50cf\u6784\u56fe\u548c\u66dd\u5149\u7b49\u591a\u79cd\u89d2\u5ea6\u8fdb\u884c\u64b0\u5199\u3002\u8bc4\u8bba\u89c6\u89d2\u7684\u591a\u6837\u6027\u4f7f\u5f97\u5f88\u96be\u552f\u4e00\u5730\u786e\u5b9a\u5bf9\u56fe\u50cf\u7684\u5355\u4e00\u6b63\u786e\u8bc4\u8bba\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6392\u540d\u76f8\u5173\u5206\u6790\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5176\u4e2d\u8bc4\u8bba\u6587\u672c\u7531\u4eba\u7c7b\u548c LVLM \u8fdb\u884c\u6392\u540d\uff0c\u7136\u540e\u6d4b\u91cf\u8fd9\u4e9b\u6392\u540d\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u6211\u4eec\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u65e8\u5728\u8bc4\u4f30\u6700\u8fd1 LVLM \u7684\u56fe\u50cf\u5ba1\u67e5\u80fd\u529b\u7684\u57fa\u51c6\u6570\u636e\u96c6\u6765\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u8fd9\u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u5bf9\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLVLM\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5728\u5176\u4ed6\u8bc4\u4f30\u73af\u5883\u4e2d\u5df2\u8bc1\u660e\u5177\u6709\u4f18\u8d8a\u6027\u7684 LVLM\uff0c\u64c5\u957f\u533a\u5206\u9ad8\u8d28\u91cf\u548c\u4e0d\u5408\u683c\u7684\u56fe\u50cf\u8bc4\u8bba\u3002|[2402.12121v1](http://arxiv.org/pdf/2402.12121v1)|null|\n", "2402.11840": "|**2024-02-19**|**An Endoscopic Chisel: Intraoperative Imaging Carves 3D Anatomical Models**|\u5185\u7aa5\u955c\u51ff\u5b50\uff1a\u672f\u4e2d\u6210\u50cf\u96d5\u523b 3D \u89e3\u5256\u6a21\u578b|Jan Emily Mangulabnan, Roger D. Soberanis-Mukul, Timo Teufel, Manish Sahu, Jose L. Porras, S. Swaroop Vedula, Masaru Ishii, Gregory Hager, Russell H. Taylor, Mathias Unberath|Purpose: Preoperative imaging plays a pivotal role in sinus surgery where CTs offer patient-specific insights of complex anatomy, enabling real-time intraoperative navigation to complement endoscopy imaging. However, surgery elicits anatomical changes not represented in the preoperative model, generating an inaccurate basis for navigation during surgery progression.   Methods: We propose a first vision-based approach to update the preoperative 3D anatomical model leveraging intraoperative endoscopic video for navigated sinus surgery where relative camera poses are known. We rely on comparisons of intraoperative monocular depth estimates and preoperative depth renders to identify modified regions. The new depths are integrated in these regions through volumetric fusion in a truncated signed distance function representation to generate an intraoperative 3D model that reflects tissue manipulation.   Results: We quantitatively evaluate our approach by sequentially updating models for a five-step surgical progression in an ex vivo specimen. We compute the error between correspondences from the updated model and ground-truth intraoperative CT in the region of anatomical modification. The resulting models show a decrease in error during surgical progression as opposed to increasing when no update is employed.   Conclusion: Our findings suggest that preoperative 3D anatomical models can be updated using intraoperative endoscopy video in navigated sinus surgery. Future work will investigate improvements to monocular depth estimation as well as removing the need for external navigation systems. The resulting ability to continuously update the patient model may provide surgeons with a more precise understanding of the current anatomical state and paves the way toward a digital twin paradigm for sinus surgery.|\u76ee\u7684\uff1a\u672f\u524d\u6210\u50cf\u5728\u9f3b\u7aa6\u624b\u672f\u4e2d\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\uff0cCT \u53ef\u4ee5\u4e3a\u60a3\u8005\u63d0\u4f9b\u590d\u6742\u89e3\u5256\u7ed3\u6784\u7684\u5177\u4f53\u89c1\u89e3\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u672f\u4e2d\u5bfc\u822a\u4ee5\u8865\u5145\u5185\u7aa5\u955c\u6210\u50cf\u3002\u7136\u800c\uff0c\u624b\u672f\u4f1a\u5f15\u8d77\u672f\u524d\u6a21\u578b\u4e2d\u672a\u4f53\u73b0\u7684\u89e3\u5256\u5b66\u53d8\u5316\uff0c\u4ece\u800c\u5728\u624b\u672f\u8fdb\u5c55\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u4e0d\u51c6\u786e\u7684\u5bfc\u822a\u57fa\u7840\u3002\u65b9\u6cd5\uff1a\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\u6765\u66f4\u65b0\u672f\u524d 3D \u89e3\u5256\u6a21\u578b\uff0c\u5229\u7528\u672f\u4e2d\u5185\u7aa5\u955c\u89c6\u9891\u8fdb\u884c\u5bfc\u822a\u9f3b\u7aa6\u624b\u672f\uff0c\u5176\u4e2d\u76f8\u5bf9\u76f8\u673a\u59ff\u52bf\u5df2\u77e5\u3002\u6211\u4eec\u4f9d\u9760\u672f\u4e2d\u5355\u773c\u6df1\u5ea6\u4f30\u8ba1\u548c\u672f\u524d\u6df1\u5ea6\u6e32\u67d3\u7684\u6bd4\u8f83\u6765\u8bc6\u522b\u4fee\u6539\u533a\u57df\u3002\u65b0\u7684\u6df1\u5ea6\u901a\u8fc7\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u8868\u793a\u4e2d\u7684\u4f53\u79ef\u878d\u5408\u96c6\u6210\u5728\u8fd9\u4e9b\u533a\u57df\u4e2d\uff0c\u4ee5\u751f\u6210\u53cd\u6620\u7ec4\u7ec7\u64cd\u4f5c\u7684\u672f\u4e2d 3D \u6a21\u578b\u3002\u7ed3\u679c\uff1a\u6211\u4eec\u901a\u8fc7\u8fde\u7eed\u66f4\u65b0\u79bb\u4f53\u6807\u672c\u4e2d\u4e94\u6b65\u624b\u672f\u8fdb\u5c55\u7684\u6a21\u578b\u6765\u5b9a\u91cf\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8ba1\u7b97\u66f4\u65b0\u6a21\u578b\u4e0e\u89e3\u5256\u4fee\u6539\u533a\u57df\u7684\u771f\u5b9e\u672f\u4e2d CT \u4e4b\u95f4\u7684\u5bf9\u5e94\u8bef\u5dee\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6a21\u578b\u663e\u793a\uff0c\u5728\u624b\u672f\u8fdb\u5c55\u8fc7\u7a0b\u4e2d\u8bef\u5dee\u6709\u6240\u51cf\u5c11\uff0c\u800c\u4e0d\u662f\u5728\u4e0d\u8fdb\u884c\u66f4\u65b0\u65f6\u8bef\u5dee\u589e\u52a0\u3002\u7ed3\u8bba\uff1a\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5bfc\u822a\u9f3b\u7aa6\u624b\u672f\u4e2d\u53ef\u4ee5\u4f7f\u7528\u672f\u4e2d\u5185\u7aa5\u955c\u89c6\u9891\u66f4\u65b0\u672f\u524d 3D \u89e3\u5256\u6a21\u578b\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u7814\u7a76\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u6539\u8fdb\u4ee5\u53ca\u6d88\u9664\u5bf9\u5916\u90e8\u5bfc\u822a\u7cfb\u7edf\u7684\u9700\u6c42\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6301\u7eed\u66f4\u65b0\u60a3\u8005\u6a21\u578b\u7684\u80fd\u529b\u53ef\u4ee5\u4f7f\u5916\u79d1\u533b\u751f\u66f4\u51c6\u786e\u5730\u4e86\u89e3\u5f53\u524d\u7684\u89e3\u5256\u72b6\u6001\uff0c\u5e76\u4e3a\u9f3b\u7aa6\u624b\u672f\u7684\u6570\u5b57\u5b6a\u751f\u8303\u5f0f\u94fa\u5e73\u9053\u8def\u3002|[2402.11840v1](http://arxiv.org/pdf/2402.11840v1)|null|\n"}, "LLM": {"2402.12259": "|**2024-02-19**|**Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships**|Open3DSG\uff1a\u6765\u81ea\u70b9\u4e91\u7684\u5f00\u653e\u8bcd\u6c47 3D \u573a\u666f\u56fe\uff0c\u5177\u6709\u53ef\u67e5\u8be2\u5bf9\u8c61\u548c\u5f00\u653e\u96c6\u5173\u7cfb|Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, Timo Ropinski|Current approaches for 3D scene graph prediction rely on labeled datasets to train models for a fixed set of known object classes and relationship categories. We present Open3DSG, an alternative approach to learn 3D scene graph prediction in an open world without requiring labeled scene graph data. We co-embed the features from a 3D scene graph prediction backbone with the feature space of powerful open world 2D vision language foundation models. This enables us to predict 3D scene graphs from 3D point clouds in a zero-shot manner by querying object classes from an open vocabulary and predicting the inter-object relationships from a grounded LLM with scene graph features and queried object classes as context. Open3DSG is the first 3D point cloud method to predict not only explicit open-vocabulary object classes, but also open-set relationships that are not limited to a predefined label set, making it possible to express rare as well as specific objects and relationships in the predicted 3D scene graph. Our experiments show that Open3DSG is effective at predicting arbitrary object classes as well as their complex inter-object relationships describing spatial, supportive, semantic and comparative relationships.|\u5f53\u524d\u7684 3D \u573a\u666f\u56fe\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6807\u8bb0\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u4e00\u7ec4\u56fa\u5b9a\u7684\u5df2\u77e5\u5bf9\u8c61\u7c7b\u548c\u5173\u7cfb\u7c7b\u522b\u7684\u6a21\u578b\u3002\u6211\u4eec\u63d0\u51fa\u4e86 Open3DSG\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u5b66\u4e60 3D \u573a\u666f\u56fe\u9884\u6d4b\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u65e0\u9700\u6807\u8bb0\u573a\u666f\u56fe\u6570\u636e\u3002\u6211\u4eec\u5c06 3D \u573a\u666f\u56fe\u9884\u6d4b\u4e3b\u5e72\u7684\u7279\u5f81\u4e0e\u5f3a\u5927\u7684\u5f00\u653e\u4e16\u754c 2D \u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u7a7a\u95f4\u5171\u540c\u5d4c\u5165\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u4ee5\u96f6\u6837\u672c\u7684\u65b9\u5f0f\u4ece 3D \u70b9\u4e91\u9884\u6d4b 3D \u573a\u666f\u56fe\uff0c\u65b9\u6cd5\u662f\u4ece\u5f00\u653e\u8bcd\u6c47\u8868\u4e2d\u67e5\u8be2\u5bf9\u8c61\u7c7b\uff0c\u5e76\u4ece\u5177\u6709\u573a\u666f\u56fe\u7279\u5f81\u548c\u67e5\u8be2\u5bf9\u8c61\u7c7b\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7684\u624e\u6839 LLM \u4e2d\u9884\u6d4b\u5bf9\u8c61\u95f4\u5173\u7cfb\u3002 Open3DSG \u662f\u7b2c\u4e00\u4e2a 3D \u70b9\u4e91\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u9884\u6d4b\u663e\u5f0f\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u7c7b\uff0c\u8fd8\u53ef\u4ee5\u9884\u6d4b\u4e0d\u9650\u4e8e\u9884\u5b9a\u4e49\u6807\u7b7e\u96c6\u7684\u5f00\u653e\u96c6\u5173\u7cfb\uff0c\u4ece\u800c\u53ef\u4ee5\u8868\u8fbe\u7f55\u89c1\u4ee5\u53ca\u7279\u5b9a\u7684\u5bf9\u8c61\u548c\u5173\u7cfb\u3002\u9884\u6d4b\u7684 3D \u573a\u666f\u56fe\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOpen3DSG \u53ef\u4ee5\u6709\u6548\u5730\u9884\u6d4b\u4efb\u610f\u5bf9\u8c61\u7c7b\u522b\u53ca\u5176\u63cf\u8ff0\u7a7a\u95f4\u3001\u652f\u6301\u3001\u8bed\u4e49\u548c\u6bd4\u8f83\u5173\u7cfb\u7684\u590d\u6742\u5bf9\u8c61\u95f4\u5173\u7cfb\u3002|[2402.12259v1](http://arxiv.org/pdf/2402.12259v1)|null|\n"}, "Transformer": {"2402.12043": "|**2024-02-19**|**A Lightweight Parallel Framework for Blind Image Quality Assessment**|\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5e76\u884c\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6|Qunyue Huang, Bin Fang|Existing blind image quality assessment (BIQA) methods focus on designing complicated networks based on convolutional neural networks (CNNs) or transformer. In addition, some BIQA methods enhance the performance of the model in a two-stage training manner. Despite the significant advancements, these methods remarkably raise the parameter count of the model, thus requiring more training time and computational resources. To tackle the above issues, we propose a lightweight parallel framework (LPF) for BIQA. First, we extract the visual features using a pre-trained feature extraction network. Furthermore, we construct a simple yet effective feature embedding network (FEN) to transform the visual features, aiming to generate the latent representations that contain salient distortion information. To improve the robustness of the latent representations, we present two novel self-supervised subtasks, including a sample-level category prediction task and a batch-level quality comparison task. The sample-level category prediction task is presented to help the model with coarse-grained distortion perception. The batch-level quality comparison task is formulated to enhance the training data and thus improve the robustness of the latent representations. Finally, the latent representations are fed into a distortion-aware quality regression network (DaQRN), which simulates the human vision system (HVS) and thus generates accurate quality scores. Experimental results on multiple benchmark datasets demonstrate that the proposed method achieves superior performance over state-of-the-art approaches. Moreover, extensive analyses prove that the proposed method has lower computational complexity and faster convergence speed.|\u73b0\u6709\u7684\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08BIQA\uff09\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u8bbe\u8ba1\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6216\u53d8\u538b\u5668\u7684\u590d\u6742\u7f51\u7edc\u3002\u6b64\u5916\uff0c\u4e00\u4e9bBIQA\u65b9\u6cd5\u4ee5\u4e24\u9636\u6bb5\u8bad\u7ec3\u7684\u65b9\u5f0f\u589e\u5f3a\u6a21\u578b\u7684\u6027\u80fd\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u6b65\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u663e\u7740\u589e\u52a0\u4e86\u6a21\u578b\u7684\u53c2\u6570\u6570\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u591a\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e BIQA \u7684\u8f7b\u91cf\u7ea7\u5e76\u884c\u6846\u67b6\uff08LPF\uff09\u3002\u9996\u5148\uff0c\u6211\u4eec\u4f7f\u7528\u9884\u5148\u8bad\u7ec3\u7684\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u7279\u5f81\u5d4c\u5165\u7f51\u7edc\uff08FEN\uff09\u6765\u8f6c\u6362\u89c6\u89c9\u7279\u5f81\uff0c\u65e8\u5728\u751f\u6210\u5305\u542b\u663e\u7740\u5931\u771f\u4fe1\u606f\u7684\u6f5c\u5728\u8868\u793a\u3002\u4e3a\u4e86\u63d0\u9ad8\u6f5c\u5728\u8868\u793a\u7684\u9c81\u68d2\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5b50\u4efb\u52a1\uff0c\u5305\u62ec\u6837\u672c\u7ea7\u7c7b\u522b\u9884\u6d4b\u4efb\u52a1\u548c\u6279\u6b21\u7ea7\u8d28\u91cf\u6bd4\u8f83\u4efb\u52a1\u3002\u63d0\u51fa\u6837\u672c\u7ea7\u7c7b\u522b\u9884\u6d4b\u4efb\u52a1\u6765\u5e2e\u52a9\u6a21\u578b\u8fdb\u884c\u7c97\u7c92\u5ea6\u7684\u5931\u771f\u611f\u77e5\u3002\u5236\u5b9a\u6279\u6b21\u7ea7\u8d28\u91cf\u6bd4\u8f83\u4efb\u52a1\u662f\u4e3a\u4e86\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u63d0\u9ad8\u6f5c\u5728\u8868\u793a\u7684\u9c81\u68d2\u6027\u3002\u6700\u540e\uff0c\u6f5c\u5728\u8868\u793a\u88ab\u8f93\u5165\u5931\u771f\u611f\u77e5\u8d28\u91cf\u56de\u5f52\u7f51\u7edc\uff08DaQRN\uff09\uff0c\u8be5\u7f51\u7edc\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\uff08HVS\uff09\uff0c\u4ece\u800c\u751f\u6210\u51c6\u786e\u7684\u8d28\u91cf\u5206\u6570\u3002\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5927\u91cf\u5206\u6790\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u8f83\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002|[2402.12043v1](http://arxiv.org/pdf/2402.12043v1)|null|\n", "2402.12041": "|**2024-02-19**|**Surround-View Fisheye Optics in Computer Vision and Simulation: Survey and Challenge**|\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u4eff\u771f\u4e2d\u7684\u73af\u89c6\u9c7c\u773c\u5149\u5b66\u5668\u4ef6\uff1a\u8c03\u67e5\u548c\u6311\u6218|Daniel Jakab, Brian Michael Deegan, Sushil Sharma, Eoin Martino Grua, Jonathan Horgan, Enda Ward, Pepijn Van De Ven, Anthony Scanlan, Ciaran Eising|In this paper, we provide a survey on automotive surround-view fisheye optics, with an emphasis on the impact of optical artifacts on computer vision tasks in autonomous driving and ADAS. The automotive industry has advanced in applying state-of-the-art computer vision to enhance road safety and provide automated driving functionality. When using camera systems on vehicles, there is a particular need for a wide field of view to capture the entire vehicle's surroundings, in areas such as low-speed maneuvering, automated parking, and cocoon sensing. However, one crucial challenge in surround-view cameras is the strong optical aberrations of the fisheye camera, which is an area that has received little attention in the literature. Additionally, a comprehensive dataset is needed for testing safety-critical scenarios in vehicle automation. The industry has turned to simulation as a cost-effective strategy for creating synthetic datasets with surround-view camera imagery. We examine different simulation methods (such as model-driven and data-driven simulations) and discuss the simulators' ability (or lack thereof) to model real-world optical performance. Overall, this paper highlights the optical aberrations in automotive fisheye datasets, and the limitations of optical reality in simulated fisheye datasets, with a focus on computer vision in surround-view optical systems.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u6c7d\u8f66\u73af\u89c6\u9c7c\u773c\u5149\u5b66\u8fdb\u884c\u4e86\u8c03\u67e5\uff0c\u91cd\u70b9\u5173\u6ce8\u5149\u5b66\u4f2a\u5f71\u5bf9\u81ea\u52a8\u9a7e\u9a76\u548c ADAS \u4e2d\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u5f71\u54cd\u3002\u6c7d\u8f66\u884c\u4e1a\u5728\u5e94\u7528\u6700\u5148\u8fdb\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6765\u589e\u5f3a\u9053\u8def\u5b89\u5168\u5e76\u63d0\u4f9b\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u6b65\u3002\u5728\u8f66\u8f86\u4e0a\u4f7f\u7528\u6444\u50cf\u5934\u7cfb\u7edf\u65f6\uff0c\u7279\u522b\u9700\u8981\u5bbd\u89c6\u91ce\u6765\u6355\u6349\u6574\u4e2a\u8f66\u8f86\u7684\u5468\u56f4\u73af\u5883\uff0c\u4f8b\u5982\u4f4e\u901f\u64cd\u7eb5\u3001\u81ea\u52a8\u505c\u8f66\u548c\u8327\u611f\u6d4b\u7b49\u9886\u57df\u3002\u7136\u800c\uff0c\u73af\u89c6\u76f8\u673a\u9762\u4e34\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u9c7c\u773c\u76f8\u673a\u7684\u5f3a\u70c8\u5149\u5b66\u50cf\u5dee\uff0c\u8fd9\u4e00\u9886\u57df\u5728\u6587\u732e\u4e2d\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\u3002\u6b64\u5916\uff0c\u8fd8\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u6d4b\u8bd5\u8f66\u8f86\u81ea\u52a8\u5316\u4e2d\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\u3002\u8be5\u884c\u4e1a\u5df2\u5c06\u6a21\u62df\u4f5c\u4e3a\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u521b\u5efa\u5177\u6709\u73af\u89c6\u6444\u50cf\u673a\u56fe\u50cf\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u6211\u4eec\u7814\u7a76\u4e0d\u540c\u7684\u6a21\u62df\u65b9\u6cd5\uff08\u4f8b\u5982\u6a21\u578b\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u6a21\u62df\uff09\uff0c\u5e76\u8ba8\u8bba\u6a21\u62df\u5668\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5149\u5b66\u6027\u80fd\u7684\u80fd\u529b\uff08\u6216\u7f3a\u4e4f\u80fd\u529b\uff09\u3002\u603b\u7684\u6765\u8bf4\uff0c\u672c\u6587\u5f3a\u8c03\u4e86\u6c7d\u8f66\u9c7c\u773c\u6570\u636e\u96c6\u4e2d\u7684\u5149\u5b66\u50cf\u5dee\uff0c\u4ee5\u53ca\u6a21\u62df\u9c7c\u773c\u6570\u636e\u96c6\u4e2d\u5149\u5b66\u73b0\u5b9e\u7684\u5c40\u9650\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u73af\u89c6\u5149\u5b66\u7cfb\u7edf\u4e2d\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u3002|[2402.12041v1](http://arxiv.org/pdf/2402.12041v1)|null|\n", "2402.11940": "|**2024-02-19**|**AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization**|AICAtack\uff1a\u57fa\u4e8e\u6ce8\u610f\u529b\u4f18\u5316\u7684\u5bf9\u6297\u6027\u56fe\u50cf\u5b57\u5e55\u653b\u51fb|Jiyao Li, Mingze Ni, Yifei Dong, Tianqing Zhu, Wei Liu|Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchmark datasets with multiple victim models. The experimental results demonstrate that our method surpasses current leading-edge techniques by effectively distributing the alignment and semantics of words in the output.|\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u7684\u6700\u65b0\u8fdb\u5c55\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7684\u8bb8\u591a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u5c31\u3002 CV \u548c NLP \u7684\u4ea4\u53c9\u70b9\u662f\u56fe\u50cf\u63cf\u8ff0\u95ee\u9898\uff0c\u5176\u4e2d\u76f8\u5173\u6a21\u578b\u9488\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5f88\u597d\u7684\u7814\u7a76\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u6027\u653b\u51fb\u7b56\u7565\uff0c\u79f0\u4e3a AICAtack\uff08\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u56fe\u50cf\u5b57\u5e55\u653b\u51fb\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u5bf9\u56fe\u50cf\u7684\u5fae\u5999\u6270\u52a8\u6765\u653b\u51fb\u56fe\u50cf\u5b57\u5e55\u6a21\u578b\u3002\u5728\u9ed1\u76d2\u653b\u51fb\u573a\u666f\u4e2d\u8fd0\u884c\uff0c\u6211\u4eec\u7684\u7b97\u6cd5\u4e0d\u9700\u8981\u8bbf\u95ee\u76ee\u6807\u6a21\u578b\u7684\u67b6\u6784\u3001\u53c2\u6570\u6216\u68af\u5ea6\u4fe1\u606f\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5019\u9009\u9009\u62e9\u673a\u5236\uff0c\u8be5\u673a\u5236\u53ef\u8bc6\u522b\u8981\u653b\u51fb\u7684\u6700\u4f73\u50cf\u7d20\uff0c\u7136\u540e\u91c7\u7528\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u6765\u6270\u52a8\u50cf\u7d20\u7684 RGB \u503c\u3002\u6211\u4eec\u901a\u8fc7\u5bf9\u5177\u6709\u591a\u4e2a\u53d7\u5bb3\u8005\u6a21\u578b\u7684\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6765\u8bc1\u660e AICAtack \u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5730\u5206\u5e03\u8f93\u51fa\u4e2d\u5355\u8bcd\u7684\u5bf9\u9f50\u548c\u8bed\u4e49\uff0c\u8d85\u8d8a\u4e86\u5f53\u524d\u7684\u524d\u6cbf\u6280\u672f\u3002|[2402.11940v1](http://arxiv.org/pdf/2402.11940v1)|null|\n", "2402.11913": "|**2024-02-19**|**PhySU-Net: Long Temporal Context Transformer for rPPG with Self-Supervised Pre-training**|PhySU-Net\uff1a\u5177\u6709\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684 rPPG \u957f\u65f6\u6001\u4e0a\u4e0b\u6587\u8f6c\u6362\u5668|Marko Savic, Guoying Zhao|Remote photoplethysmography (rPPG) is a promising technology that consists of contactless measuring of cardiac activity from facial videos. Most recent approaches utilize convolutional networks with limited temporal modeling capability or ignore long temporal context. Supervised rPPG methods are also severely limited by scarce data availability. In this work, we propose PhySU-Net, the first long spatial-temporal map rPPG transformer network and a self-supervised pre-training strategy that exploits unlabeled data to improve our model. Our strategy leverages traditional methods and image masking to provide pseudo-labels for self-supervised pre-training. Our model is tested on two public datasets (OBF and VIPL-HR) and shows superior performance in supervised training. Furthermore, we demonstrate that our self-supervised pre-training strategy further improves our model's performance by leveraging representations learned from unlabeled data.|\u8fdc\u7a0b\u5149\u7535\u4f53\u79ef\u63cf\u8bb0\u6cd5 (rPPG) \u662f\u4e00\u9879\u5f88\u6709\u524d\u9014\u7684\u6280\u672f\uff0c\u5b83\u901a\u8fc7\u9762\u90e8\u89c6\u9891\u975e\u63a5\u89e6\u5f0f\u6d4b\u91cf\u5fc3\u810f\u6d3b\u52a8\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u5229\u7528\u5177\u6709\u6709\u9650\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u7684\u5377\u79ef\u7f51\u7edc\u6216\u5ffd\u7565\u957f\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u6709\u76d1\u7763\u7684 rPPG \u65b9\u6cd5\u4e5f\u53d7\u5230\u6570\u636e\u7a00\u7f3a\u7684\u4e25\u91cd\u9650\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 PhySU-Net\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u957f\u65f6\u7a7a\u56fe rPPG \u53d8\u6362\u7f51\u7edc\u548c\u4e00\u79cd\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u672a\u6807\u8bb0\u7684\u6570\u636e\u6765\u6539\u8fdb\u6211\u4eec\u7684\u6a21\u578b\u3002\u6211\u4eec\u7684\u7b56\u7565\u5229\u7528\u4f20\u7edf\u65b9\u6cd5\u548c\u56fe\u50cf\u63a9\u853d\u4e3a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u63d0\u4f9b\u4f2a\u6807\u7b7e\u3002\u6211\u4eec\u7684\u6a21\u578b\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff08OBF \u548c VIPL-HR\uff09\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u5728\u76d1\u7763\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u81ea\u6211\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\u901a\u8fc7\u5229\u7528\u4ece\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u8868\u793a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6211\u4eec\u6a21\u578b\u7684\u6027\u80fd\u3002|[2402.11913v1](http://arxiv.org/pdf/2402.11913v1)|null|\n", "2402.11874": "|**2024-02-19**|**Language-guided Image Reflection Separation**|\u8bed\u8a00\u5f15\u5bfc\u7684\u56fe\u50cf\u53cd\u5c04\u5206\u79bb|Haofeng Zhong, Yuchen Hong, Shuchen Weng, Jinxiu Liang, Boxin Shi|This paper studies the problem of language-guided reflection separation, which aims at addressing the ill-posed reflection separation problem by introducing language descriptions to provide layer content. We propose a unified framework to solve this problem, which leverages the cross-attention mechanism with contrastive learning strategies to construct the correspondence between language descriptions and image layers. A gated network design and a randomized training strategy are employed to tackle the recognizable layer ambiguity. The effectiveness of the proposed method is validated by the significant performance advantage over existing reflection separation methods on both quantitative and qualitative comparisons.|\u672c\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u53cd\u5c04\u5206\u79bb\u95ee\u9898\uff0c\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u8bed\u8a00\u63cf\u8ff0\u6765\u63d0\u4f9b\u5206\u5c42\u5185\u5bb9\u6765\u89e3\u51b3\u4e0d\u9002\u5b9a\u7684\u53cd\u5c04\u5206\u79bb\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5b83\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u6765\u6784\u5efa\u8bed\u8a00\u63cf\u8ff0\u548c\u56fe\u50cf\u5c42\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u91c7\u7528\u95e8\u63a7\u7f51\u7edc\u8bbe\u8ba1\u548c\u968f\u673a\u8bad\u7ec3\u7b56\u7565\u6765\u89e3\u51b3\u53ef\u8bc6\u522b\u5c42\u7684\u6a21\u7cca\u6027\u3002\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u901a\u8fc7\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u53cd\u5c04\u5206\u79bb\u65b9\u6cd5\u7684\u663e\u7740\u6027\u80fd\u4f18\u52bf\u5f97\u5230\u9a8c\u8bc1\u3002|[2402.11874v1](http://arxiv.org/pdf/2402.11874v1)|null|\n"}, "3D/CG": {"2402.12225": "|**2024-02-19**|**Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability**|\u4ee5\u5bb9\u91cf\u548c\u53ef\u6269\u5c55\u6027\u63a8\u52a8 3D \u5f62\u72b6\u751f\u6210\u7684\u81ea\u56de\u5f52\u6a21\u578b|Xuelin Qian, Yu Wang, Simian Luo, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chengjie Wang, Xiangyang Xue, Bo Zhao, Tiejun Huang, et.al.|Auto-regressive models have achieved impressive results in 2D image generation by modeling joint distributions in grid space. In this paper, we extend auto-regressive models to 3D domains, and seek a stronger ability of 3D shape generation by improving auto-regressive models at capacity and scalability simultaneously. Firstly, we leverage an ensemble of publicly available 3D datasets to facilitate the training of large-scale models. It consists of a comprehensive collection of approximately 900,000 objects, with multiple properties of meshes, points, voxels, rendered images, and text captions. This diverse labeled dataset, termed Objaverse-Mix, empowers our model to learn from a wide range of object variations. However, directly applying 3D auto-regression encounters critical challenges of high computational demands on volumetric grids and ambiguous auto-regressive order along grid dimensions, resulting in inferior quality of 3D shapes. To this end, we then present a novel framework Argus3D in terms of capacity. Concretely, our approach introduces discrete representation learning based on a latent vector instead of volumetric grids, which not only reduces computational costs but also preserves essential geometric details by learning the joint distributions in a more tractable order. The capacity of conditional generation can thus be realized by simply concatenating various conditioning inputs to the latent vector, such as point clouds, categories, images, and texts. In addition, thanks to the simplicity of our model architecture, we naturally scale up our approach to a larger model with an impressive 3.6 billion parameters, further enhancing the quality of versatile 3D generation. Extensive experiments on four generation tasks demonstrate that Argus3D can synthesize diverse and faithful shapes across multiple categories, achieving remarkable performance.|\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u5bf9\u7f51\u683c\u7a7a\u95f4\u4e2d\u7684\u8054\u5408\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u5728\u4e8c\u7ef4\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u81ea\u56de\u5f52\u6a21\u578b\u6269\u5c55\u52303D\u9886\u57df\uff0c\u5e76\u901a\u8fc7\u540c\u65f6\u6539\u8fdb\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5bb9\u91cf\u548c\u53ef\u6269\u5c55\u6027\u6765\u5bfb\u6c42\u66f4\u5f3a\u76843D\u5f62\u72b6\u751f\u6210\u80fd\u529b\u3002\u9996\u5148\uff0c\u6211\u4eec\u5229\u7528\u516c\u5f00\u7684 3D \u6570\u636e\u96c6\u6765\u4fc3\u8fdb\u5927\u578b\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u5b83\u7531\u5927\u7ea6 900,000 \u4e2a\u5bf9\u8c61\u7684\u5168\u9762\u96c6\u5408\u7ec4\u6210\uff0c\u5177\u6709\u7f51\u683c\u3001\u70b9\u3001\u4f53\u7d20\u3001\u6e32\u67d3\u56fe\u50cf\u548c\u6587\u672c\u6807\u9898\u7b49\u591a\u79cd\u5c5e\u6027\u3002\u8fd9\u4e2a\u591a\u6837\u5316\u7684\u6807\u8bb0\u6570\u636e\u96c6\u88ab\u79f0\u4e3a Objaverse-Mix\uff0c\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u4ece\u5404\u79cd\u5bf9\u8c61\u53d8\u5316\u4e2d\u5b66\u4e60\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5e94\u7528 3D \u81ea\u56de\u5f52\u9047\u5230\u4e86\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u5373\u5bf9\u4f53\u79ef\u7f51\u683c\u7684\u9ad8\u8ba1\u7b97\u8981\u6c42\u4ee5\u53ca\u6cbf\u7f51\u683c\u7ef4\u5ea6\u7684\u81ea\u56de\u5f52\u987a\u5e8f\u4e0d\u660e\u786e\uff0c\u5bfc\u81f4 3D \u5f62\u72b6\u8d28\u91cf\u8f83\u5dee\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5728\u5bb9\u91cf\u65b9\u9762\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6Argus3D\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u57fa\u4e8e\u6f5c\u5728\u5411\u91cf\u800c\u4e0d\u662f\u4f53\u79ef\u7f51\u683c\u7684\u79bb\u6563\u8868\u793a\u5b66\u4e60\uff0c\u8fd9\u4e0d\u4ec5\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u800c\u4e14\u8fd8\u901a\u8fc7\u4ee5\u66f4\u5bb9\u6613\u5904\u7406\u7684\u987a\u5e8f\u5b66\u4e60\u8054\u5408\u5206\u5e03\u6765\u4fdd\u7559\u57fa\u672c\u7684\u51e0\u4f55\u7ec6\u8282\u3002\u56e0\u6b64\uff0c\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u5730\u5c06\u5404\u79cd\u6761\u4ef6\u8f93\u5165\uff08\u4f8b\u5982\u70b9\u4e91\u3001\u7c7b\u522b\u3001\u56fe\u50cf\u548c\u6587\u672c\uff09\u8fde\u63a5\u5230\u6f5c\u5728\u5411\u91cf\u6765\u5b9e\u73b0\u6761\u4ef6\u751f\u6210\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u6211\u4eec\u6a21\u578b\u67b6\u6784\u7684\u7b80\u5355\u6027\uff0c\u6211\u4eec\u81ea\u7136\u5730\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u6269\u5c55\u5230\u5177\u6709\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684 36 \u4ebf\u4e2a\u53c2\u6570\u7684\u66f4\u5927\u6a21\u578b\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u591a\u529f\u80fd 3D \u751f\u6210\u7684\u8d28\u91cf\u3002\u5bf9\u56db\u4ee3\u4efb\u52a1\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cArgus3D \u53ef\u4ee5\u8de8\u591a\u4e2a\u7c7b\u522b\u5408\u6210\u591a\u6837\u5316\u4e14\u5fe0\u5b9e\u7684\u5f62\u72b6\uff0c\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002|[2402.12225v1](http://arxiv.org/pdf/2402.12225v1)|null|\n", "2402.12192": "|**2024-02-19**|**Pan-Mamba: Effective pan-sharpening with State Space Model**|Pan-Mamba\uff1a\u4f7f\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u6709\u6548\u7684\u5168\u8272\u9510\u5316|Xuanhua He, Ke Cao, Keyu Yan, Rui Li, Chengjun Xie, Jie Zhang, Man Zhou|Pan-sharpening involves integrating information from lowresolution multi-spectral and high-resolution panchromatic images to generate high-resolution multi-spectral counterparts. While recent advancements in the state space model, particularly the efficient long-range dependency modeling achieved by Mamba, have revolutionized computer vision community, its untapped potential in pan-sharpening motivates our exploration. Our contribution, Pan-Mamba, represents a novel pansharpening network that leverages the efficiency of the Mamba model in global information modeling. In Pan-Mamba, we customize two core components: channel swapping Mamba and cross-modal Mamba, strategically designed for efficient cross-modal information exchange and fusion. The former initiates a lightweight cross-modal interaction through the exchange of partial panchromatic and multispectral channels, while the latter facilities the information representation capability by exploiting inherent cross-modal relationships. Through extensive experiments across diverse datasets, our proposed approach surpasses state-of-theart methods, showcasing superior fusion results in pan-sharpening. To the best of our knowledge, this work is the first attempt in exploring the potential of the Mamba model and establishes a new frontier in the pan-sharpening techniques. The source code is available at https://github.com/alexhe101/Pan-Mamba .|\u5168\u8272\u9510\u5316\u6d89\u53ca\u6574\u5408\u6765\u81ea\u4f4e\u5206\u8fa8\u7387\u591a\u5149\u8c31\u548c\u9ad8\u5206\u8fa8\u7387\u5168\u8272\u56fe\u50cf\u7684\u4fe1\u606f\u4ee5\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u591a\u5149\u8c31\u5bf9\u5e94\u7269\u3002\u867d\u7136\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f Mamba \u5b9e\u73b0\u7684\u9ad8\u6548\u8fdc\u7a0b\u4f9d\u8d56\u5efa\u6a21\uff0c\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u793e\u533a\uff0c\u4f46\u5176\u5728\u5168\u8272\u9510\u5316\u65b9\u9762\u5c1a\u672a\u5f00\u53d1\u7684\u6f5c\u529b\u6fc0\u53d1\u4e86\u6211\u4eec\u7684\u63a2\u7d22\u3002\u6211\u4eec\u7684\u8d21\u732e Pan-Mamba \u4ee3\u8868\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5168\u8272\u9510\u5316\u7f51\u7edc\uff0c\u5b83\u5229\u7528\u4e86 Mamba \u6a21\u578b\u5728\u5168\u5c40\u4fe1\u606f\u5efa\u6a21\u4e2d\u7684\u6548\u7387\u3002\u5728 Pan-Mamba \u4e2d\uff0c\u6211\u4eec\u5b9a\u5236\u4e86\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u6e20\u9053\u4ea4\u6362 Mamba \u548c\u8de8\u6a21\u6001 Mamba\uff0c\u4ece\u6218\u7565\u4e0a\u65e8\u5728\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u4fe1\u606f\u4ea4\u6362\u548c\u878d\u5408\u3002\u524d\u8005\u901a\u8fc7\u90e8\u5206\u5168\u8272\u548c\u591a\u5149\u8c31\u901a\u9053\u7684\u4ea4\u6362\u542f\u52a8\u8f7b\u91cf\u7ea7\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u800c\u540e\u8005\u5219\u901a\u8fc7\u5229\u7528\u56fa\u6709\u7684\u8de8\u6a21\u6001\u5173\u7cfb\u6765\u4fc3\u8fdb\u4fe1\u606f\u8868\u793a\u80fd\u529b\u3002\u901a\u8fc7\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u5168\u8272\u9510\u5316\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u878d\u5408\u7ed3\u679c\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u9879\u5de5\u4f5c\u662f\u63a2\u7d22 Mamba \u6a21\u578b\u6f5c\u529b\u7684\u9996\u6b21\u5c1d\u8bd5\uff0c\u5e76\u5efa\u7acb\u4e86\u5168\u8272\u9510\u5316\u6280\u672f\u7684\u65b0\u9886\u57df\u3002\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/alexhe101/Pan-Mamba \u83b7\u53d6\u3002|[2402.12192v1](http://arxiv.org/pdf/2402.12192v1)|null|\n", "2402.12114": "|**2024-02-19**|**A Spatiotemporal Illumination Model for 3D Image Fusion in Optical Coherence Tomography**|\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u4e2d 3D \u56fe\u50cf\u878d\u5408\u7684\u65f6\u7a7a\u7167\u660e\u6a21\u578b|Stefan Ploner, Jungeun Won, Julia Schottenhamml, Jessica Girgis, Kenneth Lam, Nadia Waheed, James Fujimoto, Andreas Maier|Optical coherence tomography (OCT) is a non-invasive, micrometer-scale imaging modality that has become a clinical standard in ophthalmology. By raster-scanning the retina, sequential cross-sectional image slices are acquired to generate volumetric data. In-vivo imaging suffers from discontinuities between slices that show up as motion and illumination artifacts. We present a new illumination model that exploits continuity in orthogonally raster-scanned volume data. Our novel spatiotemporal parametrization adheres to illumination continuity both temporally, along the imaged slices, as well as spatially, in the transverse directions. Yet, our formulation does not make inter-slice assumptions, which could have discontinuities. This is the first optimization of a 3D inverse model in an image reconstruction context in OCT. Evaluation in 68 volumes from eyes with pathology showed reduction of illumination artifacts in 88\\% of the data, and only 6\\% showed moderate residual illumination artifacts. The method enables the use of forward-warped motion corrected data, which is more accurate, and enables supersampling and advanced 3D image reconstruction in OCT.|\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf (OCT) \u662f\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u5fae\u7c73\u7ea7\u6210\u50cf\u65b9\u5f0f\uff0c\u5df2\u6210\u4e3a\u773c\u79d1\u4e34\u5e8a\u6807\u51c6\u3002\u901a\u8fc7\u5149\u6805\u626b\u63cf\u89c6\u7f51\u819c\uff0c\u83b7\u53d6\u8fde\u7eed\u7684\u6a2a\u622a\u9762\u56fe\u50cf\u5207\u7247\u4ee5\u751f\u6210\u4f53\u79ef\u6570\u636e\u3002\u4f53\u5185\u6210\u50cf\u5b58\u5728\u5207\u7247\u4e4b\u95f4\u7684\u4e0d\u8fde\u7eed\u6027\uff0c\u8868\u73b0\u4e3a\u8fd0\u52a8\u548c\u7167\u660e\u4f2a\u5f71\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7167\u660e\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u6b63\u4ea4\u5149\u6805\u626b\u63cf\u4f53\u6570\u636e\u7684\u8fde\u7eed\u6027\u3002\u6211\u4eec\u65b0\u9896\u7684\u65f6\u7a7a\u53c2\u6570\u5316\u5728\u65f6\u95f4\u4e0a\uff08\u6cbf\u7740\u6210\u50cf\u5207\u7247\uff09\u4ee5\u53ca\u7a7a\u95f4\u4e0a\uff08\u5728\u6a2a\u5411\uff09\u4e0a\u90fd\u9075\u5faa\u7167\u660e\u8fde\u7eed\u6027\u3002\u7136\u800c\uff0c\u6211\u4eec\u7684\u516c\u5f0f\u6ca1\u6709\u505a\u51fa\u5207\u7247\u95f4\u5047\u8bbe\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5b58\u5728\u4e0d\u8fde\u7eed\u6027\u3002\u8fd9\u662f OCT \u56fe\u50cf\u91cd\u5efa\u80cc\u666f\u4e0b 3D \u9006\u6a21\u578b\u7684\u9996\u6b21\u4f18\u5316\u3002\u5bf9\u6765\u81ea\u75c5\u7406\u5b66\u773c\u775b\u7684 68 \u4e2a\u4f53\u79ef\u7684\u8bc4\u4f30\u663e\u793a\uff0c88% \u7684\u6570\u636e\u4e2d\u7167\u660e\u4f2a\u5f71\u51cf\u5c11\uff0c\u53ea\u6709 6% \u7684\u6570\u636e\u663e\u793a\u4e2d\u5ea6\u6b8b\u4f59\u7167\u660e\u4f2a\u5f71\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4f7f\u7528\u66f4\u51c6\u786e\u7684\u524d\u5411\u626d\u66f2\u8fd0\u52a8\u6821\u6b63\u6570\u636e\uff0c\u5e76\u5728 OCT \u4e2d\u5b9e\u73b0\u8d85\u91c7\u6837\u548c\u9ad8\u7ea7 3D \u56fe\u50cf\u91cd\u5efa\u3002|[2402.12114v1](http://arxiv.org/pdf/2402.12114v1)|null|\n", "2402.11866": "|**2024-02-19**|**Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic**|\u4e24\u79cd\u57fa\u4e8e\u5c42\u6b21\u5206\u6790\u6cd5\u548c\u6a21\u7cca\u903b\u8f91\u7684\u5728\u7ebf\u5730\u56fe\u5339\u914d\u7b97\u6cd5|Jeremy J. Lin, Tomoro Mochida, Riley C. W. O'Neill, Atsuro Yoshida, Masashi Yamazaki, Akinobu Sasada|Our aim of this paper is to develop new map matching algorithms and to improve upon previous work. We address two key approaches: Analytic Hierarchy Process (AHP) map matching and fuzzy logic map matching. AHP is a decision-making method that combines mathematical analysis with human judgment, and fuzzy logic is an approach to computing based on the degree of truth and aims at modeling the imprecise modes of reasoning from 0 to 1 rather than the usual boolean logic. Of these algorithms, the way of our applying AHP to map matching is newly developed in this paper, meanwhile, our application of fuzzy logic to map matching is mostly the same as existing research except for some small changes. Because of the common characteristic that both methods are designed to handle imprecise information and simplicity for implementation, we decided to use these methods.|\u6211\u4eec\u672c\u6587\u7684\u76ee\u7684\u662f\u5f00\u53d1\u65b0\u7684\u5730\u56fe\u5339\u914d\u7b97\u6cd5\u5e76\u6539\u8fdb\u4ee5\u524d\u7684\u5de5\u4f5c\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u65b9\u6cd5\uff1a\u5c42\u6b21\u5206\u6790\u6cd5\uff08AHP\uff09\u56fe\u5339\u914d\u548c\u6a21\u7cca\u903b\u8f91\u56fe\u5339\u914d\u3002\u5c42\u6b21\u5206\u6790\u6cd5\u662f\u4e00\u79cd\u5c06\u6570\u5b66\u5206\u6790\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u7ed3\u5408\u7684\u51b3\u7b56\u65b9\u6cd5\uff0c\u800c\u6a21\u7cca\u903b\u8f91\u662f\u4e00\u79cd\u57fa\u4e8e\u771f\u5b9e\u7a0b\u5ea6\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u65e8\u5728\u5bf9\u4ece0\u52301\u7684\u4e0d\u7cbe\u786e\u63a8\u7406\u6a21\u5f0f\u8fdb\u884c\u5efa\u6a21\uff0c\u800c\u4e0d\u662f\u901a\u5e38\u7684\u5e03\u5c14\u903b\u8f91\u3002\u5728\u8fd9\u4e9b\u7b97\u6cd5\u4e2d\uff0c\u6211\u4eec\u5c06\u5c42\u6b21\u5206\u6790\u6cd5\u5e94\u7528\u4e8e\u5730\u56fe\u5339\u914d\u7684\u65b9\u6cd5\u662f\u672c\u6587\u65b0\u5f00\u53d1\u7684\uff0c\u540c\u65f6\uff0c\u6211\u4eec\u5c06\u6a21\u7cca\u903b\u8f91\u5e94\u7528\u4e8e\u5730\u56fe\u5339\u914d\u7684\u65b9\u6cd5\u9664\u4e86\u4e00\u4e9b\u5c0f\u7684\u53d8\u5316\u5916\u4e0e\u73b0\u6709\u7684\u7814\u7a76\u57fa\u672c\u76f8\u540c\u3002\u7531\u4e8e\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u5171\u540c\u7279\u70b9\u662f\u5904\u7406\u4e0d\u7cbe\u786e\u7684\u4fe1\u606f\u4e14\u5b9e\u73b0\u7b80\u5355\uff0c\u56e0\u6b64\u6211\u4eec\u51b3\u5b9a\u4f7f\u7528\u8fd9\u4e9b\u65b9\u6cd5\u3002|[2402.11866v1](http://arxiv.org/pdf/2402.11866v1)|null|\n", "2402.11836": "|**2024-02-19**|**DIO: Dataset of 3D Mesh Models of Indoor Objects for Robotics and Computer Vision Applications**|DIO\uff1a\u7528\u4e8e\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u7684\u5ba4\u5185\u7269\u4f53 3D \u7f51\u683c\u6a21\u578b\u6570\u636e\u96c6|Nillan Nimal, Wenbin Li, Ronald Clark, Sajad Saeedi|The creation of accurate virtual models of real-world objects is imperative to robotic simulations and applications such as computer vision, artificial intelligence, and machine learning. This paper documents the different methods employed for generating a database of mesh models of real-world objects. These methods address the tedious and time-intensive process of manually generating the models using CAD software. Essentially, DSLR/phone cameras were employed to acquire images of target objects. These images were processed using a photogrammetry software known as Meshroom to generate a dense surface reconstruction of the scene. The result produced by Meshroom was edited and simplified using MeshLab, a mesh-editing software to produce the final model. Based on the obtained models, this process was effective in modelling the geometry and texture of real-world objects with high fidelity. An active 3D scanner was also utilized to accelerate the process for large objects. All generated models and captured images are made available on the website of the project.|\u521b\u5efa\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u7684\u51c6\u786e\u865a\u62df\u6a21\u578b\u5bf9\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u7b49\u673a\u5668\u4eba\u6a21\u62df\u548c\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u8bb0\u5f55\u4e86\u7528\u4e8e\u751f\u6210\u73b0\u5b9e\u4e16\u754c\u5bf9\u8c61\u7f51\u683c\u6a21\u578b\u6570\u636e\u5e93\u7684\u4e0d\u540c\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f7f\u7528 CAD \u8f6f\u4ef6\u624b\u52a8\u751f\u6210\u6a21\u578b\u7684\u7e41\u7410\u4e14\u8017\u65f6\u7684\u8fc7\u7a0b\u3002\u672c\u8d28\u4e0a\uff0c\u6570\u7801\u5355\u53cd\u76f8\u673a/\u624b\u673a\u6444\u50cf\u5934\u7528\u4e8e\u83b7\u53d6\u76ee\u6807\u7269\u4f53\u7684\u56fe\u50cf\u3002\u8fd9\u4e9b\u56fe\u50cf\u4f7f\u7528\u540d\u4e3a Meshroom \u7684\u6444\u5f71\u6d4b\u91cf\u8f6f\u4ef6\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u751f\u6210\u573a\u666f\u7684\u5bc6\u96c6\u8868\u9762\u91cd\u5efa\u3002 Meshroom \u751f\u6210\u7684\u7ed3\u679c\u4f7f\u7528\u7f51\u683c\u7f16\u8f91\u8f6f\u4ef6 MeshLab \u8fdb\u884c\u7f16\u8f91\u548c\u7b80\u5316\uff0c\u4ee5\u751f\u6210\u6700\u7ec8\u6a21\u578b\u3002\u57fa\u4e8e\u6240\u83b7\u5f97\u7684\u6a21\u578b\uff0c\u8be5\u8fc7\u7a0b\u53ef\u4ee5\u6709\u6548\u5730\u5bf9\u73b0\u5b9e\u4e16\u754c\u5bf9\u8c61\u7684\u51e0\u4f55\u548c\u7eb9\u7406\u8fdb\u884c\u9ad8\u4fdd\u771f\u5ea6\u5efa\u6a21\u3002\u8fd8\u5229\u7528\u4e3b\u52a8 3D \u626b\u63cf\u4eea\u6765\u52a0\u901f\u5927\u578b\u7269\u4f53\u7684\u5904\u7406\u8fc7\u7a0b\u3002\u6240\u6709\u751f\u6210\u7684\u6a21\u578b\u548c\u6355\u83b7\u7684\u56fe\u50cf\u90fd\u53ef\u4ee5\u5728\u8be5\u9879\u76ee\u7684\u7f51\u7ad9\u4e0a\u83b7\u53d6\u3002|[2402.11836v1](http://arxiv.org/pdf/2402.11836v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.11816": "|**2024-02-19**|**Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before**|\u907f\u514d\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u6291\u5236\uff1a\u5b66\u4e60\u4ee5\u524d\u6ca1\u6709\u5b66\u8fc7\u7684\u4e1c\u897f|Jihai Zhang, Xiang Lan, Xiaoye Qu, Yu Cheng, Mengling Feng, Bryan Hooi|Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-learned features. Extensive experiments conducted on various publicly available benchmarks validate the effectiveness of our proposed framework. In addition, we demonstrate that the proposed MCL can be adapted to a variety of popular contrastive learning backbones and boost their performance by learning features that could not be gained from standard contrastive learning procedures.|\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u5df2\u6210\u4e3a\u4ece\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u83b7\u53d6\u9ad8\u8d28\u91cf\u8868\u793a\u7684\u5f3a\u5927\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u6700\u8fd1\u5728\u6807\u51c6\u5bf9\u6bd4\u5b66\u4e60\uff08\u4f8b\u5982\uff0cSimCLR\u3001CLIP\uff09\u4e2d\u53d1\u73b0\u4e86\u7279\u5f81\u6291\u5236\uff1a\u5728\u5355\u4e2a\u7aef\u5230\u7aef\u8bad\u7ec3\u9636\u6bb5\uff0c\u5bf9\u6bd4\u6a21\u578b\u4ec5\u6355\u83b7\u5bf9\u6bd4\u89c6\u56fe\u4e2d\u7684\u90e8\u5206\u5171\u4eab\u4fe1\u606f\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u53ef\u80fd\u6709\u7528\u7684\u4fe1\u606f\u3002\u901a\u8fc7\u7279\u5f81\u6291\u5236\uff0c\u5bf9\u6bd4\u6a21\u578b\u901a\u5e38\u65e0\u6cd5\u5b66\u4e60\u80fd\u591f\u5b8c\u6210\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u8db3\u591f\u8868\u793a\u3002\u4e3a\u4e86\u7f13\u89e3\u7279\u5f81\u6291\u5236\u95ee\u9898\u5e76\u786e\u4fdd\u5bf9\u6bd4\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5168\u9762\u7684\u8868\u793a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5\u5bf9\u6bd4\u5b66\u4e60\uff08MCL\uff09\u6846\u67b6\u3002\u4e0e\u901a\u5e38\u5bfc\u81f4\u7279\u5f81\u6291\u5236\u7684\u6807\u51c6\u5bf9\u6bd4\u5b66\u4e60\u4e0d\u540c\uff0cMCL \u9010\u6b65\u5b66\u4e60\u524d\u4e00\u9636\u6bb5\u5c1a\u672a\u63a2\u7d22\u7684\u65b0\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u7559\u5df2\u5b66\u8fc7\u7684\u7279\u5f81\u3002\u5728\u5404\u79cd\u516c\u5f00\u53ef\u7528\u7684\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 MCL \u53ef\u4ee5\u9002\u5e94\u5404\u79cd\u6d41\u884c\u7684\u5bf9\u6bd4\u5b66\u4e60\u4e3b\u5e72\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u6807\u51c6\u5bf9\u6bd4\u5b66\u4e60\u7a0b\u5e8f\u65e0\u6cd5\u83b7\u5f97\u7684\u7279\u5f81\u6765\u63d0\u9ad8\u5176\u6027\u80fd\u3002|[2402.11816v1](http://arxiv.org/pdf/2402.11816v1)|null|\n"}, "\u5176\u4ed6": {"2402.12292": "|**2024-02-19**|**Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling**|\u901a\u8fc7\u53bb\u566a\u8fdb\u884c\u6b63\u5219\u5316\uff1a\u8d1d\u53f6\u65af\u6a21\u578b\u548c Langevin-within-split Gibbs \u91c7\u6837|Elhadji C. Faye, Mame Diarra Fall, Nicolas Dobigeon|This paper introduces a Bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It additionally implements a Monte Carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact data augmentation (AXDA). The proposed algorithm is an approximate instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo step. The proposed method is applied to common imaging tasks such as deblurring, inpainting and super-resolution, demonstrating its efficacy through extensive numerical experiments. These contributions advance Bayesian inference in imaging by leveraging data-driven regularization strategies within a probabilistic framework.|\u672c\u6587\u901a\u8fc7\u63a8\u5bfc\u53bb\u566a\u6b63\u5219\u5316 (RED) \u8303\u5f0f\u7684\u6982\u7387\u5bf9\u5e94\u9879\uff0c\u4ecb\u7ecd\u4e86\u7528\u4e8e\u56fe\u50cf\u53cd\u6f14\u7684\u8d1d\u53f6\u65af\u6846\u67b6\u3002\u5b83\u8fd8\u5b9e\u73b0\u4e86\u4e13\u95e8\u9488\u5bf9\u57fa\u4e8e\u6e10\u8fd1\u7cbe\u786e\u6570\u636e\u589e\u5f3a (AXDA) \u4ece\u6240\u5f97\u540e\u9a8c\u5206\u5e03\u8fdb\u884c\u91c7\u6837\u800c\u5b9a\u5236\u7684\u8499\u7279\u5361\u7f57\u7b97\u6cd5\u3002\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u662f\u5206\u5272\u5409\u5e03\u65af\u91c7\u6837 (SGS) \u7684\u8fd1\u4f3c\u5b9e\u4f8b\uff0c\u5176\u4e2d\u5d4c\u5165\u4e86\u4e00\u4e2a Langevin Monte Carlo \u6b65\u9aa4\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u53bb\u6a21\u7cca\u3001\u4fee\u590d\u548c\u8d85\u5206\u8fa8\u7387\u7b49\u5e38\u89c1\u6210\u50cf\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u8fd9\u4e9b\u8d21\u732e\u901a\u8fc7\u5728\u6982\u7387\u6846\u67b6\u5185\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684\u6b63\u5219\u5316\u7b56\u7565\u63a8\u8fdb\u4e86\u6210\u50cf\u4e2d\u7684\u8d1d\u53f6\u65af\u63a8\u7406\u3002|[2402.12292v1](http://arxiv.org/pdf/2402.12292v1)|null|\n", "2402.12289": "|**2024-02-19**|**DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models**|DriveVLM\uff1a\u81ea\u52a8\u9a7e\u9a76\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u878d\u5408|Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, Hang Zhao|A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of chain-of-thought (CoT) modules for scene description, scene analysis, and hierarchical planning. Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline. DriveVLM-Dual achieves robust spatial understanding and real-time inference speed. Extensive experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the effectiveness of DriveVLM and the enhanced performance of DriveVLM-Dual, surpassing existing methods in complex and unpredictable driving conditions.|\u57ce\u5e02\u73af\u5883\u4e2d\u81ea\u52a8\u9a7e\u9a76\u7684\u4e3b\u8981\u969c\u788d\u662f\u7406\u89e3\u590d\u6742\u7684\u957f\u5c3e\u573a\u666f\uff0c\u4f8b\u5982\u5177\u6709\u6311\u6218\u6027\u7684\u9053\u8def\u6761\u4ef6\u548c\u5fae\u5999\u7684\u4eba\u7c7b\u884c\u4e3a\u3002\u6211\u4eec\u63a8\u51fa DriveVLM\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u6765\u589e\u5f3a\u573a\u666f\u7406\u89e3\u548c\u89c4\u5212\u80fd\u529b\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002 DriveVLM \u96c6\u6210\u4e86\u7528\u4e8e\u573a\u666f\u63cf\u8ff0\u3001\u573a\u666f\u5206\u6790\u548c\u5206\u5c42\u89c4\u5212\u7684\u601d\u60f3\u94fe (CoT) \u6a21\u5757\u7684\u72ec\u7279\u7ec4\u5408\u3002\u6b64\u5916\uff0c\u8ba4\u8bc6\u5230 VLM \u5728\u7a7a\u95f4\u63a8\u7406\u548c\u7e41\u91cd\u8ba1\u7b97\u8981\u6c42\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DriveVLM-Dual\uff0c\u8fd9\u662f\u4e00\u79cd\u6df7\u5408\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5c06 DriveVLM \u4e0e\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u7ba1\u9053\u7684\u4f18\u52bf\u76f8\u7ed3\u5408\u3002 DriveVLM-Dual \u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u7a7a\u95f4\u7406\u89e3\u548c\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002\u5bf9 nuScenes \u6570\u636e\u96c6\u548c SUP-AD \u6570\u636e\u96c6\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86 DriveVLM \u7684\u6709\u6548\u6027\u4ee5\u53ca DriveVLM-Dual \u7684\u589e\u5f3a\u6027\u80fd\uff0c\u5728\u590d\u6742\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u9a7e\u9a76\u6761\u4ef6\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002|[2402.12289v1](http://arxiv.org/pdf/2402.12289v1)|null|\n", "2402.12181": "|**2024-02-19**|**Revisiting Data Augmentation in Deep Reinforcement Learning**|\u91cd\u65b0\u5ba1\u89c6\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u589e\u5f3a|Jianshu Hu, Yunpeng Jiang, Paul Weng|Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge. We evaluate our proposition and validate our analysis in several domains. Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments.|\u6700\u8fd1\u5728\u57fa\u4e8e\u56fe\u50cf\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4e2d\u63d0\u51fa\u4e86\u5404\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002\u5c3d\u7ba1\u4ed6\u4eec\u51ed\u7ecf\u9a8c\u8bc1\u660e\u4e86\u6570\u636e\u589e\u5f3a\u5bf9\u4e8e\u63d0\u9ad8\u6837\u672c\u6548\u7387\u6216\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u6027\uff0c\u4f46\u5e94\u8be5\u9996\u9009\u54ea\u79cd\u6280\u672f\u5e76\u4e0d\u603b\u662f\u660e\u786e\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5206\u6790\u4e86\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5b83\u4eec\u5e76\u63ed\u793a\u5b83\u4eec\u4e4b\u95f4\u7684\u8054\u7cfb\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u901a\u8fc7\u8868\u8fbe\u8fd9\u4e9b\u65b9\u6cd5\u7684 Q \u76ee\u6807\u7684\u65b9\u5dee\u548c\u7ecf\u9a8c\u53c2\u4e0e\u8005/\u6279\u8bc4\u8005\u635f\u5931\u7684\u65b9\u5dee\uff0c\u6211\u4eec\u53ef\u4ee5\u5206\u6790\u5b83\u4eec\u4e0d\u540c\u7ec4\u6210\u90e8\u5206\u7684\u6548\u679c\u5e76\u8fdb\u884c\u6bd4\u8f83\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u89e3\u91ca\u4e86\u5728\u8ba1\u7b97\u76ee\u6807 Q \u503c\u65f6\u9009\u62e9\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u53d8\u6362\u5982\u4f55\u5f71\u54cd\u8fd9\u4e9b\u65b9\u6cd5\u3002\u8be5\u5206\u6790\u5c31\u5982\u4f55\u4ee5\u66f4\u539f\u5219\u6027\u7684\u65b9\u5f0f\u5229\u7528\u6570\u636e\u589e\u5f3a\u63d0\u51fa\u4e86\u5efa\u8bae\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a tangent prop \u7684\u6b63\u5219\u5316\u672f\u8bed\uff0c\u8be5\u672f\u8bed\u5148\u524d\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u63d0\u51fa\uff0c\u4f46\u636e\u6211\u4eec\u6240\u77e5\uff0c\u5b83\u5bf9 DRL \u7684\u9002\u5e94\u662f\u65b0\u9896\u7684\u3002\u6211\u4eec\u8bc4\u4f30\u6211\u4eec\u7684\u4e3b\u5f20\u5e76\u5728\u591a\u4e2a\u9886\u57df\u9a8c\u8bc1\u6211\u4eec\u7684\u5206\u6790\u3002\u4e0e\u4e0d\u540c\u7684\u76f8\u5173\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u8bc1\u660e\u5b83\u5728\u5927\u591a\u6570\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4e00\u4e9b\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002|[2402.12181v1](http://arxiv.org/pdf/2402.12181v1)|null|\n", "2402.12179": "|**2024-02-19**|**Examining Monitoring System: Detecting Abnormal Behavior In Online Examinations**|\u8003\u8bd5\u76d1\u63a7\u7cfb\u7edf\uff1a\u68c0\u6d4b\u5728\u7ebf\u8003\u8bd5\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a|Dinh An Ngo, Thanh Dat Nguyen, Thi Le Chi Dang, Huy Hoan Le, Ton Bao Ho, Vo Thanh Khang Nguyen, Truong Thanh Hung Nguyen|Cheating in online exams has become a prevalent issue over the past decade, especially during the COVID-19 pandemic. To address this issue of academic dishonesty, our \"Exam Monitoring System: Detecting Abnormal Behavior in Online Examinations\" is designed to assist proctors in identifying unusual student behavior. Our system demonstrates high accuracy and speed in detecting cheating in real-time scenarios, providing valuable information, and aiding proctors in decision-making. This article outlines our methodology and the effectiveness of our system in mitigating the widespread problem of cheating in online exams.|\u8fc7\u53bb\u5341\u5e74\u4e2d\uff0c\u5728\u7ebf\u8003\u8bd5\u4f5c\u5f0a\u5df2\u6210\u4e3a\u4e00\u4e2a\u666e\u904d\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u65b0\u51a0\u80ba\u708e (COVID-19) \u5927\u6d41\u884c\u671f\u95f4\u3002\u4e3a\u4e86\u89e3\u51b3\u5b66\u672f\u4e0d\u8bda\u5b9e\u95ee\u9898\uff0c\u6211\u4eec\u7684\u201c\u8003\u8bd5\u76d1\u63a7\u7cfb\u7edf\uff1a\u68c0\u6d4b\u5728\u7ebf\u8003\u8bd5\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\u201d\u65e8\u5728\u5e2e\u52a9\u76d1\u8003\u4eba\u5458\u8bc6\u522b\u5b66\u751f\u7684\u5f02\u5e38\u884c\u4e3a\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u68c0\u6d4b\u4f5c\u5f0a\u3001\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u5e76\u5e2e\u52a9\u76d1\u8003\u4eba\u5458\u51b3\u7b56\u65b9\u9762\u8868\u73b0\u51fa\u5f88\u9ad8\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002\u672c\u6587\u6982\u8ff0\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u548c\u7cfb\u7edf\u5728\u7f13\u89e3\u5728\u7ebf\u8003\u8bd5\u4e2d\u666e\u904d\u5b58\u5728\u7684\u4f5c\u5f0a\u95ee\u9898\u65b9\u9762\u7684\u6709\u6548\u6027\u3002|[2402.12179v1](http://arxiv.org/pdf/2402.12179v1)|null|\n", "2402.12095": "|**2024-02-19**|**Major TOM: Expandable Datasets for Earth Observation**|\u4e3b\u8981 TOM\uff1a\u53ef\u6269\u5c55\u7684\u5730\u7403\u89c2\u6d4b\u6570\u636e\u96c6|Alistair Francis, Mikolaj Czerkawski|Deep learning models are increasingly data-hungry, requiring significant resources to collect and compile the datasets needed to train them, with Earth Observation (EO) models being no exception. However, the landscape of datasets in EO is relatively atomised, with interoperability made difficult by diverse formats and data structures. If ever larger datasets are to be built, and duplication of effort minimised, then a shared framework that allows users to combine and access multiple datasets is needed. Here, Major TOM (Terrestrial Observation Metaset) is proposed as this extensible framework. Primarily, it consists of a geographical indexing system based on a set of grid points and a metadata structure that allows multiple datasets with different sources to be merged. Besides the specification of Major TOM as a framework, this work also presents a large, open-access dataset, MajorTOM-Core, which covers the vast majority of the Earth's land surface. This dataset provides the community with both an immediately useful resource, as well as acting as a template for future additions to the Major TOM ecosystem. Access: https://huggingface.co/Major-TOM|\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u6570\u636e\u7684\u9700\u6c42\u8d8a\u6765\u8d8a\u5927\uff0c\u9700\u8981\u5927\u91cf\u8d44\u6e90\u6765\u6536\u96c6\u548c\u7f16\u8bd1\u8bad\u7ec3\u5b83\u4eec\u6240\u9700\u7684\u6570\u636e\u96c6\uff0c\u5730\u7403\u89c2\u6d4b (EO) \u6a21\u578b\u4e5f\u4e0d\u4f8b\u5916\u3002\u7136\u800c\uff0cEO \u4e2d\u7684\u6570\u636e\u96c6\u683c\u5c40\u76f8\u5bf9\u5206\u6563\uff0c\u4e0d\u540c\u7684\u683c\u5f0f\u548c\u6570\u636e\u7ed3\u6784\u4f7f\u5f97\u4e92\u64cd\u4f5c\u6027\u53d8\u5f97\u56f0\u96be\u3002\u5982\u679c\u8981\u6784\u5efa\u66f4\u5927\u7684\u6570\u636e\u96c6\uff0c\u5e76\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u91cd\u590d\u5de5\u4f5c\uff0c\u5219\u9700\u8981\u4e00\u4e2a\u5141\u8bb8\u7528\u6237\u7ec4\u5408\u548c\u8bbf\u95ee\u591a\u4e2a\u6570\u636e\u96c6\u7684\u5171\u4eab\u6846\u67b6\u3002\u8fd9\u91cc\uff0cMajor TOM\uff08\u9646\u5730\u89c2\u6d4b\u5143\u96c6\uff09\u88ab\u63d0\u8bae\u4f5c\u4e3a\u8fd9\u79cd\u53ef\u6269\u5c55\u6846\u67b6\u3002\u5b83\u4e3b\u8981\u7531\u57fa\u4e8e\u4e00\u7ec4\u7f51\u683c\u70b9\u7684\u5730\u7406\u7d22\u5f15\u7cfb\u7edf\u548c\u5141\u8bb8\u5408\u5e76\u5177\u6709\u4e0d\u540c\u6765\u6e90\u7684\u591a\u4e2a\u6570\u636e\u96c6\u7684\u5143\u6570\u636e\u7ed3\u6784\u7ec4\u6210\u3002\u9664\u4e86\u5c06 Major TOM \u89c4\u8303\u4f5c\u4e3a\u6846\u67b6\u4e4b\u5916\uff0c\u8fd9\u9879\u5de5\u4f5c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u578b\u7684\u5f00\u653e\u8bbf\u95ee\u6570\u636e\u96c6 MajorTOM-Core\uff0c\u5b83\u8986\u76d6\u4e86\u5730\u7403\u9646\u5730\u8868\u9762\u7684\u7edd\u5927\u591a\u6570\u3002\u8be5\u6570\u636e\u96c6\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u7acb\u5373\u53ef\u7528\u7684\u8d44\u6e90\uff0c\u5e76\u5145\u5f53\u4e3b\u8981 TOM \u751f\u6001\u7cfb\u7edf\u672a\u6765\u6dfb\u52a0\u5185\u5bb9\u7684\u6a21\u677f\u3002\u8bbf\u95ee\uff1ahttps://huggingface.co/Major-TOM|[2402.12095v1](http://arxiv.org/pdf/2402.12095v1)|null|\n", "2402.12072": "|**2024-02-19**|**Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview**|\u53cd\u95ee\u9898\u53d8\u5206\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u63a2\u7d22\uff1a\u6982\u8ff0|Alexander Auras, Kanchana Vaishnavi Gandikota, Hannah Droege, Michael Moeller|This paper attempts to provide an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning. A special focus lies on point estimators and their robustness against adversarial perturbations. In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees. Another focus of this review is the exploration of the subspace of data consistent solutions through explicit guidance to satisfy specific semantic or textural properties.|\u672c\u6587\u8bd5\u56fe\u6982\u8ff0\u4f7f\u7528\u53d8\u5206\u65b9\u6cd5\u548c\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u6210\u50cf\u9006\u95ee\u9898\u7684\u5f53\u524d\u65b9\u6cd5\u3002\u7279\u522b\u5173\u6ce8\u70b9\u4f30\u8ba1\u5668\u53ca\u5176\u5bf9\u6297\u5bf9\u6297\u6027\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u4e86\u4e00\u7ef4\u73a9\u5177\u95ee\u9898\u7684\u6570\u503c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u663e\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u5e76\u901a\u8fc7\u7ecf\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002\u672c\u6b21\u5ba1\u67e5\u7684\u53e6\u4e00\u4e2a\u91cd\u70b9\u662f\u901a\u8fc7\u660e\u786e\u7684\u6307\u5bfc\u6765\u63a2\u7d22\u6570\u636e\u4e00\u81f4\u6027\u89e3\u51b3\u65b9\u6848\u7684\u5b50\u7a7a\u95f4\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u7684\u8bed\u4e49\u6216\u7eb9\u7406\u5c5e\u6027\u3002|[2402.12072v1](http://arxiv.org/pdf/2402.12072v1)|null|\n", "2402.11883": "|**2024-02-19**|**InMD-X: Large Language Models for Internal Medicine Doctors**|InMD-X\uff1a\u5185\u79d1\u533b\u751f\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b|Hansle Gwon, Imjin Ahn, Hyoje Jung, Byeolhee Kim, Young-Hak Kim, Tae Joon Jun|In this paper, we introduce InMD-X, a collection of multiple large language models specifically designed to cater to the unique characteristics and demands of Internal Medicine Doctors (IMD). InMD-X represents a groundbreaking development in natural language processing, offering a suite of language models fine-tuned for various aspects of the internal medicine field. These models encompass a wide range of medical sub-specialties, enabling IMDs to perform more efficient and accurate research, diagnosis, and documentation. InMD-X's versatility and adaptability make it a valuable tool for improving the healthcare industry, enhancing communication between healthcare professionals, and advancing medical research. Each model within InMD-X is meticulously tailored to address specific challenges faced by IMDs, ensuring the highest level of precision and comprehensiveness in clinical text analysis and decision support. This paper provides an overview of the design, development, and evaluation of InMD-X, showcasing its potential to revolutionize the way internal medicine practitioners interact with medical data and information. We present results from extensive testing, demonstrating the effectiveness and practical utility of InMD-X in real-world medical scenarios.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 InMD-X\uff0c\u8fd9\u662f\u4e13\u95e8\u4e3a\u6ee1\u8db3\u5185\u79d1\u533b\u751f (IMD) \u7684\u72ec\u7279\u7279\u5f81\u548c\u9700\u6c42\u800c\u8bbe\u8ba1\u7684\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u5408\u3002 InMD-X \u4ee3\u8868\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u7a81\u7834\u6027\u53d1\u5c55\uff0c\u63d0\u4f9b\u4e86\u4e00\u5957\u9488\u5bf9\u5185\u79d1\u9886\u57df\u5404\u4e2a\u65b9\u9762\u8fdb\u884c\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u6db5\u76d6\u5e7f\u6cdb\u7684\u533b\u5b66\u5b50\u4e13\u4e1a\uff0c\u4f7f IMD \u80fd\u591f\u8fdb\u884c\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u7814\u7a76\u3001\u8bca\u65ad\u548c\u8bb0\u5f55\u3002 InMD-X \u7684\u591a\u529f\u80fd\u6027\u548c\u9002\u5e94\u6027\u4f7f\u5176\u6210\u4e3a\u6539\u5584\u533b\u7597\u4fdd\u5065\u884c\u4e1a\u3001\u52a0\u5f3a\u533b\u7597\u4fdd\u5065\u4e13\u4e1a\u4eba\u5458\u4e4b\u95f4\u7684\u6c9f\u901a\u548c\u63a8\u8fdb\u533b\u5b66\u7814\u7a76\u7684\u5b9d\u8d35\u5de5\u5177\u3002 InMD-X \u4e2d\u7684\u6bcf\u4e2a\u6a21\u578b\u90fd\u7ecf\u8fc7\u7cbe\u5fc3\u5b9a\u5236\uff0c\u4ee5\u89e3\u51b3 IMD \u9762\u4e34\u7684\u7279\u5b9a\u6311\u6218\uff0c\u786e\u4fdd\u4e34\u5e8a\u6587\u672c\u5206\u6790\u548c\u51b3\u7b56\u652f\u6301\u7684\u6700\u9ad8\u7cbe\u786e\u5ea6\u548c\u5168\u9762\u6027\u3002\u672c\u6587\u6982\u8ff0\u4e86 InMD-X \u7684\u8bbe\u8ba1\u3001\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5176\u5f7b\u5e95\u6539\u53d8\u5185\u79d1\u533b\u751f\u4e0e\u533b\u7597\u6570\u636e\u548c\u4fe1\u606f\u4ea4\u4e92\u65b9\u5f0f\u7684\u6f5c\u529b\u3002\u6211\u4eec\u5c55\u793a\u4e86\u5e7f\u6cdb\u6d4b\u8bd5\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86 InMD-X \u5728\u73b0\u5b9e\u533b\u7597\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002|[2402.11883v1](http://arxiv.org/pdf/2402.11883v1)|null|\n"}}