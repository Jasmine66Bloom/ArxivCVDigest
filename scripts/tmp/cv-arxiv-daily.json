{"\u751f\u6210\u6a21\u578b": {"2402.19481": "|**2024-02-29**|**DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models**|DistriFusion\uff1a\u9ad8\u5206\u8fa8\u7387\u6269\u6563\u6a21\u578b\u7684\u5206\u5e03\u5f0f\u5e76\u884c\u63a8\u7406|Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, Song Han|Diffusion models have achieved great success in synthesizing high-quality images. However, generating high-resolution images with diffusion models is still challenging due to the enormous computational costs, resulting in a prohibitive latency for interactive applications. In this paper, we propose DistriFusion to tackle this problem by leveraging parallelism across multiple GPUs. Our method splits the model input into multiple patches and assigns each patch to a GPU. However, na\\\"{\\i}vely implementing such an algorithm breaks the interaction between patches and loses fidelity, while incorporating such an interaction will incur tremendous communication overhead. To overcome this dilemma, we observe the high similarity between the input from adjacent diffusion steps and propose displaced patch parallelism, which takes advantage of the sequential nature of the diffusion process by reusing the pre-computed feature maps from the previous timestep to provide context for the current step. Therefore, our method supports asynchronous communication, which can be pipelined by computation. Extensive experiments show that our method can be applied to recent Stable Diffusion XL with no quality degradation and achieve up to a 6.1$\\times$ speedup on eight NVIDIA A100s compared to one. Our code is publicly available at https://github.com/mit-han-lab/distrifuser.|\u6269\u6563\u6a21\u578b\u5728\u5408\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5bfc\u81f4\u4ea4\u4e92\u5f0f\u5e94\u7528\u7a0b\u5e8f\u7684\u5ef6\u8fdf\u8fc7\u9ad8\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae DistriFusion \u901a\u8fc7\u5229\u7528\u591a\u4e2a GPU \u7684\u5e76\u884c\u6027\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u6a21\u578b\u8f93\u5165\u62c6\u5206\u4e3a\u591a\u4e2a\u8865\u4e01\uff0c\u5e76\u5c06\u6bcf\u4e2a\u8865\u4e01\u5206\u914d\u7ed9 GPU\u3002\u7136\u800c\uff0c\u7b80\u5355\u5730\u5b9e\u73b0\u8fd9\u6837\u7684\u7b97\u6cd5\u4f1a\u7834\u574f\u8865\u4e01\u4e4b\u95f4\u7684\u4ea4\u4e92\u5e76\u5931\u53bb\u4fdd\u771f\u5ea6\uff0c\u800c\u5408\u5e76\u8fd9\u6837\u7684\u4ea4\u4e92\u5c06\u4ea7\u751f\u5de8\u5927\u7684\u901a\u4fe1\u5f00\u9500\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u56f0\u5883\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u76f8\u90bb\u6269\u6563\u7684\u8f93\u5165\u4e4b\u95f4\u7684\u9ad8\u5ea6\u76f8\u4f3c\u6027\u6b65\u9aa4\u5e76\u63d0\u51fa\u7f6e\u6362\u8865\u4e01\u5e76\u884c\u6027\uff0c\u5b83\u901a\u8fc7\u91cd\u7528\u524d\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\u9884\u5148\u8ba1\u7b97\u7684\u7279\u5f81\u56fe\u6765\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u7684\u987a\u5e8f\u6027\u8d28\uff0c\u4e3a\u5f53\u524d\u6b65\u9aa4\u63d0\u4f9b\u4e0a\u4e0b\u6587\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u652f\u6301\u5f02\u6b65\u901a\u4fe1\uff0c\u53ef\u4ee5\u6d41\u6c34\u7ebf\u5316\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5e94\u7528\u4e8e\u6700\u8fd1\u7684 Stable Diffusion XL\uff0c\u800c\u4e0d\u4f1a\u964d\u4f4e\u8d28\u91cf\uff0c\u5e76\u4e14\u5728 8 \u53f0 NVIDIA A100 \u4e0a\u6bd4\u4e00\u53f0 NVIDIA A100 \u5b9e\u73b0\u9ad8\u8fbe 6.1$\\times$ \u7684\u52a0\u901f\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https:// \u516c\u5f00\u83b7\u53d6github.com/mit-han-lab/distrifuser\u3002|[2402.19481v1](http://arxiv.org/pdf/2402.19481v1)|null|\n", "2402.19470": "|**2024-02-29**|**Towards Generalizable Tumor Synthesis**|\u8fc8\u5411\u53ef\u63a8\u5e7f\u7684\u80bf\u7624\u5408\u6210|Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan Yuille, Chen Wei, Zongwei Zhou|Tumor synthesis enables the creation of artificial tumors in medical images, facilitating the training of AI models for tumor detection and segmentation. However, success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, furthermore, the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g., hospitals). This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (< 2cm) tend to have similar imaging characteristics in computed tomography (CT), whether they originate in the liver, pancreas, or kidneys. We have ascertained that generative AI models, e.g., Diffusion Models, can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ. Moreover, we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes, encompassing a broad spectrum of patient demographics, imaging protocols, and healthcare facilities.|\u80bf\u7624\u5408\u6210\u53ef\u4ee5\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u521b\u5efa\u4eba\u9020\u80bf\u7624\uff0c\u4ece\u800c\u4fc3\u8fdb\u7528\u4e8e\u80bf\u7624\u68c0\u6d4b\u548c\u5206\u5272\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u80bf\u7624\u5408\u6210\u7684\u6210\u529f\u53d6\u51b3\u4e8e\u521b\u5efa\u89c6\u89c9\u4e0a\u771f\u5b9e\u7684\u80bf\u7624\uff0c\u8fd9\u4e9b\u80bf\u7624\u53ef\u4ee5\u63a8\u5e7f\u5230\u591a\u4e2a\u5668\u5b98\uff0c\u6b64\u5916\uff0c\u7531\u6b64\u4ea7\u751f\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u80fd\u591f\u68c0\u6d4b\u6765\u81ea\u4e0d\u540c\u9886\u57df\uff08\u4f8b\u5982\u533b\u9662\uff09\u7684\u56fe\u50cf\u4e2d\u7684\u771f\u5b9e\u80bf\u7624\u3002\u672c\u6587\u5229\u7528\u4e00\u9879\u91cd\u8981\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u5728\u5e7f\u4e49\u80bf\u7624\u5408\u6210\u65b9\u9762\u8fc8\u51fa\u4e86\u4e00\u6b65\uff1a\u65e9\u671f\u80bf\u7624\uff08< 2cm\uff09\u5728\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u4e2d\u5f80\u5f80\u5177\u6709\u76f8\u4f3c\u7684\u6210\u50cf\u7279\u5f81\uff0c\u65e0\u8bba\u5b83\u4eec\u8d77\u6e90\u4e8e\u809d\u810f\u3001\u80f0\u817a\u8fd8\u662f\u80be\u810f\u3002\u6211\u4eec\u5df2\u7ecf\u786e\u5b9a\uff0c\u751f\u6210\u5f0f AI \u6a21\u578b\uff08\u4f8b\u5982\u6269\u6563\u6a21\u578b\uff09\u53ef\u4ee5\u521b\u5efa\u6cdb\u5316\u5230\u4e00\u7cfb\u5217\u5668\u5b98\u7684\u771f\u5b9e\u80bf\u7624\uff0c\u5373\u4f7f\u4ec5\u5bf9\u6765\u81ea\u4e00\u4e2a\u5668\u5b98\u7684\u6709\u9650\u6570\u91cf\u7684\u80bf\u7624\u793a\u4f8b\u8fdb\u884c\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u5728\u8fd9\u4e9b\u5408\u6210\u80bf\u7624\u4e0a\u8bad\u7ec3\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u53ef\u4ee5\u63a8\u5e7f\u5230\u4ece CT \u4f53\u79ef\u4e2d\u68c0\u6d4b\u548c\u5206\u5272\u771f\u5b9e\u80bf\u7624\uff0c\u6db5\u76d6\u5e7f\u6cdb\u7684\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u3001\u6210\u50cf\u65b9\u6848\u548c\u533b\u7597\u8bbe\u65bd\u3002|[2402.19470v1](http://arxiv.org/pdf/2402.19470v1)|null|\n", "2402.19469": "|**2024-02-29**|**Humanoid Locomotion as Next Token Prediction**|\u4eba\u5f62\u8fd0\u52a8\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b|Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, Jitendra Malik|We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.|\u6211\u4eec\u5c06\u73b0\u5b9e\u4e16\u754c\u7684\u4eba\u5f62\u63a7\u5236\u89c6\u4e3a\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u95ee\u9898\uff0c\u7c7b\u4f3c\u4e8e\u9884\u6d4b\u8bed\u8a00\u4e2d\u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd\u3002\u6211\u4eec\u7684\u6a21\u578b\u662f\u901a\u8fc7\u611f\u89c9\u8fd0\u52a8\u8f68\u8ff9\u7684\u81ea\u56de\u5f52\u9884\u6d4b\u8bad\u7ec3\u7684\u56e0\u679c\u53d8\u6362\u5668\u3002\u4e3a\u4e86\u8003\u8651\u6570\u636e\u7684\u591a\u6a21\u6001\u6027\u8d28\uff0c\u6211\u4eec\u4ee5\u6a21\u6001\u5bf9\u9f50\u7684\u65b9\u5f0f\u6267\u884c\u9884\u6d4b\uff0c\u5e76\u4e14\u5bf9\u4e8e\u6bcf\u4e2a\u8f93\u5165\u6807\u8bb0\u4ece\u76f8\u540c\u6a21\u6001\u9884\u6d4b\u4e0b\u4e00\u4e2a\u6807\u8bb0\u3002\u8fd9\u79cd\u901a\u7528\u7684\u516c\u5f0f\u4f7f\u6211\u4eec\u80fd\u591f\u5229\u7528\u7f3a\u5c11\u6a21\u5f0f\u7684\u6570\u636e\uff0c\u4f8b\u5982\u6ca1\u6709\u52a8\u4f5c\u7684\u89c6\u9891\u8f68\u8ff9\u3002\u6211\u4eec\u6839\u636e\u6765\u81ea\u5148\u524d\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u3001\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u5668\u3001\u52a8\u4f5c\u6355\u6349\u6570\u636e\u548c\u4eba\u7c7b YouTube \u89c6\u9891\u7684\u4e00\u7ec4\u6a21\u62df\u8f68\u8ff9\u6765\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u8ba9\u5168\u5c3a\u5bf8\u7684\u4eba\u5f62\u673a\u5668\u4eba\u96f6\u5c04\u51fb\u5730\u5728\u65e7\u91d1\u5c71\u884c\u8d70\u3002\u5373\u4f7f\u4ec5\u4f7f\u7528 27 \u5c0f\u65f6\u7684\u6b65\u884c\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4e5f\u53ef\u4ee5\u8f6c\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\uff0c\u5e76\u4e14\u53ef\u4ee5\u6cdb\u5316\u5230\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u8fc7\u7684\u547d\u4ee4\uff0c\u4f8b\u5982\u5012\u9000\u884c\u8d70\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u901a\u8fc7\u611f\u89c9\u8fd0\u52a8\u8f68\u8ff9\u7684\u751f\u6210\u6a21\u578b\u6765\u5b66\u4e60\u5177\u6709\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u63a7\u5236\u4efb\u52a1\u662f\u4e00\u6761\u6709\u5e0c\u671b\u7684\u9053\u8def\u3002|[2402.19469v1](http://arxiv.org/pdf/2402.19469v1)|null|\n", "2402.19455": "|**2024-02-29**|**Listening to the Noise: Blind Denoising with Gibbs Diffusion**|\u8046\u542c\u566a\u97f3\uff1a\u4f7f\u7528\u5409\u5e03\u65af\u6269\u6563\u8fdb\u884c\u76f2\u964d\u566a|David Heurtel-Depeiges, Charles C. Margossian, Ruben Ohana, Bruno R\u00e9galdo-Saint Blancard|In recent years, denoising problems have become intertwined with the development of deep generative models. In particular, diffusion models are trained like denoisers, and the distribution they model coincide with denoising priors in the Bayesian picture. However, denoising through diffusion-based posterior sampling requires the noise level and covariance to be known, preventing blind denoising. We overcome this limitation by introducing Gibbs Diffusion (GDiff), a general methodology addressing posterior sampling of both the signal and the noise parameters. Assuming arbitrary parametric Gaussian noise, we develop a Gibbs algorithm that alternates sampling steps from a conditional diffusion model trained to map the signal prior to the family of noise distributions, and a Monte Carlo sampler to infer the noise parameters. Our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the Gibbs stationary distribution caused by the diffusion model. We showcase our method for 1) blind denoising of natural images involving colored noises with unknown amplitude and spectral index, and 2) a cosmology problem, namely the analysis of cosmic microwave background data, where Bayesian inference of \"noise\" parameters means constraining models of the evolution of the Universe.|\u8fd1\u5e74\u6765\uff0c\u53bb\u566a\u95ee\u9898\u4e0e\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u4ea4\u7ec7\u5728\u4e00\u8d77\u3002\u7279\u522b\u662f\uff0c\u6269\u6563\u6a21\u578b\u50cf\u964d\u566a\u5668\u4e00\u6837\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4e14\u5b83\u4eec\u5efa\u6a21\u7684\u5206\u5e03\u4e0e\u8d1d\u53f6\u65af\u56fe\u50cf\u4e2d\u7684\u964d\u566a\u5148\u9a8c\u4e00\u81f4\u3002\u7136\u800c\uff0c\u901a\u8fc7\u57fa\u4e8e\u6269\u6563\u7684\u540e\u9a8c\u91c7\u6837\u53bb\u566a\u9700\u8981\u5df2\u77e5\u566a\u58f0\u6c34\u5e73\u548c\u534f\u65b9\u5dee\uff0c\u4ece\u800c\u9632\u6b62\u76f2\u76ee\u53bb\u566a\u3002\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u5409\u5e03\u65af\u6269\u6563\uff08GDiff\uff09\u514b\u670d\u4e86\u8fd9\u4e00\u9650\u5236\uff0c\u5409\u5e03\u65af\u6269\u6563\u662f\u4e00\u79cd\u89e3\u51b3\u4fe1\u53f7\u548c\u566a\u58f0\u53c2\u6570\u540e\u91c7\u6837\u95ee\u9898\u7684\u901a\u7528\u65b9\u6cd5\u3002\u5047\u8bbe\u5b58\u5728\u4efb\u610f\u53c2\u6570\u9ad8\u65af\u566a\u58f0\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u5409\u5e03\u65af\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4ece\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e2d\u4ea4\u66ff\u91c7\u6837\u6b65\u9aa4\uff0c\u4ee5\u5c06\u4fe1\u53f7\u6620\u5c04\u5230\u566a\u58f0\u5206\u5e03\u65cf\u4e4b\u524d\uff0c\u5e76\u4f7f\u7528\u8499\u7279\u5361\u7f57\u91c7\u6837\u5668\u6765\u63a8\u65ad\u566a\u58f0\u53c2\u6570\u3002\u6211\u4eec\u7684\u7406\u8bba\u5206\u6790\u5f3a\u8c03\u4e86\u6f5c\u5728\u7684\u9677\u9631\uff0c\u6307\u5bfc\u8bca\u65ad\u4f7f\u7528\uff0c\u5e76\u91cf\u5316\u7531\u6269\u6563\u6a21\u578b\u5f15\u8d77\u7684\u5409\u5e03\u65af\u5e73\u7a33\u5206\u5e03\u4e2d\u7684\u8bef\u5dee\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff1a1\uff09\u6d89\u53ca\u5177\u6709\u672a\u77e5\u5e45\u5ea6\u548c\u5149\u8c31\u6307\u6570\u7684\u6709\u8272\u566a\u58f0\u7684\u81ea\u7136\u56fe\u50cf\u7684\u76f2\u53bb\u566a\uff0c\u4ee5\u53ca2\uff09\u5b87\u5b99\u5b66\u95ee\u9898\uff0c\u5373\u5b87\u5b99\u5fae\u6ce2\u80cc\u666f\u6570\u636e\u7684\u5206\u6790\uff0c\u5176\u4e2d\u201c\u566a\u58f0\u201d\u53c2\u6570\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u610f\u5473\u7740\u7ea6\u675f\u6a21\u578b\u5b87\u5b99\u7684\u6f14\u5316\u3002|[2402.19455v1](http://arxiv.org/pdf/2402.19455v1)|null|\n", "2402.19387": "|**2024-02-29**|**SeD: Semantic-Aware Discriminator for Image Super-Resolution**|SeD\uff1a\u7528\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u8bed\u4e49\u611f\u77e5\u9274\u522b\u5668|Bingchen Li, Xin Li, Hanxin Zhu, Yeying Jin, Ruoyu Feng, Zhizheng Zhang, Zhibo Chen|Generative Adversarial Networks (GANs) have been widely used to recover vivid textures in image super-resolution (SR) tasks. In particular, one discriminator is utilized to enable the SR network to learn the distribution of real-world high-quality images in an adversarial training manner. However, the distribution learning is overly coarse-grained, which is susceptible to virtual textures and causes counter-intuitive generation results. To mitigate this, we propose the simple and effective Semantic-aware Discriminator (denoted as SeD), which encourages the SR network to learn the fine-grained distributions by introducing the semantics of images as a condition. Concretely, we aim to excavate the semantics of images from a well-trained semantic extractor. Under different semantics, the discriminator is able to distinguish the real-fake images individually and adaptively, which guides the SR network to learn the more fine-grained semantic-aware textures. To obtain accurate and abundant semantics, we take full advantage of recently popular pretrained vision models (PVMs) with extensive datasets, and then incorporate its semantic features into the discriminator through a well-designed spatial cross-attention module. In this way, our proposed semantic-aware discriminator empowered the SR network to produce more photo-realistic and pleasing images. Extensive experiments on two typical tasks, i.e., SR and Real SR have demonstrated the effectiveness of our proposed methods.|\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u5df2\u88ab\u5e7f\u6cdb\u7528\u4e8e\u6062\u590d\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u4efb\u52a1\u4e2d\u7684\u751f\u52a8\u7eb9\u7406\u3002\u7279\u522b\u662f\uff0c\u5229\u7528\u4e00\u4e2a\u5224\u522b\u5668\u4f7f SR \u7f51\u7edc\u80fd\u591f\u4ee5\u5bf9\u6297\u6027\u8bad\u7ec3\u7684\u65b9\u5f0f\u5b66\u4e60\u73b0\u5b9e\u4e16\u754c\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u5206\u5e03\u3002\u7136\u800c\uff0c\u5206\u5e03\u5b66\u4e60\u8fc7\u4e8e\u7c97\u7c92\u5ea6\uff0c\u5bb9\u6613\u53d7\u5230\u865a\u62df\u7eb9\u7406\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u8fdd\u53cd\u76f4\u89c9\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7b80\u5355\u6709\u6548\u7684\u8bed\u4e49\u611f\u77e5\u5224\u522b\u5668\uff08\u8868\u793a\u4e3a SeD\uff09\uff0c\u5b83\u9f13\u52b1 SR \u7f51\u7edc\u901a\u8fc7\u5f15\u5165\u56fe\u50cf\u8bed\u4e49\u4f5c\u4e3a\u6761\u4ef6\u6765\u5b66\u4e60\u7ec6\u7c92\u5ea6\u5206\u5e03\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u4ece\u8bad\u7ec3\u6709\u7d20\u7684\u8bed\u4e49\u63d0\u53d6\u5668\u4e2d\u6316\u6398\u56fe\u50cf\u7684\u8bed\u4e49\u3002\u5728\u4e0d\u540c\u7684\u8bed\u4e49\u4e0b\uff0c\u9274\u522b\u5668\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5355\u72ec\u533a\u5206\u771f\u5047\u56fe\u50cf\uff0c\u4ece\u800c\u5f15\u5bfc SR \u7f51\u7edc\u5b66\u4e60\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u611f\u77e5\u7eb9\u7406\u3002\u4e3a\u4e86\u83b7\u5f97\u51c6\u786e\u548c\u4e30\u5bcc\u7684\u8bed\u4e49\uff0c\u6211\u4eec\u5145\u5206\u5229\u7528\u6700\u8fd1\u6d41\u884c\u7684\u5177\u6709\u5e7f\u6cdb\u6570\u636e\u96c6\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff08PVM\uff09\uff0c\u7136\u540e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7a7a\u95f4\u4ea4\u53c9\u6ce8\u610f\u6a21\u5757\u5c06\u5176\u8bed\u4e49\u7279\u5f81\u5408\u5e76\u5230\u9274\u522b\u5668\u4e2d\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u8bed\u4e49\u611f\u77e5\u9274\u522b\u5668\u4f7f SR \u7f51\u7edc\u80fd\u591f\u751f\u6210\u66f4\u52a0\u903c\u771f\u548c\u4ee4\u4eba\u6109\u60a6\u7684\u56fe\u50cf\u3002\u5bf9\u4e24\u4e2a\u5178\u578b\u4efb\u52a1\uff08\u5373 SR \u548c Real SR\uff09\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2402.19387v1](http://arxiv.org/pdf/2402.19387v1)|null|\n", "2402.19369": "|**2024-02-29**|**Structure Preserving Diffusion Models**|\u7ed3\u6784\u4fdd\u6301\u6269\u6563\u6a21\u578b|Haoye Lu, Spencer Szabados, Yaoliang Yu|Diffusion models have become the leading distribution-learning method in recent years. Herein, we introduce structure-preserving diffusion processes, a family of diffusion processes for learning distributions that possess additional structure, such as group symmetries, by developing theoretical conditions under which the diffusion transition steps preserve said symmetry. While also enabling equivariant data sampling trajectories, we exemplify these results by developing a collection of different symmetry equivariant diffusion models capable of learning distributions that are inherently symmetric. Empirical studies, over both synthetic and real-world datasets, are used to validate the developed models adhere to the proposed theory and are capable of achieving improved performance over existing methods in terms of sample equality. We also show how the proposed models can be used to achieve theoretically guaranteed equivariant image noise reduction without prior knowledge of the image orientation.|\u8fd1\u5e74\u6765\uff0c\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u9886\u5148\u7684\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u7ed3\u6784\u4fdd\u6301\u6269\u6563\u8fc7\u7a0b\uff0c\u8fd9\u662f\u4e00\u7cfb\u5217\u7528\u4e8e\u5b66\u4e60\u5177\u6709\u9644\u52a0\u7ed3\u6784\uff08\u4f8b\u5982\u7fa4\u5bf9\u79f0\u6027\uff09\u7684\u5206\u5e03\u7684\u6269\u6563\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5f00\u53d1\u6269\u6563\u8fc7\u6e21\u6b65\u9aa4\u4fdd\u6301\u6240\u8ff0\u5bf9\u79f0\u6027\u7684\u7406\u8bba\u6761\u4ef6\u3002\u5728\u5b9e\u73b0\u7b49\u53d8\u6570\u636e\u91c7\u6837\u8f68\u8ff9\u7684\u540c\u65f6\uff0c\u6211\u4eec\u901a\u8fc7\u5f00\u53d1\u4e00\u7cfb\u5217\u80fd\u591f\u5b66\u4e60\u672c\u8d28\u4e0a\u5bf9\u79f0\u7684\u5206\u5e03\u7684\u4e0d\u540c\u5bf9\u79f0\u7b49\u53d8\u6269\u6563\u6a21\u578b\u6765\u4e3e\u4f8b\u8bf4\u660e\u8fd9\u4e9b\u7ed3\u679c\u3002\u5bf9\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5b9e\u8bc1\u7814\u7a76\u7528\u4e8e\u9a8c\u8bc1\u5f00\u53d1\u7684\u6a21\u578b\u662f\u5426\u7b26\u5408\u6240\u63d0\u51fa\u7684\u7406\u8bba\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u6837\u672c\u5e73\u7b49\u65b9\u9762\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u66f4\u9ad8\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u6a21\u578b\u6765\u5b9e\u73b0\u7406\u8bba\u4e0a\u4fdd\u8bc1\u7684\u7b49\u53d8\u56fe\u50cf\u964d\u566a\uff0c\u800c\u65e0\u9700\u4e8b\u5148\u4e86\u89e3\u56fe\u50cf\u65b9\u5411\u3002|[2402.19369v1](http://arxiv.org/pdf/2402.19369v1)|null|\n", "2402.19330": "|**2024-02-29**|**A Novel Approach to Industrial Defect Generation through Blended Latent Diffusion Model with Online Adaptation**|\u901a\u8fc7\u5728\u7ebf\u9002\u5e94\u7684\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u5de5\u4e1a\u7f3a\u9677\u7684\u65b0\u65b9\u6cd5|Hanxi Li, Zhengxun Zhang, Hao Chen, Lin Wu, Bo Li, Deyin Liu, Mingwen Wang|Effectively addressing the challenge of industrial Anomaly Detection (AD) necessitates an ample supply of defective samples, a constraint often hindered by their scarcity in industrial contexts. This paper introduces a novel algorithm designed to augment defective samples, thereby enhancing AD performance. The proposed method tailors the blended latent diffusion model for defect sample generation, employing a diffusion model to generate defective samples in the latent space. A feature editing process, controlled by a \"trimap\" mask and text prompts, refines the generated samples. The image generation inference process is structured into three stages: a free diffusion stage, an editing diffusion stage, and an online decoder adaptation stage. This sophisticated inference strategy yields high-quality synthetic defective samples with diverse pattern variations, leading to significantly improved AD accuracies based on the augmented training set. Specifically, on the widely recognized MVTec AD dataset, the proposed method elevates the state-of-the-art (SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD metrics AP, IAP, and IAP90, respectively. The implementation code of this work can be found at the GitHub repository https://github.com/GrandpaXun242/AdaBLDM.git|\u6709\u6548\u5e94\u5bf9\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b (AD) \u7684\u6311\u6218\u9700\u8981\u63d0\u4f9b\u5145\u8db3\u7684\u7f3a\u9677\u6837\u672c\uff0c\u800c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\uff0c\u7f3a\u9677\u6837\u672c\u7684\u7a00\u7f3a\u5f80\u5f80\u4f1a\u963b\u788d\u8fd9\u4e00\u9650\u5236\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b97\u6cd5\uff0c\u65e8\u5728\u589e\u52a0\u7f3a\u9677\u6837\u672c\uff0c\u4ece\u800c\u63d0\u9ad8 AD \u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u9488\u5bf9\u7f3a\u9677\u6837\u672c\u751f\u6210\u5b9a\u5236\u4e86\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u6269\u6563\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u7f3a\u9677\u6837\u672c\u3002\u7531\u201ctrimap\u201d\u8499\u7248\u548c\u6587\u672c\u63d0\u793a\u63a7\u5236\u7684\u7279\u5f81\u7f16\u8f91\u8fc7\u7a0b\u53ef\u7ec6\u5316\u751f\u6210\u7684\u6837\u672c\u3002\u56fe\u50cf\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u81ea\u7531\u6269\u6563\u9636\u6bb5\u3001\u7f16\u8f91\u6269\u6563\u9636\u6bb5\u548c\u5728\u7ebf\u89e3\u7801\u5668\u9002\u5e94\u9636\u6bb5\u3002\u8fd9\u79cd\u590d\u6742\u7684\u63a8\u7406\u7b56\u7565\u53ef\u4ea7\u751f\u5177\u6709\u591a\u79cd\u6a21\u5f0f\u53d8\u5316\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u7f3a\u9677\u6837\u672c\uff0c\u4ece\u800c\u57fa\u4e8e\u589e\u5f3a\u7684\u8bad\u7ec3\u96c6\u663e\u7740\u63d0\u9ad8 AD \u51c6\u786e\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u5e7f\u6cdb\u8ba4\u53ef\u7684 MVTec AD \u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06 AD \u7684\u6700\u65b0 (SOTA) \u6027\u80fd\u63d0\u5347\u4e86 1.5%\u30011.9% \u548c 3.1%\uff0c\u5176\u4e2d AD \u6307\u6807 AP\u3001IAP \u548c IAP90 \uff0c \u5206\u522b\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u5b9e\u73b0\u4ee3\u7801\u53ef\u4ee5\u5728GitHub\u5b58\u50a8\u5e93\u4e2d\u627e\u5230https://github.com/GrandpaXun242/AdaBLDM.git|[2402.19330v1](http://arxiv.org/pdf/2402.19330v1)|null|\n", "2402.19302": "|**2024-02-29**|**DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly**|DiffAssemble\uff1a\u7528\u4e8e 2D \u548c 3D \u91cd\u7ec4\u7684\u7edf\u4e00\u56fe\u6269\u6563\u6a21\u578b|Gianluca Scarpellini, Stefano Fiorini, Francesco Giuliari, Pietro Morerio, Alessio Del Bue|Reassembly tasks play a fundamental role in many fields and multiple approaches exist to solve specific reassembly problems. In this context, we posit that a general unified model can effectively address them all, irrespective of the input data type (images, 3D, etc.). We introduce DiffAssemble, a Graph Neural Network (GNN)-based architecture that learns to solve reassembly tasks using a diffusion model formulation. Our method treats the elements of a set, whether pieces of 2D patch or 3D object fragments, as nodes of a spatial graph. Training is performed by introducing noise into the position and rotation of the elements and iteratively denoising them to reconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art (SOTA) results in most 2D and 3D reassembly tasks and is the first learning-based approach that solves 2D puzzles for both rotation and translation. Furthermore, we highlight its remarkable reduction in run-time, performing 11 times faster than the quickest optimization-based method for puzzle solving. Code available at https://github.com/IIT-PAVIS/DiffAssemble|\u91cd\u7ec4\u4efb\u52a1\u5728\u8bb8\u591a\u9886\u57df\u53d1\u6325\u7740\u57fa\u7840\u4f5c\u7528\uff0c\u5e76\u4e14\u5b58\u5728\u591a\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u7279\u5b9a\u7684\u91cd\u7ec4\u95ee\u9898\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5047\u8bbe\u901a\u7528\u7684\u7edf\u4e00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u6240\u6709\u95ee\u9898\uff0c\u800c\u4e0d\u7ba1\u8f93\u5165\u6570\u636e\u7c7b\u578b\uff08\u56fe\u50cf\u30013D \u7b49\uff09\u5982\u4f55\u3002\u6211\u4eec\u5f15\u5165\u4e86 DiffAssemble\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u7684\u67b6\u6784\uff0c\u5b83\u5b66\u4e60\u4f7f\u7528\u6269\u6563\u6a21\u578b\u516c\u5f0f\u6765\u89e3\u51b3\u91cd\u7ec4\u4efb\u52a1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u96c6\u5408\u7684\u5143\u7d20\uff08\u65e0\u8bba\u662f 2D \u5757\u8fd8\u662f 3D \u5bf9\u8c61\u7247\u6bb5\uff09\u89c6\u4e3a\u7a7a\u95f4\u56fe\u7684\u8282\u70b9\u3002\u901a\u8fc7\u5c06\u566a\u58f0\u5f15\u5165\u5143\u7d20\u7684\u4f4d\u7f6e\u548c\u65cb\u8f6c\u5e76\u8fed\u4ee3\u5730\u5bf9\u5176\u8fdb\u884c\u53bb\u566a\u4ee5\u91cd\u5efa\u76f8\u5e72\u7684\u521d\u59cb\u59ff\u6001\u6765\u6267\u884c\u8bad\u7ec3\u3002 DiffAssemble \u5728\u5927\u591a\u6570 2D \u548c 3D \u91cd\u7ec4\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb (SOTA) \u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u89e3\u51b3 2D \u65cb\u8f6c\u548c\u5e73\u79fb\u96be\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f3a\u8c03\u5b83\u663e\u7740\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u95f4\uff0c\u5176\u6267\u884c\u901f\u5ea6\u6bd4\u6700\u5feb\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u89e3\u8c1c\u65b9\u6cd5\u5feb 11 \u500d\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/IIT-PAVIS/DiffAssemble \u83b7\u53d6|[2402.19302v1](http://arxiv.org/pdf/2402.19302v1)|null|\n", "2402.19215": "|**2024-02-29**|**Training Generative Image Super-Resolution Models by Wavelet-Domain Losses Enables Better Control of Artifacts**|\u901a\u8fc7\u5c0f\u6ce2\u57df\u635f\u5931\u8bad\u7ec3\u751f\u6210\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u53ef\u4ee5\u66f4\u597d\u5730\u63a7\u5236\u4f2a\u5f71|Cansu Korkmaz, A. Murat Tekalp, Zafer Dogan|Super-resolution (SR) is an ill-posed inverse problem, where the size of the set of feasible solutions that are consistent with a given low-resolution image is very large. Many algorithms have been proposed to find a \"good\" solution among the feasible solutions that strike a balance between fidelity and perceptual quality. Unfortunately, all known methods generate artifacts and hallucinations while trying to reconstruct high-frequency (HF) image details. A fundamental question is: Can a model learn to distinguish genuine image details from artifacts? Although some recent works focused on the differentiation of details and artifacts, this is a very challenging problem and a satisfactory solution is yet to be found. This paper shows that the characterization of genuine HF details versus artifacts can be better learned by training GAN-based SR models using wavelet-domain loss functions compared to RGB-domain or Fourier-space losses. Although wavelet-domain losses have been used in the literature before, they have not been used in the context of the SR task. More specifically, we train the discriminator only on the HF wavelet sub-bands instead of on RGB images and the generator is trained by a fidelity loss over wavelet subbands to make it sensitive to the scale and orientation of structures. Extensive experimental results demonstrate that our model achieves better perception-distortion trade-off according to multiple objective measures and visual evaluations.|\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u662f\u4e00\u4e2a\u4e0d\u9002\u5b9a\u7684\u53cd\u95ee\u9898\uff0c\u5176\u4e2d\u4e0e\u7ed9\u5b9a\u7684\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e00\u81f4\u7684\u53ef\u884c\u89e3\u96c6\u7684\u5927\u5c0f\u975e\u5e38\u5927\u3002\u4eba\u4eec\u5df2\u7ecf\u63d0\u51fa\u4e86\u8bb8\u591a\u7b97\u6cd5\u6765\u5728\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u4e2d\u627e\u5230\u201c\u597d\u7684\u201d\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u5728\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u6240\u6709\u5df2\u77e5\u7684\u65b9\u6cd5\u5728\u5c1d\u8bd5\u91cd\u5efa\u9ad8\u9891\uff08HF\uff09\u56fe\u50cf\u7ec6\u8282\u65f6\u90fd\u4f1a\u4ea7\u751f\u4f2a\u5f71\u548c\u5e7b\u89c9\u3002\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u662f\uff1a\u6a21\u578b\u80fd\u5426\u5b66\u4f1a\u533a\u5206\u771f\u5b9e\u56fe\u50cf\u7ec6\u8282\u548c\u4f2a\u5f71\uff1f\u5c3d\u7ba1\u6700\u8fd1\u7684\u4e00\u4e9b\u5de5\u4f5c\u4fa7\u91cd\u4e8e\u7ec6\u8282\u548c\u4f2a\u5f71\u7684\u533a\u5206\uff0c\u4f46\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5c1a\u672a\u627e\u5230\u4ee4\u4eba\u6ee1\u610f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u8868\u660e\uff0c\u4e0e RGB \u57df\u6216\u5085\u7acb\u53f6\u7a7a\u95f4\u635f\u5931\u76f8\u6bd4\uff0c\u901a\u8fc7\u4f7f\u7528\u5c0f\u6ce2\u57df\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u57fa\u4e8e GAN \u7684 SR \u6a21\u578b\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5b66\u4e60\u771f\u5b9e HF \u7ec6\u8282\u4e0e\u4f2a\u5f71\u7684\u8868\u5f81\u3002\u5c3d\u7ba1\u5c0f\u6ce2\u57df\u635f\u5931\u4e4b\u524d\u5df2\u5728\u6587\u732e\u4e2d\u4f7f\u7528\u8fc7\uff0c\u4f46\u5c1a\u672a\u5728 SR \u4efb\u52a1\u4e2d\u4f7f\u7528\u8fc7\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u4ec5\u5728 HF \u5c0f\u6ce2\u5b50\u5e26\u4e0a\u800c\u4e0d\u662f\u5728 RGB \u56fe\u50cf\u4e0a\u8bad\u7ec3\u9274\u522b\u5668\uff0c\u5e76\u4e14\u901a\u8fc7\u5c0f\u6ce2\u5b50\u5e26\u4e0a\u7684\u4fdd\u771f\u5ea6\u635f\u5931\u6765\u8bad\u7ec3\u751f\u6210\u5668\uff0c\u4ee5\u4f7f\u5176\u5bf9\u7ed3\u6784\u7684\u5c3a\u5ea6\u548c\u65b9\u5411\u654f\u611f\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u6839\u636e\u591a\u79cd\u5ba2\u89c2\u6d4b\u91cf\u548c\u89c6\u89c9\u8bc4\u4f30\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u611f\u77e5\u5931\u771f\u6743\u8861\u3002|[2402.19215v1](http://arxiv.org/pdf/2402.19215v1)|null|\n", "2402.19186": "|**2024-02-29**|**Disentangling representations of retinal images with generative models**|\u7528\u751f\u6210\u6a21\u578b\u89e3\u5f00\u89c6\u7f51\u819c\u56fe\u50cf\u7684\u8868\u793a|Sarah M\u00fcller, Lisa M. Koch, Hendrik P. A. Lensch, Philipp Berens|Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation. To achieve this, we propose a novel disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we demonstrate the effectiveness of this novel loss function in disentangling the learned subspaces. Our results show that our model provides a new perspective on the complex relationship between patient attributes and technical confounders in retinal fundus image generation.|\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u5728\u773c\u90e8\u75be\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u751a\u81f3\u8bc1\u660e\u4e86\u5b83\u4eec\u5728\u68c0\u6d4b\u5fc3\u8840\u7ba1\u5371\u9669\u56e0\u7d20\u548c\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u65b9\u9762\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u6280\u672f\u56e0\u7d20\u5bf9\u8fd9\u4e9b\u56fe\u50cf\u7684\u5f71\u54cd\u53ef\u80fd\u4f1a\u7ed9\u773c\u79d1\u4e2d\u53ef\u9760\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u5e26\u6765\u6311\u6218\u3002\u4f8b\u5982\uff0c\u5927\u578b\u773c\u5e95\u7fa4\u4f53\u5e38\u5e38\u53d7\u5230\u76f8\u673a\u7c7b\u578b\u3001\u56fe\u50cf\u8d28\u91cf\u6216\u7167\u660e\u6c34\u5e73\u7b49\u56e0\u7d20\u7684\u56f0\u6270\uff0c\u627f\u62c5\u7740\u5b66\u4e60\u6377\u5f84\u7684\u98ce\u9669\uff0c\u800c\u4e0d\u662f\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u80cc\u540e\u7684\u56e0\u679c\u5173\u7cfb\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u7fa4\u4f53\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6709\u6548\u5730\u5c06\u60a3\u8005\u5c5e\u6027\u4e0e\u76f8\u673a\u6548\u679c\u5206\u5f00\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u63a7\u4e14\u9ad8\u5ea6\u903c\u771f\u7684\u56fe\u50cf\u751f\u6210\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u76f8\u5173\u6027\u7684\u65b0\u578b\u89e3\u7f20\u7ed3\u635f\u5931\u3002\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u79cd\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\u5728\u89e3\u5f00\u5b66\u4e60\u5b50\u7a7a\u95f4\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4e3a\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u751f\u6210\u4e2d\u60a3\u8005\u5c5e\u6027\u548c\u6280\u672f\u6df7\u6742\u56e0\u7d20\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002|[2402.19186v1](http://arxiv.org/pdf/2402.19186v1)|null|\n", "2402.19062": "|**2024-02-29**|**Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach**|\u7528\u4e8e\u81ea\u52a8\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u56fe\u8bc6\u522b\u7684\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff1a\u6574\u4f53\u65b9\u6cd5|Sarina Thomas, Cristiana Tiago, B\u00f8rge Solli Andreassen, Svein-Arne Aase, Jurica Sprem, Erik Steen, Anne Solberg, Guy Ben-Yosef|To facilitate diagnosis on cardiac ultrasound (US), clinical practice has established several standard views of the heart, which serve as reference points for diagnostic measurements and define viewports from which images are acquired. Automatic view recognition involves grouping those images into classes of standard views. Although deep learning techniques have been successful in achieving this, they still struggle with fully verifying the suitability of an image for specific measurements due to factors like the correct location, pose, and potential occlusions of cardiac structures. Our approach goes beyond view classification and incorporates a 3D mesh reconstruction of the heart that enables several more downstream tasks, like segmentation and pose estimation. In this work, we explore learning 3D heart meshes via graph convolutions, using similar techniques to learn 3D meshes in natural images, such as human pose estimation. As the availability of fully annotated 3D images is limited, we generate synthetic US images from 3D meshes by training an adversarial denoising diffusion model. Experiments were conducted on synthetic and clinical cases for view recognition and structure detection. The approach yielded good performance on synthetic images and, despite being exclusively trained on synthetic data, it already showed potential when applied to clinical images. With this proof-of-concept, we aim to demonstrate the benefits of graphs to improve cardiac view recognition that can ultimately lead to better efficiency in cardiac diagnosis.|\u4e3a\u4e86\u4fc3\u8fdb\u5fc3\u810f\u8d85\u58f0 (US) \u8bca\u65ad\uff0c\u4e34\u5e8a\u5b9e\u8df5\u5efa\u7acb\u4e86\u591a\u4e2a\u5fc3\u810f\u6807\u51c6\u89c6\u56fe\uff0c\u8fd9\u4e9b\u89c6\u56fe\u53ef\u4f5c\u4e3a\u8bca\u65ad\u6d4b\u91cf\u7684\u53c2\u8003\u70b9\u5e76\u5b9a\u4e49\u4ece\u4e2d\u83b7\u53d6\u56fe\u50cf\u7684\u89c6\u53e3\u3002\u81ea\u52a8\u89c6\u56fe\u8bc6\u522b\u6d89\u53ca\u5c06\u8fd9\u4e9b\u56fe\u50cf\u5206\u7ec4\u4e3a\u6807\u51c6\u89c6\u56fe\u7c7b\u522b\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5df2\u7ecf\u6210\u529f\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\uff0c\u4f46\u7531\u4e8e\u5fc3\u810f\u7ed3\u6784\u7684\u6b63\u786e\u4f4d\u7f6e\u3001\u59ff\u52bf\u548c\u6f5c\u5728\u95ed\u585e\u7b49\u56e0\u7d20\uff0c\u5b83\u4eec\u4ecd\u7136\u96be\u4ee5\u5145\u5206\u9a8c\u8bc1\u56fe\u50cf\u5bf9\u4e8e\u7279\u5b9a\u6d4b\u91cf\u7684\u9002\u7528\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u8d85\u8d8a\u4e86\u89c6\u56fe\u5206\u7c7b\uff0c\u5e76\u7ed3\u5408\u4e86\u5fc3\u810f\u7684 3D \u7f51\u683c\u91cd\u5efa\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u591a\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f8b\u5982\u5206\u5272\u548c\u59ff\u52bf\u4f30\u8ba1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u901a\u8fc7\u56fe\u5377\u79ef\u5b66\u4e60 3D \u5fc3\u810f\u7f51\u683c\uff0c\u4f7f\u7528\u7c7b\u4f3c\u7684\u6280\u672f\u6765\u5b66\u4e60\u81ea\u7136\u56fe\u50cf\u4e2d\u7684 3D \u7f51\u683c\uff0c\u4f8b\u5982\u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u3002\u7531\u4e8e\u5b8c\u5168\u6ce8\u91ca\u7684 3D \u56fe\u50cf\u7684\u53ef\u7528\u6027\u6709\u9650\uff0c\u6211\u4eec\u901a\u8fc7\u8bad\u7ec3\u5bf9\u6297\u6027\u53bb\u566a\u6269\u6563\u6a21\u578b\u4ece \u200b\u200b3D \u7f51\u683c\u751f\u6210\u5408\u6210 US \u56fe\u50cf\u3002\u5728\u5408\u6210\u548c\u4e34\u5e8a\u6848\u4f8b\u4e0a\u8fdb\u884c\u4e86\u89c6\u56fe\u8bc6\u522b\u548c\u7ed3\u6784\u68c0\u6d4b\u7684\u5b9e\u9a8c\u3002\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u56fe\u50cf\u4e0a\u4ea7\u751f\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u5c3d\u7ba1\u4e13\u95e8\u9488\u5bf9\u5408\u6210\u6570\u636e\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u4f46\u5b83\u5728\u5e94\u7528\u4e8e\u4e34\u5e8a\u56fe\u50cf\u65f6\u5df2\u7ecf\u663e\u793a\u51fa\u6f5c\u529b\u3002\u901a\u8fc7\u8fd9\u4e00\u6982\u5ff5\u9a8c\u8bc1\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u5c55\u793a\u56fe\u8868\u5728\u6539\u5584\u5fc3\u810f\u89c6\u56fe\u8bc6\u522b\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u6700\u7ec8\u63d0\u9ad8\u5fc3\u810f\u8bca\u65ad\u7684\u6548\u7387\u3002|[2402.19062v1](http://arxiv.org/pdf/2402.19062v1)|null|\n", "2402.19043": "|**2024-02-29**|**WDM: 3D Wavelet Diffusion Models for High-Resolution Medical Image Synthesis**|WDM\uff1a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u533b\u5b66\u56fe\u50cf\u5408\u6210\u7684 3D \u5c0f\u6ce2\u6269\u6563\u6a21\u578b|Paul Friedrich, Julia Wolleb, Florentin Bieder, Alicia Durrer, Philippe C. Cattin|Due to the three-dimensional nature of CT- or MR-scans, generative modeling of medical images is a particularly challenging task. Existing approaches mostly apply patch-wise, slice-wise, or cascaded generation techniques to fit the high-dimensional data into the limited GPU memory. However, these approaches may introduce artifacts and potentially restrict the model's applicability for certain downstream tasks. This work presents WDM, a wavelet-based medical image synthesis framework that applies a diffusion model on wavelet decomposed images. The presented approach is a simple yet effective way of scaling diffusion models to high resolutions and can be trained on a single 40 GB GPU. Experimental results on BraTS and LIDC-IDRI unconditional image generation at a resolution of $128 \\times 128 \\times 128$ show state-of-the-art image fidelity (FID) and sample diversity (MS-SSIM) scores compared to GANs, Diffusion Models, and Latent Diffusion Models. Our proposed method is the only one capable of generating high-quality images at a resolution of $256 \\times 256 \\times 256$.|\u7531\u4e8e CT \u6216 MR \u626b\u63cf\u7684\u4e09\u7ef4\u6027\u8d28\uff0c\u533b\u5b66\u56fe\u50cf\u7684\u751f\u6210\u5efa\u6a21\u662f\u4e00\u9879\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u91c7\u7528 patch-wise\u3001slice-wise \u6216\u7ea7\u8054\u751f\u6210\u6280\u672f\u6765\u5c06\u9ad8\u7ef4\u6570\u636e\u653e\u5165\u6709\u9650\u7684 GPU \u5185\u5b58\u4e2d\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u4f1a\u5f15\u5165\u5de5\u4ef6\uff0c\u5e76\u53ef\u80fd\u9650\u5236\u6a21\u578b\u5bf9\u67d0\u4e9b\u4e0b\u6e38\u4efb\u52a1\u7684\u9002\u7528\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86 WDM\uff0c\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7684\u533b\u5b66\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u5b83\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u5c0f\u6ce2\u5206\u89e3\u56fe\u50cf\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u5c06\u6269\u6563\u6a21\u578b\u6269\u5c55\u5230\u9ad8\u5206\u8fa8\u7387\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5355\u4e2a 40 GB GPU \u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u5206\u8fa8\u7387\u4e3a $128 \u00d7 128 \u00d7 128$ \u7684 BraTS \u548c LIDC-IDRI \u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e GAN\u3001\u6269\u6563\u76f8\u6bd4\uff0c\u5177\u6709\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u4fdd\u771f\u5ea6 (FID) \u548c\u6837\u672c\u591a\u6837\u6027 (MS-SSIM) \u5206\u6570\u6a21\u578b\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u552f\u4e00\u80fd\u591f\u751f\u6210\u5206\u8fa8\u7387\u4e3a 256 \u00d7 256 \u00d7 256$ \u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u65b9\u6cd5\u3002|[2402.19043v1](http://arxiv.org/pdf/2402.19043v1)|null|\n", "2402.18879": "|**2024-02-29**|**Dose Prediction Driven Radiotherapy Paramters Regression via Intra- and Inter-Relation Modeling**|\u901a\u8fc7\u5185\u5173\u7cfb\u548c\u76f8\u4e92\u5173\u7cfb\u5efa\u6a21\u8fdb\u884c\u5242\u91cf\u9884\u6d4b\u9a71\u52a8\u7684\u653e\u5c04\u6cbb\u7597\u53c2\u6570\u56de\u5f52|Jiaqi Cui, Yuanyuan Xu, Jianghong Xiao, Yuchen Fei, Jiliu Zhou, Xingcheng Peng, Yan Wang|Deep learning has facilitated the automation of radiotherapy by predicting accurate dose distribution maps. However, existing methods fail to derive the desirable radiotherapy parameters that can be directly input into the treatment planning system (TPS), impeding the full automation of radiotherapy. To enable more thorough automatic radiotherapy, in this paper, we propose a novel two-stage framework to directly regress the radiotherapy parameters, including a dose map prediction stage and a radiotherapy parameters regression stage. In stage one, we combine transformer and convolutional neural network (CNN) to predict realistic dose maps with rich global and local information, providing accurate dosimetric knowledge for the subsequent parameters regression. In stage two, two elaborate modules, i.e., an intra-relation modeling (Intra-RM) module and an inter-relation modeling (Inter-RM) module, are designed to exploit the organ-specific and organ-shared features for precise parameters regression. Experimental results on a rectal cancer dataset demonstrate the effectiveness of our method.|\u6df1\u5ea6\u5b66\u4e60\u901a\u8fc7\u9884\u6d4b\u51c6\u786e\u7684\u5242\u91cf\u5206\u5e03\u56fe\u4fc3\u8fdb\u4e86\u653e\u5c04\u6cbb\u7597\u7684\u81ea\u52a8\u5316\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5f97\u51fa\u53ef\u76f4\u63a5\u8f93\u5165\u6cbb\u7597\u8ba1\u5212\u7cfb\u7edf\uff08TPS\uff09\u7684\u7406\u60f3\u653e\u7597\u53c2\u6570\uff0c\u963b\u788d\u4e86\u653e\u7597\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u3002\u4e3a\u4e86\u5b9e\u73b0\u66f4\u5f7b\u5e95\u7684\u81ea\u52a8\u653e\u7597\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u6765\u76f4\u63a5\u56de\u5f52\u653e\u7597\u53c2\u6570\uff0c\u5305\u62ec\u5242\u91cf\u56fe\u9884\u6d4b\u9636\u6bb5\u548c\u653e\u7597\u53c2\u6570\u56de\u5f52\u9636\u6bb5\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u7ed3\u5408\u53d8\u538b\u5668\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6765\u9884\u6d4b\u5177\u6709\u4e30\u5bcc\u7684\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u7684\u771f\u5b9e\u5242\u91cf\u56fe\uff0c\u4e3a\u540e\u7eed\u53c2\u6570\u56de\u5f52\u63d0\u4f9b\u51c6\u786e\u7684\u5242\u91cf\u5b66\u77e5\u8bc6\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u590d\u6742\u7684\u6a21\u5757\uff0c\u5373\u5185\u90e8\u5173\u7cfb\u5efa\u6a21\uff08Intra-RM\uff09\u6a21\u5757\u548c\u76f8\u4e92\u5173\u7cfb\u5efa\u6a21\uff08Inter-RM\uff09\u6a21\u5757\uff0c\u4ee5\u5229\u7528\u5668\u5b98\u7279\u5b9a\u548c\u5668\u5b98\u5171\u4eab\u7684\u7279\u5f81\u6765\u83b7\u53d6\u7cbe\u786e\u7684\u53c2\u6570\u56de\u5f52\u3002\u76f4\u80a0\u764c\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2402.18879v1](http://arxiv.org/pdf/2402.18879v1)|null|\n", "2402.18849": "|**2024-02-29**|**Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence**|\u589e\u5f3a\u9690\u5199\u6587\u672c\u63d0\u53d6\uff1a\u8bc4\u4f30 NLP \u6a21\u578b\u5bf9\u51c6\u786e\u6027\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u5f71\u54cd|Mingyang Li, Maoqin Yuan, Luyao Li, Han Pengsihua|This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of information hiding. The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems.|\u672c\u7814\u7a76\u8ba8\u8bba\u4e86\u4e00\u79cd\u5c06\u56fe\u50cf\u9690\u5199\u6280\u672f\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5927\u578b\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u63d0\u53d6\u9690\u5199\u6587\u672c\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u4f20\u7edf\u7684\u6700\u4f4e\u6709\u6548\u4f4d\uff08LSB\uff09\u9690\u5199\u6280\u672f\u5728\u5904\u7406\u590d\u6742\u7684\u5b57\u7b26\u7f16\u7801\uff08\u4f8b\u5982\u6c49\u5b57\uff09\u65f6\u9762\u4e34\u4fe1\u606f\u63d0\u53d6\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u7684\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684 LSB-NLP \u6df7\u5408\u6846\u67b6\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86NLP\u5927\u578b\u6a21\u578b\u7684\u5148\u8fdb\u80fd\u529b\uff0c\u5982\u9519\u8bef\u68c0\u6d4b\u3001\u7ea0\u6b63\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u5206\u6790\u4ee5\u53ca\u4fe1\u606f\u91cd\u6784\u6280\u672f\uff0c\u4ece\u800c\u663e\u7740\u589e\u5f3a\u4e86\u9690\u5199\u6587\u672c\u63d0\u53d6\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLSB-NLP\u6df7\u5408\u6846\u67b6\u5728\u63d0\u9ad8\u9690\u5199\u6587\u672c\u7684\u63d0\u53d6\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6c49\u5b57\u65b9\u9762\u3002\u8be5\u7814\u7a76\u7ed3\u679c\u4e0d\u4ec5\u8bc1\u5b9e\u4e86\u56fe\u50cf\u9690\u5199\u6280\u672f\u4e0eNLP\u5927\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u800c\u4e14\u4e3a\u4fe1\u606f\u9690\u85cf\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u51fa\u4e86\u65b0\u7684\u601d\u8def\u3002\u8fd9\u79cd\u8de8\u5b66\u79d1\u65b9\u6cd5\u7684\u6210\u529f\u5b9e\u65bd\u5c55\u793a\u4e86\u56fe\u50cf\u9690\u5199\u6280\u672f\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u76f8\u7ed3\u5408\u5728\u89e3\u51b3\u590d\u6742\u4fe1\u606f\u5904\u7406\u95ee\u9898\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002|[2402.18849v1](http://arxiv.org/pdf/2402.18849v1)|null|\n", "2402.18842": "|**2024-02-29**|**ViewFusion: Towards Multi-View Consistency via Interpolated Denoising**|ViewFusion\uff1a\u901a\u8fc7\u63d2\u503c\u53bb\u566a\u5b9e\u73b0\u591a\u89c6\u56fe\u4e00\u81f4\u6027|Xianghui Yang, Yan Zuo, Sameera Ramasinghe, Loris Bazzani, Gil Avraham, Anton van den Hengel|Novel-view synthesis through diffusion models has demonstrated remarkable potential for generating diverse and high-quality images. Yet, the independent process of image generation in these prevailing methods leads to challenges in maintaining multiple-view consistency. To address this, we introduce ViewFusion, a novel, training-free algorithm that can be seamlessly integrated into existing pre-trained diffusion models. Our approach adopts an auto-regressive method that implicitly leverages previously generated views as context for the next view generation, ensuring robust multi-view consistency during the novel-view generation process. Through a diffusion process that fuses known-view information via interpolated denoising, our framework successfully extends single-view conditioned models to work in multiple-view conditional settings without any additional fine-tuning. Extensive experimental results demonstrate the effectiveness of ViewFusion in generating consistent and detailed novel views.|\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u65b0\u9896\u89c6\u56fe\u5408\u6210\u5df2\u663e\u793a\u51fa\u751f\u6210\u591a\u6837\u5316\u548c\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6d41\u884c\u65b9\u6cd5\u4e2d\u56fe\u50cf\u751f\u6210\u7684\u72ec\u7acb\u8fc7\u7a0b\u7ed9\u7ef4\u6301\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u5e26\u6765\u4e86\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ViewFusion\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u514d\u8bad\u7ec3\u7b97\u6cd5\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u81ea\u56de\u5f52\u65b9\u6cd5\uff0c\u9690\u5f0f\u5730\u5229\u7528\u5148\u524d\u751f\u6210\u7684\u89c6\u56fe\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u89c6\u56fe\u751f\u6210\u7684\u4e0a\u4e0b\u6587\uff0c\u786e\u4fdd\u65b0\u9896\u89c6\u56fe\u751f\u6210\u8fc7\u7a0b\u4e2d\u7a33\u5065\u7684\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u901a\u8fc7\u63d2\u503c\u53bb\u566a\u878d\u5408\u5df2\u77e5\u89c6\u56fe\u4fe1\u606f\u7684\u6269\u6563\u8fc7\u7a0b\uff0c\u6211\u4eec\u7684\u6846\u67b6\u6210\u529f\u5730\u5c06\u5355\u89c6\u56fe\u6761\u4ef6\u6a21\u578b\u6269\u5c55\u4e3a\u5728\u591a\u89c6\u56fe\u6761\u4ef6\u8bbe\u7f6e\u4e2d\u5de5\u4f5c\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u989d\u5916\u7684\u5fae\u8c03\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86 ViewFusion \u5728\u751f\u6210\u4e00\u81f4\u4e14\u8be6\u7ec6\u7684\u65b0\u9896\u89c6\u56fe\u65b9\u9762\u7684\u6709\u6548\u6027\u3002|[2402.18842v1](http://arxiv.org/pdf/2402.18842v1)|null|\n", "2402.18780": "|**2024-02-29**|**A Quantitative Evaluation of Score Distillation Sampling Based Text-to-3D**|\u57fa\u4e8e\u6587\u672c\u8f6c3D\u7684\u5206\u6570\u84b8\u998f\u91c7\u6837\u7684\u5b9a\u91cf\u8bc4\u4f30|Xiaohan Fei, Chethan Parameshwara, Jiawei Mo, Xiaolong Li, Ashwin Swaminathan, CJ Taylor, Paolo Favaro, Stefano Soatto|The development of generative models that create 3D content from a text prompt has made considerable strides thanks to the use of the score distillation sampling (SDS) method on pre-trained diffusion models for image generation. However, the SDS method is also the source of several artifacts, such as the Janus problem, the misalignment between the text prompt and the generated 3D model, and 3D model inaccuracies. While existing methods heavily rely on the qualitative assessment of these artifacts through visual inspection of a limited set of samples, in this work we propose more objective quantitative evaluation metrics, which we cross-validate via human ratings, and show analysis of the failure cases of the SDS technique. We demonstrate the effectiveness of this analysis by designing a novel computationally efficient baseline model that achieves state-of-the-art performance on the proposed metrics while addressing all the above-mentioned artifacts.|\u7531\u4e8e\u5728\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e0a\u4f7f\u7528\u4e86\u5206\u6570\u84b8\u998f\u91c7\u6837 (SDS) \u65b9\u6cd5\uff0c\u4ece\u6587\u672c\u63d0\u793a\u521b\u5efa 3D \u5185\u5bb9\u7684\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u53d6\u5f97\u4e86\u957f\u8db3\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0cSDS \u65b9\u6cd5\u4e5f\u662f\u4e00\u4e9b\u4f2a\u5f71\u7684\u6839\u6e90\uff0c\u4f8b\u5982 Janus \u95ee\u9898\u3001\u6587\u672c\u63d0\u793a\u4e0e\u751f\u6210\u7684 3D \u6a21\u578b\u4e4b\u95f4\u7684\u9519\u4f4d\u4ee5\u53ca 3D \u6a21\u578b\u4e0d\u51c6\u786e\u3002\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4e8e\u901a\u8fc7\u5bf9\u6709\u9650\u6837\u672c\u96c6\u8fdb\u884c\u76ee\u89c6\u68c0\u67e5\u6765\u5bf9\u8fd9\u4e9b\u5de5\u4ef6\u8fdb\u884c\u5b9a\u6027\u8bc4\u4f30\uff0c\u4f46\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u66f4\u5ba2\u89c2\u7684\u5b9a\u91cf\u8bc4\u4f30\u6307\u6807\uff0c\u6211\u4eec\u901a\u8fc7\u4eba\u5de5\u8bc4\u7ea7\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5e76\u663e\u793a\u5bf9\u5931\u8d25\u6848\u4f8b\u7684\u5206\u6790SDS \u6280\u672f\u3002\u6211\u4eec\u901a\u8fc7\u8bbe\u8ba1\u4e00\u79cd\u65b0\u9896\u7684\u8ba1\u7b97\u9ad8\u6548\u7684\u57fa\u7ebf\u6a21\u578b\u6765\u8bc1\u660e\u8fd9\u79cd\u5206\u6790\u7684\u6709\u6548\u6027\uff0c\u8be5\u6a21\u578b\u5728\u89e3\u51b3\u6240\u6709\u4e0a\u8ff0\u5de5\u4ef6\u7684\u540c\u65f6\uff0c\u5728\u6240\u63d0\u51fa\u7684\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2402.18780v1](http://arxiv.org/pdf/2402.18780v1)|null|\n"}, "\u591a\u6a21\u6001": {"2402.19479": "|**2024-02-29**|**Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers**|Panda-70M\uff1a\u4e0e\u591a\u4e2a\u8de8\u6a21\u6001\u6559\u5e08\u4e00\u8d77\u4e3a 70M \u89c6\u9891\u6dfb\u52a0\u5b57\u5e55|Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et.al.|The quality of the data and annotation upper-bounds the quality of a downstream model. While there exist large text corpora and image-text pairs, high-quality video-text data is much harder to collect. First of all, manual labeling is more time-consuming, as it requires an annotator to watch an entire video. Second, videos have a temporal dimension, consisting of several scenes stacked together, and showing multiple actions. Accordingly, to establish a video dataset with high-quality captions, we propose an automatic approach leveraging multimodal inputs, such as textual video description, subtitles, and individual video frames. Specifically, we curate 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. We then split them into semantically consistent video clips, and apply multiple cross-modality teacher models to obtain captions for each video. Next, we finetune a retrieval model on a small subset where the best caption of each video is manually selected and then employ the model in the whole dataset to select the best caption as the annotation. In this way, we get 70M videos paired with high-quality text captions. We dub the dataset as Panda-70M. We show the value of the proposed dataset on three downstream tasks: video captioning, video and text retrieval, and text-driven video generation. The models trained on the proposed data score substantially better on the majority of metrics across all the tasks.|\u6570\u636e\u548c\u6ce8\u91ca\u7684\u8d28\u91cf\u9650\u5236\u4e86\u4e0b\u6e38\u6a21\u578b\u7684\u8d28\u91cf\u3002\u867d\u7136\u5b58\u5728\u5927\u578b\u6587\u672c\u8bed\u6599\u5e93\u548c\u56fe\u50cf\u6587\u672c\u5bf9\uff0c\u4f46\u6536\u96c6\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u6587\u672c\u6570\u636e\u8981\u56f0\u96be\u5f97\u591a\u3002\u9996\u5148\uff0c\u624b\u52a8\u6807\u6ce8\u6bd4\u8f83\u8017\u65f6\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u6ce8\u91ca\u8005\u89c2\u770b\u6574\u4e2a\u89c6\u9891\u3002\u5176\u6b21\uff0c\u89c6\u9891\u5177\u6709\u65f6\u95f4\u7ef4\u5ea6\uff0c\u7531\u5806\u53e0\u5728\u4e00\u8d77\u7684\u591a\u4e2a\u573a\u666f\u7ec4\u6210\uff0c\u5e76\u663e\u793a\u591a\u4e2a\u52a8\u4f5c\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u5efa\u7acb\u5177\u6709\u9ad8\u8d28\u91cf\u5b57\u5e55\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6a21\u5f0f\u8f93\u5165\u7684\u81ea\u52a8\u65b9\u6cd5\uff0c\u4f8b\u5982\u6587\u672c\u89c6\u9891\u63cf\u8ff0\u3001\u5b57\u5e55\u548c\u5355\u4e2a\u89c6\u9891\u5e27\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4ece\u516c\u5f00\u7684 HD-VILA-100M \u6570\u636e\u96c6\u4e2d\u7cbe\u9009\u4e86 380 \u4e07\u4e2a\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u5b83\u4eec\u5206\u5272\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u89c6\u9891\u526a\u8f91\uff0c\u5e76\u5e94\u7528\u591a\u4e2a\u8de8\u6a21\u6001\u6559\u5e08\u6a21\u578b\u6765\u83b7\u53d6\u6bcf\u4e2a\u89c6\u9891\u7684\u5b57\u5e55\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5728\u4e00\u4e2a\u5c0f\u5b50\u96c6\u4e0a\u5fae\u8c03\u68c0\u7d22\u6a21\u578b\uff0c\u5176\u4e2d\u624b\u52a8\u9009\u62e9\u6bcf\u4e2a\u89c6\u9891\u7684\u6700\u4f73\u6807\u9898\uff0c\u7136\u540e\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e2d\u4f7f\u7528\u8be5\u6a21\u578b\u6765\u9009\u62e9\u6700\u4f73\u6807\u9898\u4f5c\u4e3a\u6ce8\u91ca\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u83b7\u5f97\u4e86 7000 \u4e07\u4e2a\u89c6\u9891\u4ee5\u53ca\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5b57\u5e55\u3002\u6211\u4eec\u5c06\u8be5\u6570\u636e\u96c6\u547d\u540d\u4e3a Panda-70M\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u5728\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u4ef7\u503c\uff1a\u89c6\u9891\u5b57\u5e55\u3001\u89c6\u9891\u548c\u6587\u672c\u68c0\u7d22\u4ee5\u53ca\u6587\u672c\u9a71\u52a8\u7684\u89c6\u9891\u751f\u6210\u3002\u6839\u636e\u6240\u63d0\u8bae\u7684\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u7684\u5927\u591a\u6570\u6307\u6807\u4e0a\u5f97\u5206\u660e\u663e\u66f4\u9ad8\u3002|[2402.19479v1](http://arxiv.org/pdf/2402.19479v1)|null|\n", "2402.19474": "|**2024-02-29**|**The All-Seeing Project V2: Towards General Relation Comprehension of the Open World**|\u5168\u89c6\u8ba1\u5212V2\uff1a\u8fc8\u5411\u5f00\u653e\u4e16\u754c\u7684\u4e00\u822c\u5173\u7cfb\u7406\u89e3|Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et.al.|We present the All-Seeing Project V2: a new model and dataset designed for understanding object relations in images. Specifically, we propose the All-Seeing Model V2 (ASMv2) that integrates the formulation of text generation, object localization, and relation comprehension into a relation conversation (ReC) task. Leveraging this unified task, our model excels not only in perceiving and recognizing all objects within the image but also in grasping the intricate relation graph between them, diminishing the relation hallucination often encountered by Multi-modal Large Language Models (MLLMs). To facilitate training and evaluation of MLLMs in relation understanding, we created the first high-quality ReC dataset ({AS-V2) which is aligned with the format of standard instruction tuning data. In addition, we design a new benchmark, termed Circular-based Relation Probing Evaluation (CRPE) for comprehensively evaluating the relation comprehension capabilities of MLLMs. Notably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware benchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that our work can inspire more future research and contribute to the evolution towards artificial general intelligence. Our project is released at https://github.com/OpenGVLab/all-seeing.|\u6211\u4eec\u63d0\u51fa\u4e86 All-Seeing Project V2\uff1a\u4e00\u4e2a\u65b0\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u65e8\u5728\u7406\u89e3\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u5173\u7cfb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 All-Seeing Model V2 (ASMv2)\uff0c\u5b83\u5c06\u6587\u672c\u751f\u6210\u3001\u5bf9\u8c61\u5b9a\u4f4d\u548c\u5173\u7cfb\u7406\u89e3\u7684\u516c\u5f0f\u96c6\u6210\u5230\u5173\u7cfb\u5bf9\u8bdd (ReC) \u4efb\u52a1\u4e2d\u3002\u5229\u7528\u8fd9\u4e2a\u7edf\u4e00\u7684\u4efb\u52a1\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4e0d\u4ec5\u5728\u611f\u77e5\u548c\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u6240\u6709\u5bf9\u8c61\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u800c\u4e14\u5728\u638c\u63e1\u5b83\u4eec\u4e4b\u95f4\u590d\u6742\u7684\u5173\u7cfb\u56fe\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7ecf\u5e38\u9047\u5230\u7684\u5173\u7cfb\u5e7b\u89c9\u3002\u4e3a\u4e86\u4fc3\u8fdb\u5173\u7cfb\u7406\u89e3\u65b9\u9762\u7684 MLLM \u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u7b2c\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684 ReC \u6570\u636e\u96c6\uff08{AS-V2\uff09\uff0c\u8be5\u6570\u636e\u96c6\u4e0e\u6807\u51c6\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u7684\u683c\u5f0f\u4fdd\u6301\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u79f0\u4e3a\u57fa\u4e8e\u5faa\u73af\u7684\u5173\u7cfb\u63a2\u6d4b\u8bc4\u4f30\uff08CRPE\uff09\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30 MLLM \u7684\u5173\u7cfb\u7406\u89e3\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684 ASMv2 \u5728\u8fd9\u4e2a\u5173\u7cfb\u611f\u77e5\u57fa\u51c6\u4e0a\u7684\u6574\u4f53\u51c6\u786e\u7387\u8fbe\u5230\u4e86 52.04\uff0c\u5927\u5927\u8d85\u8fc7\u4e86 LLaVA-1.5 \u7684 43.14\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u5de5\u4f5c\u80fd\u591f\u6fc0\u53d1\u66f4\u591a\u672a\u6765\u7684\u7814\u7a76\uff0c\u5e76\u4e3a\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u505a\u51fa\u8d21\u732e\u3002\u6211\u4eec\u7684\u9879\u76ee\u53d1\u5e03\u4e8e https://github.com/OpenGVLab/all-seeing\u3002|[2402.19474v1](http://arxiv.org/pdf/2402.19474v1)|null|\n", "2402.19467": "|**2024-02-29**|**TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning**|TV-TREES\uff1a\u7528\u4e8e\u795e\u7ecf\u7b26\u53f7\u89c6\u9891\u63a8\u7406\u7684\u591a\u6a21\u6001\u8574\u6db5\u6811|Kate Sanders, Nathaniel Weir, Benjamin Van Durme|It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.|\u5bf9\u7535\u89c6\u526a\u8f91\u7b49\u590d\u6742\u7684\u591a\u6a21\u5f0f\u5185\u5bb9\u8fdb\u884c\u95ee\u7b54\u5177\u6709\u6311\u6218\u6027\u3002\u90e8\u5206\u539f\u56e0\u662f\u5f53\u524d\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u4e8e\u5355\u4e00\u6a21\u6001\u63a8\u7406\uff0c\u5728\u957f\u8f93\u5165\u4e0a\u7684\u6027\u80fd\u8f83\u4f4e\uff0c\u5e76\u4e14\u7f3a\u4e4f\u4e92\u7528\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86 TV-TREES\uff0c\u7b2c\u4e00\u4e2a\u591a\u6a21\u6001\u8574\u6db5\u6811\u751f\u6210\u5668\u3002 TV-TREES \u662f\u4e00\u79cd\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u89c6\u9891\u76f4\u63a5\u8574\u6db5\u7684\u7b80\u5355\u524d\u63d0\u548c\u66f4\u9ad8\u5c42\u6b21\u7684\u7ed3\u8bba\u4e4b\u95f4\u751f\u6210\u8574\u6db5\u5173\u7cfb\u6811\uff0c\u4fc3\u8fdb\u53ef\u89e3\u91ca\u7684\u8054\u5408\u6a21\u6001\u63a8\u7406\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f15\u5165\u591a\u6a21\u6001\u8574\u6db5\u6811\u751f\u6210\u7684\u4efb\u52a1\u6765\u8bc4\u4f30\u6b64\u7c7b\u65b9\u6cd5\u7684\u63a8\u7406\u8d28\u91cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684 TVQA \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u5728\u5b8c\u6574\u89c6\u9891\u526a\u8f91\u4e0a\u7684\u4e0d\u53ef\u89e3\u91ca\u7684\u3001\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u8bf4\u660e\u4e86\u4e0e\u9ed1\u76d2\u65b9\u6cd5\u7684\u4e24\u5168\u5176\u7f8e\u7684\u5bf9\u6bd4\u3002|[2402.19467v1](http://arxiv.org/pdf/2402.19467v1)|null|\n", "2402.19405": "|**2024-02-29**|**Navigating Hallucinations for Reasoning of Unintentional Activities**|\u5bfc\u822a\u5e7b\u89c9\u4ee5\u63a8\u7406\u65e0\u610f\u8bc6\u7684\u6d3b\u52a8|Shresth Grover, Vibhav Vineet, Yogesh S Rawat|In this work we present a novel task of understanding unintentional human activities in videos. We formalize this problem as a reasoning task under zero-shot scenario, where given a video of an unintentional activity we want to know why it transitioned from intentional to unintentional. We first evaluate the effectiveness of current state-of-the-art Large Multimodal Models on this reasoning task and observe that they suffer from hallucination. We further propose a novel prompting technique,termed as Dream of Thoughts (DoT), which allows the model to navigate through hallucinated thoughts to achieve better reasoning. To evaluate the performance on this task, we also introduce three different specialized metrics designed to quantify the models reasoning capability. We perform our experiments on two different datasets, OOPs and UCF-Crimes, and our findings show that DOT prompting technique is able to outperform standard prompting, while minimizing hallucinations.|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u9879\u7406\u89e3\u89c6\u9891\u4e2d\u65e0\u610f\u8bc6\u7684\u4eba\u7c7b\u6d3b\u52a8\u7684\u65b0\u4efb\u52a1\u3002\u6211\u4eec\u5c06\u8fd9\u4e2a\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u65e0\u610f\u8bc6\u6d3b\u52a8\u7684\u89c6\u9891\uff0c\u6211\u4eec\u60f3\u77e5\u9053\u4e3a\u4ec0\u4e48\u5b83\u4ece\u6709\u610f\u8f6c\u53d8\u4e3a\u65e0\u610f\u3002\u6211\u4eec\u9996\u5148\u8bc4\u4f30\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u8fd9\u4e00\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u89c2\u5bdf\u5230\u5b83\u4eec\u60a3\u6709\u5e7b\u89c9\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63d0\u793a\u6280\u672f\uff0c\u79f0\u4e3a\u201c\u601d\u60f3\u4e4b\u68a6\u201d\uff08DoT\uff09\uff0c\u5b83\u5141\u8bb8\u6a21\u578b\u5bfc\u822a\u5e7b\u89c9\u601d\u60f3\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u63a8\u7406\u3002\u4e3a\u4e86\u8bc4\u4f30\u6b64\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u4e13\u95e8\u6307\u6807\uff0c\u65e8\u5728\u91cf\u5316\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\uff08OOP \u548c UCF-Crimes\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cDOT \u63d0\u793a\u6280\u672f\u80fd\u591f\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5e7b\u89c9\u3002|[2402.19405v1](http://arxiv.org/pdf/2402.19405v1)|null|\n", "2402.19404": "|**2024-02-29**|**Entity-Aware Multimodal Alignment Framework for News Image Captioning**|\u7528\u4e8e\u65b0\u95fb\u56fe\u50cf\u5b57\u5e55\u7684\u5b9e\u4f53\u611f\u77e5\u591a\u6a21\u6001\u5bf9\u9f50\u6846\u67b6|Junzhe Zhang, Huixuan Zhang, Xiaojun Wan|News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -> 86.29) on GoodNews dataset and (70.83 -> 85.61) on NYTimes800k dataset.|\u65b0\u95fb\u56fe\u50cf\u5b57\u5e55\u4efb\u52a1\u662f\u56fe\u50cf\u5b57\u5e55\u4efb\u52a1\u7684\u4e00\u79cd\u53d8\u4f53\uff0c\u5b83\u9700\u8981\u6a21\u578b\u7528\u65b0\u95fb\u56fe\u50cf\u548c\u76f8\u5173\u65b0\u95fb\u6587\u7ae0\u751f\u6210\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u5b57\u5e55\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fd1\u5e74\u6765\u53d1\u5c55\u8fc5\u901f\uff0c\u5728\u65b0\u95fb\u56fe\u50cf\u5b57\u5e55\u4efb\u52a1\u4e2d\u5177\u6709\u5e7f\u9614\u7684\u524d\u666f\u3002\u7136\u800c\uff0c\u6839\u636e\u6211\u4eec\u7684\u5b9e\u9a8c\uff0c\u5e38\u89c1\u7684 MLLM \u4e0d\u64c5\u957f\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u751f\u6210\u5b9e\u4f53\u3002\u5728\u65b0\u95fb\u56fe\u50cf\u5b57\u5e55\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7b80\u5355\u5fae\u8c03\u540e\uff0c\u5b83\u4eec\u5904\u7406\u5b9e\u4f53\u4fe1\u606f\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u4e3a\u4e86\u83b7\u5f97\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u6765\u5904\u7406\u591a\u6a21\u6001\u5b9e\u4f53\u4fe1\u606f\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u591a\u6a21\u6001\u5b9e\u4f53\u611f\u77e5\u5bf9\u9f50\u4efb\u52a1\u548c\u4e00\u4e2a\u5bf9\u9f50\u6846\u67b6\u6765\u5bf9\u9f50\u6a21\u578b\u5e76\u751f\u6210\u65b0\u95fb\u56fe\u50cf\u6807\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 GoodNews \u6570\u636e\u96c6\u4e0a\u7684 CIDEr \u5206\u6570\uff0872.33 -> 86.29\uff09\u548c NYTimes800k \u6570\u636e\u96c6\u4e0a\u7684 CIDEr \u5206\u6570\uff0870.83 -> 85.61\uff09\u53d6\u5f97\u4e86\u6bd4\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u66f4\u597d\u7684\u7ed3\u679c\u3002|[2402.19404v1](http://arxiv.org/pdf/2402.19404v1)|null|\n", "2402.19298": "|**2024-02-29**|**Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing**|\u6291\u5236\u548c\u91cd\u65b0\u5e73\u8861\uff1a\u8fc8\u5411\u5e7f\u4e49\u591a\u6a21\u6001\u4eba\u8138\u53cd\u6b3a\u9a97|Xun Lin, Shuai Wang, Rizhao Cai, Yizhong Liu, Ying Fu, Zitong Yu, Wenzhong Tang, Alex Kot|Face Anti-Spoofing (FAS) is crucial for securing face recognition systems against presentation attacks. With advancements in sensor manufacture and multi-modal learning techniques, many multi-modal FAS approaches have emerged. However, they face challenges in generalizing to unseen attacks and deployment conditions. These challenges arise from (1) modality unreliability, where some modality sensors like depth and infrared undergo significant domain shifts in varying environments, leading to the spread of unreliable information during cross-modal feature fusion, and (2) modality imbalance, where training overly relies on a dominant modality hinders the convergence of others, reducing effectiveness against attack types that are indistinguishable sorely using the dominant modality. To address modality unreliability, we propose the Uncertainty-Guided Cross-Adapter (U-Adapter) to recognize unreliably detected regions within each modality and suppress the impact of unreliable regions on other modalities. For modality imbalance, we propose a Rebalanced Modality Gradient Modulation (ReGrad) strategy to rebalance the convergence speed of all modalities by adaptively adjusting their gradients. Besides, we provide the first large-scale benchmark for evaluating multi-modal FAS performance under domain generalization scenarios. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. Source code and protocols will be released on https://github.com/OMGGGGG/mmdg.|\u4eba\u8138\u53cd\u6b3a\u9a97 (FAS) \u5bf9\u4e8e\u4fdd\u62a4\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u514d\u53d7\u6f14\u793a\u653b\u51fb\u81f3\u5173\u91cd\u8981\u3002\u968f\u7740\u4f20\u611f\u5668\u5236\u9020\u548c\u591a\u6a21\u6001\u5b66\u4e60\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u8bb8\u591a\u591a\u6a21\u6001 FAS \u65b9\u6cd5\u5df2\u7ecf\u51fa\u73b0\u3002\u7136\u800c\uff0c\u4ed6\u4eec\u5728\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u653b\u51fb\u548c\u90e8\u7f72\u6761\u4ef6\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u8fd9\u4e9b\u6311\u6218\u6e90\u4e8e\uff081\uff09\u6a21\u6001\u4e0d\u53ef\u9760\u6027\uff0c\u6df1\u5ea6\u548c\u7ea2\u5916\u7b49\u4e00\u4e9b\u6a21\u6001\u4f20\u611f\u5668\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e2d\u7ecf\u5386\u663e\u7740\u7684\u57df\u53d8\u5316\uff0c\u5bfc\u81f4\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u671f\u95f4\u4e0d\u53ef\u9760\u4fe1\u606f\u7684\u4f20\u64ad\uff1b\uff082\uff09\u6a21\u6001\u4e0d\u5e73\u8861\uff0c\u5176\u4e2d\u8bad\u7ec3\u8fc7\u5ea6\u4f9d\u8d56\u4e3b\u5bfc\u6a21\u5f0f\u4f1a\u963b\u788d\u5176\u4ed6\u6a21\u5f0f\u7684\u878d\u5408\uff0c\u4ece\u800c\u964d\u4f4e\u9488\u5bf9\u4ec5\u4f7f\u7528\u4e3b\u5bfc\u6a21\u5f0f\u65e0\u6cd5\u533a\u5206\u7684\u653b\u51fb\u7c7b\u578b\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u6a21\u6001\u4e0d\u53ef\u9760\u6027\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u4ea4\u53c9\u9002\u914d\u5668\uff08U-Adapter\uff09\u6765\u8bc6\u522b\u6bcf\u79cd\u6a21\u6001\u4e2d\u4e0d\u53ef\u9760\u7684\u68c0\u6d4b\u533a\u57df\uff0c\u5e76\u6291\u5236\u4e0d\u53ef\u9760\u533a\u57df\u5bf9\u5176\u4ed6\u6a21\u6001\u7684\u5f71\u54cd\u3002\u5bf9\u4e8e\u6a21\u6001\u4e0d\u5e73\u8861\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u65b0\u5e73\u8861\u6a21\u6001\u68af\u5ea6\u8c03\u5236\uff08ReGrad\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u68af\u5ea6\u6765\u91cd\u65b0\u5e73\u8861\u6240\u6709\u6a21\u6001\u7684\u6536\u655b\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u9886\u57df\u6cdb\u5316\u573a\u666f\u4e0b\u591a\u6a21\u5f0f FAS \u6027\u80fd\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6e90\u4ee3\u7801\u548c\u534f\u8bae\u5c06\u5728 https://github.com/OMGGGGGG/mmdg \u4e0a\u200b\u200b\u53d1\u5e03\u3002|[2402.19298v1](http://arxiv.org/pdf/2402.19298v1)|null|\n", "2402.19276": "|**2024-02-29**|**Modular Blind Video Quality Assessment**|\u6a21\u5757\u5316\u76f2\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30|Wen Wen, Mu Li, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang, Kede Ma|Blind video quality assessment (BVQA) plays a pivotal role in evaluating and improving the viewing experience of end-users across a wide range of video-based platforms and services. Contemporary deep learning-based models primarily analyze the video content in its aggressively downsampled format, while being blind to the impact of actual spatial resolution and frame rate on video quality. In this paper, we propose a modular BVQA model, and a method of training it to improve its modularity. Specifically, our model comprises a base quality predictor, a spatial rectifier, and a temporal rectifier, responding to the visual content and distortion, spatial resolution, and frame rate changes on video quality, respectively. During training, spatial and temporal rectifiers are dropped out with some probabilities so as to make the base quality predictor a standalone BVQA model, which should work better with the rectifiers. Extensive experiments on both professionally-generated content and user generated content video databases show that our quality model achieves superior or comparable performance to current methods. Furthermore, the modularity of our model offers a great opportunity to analyze existing video quality databases in terms of their spatial and temporal complexities. Last, our BVQA model is cost-effective to add other quality-relevant video attributes such as dynamic range and color gamut as additional rectifiers.|\u76f2\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30 (BVQA) \u5728\u8bc4\u4f30\u548c\u6539\u5584\u5404\u79cd\u57fa\u4e8e\u89c6\u9891\u7684\u5e73\u53f0\u548c\u670d\u52a1\u7684\u6700\u7ec8\u7528\u6237\u7684\u89c2\u770b\u4f53\u9a8c\u65b9\u9762\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u3002\u5f53\u4ee3\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u4e3b\u8981\u4ee5\u79ef\u6781\u4e0b\u91c7\u6837\u7684\u683c\u5f0f\u5206\u6790\u89c6\u9891\u5185\u5bb9\uff0c\u800c\u5ffd\u89c6\u4e86\u5b9e\u9645\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u5e27\u901f\u7387\u5bf9\u89c6\u9891\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316 BVQA \u6a21\u578b\uff0c\u4ee5\u53ca\u4e00\u79cd\u8bad\u7ec3\u5b83\u4ee5\u63d0\u9ad8\u5176\u6a21\u5757\u5316\u6027\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5305\u62ec\u57fa\u672c\u8d28\u91cf\u9884\u6d4b\u5668\u3001\u7a7a\u95f4\u6574\u6d41\u5668\u548c\u65f6\u95f4\u6574\u6d41\u5668\uff0c\u5206\u522b\u54cd\u5e94\u89c6\u9891\u8d28\u91cf\u7684\u89c6\u89c9\u5185\u5bb9\u548c\u5931\u771f\u3001\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u5e27\u901f\u7387\u53d8\u5316\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u7a7a\u95f4\u548c\u65f6\u95f4\u6574\u6d41\u5668\u4f1a\u4ee5\u4e00\u5b9a\u7684\u6982\u7387\u88ab\u4e22\u5f03\uff0c\u4ee5\u4f7f\u57fa\u672c\u8d28\u91cf\u9884\u6d4b\u5668\u6210\u4e3a\u72ec\u7acb\u7684 BVQA \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5e94\u8be5\u4e0e\u6574\u6d41\u5668\u4e00\u8d77\u66f4\u597d\u5730\u5de5\u4f5c\u3002\u5bf9\u4e13\u4e1a\u751f\u6210\u7684\u5185\u5bb9\u548c\u7528\u6237\u751f\u6210\u7684\u5185\u5bb9\u89c6\u9891\u6570\u636e\u5e93\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u8d28\u91cf\u6a21\u578b\u5b9e\u73b0\u4e86\u4f18\u4e8e\u5f53\u524d\u65b9\u6cd5\u6216\u53ef\u6bd4\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6a21\u578b\u7684\u6a21\u5757\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f88\u597d\u7684\u673a\u4f1a\u6765\u5206\u6790\u73b0\u6709\u89c6\u9891\u8d28\u91cf\u6570\u636e\u5e93\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u590d\u6742\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u7684 BVQA \u6a21\u578b\u53ef\u4ee5\u7ecf\u6d4e\u9ad8\u6548\u5730\u6dfb\u52a0\u5176\u4ed6\u4e0e\u8d28\u91cf\u76f8\u5173\u7684\u89c6\u9891\u5c5e\u6027\uff0c\u4f8b\u5982\u52a8\u6001\u8303\u56f4\u548c\u8272\u57df\u4f5c\u4e3a\u989d\u5916\u7684\u6574\u6d41\u5668\u3002|[2402.19276v1](http://arxiv.org/pdf/2402.19276v1)|null|\n", "2402.19258": "|**2024-02-29**|**MaskFi: Unsupervised Learning of WiFi and Vision Representations for Multimodal Human Activity Recognition**|MaskFi\uff1a\u7528\u4e8e\u591a\u6a21\u6001\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684 WiFi \u548c\u89c6\u89c9\u8868\u793a\u7684\u65e0\u76d1\u7763\u5b66\u4e60|Jianfei Yang, Shijie Tang, Yuecong Xu, Yunjiao Zhou, Lihua Xie|Human activity recognition (HAR) has been playing an increasingly important role in various domains such as healthcare, security monitoring, and metaverse gaming. Though numerous HAR methods based on computer vision have been developed to show prominent performance, they still suffer from poor robustness in adverse visual conditions in particular low illumination, which motivates WiFi-based HAR to serve as a good complementary modality. Existing solutions using WiFi and vision modalities rely on massive labeled data that are very cumbersome to collect. In this paper, we propose a novel unsupervised multimodal HAR solution, MaskFi, that leverages only unlabeled video and WiFi activity data for model training. We propose a new algorithm, masked WiFi-vision modeling (MI2M), that enables the model to learn cross-modal and single-modal features by predicting the masked sections in representation learning. Benefiting from our unsupervised learning procedure, the network requires only a small amount of annotated data for finetuning and can adapt to the new environment with better performance. We conduct extensive experiments on two WiFi-vision datasets collected in-house, and our method achieves human activity recognition and human identification in terms of both robustness and accuracy.|\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u5728\u533b\u7597\u4fdd\u5065\u3001\u5b89\u5168\u76d1\u63a7\u548c\u5143\u5b87\u5b99\u6e38\u620f\u7b49\u5404\u4e2a\u9886\u57df\u53d1\u6325\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5c3d\u7ba1\u5df2\u7ecf\u5f00\u53d1\u51fa\u8bb8\u591a\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684 HAR \u65b9\u6cd5\u5e76\u663e\u793a\u51fa\u7a81\u51fa\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u4e0d\u5229\u7684\u89c6\u89c9\u6761\u4ef6\uff08\u7279\u522b\u662f\u4f4e\u7167\u5ea6\uff09\u4e0b\u4ecd\u7136\u5b58\u5728\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\uff0c\u8fd9\u4fc3\u4f7f\u57fa\u4e8e WiFi \u7684 HAR \u6210\u4e3a\u4e00\u79cd\u826f\u597d\u7684\u8865\u5145\u6a21\u5f0f\u3002\u4f7f\u7528 WiFi \u548c\u89c6\u89c9\u6a21\u5f0f\u7684\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u4e8e\u6536\u96c6\u8d77\u6765\u975e\u5e38\u9ebb\u70e6\u7684\u5927\u91cf\u6807\u8bb0\u6570\u636e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76d1\u7763\u591a\u6a21\u5f0f HAR \u89e3\u51b3\u65b9\u6848 MaskFi\uff0c\u5b83\u4ec5\u5229\u7528\u672a\u6807\u8bb0\u7684\u89c6\u9891\u548c WiFi \u6d3b\u52a8\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u5373\u5c4f\u853d WiFi \u89c6\u89c9\u5efa\u6a21 (MI2M)\uff0c\u8be5\u7b97\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u9884\u6d4b\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u5c4f\u853d\u90e8\u5206\u6765\u5b66\u4e60\u8de8\u6a21\u6001\u548c\u5355\u6a21\u6001\u7279\u5f81\u3002\u53d7\u76ca\u4e8e\u6211\u4eec\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u8fc7\u7a0b\uff0c\u7f51\u7edc\u53ea\u9700\u8981\u5c11\u91cf\u7684\u6ce8\u91ca\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ee5\u66f4\u597d\u7684\u6027\u80fd\u9002\u5e94\u65b0\u73af\u5883\u3002\u6211\u4eec\u5bf9\u5185\u90e8\u6536\u96c6\u7684\u4e24\u4e2a WiFi \u89c6\u89c9\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u548c\u4eba\u7c7b\u8eab\u4efd\u8bc6\u522b\u3002|[2402.19258v1](http://arxiv.org/pdf/2402.19258v1)|null|\n", "2402.19150": "|**2024-02-29**|**Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts**|\u5927\u578b\u591a\u6a21\u5f0f\u6a21\u578b\u4e2d\u7684\u5370\u5237\u653b\u51fb\u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u66f4\u591a\u4fe1\u606f\u7684\u63d0\u793a\u6765\u7f13\u89e3|Hao Cheng, Erjia Xiao, Renjing Xu|Large Multimodal Models (LMMs) rely on pre-trained Vision Language Models (VLMs) and Large Language Models (LLMs) to perform amazing emergent abilities on various multimodal tasks in the joint space of vision and language. However, the Typographic Attack, which shows disruption to VLMs, has also been certified as a security vulnerability to LMMs. In this work, we first comprehensively investigate the distractibility of LMMs by typography. In particular, we introduce the Typographic Dataset designed to evaluate distractibility across various multi-modal subtasks, such as object recognition, visual attributes detection, enumeration, arithmetic computation, and commonsense reasoning. To further study the effect of typographic patterns on performance, we also scrutinize the effect of tuning various typographic factors, encompassing font size, color, opacity, and spatial positioning of typos. We discover that LMMs can partially distinguish visual contents and typos when confronting typographic attacks, which suggests that embeddings from vision encoders contain enough information to distinguish visual contents and typos in images. Inspired by such phenomena, we demonstrate that CLIP's performance of zero-shot classification on typo-ridden images can be significantly improved by providing more informative texts to match images. Furthermore, we also prove that LMMs can utilize more informative prompts to leverage information in embeddings to differentiate between visual content and typos. Finally, we propose a prompt information enhancement method that can effectively mitigate the effects of typography.|\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u4f9d\u9760\u9884\u5148\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u8054\u5408\u7a7a\u95f4\u4e2d\u7684\u5404\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u6d8c\u73b0\u80fd\u529b\u3002\u7136\u800c\uff0c\u663e\u793a\u5bf9 VLM \u9020\u6210\u7834\u574f\u7684\u5370\u5237\u653b\u51fb\u4e5f\u5df2\u88ab\u8ba4\u8bc1\u4e3a LMM \u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u6392\u7248\u5168\u9762\u7814\u7a76 LMM \u7684\u5206\u6563\u6027\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5370\u5237\u6570\u636e\u96c6\uff0c\u65e8\u5728\u8bc4\u4f30\u5404\u79cd\u591a\u6a21\u5f0f\u5b50\u4efb\u52a1\u7684\u5206\u6563\u6027\uff0c\u4f8b\u5982\u5bf9\u8c61\u8bc6\u522b\u3001\u89c6\u89c9\u5c5e\u6027\u68c0\u6d4b\u3001\u679a\u4e3e\u3001\u7b97\u672f\u8ba1\u7b97\u548c\u5e38\u8bc6\u63a8\u7406\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u5370\u5237\u56fe\u6848\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u8fd8\u4ed4\u7ec6\u7814\u7a76\u4e86\u8c03\u6574\u5404\u79cd\u5370\u5237\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u5b57\u4f53\u5927\u5c0f\u3001\u989c\u8272\u3001\u4e0d\u900f\u660e\u5ea6\u548c\u5370\u5237\u9519\u8bef\u7684\u7a7a\u95f4\u5b9a\u4f4d\u3002\u6211\u4eec\u53d1\u73b0 LMM \u5728\u9762\u5bf9\u6392\u7248\u653b\u51fb\u65f6\u53ef\u4ee5\u90e8\u5206\u5730\u533a\u5206\u89c6\u89c9\u5185\u5bb9\u548c\u62fc\u5199\u9519\u8bef\uff0c\u8fd9\u8868\u660e\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5d4c\u5165\u5305\u542b\u8db3\u591f\u7684\u4fe1\u606f\u6765\u533a\u5206\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u5185\u5bb9\u548c\u62fc\u5199\u9519\u8bef\u3002\u53d7\u6b64\u7c7b\u73b0\u8c61\u7684\u542f\u53d1\uff0c\u6211\u4eec\u8bc1\u660e\uff0c\u901a\u8fc7\u63d0\u4f9b\u66f4\u591a\u4fe1\u606f\u6027\u6587\u672c\u6765\u5339\u914d\u56fe\u50cf\uff0c\u53ef\u4ee5\u663e\u7740\u63d0\u9ad8 CLIP \u5bf9\u5145\u6ee1\u62fc\u5199\u9519\u8bef\u7684\u56fe\u50cf\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bc1\u660e LMM \u53ef\u4ee5\u5229\u7528\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u63d0\u793a\u6765\u5229\u7528\u5d4c\u5165\u4e2d\u7684\u4fe1\u606f\u6765\u533a\u5206\u89c6\u89c9\u5185\u5bb9\u548c\u62fc\u5199\u9519\u8bef\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u793a\u4fe1\u606f\u589e\u5f3a\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u8f7b\u6392\u7248\u7684\u5f71\u54cd\u3002|[2402.19150v1](http://arxiv.org/pdf/2402.19150v1)|null|\n", "2402.19014": "|**2024-02-29**|**Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models**|\u901a\u8fc7\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u89c6\u89c9\u6587\u6863\u7406\u89e3|Xin Li, Yunfei Wu, Xinghua Jiang, Zhihao Guo, Mingming Gong, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun|Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional vision-language tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements. Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios. In this paper, we abbreviate it as the fine-grained feature collapse issue. With the aim of filling this gap, we propose a contrastive learning framework, termed Document Object COntrastive learning (DoCo), specifically tailored for the downstream tasks of VDU. DoCo leverages an auxiliary multimodal encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios. It can represent that the contrastive learning between the visual holistic representations and the multimodal fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs. We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process. Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic vision-language tasks.|\u6700\u8fd1\uff0c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u51fa\u73b0\u53d7\u5230\u4e86\u5404\u4e2a\u9886\u57df\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u6587\u6863\u7406\u89e3\uff08VDU\uff09\u9886\u57df\u3002\u4e0e\u4f20\u7edf\u7684\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0d\u540c\uff0cVDU \u7279\u522b\u5173\u6ce8\u5305\u542b\u4e30\u5bcc\u6587\u6863\u5143\u7d20\u7684\u6587\u672c\u4e30\u5bcc\u573a\u666f\u3002\u7136\u800c\uff0c\u7ec6\u7c92\u5ea6\u7279\u5f81\u7684\u91cd\u8981\u6027\u5728 LVLM \u793e\u533a\u4e2d\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\uff0c\u5bfc\u81f4\u5728\u6587\u672c\u4e30\u5bcc\u7684\u573a\u666f\u4e2d\u6027\u80fd\u4e0d\u4f73\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u5176\u7f29\u5199\u4e3a\u7ec6\u7c92\u5ea6\u7279\u5f81\u5d29\u6e83\u95ee\u9898\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u79f0\u4e3a\u6587\u6863\u5bf9\u8c61\u5bf9\u6bd4\u5b66\u4e60\uff08DoCo\uff09\uff0c\u4e13\u95e8\u4e3a VDU \u7684\u4e0b\u6e38\u4efb\u52a1\u91cf\u8eab\u5b9a\u5236\u3002 DoCo\u5229\u7528\u8f85\u52a9\u591a\u6a21\u6001\u7f16\u7801\u5668\u6765\u83b7\u53d6\u6587\u6863\u5bf9\u8c61\u7684\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u4e0eLVLM\u89c6\u89c9\u7f16\u7801\u5668\u751f\u6210\u7684\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u6587\u672c\u4e30\u5bcc\u573a\u666f\u4e2d\u7684\u89c6\u89c9\u8868\u793a\u3002\u5b83\u53ef\u4ee5\u8868\u793a\u89c6\u89c9\u6574\u4f53\u8868\u793a\u4e0e\u6587\u6863\u5bf9\u8c61\u7684\u591a\u6a21\u6001\u7ec6\u7c92\u5ea6\u7279\u5f81\u4e4b\u95f4\u7684\u5bf9\u6bd4\u5b66\u4e60\u53ef\u4ee5\u5e2e\u52a9\u89c6\u89c9\u7f16\u7801\u5668\u83b7\u53d6\u66f4\u6709\u6548\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ece\u800c\u589e\u5f3a LVLM \u4e2d\u6587\u672c\u4e30\u5bcc\u6587\u6863\u7684\u7406\u89e3\u3002\u6211\u4eec\u8fd8\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 DoCo \u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u5404\u79cd LVLM \u7684\u9884\u8bad\u7ec3\uff0c\u800c\u4e0d\u4f1a\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u6027\u3002 VDU \u591a\u4e2a\u57fa\u51c6\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u914d\u5907\u6211\u4eec\u63d0\u51fa\u7684 DoCo \u7684 LVLM \u53ef\u4ee5\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u7f29\u5c0f VDU \u548c\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002|[2402.19014v1](http://arxiv.org/pdf/2402.19014v1)|null|\n", "2402.19002": "|**2024-02-29**|**GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction**|GoalNet\uff1a\u9762\u5411\u76ee\u6807\u533a\u57df\u7684\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b|Ching-Lin Lee, Zhi-Xuan Wang, Kuan-Ting Lai, Amar Fadillah|Predicting the future trajectories of pedestrians on the road is an important task for autonomous driving. The pedestrian trajectory prediction is affected by scene paths, pedestrian's intentions and decision-making, which is a multi-modal problem. Most recent studies use past trajectories to predict a variety of potential future trajectory distributions, which do not account for the scene context and pedestrian targets. Instead of predicting the future trajectory directly, we propose to use scene context and observed trajectory to predict the goal points first, and then reuse the goal points to predict the future trajectories. By leveraging the information from scene context and observed trajectory, the uncertainty can be limited to a few target areas, which represent the \"goals\" of the pedestrians. In this paper, we propose GoalNet, a new trajectory prediction neural network based on the goal areas of a pedestrian. Our network can predict both pedestrian's trajectories and bounding boxes. The overall model is efficient and modular, and its outputs can be changed according to the usage scenario. Experimental results show that GoalNet significantly improves the previous state-of-the-art performance by 48.7% on the JAAD and 40.8% on the PIE dataset.|\u9884\u6d4b\u9053\u8def\u4e0a\u884c\u4eba\u7684\u672a\u6765\u8f68\u8ff9\u662f\u81ea\u52a8\u9a7e\u9a76\u7684\u4e00\u9879\u91cd\u8981\u4efb\u52a1\u3002\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u53d7\u5230\u573a\u666f\u8def\u5f84\u3001\u884c\u4eba\u610f\u56fe\u548c\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u95ee\u9898\u3002\u6700\u8fd1\u7684\u5927\u591a\u6570\u7814\u7a76\u4f7f\u7528\u8fc7\u53bb\u7684\u8f68\u8ff9\u6765\u9884\u6d4b\u5404\u79cd\u6f5c\u5728\u7684\u672a\u6765\u8f68\u8ff9\u5206\u5e03\uff0c\u8fd9\u6ca1\u6709\u8003\u8651\u573a\u666f\u80cc\u666f\u548c\u884c\u4eba\u76ee\u6807\u3002\u6211\u4eec\u5efa\u8bae\u9996\u5148\u4f7f\u7528\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u89c2\u5bdf\u5230\u7684\u8f68\u8ff9\u6765\u9884\u6d4b\u76ee\u6807\u70b9\uff0c\u7136\u540e\u91cd\u7528\u76ee\u6807\u70b9\u6765\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\u3002\u901a\u8fc7\u5229\u7528\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u89c2\u5bdf\u5230\u7684\u8f68\u8ff9\u7684\u4fe1\u606f\uff0c\u53ef\u4ee5\u5c06\u4e0d\u786e\u5b9a\u6027\u9650\u5236\u5728\u51e0\u4e2a\u76ee\u6807\u533a\u57df\uff0c\u8fd9\u4e9b\u76ee\u6807\u533a\u57df\u4ee3\u8868\u884c\u4eba\u7684\u201c\u76ee\u6807\u201d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GoalNet\uff0c\u4e00\u79cd\u57fa\u4e8e\u884c\u4eba\u76ee\u6807\u533a\u57df\u7684\u65b0\u8f68\u8ff9\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u3002\u6211\u4eec\u7684\u7f51\u7edc\u53ef\u4ee5\u9884\u6d4b\u884c\u4eba\u7684\u8f68\u8ff9\u548c\u8fb9\u754c\u6846\u3002\u6574\u4f53\u6a21\u578b\u9ad8\u6548\u3001\u6a21\u5757\u5316\uff0c\u5176\u8f93\u51fa\u53ef\u4ee5\u6839\u636e\u4f7f\u7528\u573a\u666f\u800c\u6539\u53d8\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGoalNet \u5728 JAAD \u4e0a\u5c06\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6027\u80fd\u663e\u7740\u63d0\u9ad8\u4e86 48.7%\uff0c\u5728 PIE \u6570\u636e\u96c6\u4e0a\u663e\u7740\u63d0\u9ad8\u4e86 40.8%\u3002|[2402.19002v1](http://arxiv.org/pdf/2402.19002v1)|null|\n", "2402.18951": "|**2024-02-29**|**Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition**|\u611f\u77e5\u3001\u804a\u5929\uff0c\u7136\u540e\u9002\u5e94\uff1a\u5f00\u653e\u4e16\u754c\u89c6\u9891\u8bc6\u522b\u57fa\u7840\u6a21\u578b\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb|Boyu Chen, Siran Chen, Kunchang Li, Qinglin Xu, Yu Qiao, Yali Wang|Open-world video recognition is challenging since traditional networks are not generalized well on complex environment variations. Alternatively, foundation models with rich knowledge have recently shown their generalization power. However, how to apply such knowledge has not been fully explored for open-world video recognition. To this end, we propose a generic knowledge transfer pipeline, which progressively exploits and integrates external multimodal knowledge from foundation models to boost open-world video recognition. We name it PCA, based on three stages of Percept, Chat, and Adapt. First, we perform Percept process to reduce the video domain gap and obtain external visual knowledge. Second, we generate rich linguistic semantics as external textual knowledge in Chat stage. Finally, we blend external multimodal knowledge in Adapt stage, by inserting multimodal knowledge adaptation modules into networks. We conduct extensive experiments on three challenging open-world video benchmarks, i.e., TinyVIRAT, ARID, and QV-Pipe. Our approach achieves state-of-the-art performance on all three datasets.|\u5f00\u653e\u4e16\u754c\u7684\u89c6\u9891\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4f20\u7edf\u7f51\u7edc\u4e0d\u80fd\u5f88\u597d\u5730\u9002\u5e94\u590d\u6742\u7684\u73af\u5883\u53d8\u5316\u3002\u6216\u8005\uff0c\u5177\u6709\u4e30\u5bcc\u77e5\u8bc6\u7684\u57fa\u7840\u6a21\u578b\u6700\u8fd1\u663e\u793a\u4e86\u5b83\u4eec\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u5982\u4f55\u5e94\u7528\u8fd9\u4e9b\u77e5\u8bc6\u8fdb\u884c\u5f00\u653e\u4e16\u754c\u89c6\u9891\u8bc6\u522b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u77e5\u8bc6\u8f6c\u79fb\u7ba1\u9053\uff0c\u5b83\u9010\u6b65\u5229\u7528\u548c\u96c6\u6210\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u5916\u90e8\u591a\u6a21\u6001\u77e5\u8bc6\uff0c\u4ee5\u4fc3\u8fdb\u5f00\u653e\u4e16\u754c\u7684\u89c6\u9891\u8bc6\u522b\u3002\u6211\u4eec\u5c06\u5176\u547d\u540d\u4e3a PCA\uff0c\u57fa\u4e8e Percept\u3001Chat \u548c Adapt \u4e09\u4e2a\u9636\u6bb5\u3002\u9996\u5148\uff0c\u6211\u4eec\u6267\u884c\u611f\u77e5\u8fc7\u7a0b\u4ee5\u51cf\u5c11\u89c6\u9891\u57df\u95f4\u9699\u5e76\u83b7\u53d6\u5916\u90e8\u89c6\u89c9\u77e5\u8bc6\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5728\u804a\u5929\u9636\u6bb5\u751f\u6210\u4e30\u5bcc\u7684\u8bed\u8a00\u8bed\u4e49\u4f5c\u4e3a\u5916\u90e8\u6587\u672c\u77e5\u8bc6\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u9002\u5e94\u9636\u6bb5\u6df7\u5408\u5916\u90e8\u591a\u6a21\u6001\u77e5\u8bc6\uff0c\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u77e5\u8bc6\u9002\u5e94\u6a21\u5757\u63d2\u5165\u7f51\u7edc\u4e2d\u3002\u6211\u4eec\u5bf9\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5f00\u653e\u4e16\u754c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\uff08TinyVIRAT\u3001ARID \u548c QV-Pipe\uff09\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2402.18951v1](http://arxiv.org/pdf/2402.18951v1)|null|\n", "2402.18933": "|**2024-02-29**|**Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration**|\u7528\u4e8e\u53ef\u53d8\u5f62\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u6a21\u6001\u4e0d\u53ef\u77e5\u7ed3\u6784\u56fe\u50cf\u8868\u793a\u5b66\u4e60|Tony C. W. Mok, Zi Li, Yunhao Bai, Jianpeng Zhang, Wei Liu, Yan-Jie Zhou, Ke Yan, Dakai Jin, Yu Shi, Xiaoli Yin, et.al.|Establishing dense anatomical correspondence across distinct imaging modalities is a foundational yet challenging procedure for numerous medical image analysis studies and image-guided radiotherapy. Existing multi-modality image registration algorithms rely on statistical-based similarity measures or local structural image representations. However, the former is sensitive to locally varying noise, while the latter is not discriminative enough to cope with complex anatomical structures in multimodal scans, causing ambiguity in determining the anatomical correspondence across scans with different modalities. In this paper, we propose a modality-agnostic structural representation learning method, which leverages Deep Neighbourhood Self-similarity (DNS) and anatomy-aware contrastive learning to learn discriminative and contrast-invariance deep structural image representations (DSIR) without the need for anatomical delineations or pre-aligned training images. We evaluate our method on multiphase CT, abdomen MR-CT, and brain MR T1w-T2w registration. Comprehensive results demonstrate that our method is superior to the conventional local structural representation and statistical-based similarity measures in terms of discriminability and accuracy.|\u5bf9\u4e8e\u4f17\u591a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7814\u7a76\u548c\u56fe\u50cf\u5f15\u5bfc\u653e\u5c04\u6cbb\u7597\u6765\u8bf4\uff0c\u5728\u4e0d\u540c\u7684\u6210\u50cf\u6a21\u5f0f\u4e4b\u95f4\u5efa\u7acb\u5bc6\u96c6\u7684\u89e3\u5256\u5bf9\u5e94\u5173\u7cfb\u662f\u4e00\u4e2a\u57fa\u7840\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u8fc7\u7a0b\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u56fe\u50cf\u914d\u51c6\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u7edf\u8ba1\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u6216\u5c40\u90e8\u7ed3\u6784\u56fe\u50cf\u8868\u793a\u3002\u7136\u800c\uff0c\u524d\u8005\u5bf9\u5c40\u90e8\u53d8\u5316\u7684\u566a\u58f0\u654f\u611f\uff0c\u800c\u540e\u8005\u7684\u8fa8\u522b\u529b\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u591a\u6a21\u6001\u626b\u63cf\u4e2d\u7684\u590d\u6742\u89e3\u5256\u7ed3\u6784\uff0c\u5bfc\u81f4\u5728\u786e\u5b9a\u4e0d\u540c\u6a21\u6001\u626b\u63cf\u4e4b\u95f4\u7684\u89e3\u5256\u5bf9\u5e94\u5173\u7cfb\u65f6\u4ea7\u751f\u6a21\u7cca\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u6a21\u6001\u65e0\u5173\u7684\u7ed3\u6784\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6df1\u5ea6\u90bb\u57df\u81ea\u76f8\u4f3c\u6027\uff08DNS\uff09\u548c\u89e3\u5256\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u6765\u5b66\u4e60\u5224\u522b\u6027\u548c\u5bf9\u6bd4\u5ea6\u4e0d\u53d8\u7684\u6df1\u5c42\u7ed3\u6784\u56fe\u50cf\u8868\u793a\uff08DSIR\uff09\uff0c\u800c\u65e0\u9700\u89e3\u5256\u5b66\u8f6e\u5ed3\u6216\u9884\u5148\u5bf9\u9f50\u7684\u8bad\u7ec3\u56fe\u50cf\u3002\u6211\u4eec\u5728\u591a\u76f8 CT\u3001\u8179\u90e8 MR-CT \u548c\u8111\u90e8 MR T1w-T2w \u914d\u51c6\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u7efc\u5408\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u53ef\u8fa8\u522b\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u5c40\u90e8\u7ed3\u6784\u8868\u793a\u548c\u57fa\u4e8e\u7edf\u8ba1\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u3002|[2402.18933v1](http://arxiv.org/pdf/2402.18933v1)|null|\n", "2402.18892": "|**2024-02-29**|**Aligning Knowledge Graph with Visual Perception for Object-goal Navigation**|\u5c06\u77e5\u8bc6\u56fe\u4e0e\u89c6\u89c9\u611f\u77e5\u76f8\u7ed3\u5408\u4ee5\u5b9e\u73b0\u5bf9\u8c61\u76ee\u6807\u5bfc\u822a|Nuo Xu, Wen Wang, Rong Yang, Mengjie Qin, Zheyuan Lin, Wei Song, Chunlong Zhang, Jason Gu, Chao Li|Object-goal navigation is a challenging task that requires guiding an agent to specific objects based on first-person visual observations. The ability of agent to comprehend its surroundings plays a crucial role in achieving successful object finding. However, existing knowledge-graph-based navigators often rely on discrete categorical one-hot vectors and vote counting strategy to construct graph representation of the scenes, which results in misalignment with visual images. To provide more accurate and coherent scene descriptions and address this misalignment issue, we propose the Aligning Knowledge Graph with Visual Perception (AKGVP) method for object-goal navigation. Technically, our approach introduces continuous modeling of the hierarchical scene architecture and leverages visual-language pre-training to align natural language description with visual perception. The integration of a continuous knowledge graph architecture and multimodal feature alignment empowers the navigator with a remarkable zero-shot navigation capability. We extensively evaluate our method using the AI2-THOR simulator and conduct a series of experiments to demonstrate the effectiveness and efficiency of our navigator. Code available: https://github.com/nuoxu/AKGVP.|\u5bf9\u8c61\u76ee\u6807\u5bfc\u822a\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u6839\u636e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89c9\u89c2\u5bdf\u5f15\u5bfc\u667a\u80fd\u4f53\u5230\u8fbe\u7279\u5b9a\u5bf9\u8c61\u3002\u667a\u80fd\u4f53\u7406\u89e3\u5468\u56f4\u73af\u5883\u7684\u80fd\u529b\u5bf9\u4e8e\u6210\u529f\u627e\u5230\u76ee\u6807\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u77e5\u8bc6\u56fe\u7684\u5bfc\u822a\u5668\u901a\u5e38\u4f9d\u8d56\u4e8e\u79bb\u6563\u5206\u7c7b\u5355\u70ed\u5411\u91cf\u548c\u8ba1\u7968\u7b56\u7565\u6765\u6784\u5efa\u573a\u666f\u7684\u56fe\u5f62\u8868\u793a\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u4e0e\u89c6\u89c9\u56fe\u50cf\u7684\u4e0d\u4e00\u81f4\u3002\u4e3a\u4e86\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u8fde\u8d2f\u7684\u573a\u666f\u63cf\u8ff0\u5e76\u89e3\u51b3\u8fd9\u79cd\u9519\u4f4d\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u5bf9\u8c61\u76ee\u6807\u5bfc\u822a\u7684\u77e5\u8bc6\u56fe\u4e0e\u89c6\u89c9\u611f\u77e5\u5bf9\u9f50\uff08AKGVP\uff09\u65b9\u6cd5\u3002\u4ece\u6280\u672f\u4e0a\u8bb2\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u5206\u5c42\u573a\u666f\u67b6\u6784\u7684\u8fde\u7eed\u5efa\u6a21\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6765\u4f7f\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e0e\u89c6\u89c9\u611f\u77e5\u4fdd\u6301\u4e00\u81f4\u3002\u8fde\u7eed\u77e5\u8bc6\u56fe\u67b6\u6784\u548c\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u7684\u96c6\u6210\u4f7f\u5bfc\u822a\u5668\u5177\u6709\u5353\u8d8a\u7684\u96f6\u6837\u672c\u5bfc\u822a\u80fd\u529b\u3002\u6211\u4eec\u4f7f\u7528 AI2-THOR \u6a21\u62df\u5668\u5e7f\u6cdb\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e00\u7cfb\u5217\u5b9e\u9a8c\u6765\u8bc1\u660e\u6211\u4eec\u7684\u5bfc\u822a\u5668\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002\u53ef\u7528\u4ee3\u7801\uff1ahttps://github.com/nuoxu/AKGVP\u3002|[2402.18892v1](http://arxiv.org/pdf/2402.18892v1)|null|\n"}, "Nerf": {}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.19264": "|**2024-02-29**|**T3DNet: Compressing Point Cloud Models for Lightweight 3D Recognition**|T3DNet\uff1a\u538b\u7f29\u70b9\u4e91\u6a21\u578b\u4ee5\u5b9e\u73b0\u8f7b\u91cf\u7ea7 3D \u8bc6\u522b|Zhiyuan Yang, Yunjiao Zhou, Lihua Xie, Jianfei Yang|3D point cloud has been widely used in many mobile application scenarios, including autonomous driving and 3D sensing on mobile devices. However, existing 3D point cloud models tend to be large and cumbersome, making them hard to deploy on edged devices due to their high memory requirements and non-real-time latency. There has been a lack of research on how to compress 3D point cloud models into lightweight models. In this paper, we propose a method called T3DNet (Tiny 3D Network with augmEntation and disTillation) to address this issue. We find that the tiny model after network augmentation is much easier for a teacher to distill. Instead of gradually reducing the parameters through techniques such as pruning or quantization, we pre-define a tiny model and improve its performance through auxiliary supervision from augmented networks and the original model. We evaluate our method on several public datasets, including ModelNet40, ShapeNet, and ScanObjectNN. Our method can achieve high compression rates without significant accuracy sacrifice, achieving state-of-the-art performances on three datasets against existing methods. Amazingly, our T3DNet is 58 times smaller and 54 times faster than the original model yet with only 1.4% accuracy descent on the ModelNet40 dataset.|3D\u70b9\u4e91\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8bb8\u591a\u79fb\u52a8\u5e94\u7528\u573a\u666f\uff0c\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\u548c\u79fb\u52a8\u8bbe\u5907\u4e0a\u76843D\u4f20\u611f\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684 3D \u70b9\u4e91\u6a21\u578b\u5f80\u5f80\u5e9e\u5927\u4e14\u7b28\u91cd\uff0c\u7531\u4e8e\u5185\u5b58\u8981\u6c42\u9ad8\u4e14\u975e\u5b9e\u65f6\u5ef6\u8fdf\uff0c\u4f7f\u5176\u96be\u4ee5\u90e8\u7f72\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002\u5982\u4f55\u5c063D\u70b9\u4e91\u6a21\u578b\u538b\u7f29\u4e3a\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e00\u76f4\u7f3a\u4e4f\u7814\u7a76\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a T3DNet\uff08\u5177\u6709\u589e\u5f3a\u548c\u84b8\u998f\u529f\u80fd\u7684\u5fae\u578b 3D \u7f51\u7edc\uff09\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u53d1\u73b0\u7f51\u7edc\u589e\u5f3a\u540e\u7684\u5fae\u5c0f\u6a21\u578b\u5bf9\u4e8e\u6559\u5e08\u6765\u8bf4\u66f4\u5bb9\u6613\u63d0\u70bc\u3002\u6211\u4eec\u6ca1\u6709\u901a\u8fc7\u526a\u679d\u6216\u91cf\u5316\u7b49\u6280\u672f\u9010\u6e10\u51cf\u5c11\u53c2\u6570\uff0c\u800c\u662f\u9884\u5148\u5b9a\u4e49\u4e00\u4e2a\u5fae\u5c0f\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u7f51\u7edc\u548c\u539f\u59cb\u6a21\u578b\u7684\u8f85\u52a9\u76d1\u7763\u6765\u63d0\u9ad8\u5176\u6027\u80fd\u3002\u6211\u4eec\u5728\u51e0\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5305\u62ec ModelNet40\u3001ShapeNet \u548c ScanObjectNN\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e0d\u663e\u7740\u727a\u7272\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\uff0c\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6211\u4eec\u7684 T3DNet \u6bd4\u539f\u59cb\u6a21\u578b\u5c0f 58 \u500d\uff0c\u901f\u5ea6\u5feb 54 \u500d\uff0c\u4f46\u5728 ModelNet40 \u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u4ec5\u4e0b\u964d 1.4%\u3002|[2402.19264v1](http://arxiv.org/pdf/2402.19264v1)|null|\n", "2402.19159": "|**2024-02-29**|**Trajectory Consistency Distillation**|\u8f68\u8ff9\u4e00\u81f4\u6027\u84b8\u998f|Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, Tat-Jen Cham|Latent Consistency Model (LCM) extends the Consistency Model to the latent space and leverages the guided consistency distillation technique to achieve impressive performance in accelerating text-to-image synthesis. However, we observed that LCM struggles to generate images with both clarity and detailed intricacy. To address this limitation, we initially delve into and elucidate the underlying causes. Our investigation identifies that the primary issue stems from errors in three distinct areas. Consequently, we introduce Trajectory Consistency Distillation (TCD), which encompasses trajectory consistency function and strategic stochastic sampling. The trajectory consistency function diminishes the distillation errors by broadening the scope of the self-consistency boundary condition and endowing the TCD with the ability to accurately trace the entire trajectory of the Probability Flow ODE. Additionally, strategic stochastic sampling is specifically designed to circumvent the accumulated errors inherent in multi-step consistency sampling, which is meticulously tailored to complement the TCD model. Experiments demonstrate that TCD not only significantly enhances image quality at low NFEs but also yields more detailed results compared to the teacher model at high NFEs.|\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\uff08LCM\uff09\u5c06\u4e00\u81f4\u6027\u6a21\u578b\u6269\u5c55\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u5f15\u5bfc\u4e00\u81f4\u6027\u84b8\u998f\u6280\u672f\u5728\u52a0\u901f\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230 LCM \u5f88\u96be\u751f\u6210\u65e2\u6e05\u6670\u53c8\u8be6\u7ec6\u7684\u56fe\u50cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u9996\u5148\u6df1\u5165\u7814\u7a76\u5e76\u9610\u660e\u6839\u672c\u539f\u56e0\u3002\u6211\u4eec\u7684\u8c03\u67e5\u53d1\u73b0\uff0c\u4e3b\u8981\u95ee\u9898\u6e90\u4e8e\u4e09\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u9519\u8bef\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8f68\u8ff9\u4e00\u81f4\u6027\u84b8\u998f\uff08TCD\uff09\uff0c\u5b83\u5305\u542b\u8f68\u8ff9\u4e00\u81f4\u6027\u51fd\u6570\u548c\u7b56\u7565\u968f\u673a\u91c7\u6837\u3002\u8f68\u8ff9\u4e00\u81f4\u6027\u51fd\u6570\u901a\u8fc7\u6269\u5927\u81ea\u6d3d\u8fb9\u754c\u6761\u4ef6\u7684\u8303\u56f4\u5e76\u8d4b\u4e88 TCD \u7cbe\u786e\u8ffd\u8e2a\u6982\u7387\u6d41 ODE \u6574\u4e2a\u8f68\u8ff9\u7684\u80fd\u529b\u6765\u51cf\u5c11\u84b8\u998f\u8bef\u5dee\u3002\u6b64\u5916\uff0c\u6218\u7565\u968f\u673a\u62bd\u6837\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u89c4\u907f\u591a\u6b65\u4e00\u81f4\u6027\u62bd\u6837\u4e2d\u56fa\u6709\u7684\u7d2f\u79ef\u8bef\u5dee\uff0c\u8be5\u62bd\u6837\u7ecf\u8fc7\u7cbe\u5fc3\u5b9a\u5236\u4ee5\u8865\u5145 TCD \u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTCD \u4e0d\u4ec5\u5728\u4f4e NFE \u4e0b\u663e\u7740\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u800c\u4e14\u5728\u9ad8 NFE \u4e0b\u4e0e\u6559\u5e08\u6a21\u578b\u76f8\u6bd4\u8fd8\u4ea7\u751f\u4e86\u66f4\u8be6\u7ec6\u7684\u7ed3\u679c\u3002|[2402.19159v1](http://arxiv.org/pdf/2402.19159v1)|null|\n", "2402.19144": "|**2024-02-29**|**Weakly Supervised Monocular 3D Detection with a Single-View Image**|\u4f7f\u7528\u5355\u89c6\u56fe\u56fe\u50cf\u7684\u5f31\u76d1\u7763\u5355\u76ee 3D \u68c0\u6d4b|Xueying Jiang, Sheng Jin, Lewei Lu, Xiaoqin Zhang, Shijian Lu|Monocular 3D detection (M3D) aims for precise 3D object localization from a single-view image which usually involves labor-intensive annotation of 3D detection boxes. Weakly supervised M3D has recently been studied to obviate the 3D annotation process by leveraging many existing 2D annotations, but it often requires extra training data such as LiDAR point clouds or multi-view images which greatly degrades its applicability and usability in various applications. We propose SKD-WM3D, a weakly supervised monocular 3D detection framework that exploits depth information to achieve M3D with a single-view image exclusively without any 3D annotations or other training data. One key design in SKD-WM3D is a self-knowledge distillation framework, which transforms image features into 3D-like representations by fusing depth information and effectively mitigates the inherent depth ambiguity in monocular scenarios with little computational overhead in inference. In addition, we design an uncertainty-aware distillation loss and a gradient-targeted transfer modulation strategy which facilitate knowledge acquisition and knowledge transfer, respectively. Extensive experiments show that SKD-WM3D surpasses the state-of-the-art clearly and is even on par with many fully supervised methods.|\u5355\u76ee 3D \u68c0\u6d4b (M3D) \u65e8\u5728\u4ece\u5355\u89c6\u56fe\u56fe\u50cf\u4e2d\u7cbe\u786e\u5b9a\u4f4d 3D \u5bf9\u8c61\uff0c\u8fd9\u901a\u5e38\u6d89\u53ca 3D \u68c0\u6d4b\u6846\u7684\u52b3\u52a8\u5bc6\u96c6\u578b\u6ce8\u91ca\u3002\u6700\u8fd1\u7814\u7a76\u4e86\u5f31\u76d1\u7763 M3D\uff0c\u901a\u8fc7\u5229\u7528\u8bb8\u591a\u73b0\u6709\u7684 2D \u6ce8\u91ca\u6765\u6d88\u9664 3D \u6ce8\u91ca\u8fc7\u7a0b\uff0c\u4f46\u5b83\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f8b\u5982 LiDAR \u70b9\u4e91\u6216\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u8fd9\u5927\u5927\u964d\u4f4e\u4e86\u5176\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u548c\u53ef\u7528\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86 SKD-WM3D\uff0c\u8fd9\u662f\u4e00\u79cd\u5f31\u76d1\u7763\u7684\u5355\u76ee 3D \u68c0\u6d4b\u6846\u67b6\uff0c\u5b83\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u4ec5\u901a\u8fc7\u5355\u89c6\u56fe\u56fe\u50cf\u5b9e\u73b0 M3D\uff0c\u65e0\u9700\u4efb\u4f55 3D \u6ce8\u91ca\u6216\u5176\u4ed6\u8bad\u7ec3\u6570\u636e\u3002 SKD-WM3D \u7684\u4e00\u4e2a\u5173\u952e\u8bbe\u8ba1\u662f\u81ea\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u878d\u5408\u6df1\u5ea6\u4fe1\u606f\u5c06\u56fe\u50cf\u7279\u5f81\u8f6c\u6362\u4e3a\u7c7b\u4f3c 3D \u7684\u8868\u793a\uff0c\u5e76\u6709\u6548\u51cf\u8f7b\u5355\u76ee\u573a\u666f\u4e2d\u56fa\u6709\u7684\u6df1\u5ea6\u6a21\u7cca\u6027\uff0c\u800c\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\u5f88\u5c0f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u84b8\u998f\u635f\u5931\u548c\u4e00\u79cd\u68af\u5ea6\u76ee\u6807\u8f6c\u79fb\u8c03\u5236\u7b56\u7565\uff0c\u5206\u522b\u4fc3\u8fdb\u77e5\u8bc6\u83b7\u53d6\u548c\u77e5\u8bc6\u8f6c\u79fb\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e SKD-WM3D \u660e\u663e\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u751a\u81f3\u4e0e\u8bb8\u591a\u5b8c\u5168\u76d1\u7763\u7684\u65b9\u6cd5\u76f8\u5f53\u3002|[2402.19144v1](http://arxiv.org/pdf/2402.19144v1)|null|\n", "2402.19118": "|**2024-02-29**|**Continuous Sign Language Recognition Based on Motor attention mechanism and frame-level Self-distillation**|\u57fa\u4e8e\u8fd0\u52a8\u6ce8\u610f\u673a\u5236\u548c\u5e27\u7ea7\u81ea\u84b8\u998f\u7684\u8fde\u7eed\u624b\u8bed\u8bc6\u522b|Qidan Zhu, Jing Li, Fei Yuan, Quan Gan|Changes in facial expression, head movement, body movement and gesture movement are remarkable cues in sign language recognition, and most of the current continuous sign language recognition(CSLR) research methods mainly focus on static images in video sequences at the frame-level feature extraction stage, while ignoring the dynamic changes in the images. In this paper, we propose a novel motor attention mechanism to capture the distorted changes in local motion regions during sign language expression, and obtain a dynamic representation of image changes. And for the first time, we apply the self-distillation method to frame-level feature extraction for continuous sign language, which improves the feature expression without increasing the computational resources by self-distilling the features of adjacent stages and using the higher-order features as teachers to guide the lower-order features. The combination of the two constitutes our proposed holistic model of CSLR Based on motor attention mechanism and frame-level Self-Distillation (MAM-FSD), which improves the inference ability and robustness of the model. We conduct experiments on three publicly available datasets, and the experimental results show that our proposed method can effectively extract the sign language motion information in videos, improve the accuracy of CSLR and reach the state-of-the-art level.|\u9762\u90e8\u8868\u60c5\u3001\u5934\u90e8\u8fd0\u52a8\u3001\u8eab\u4f53\u8fd0\u52a8\u548c\u624b\u52bf\u8fd0\u52a8\u7684\u53d8\u5316\u662f\u624b\u8bed\u8bc6\u522b\u4e2d\u7684\u663e\u7740\u7ebf\u7d22\uff0c\u76ee\u524d\u5927\u591a\u6570\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\uff08CSLR\uff09\u7814\u7a76\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u9759\u6001\u56fe\u50cf\u7684\u5e27\u7ea7\u7279\u5f81\u63d0\u53d6\u4e0a\u9636\u6bb5\uff0c\u800c\u5ffd\u7565\u4e86\u56fe\u50cf\u7684\u52a8\u6001\u53d8\u5316\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fd0\u52a8\u6ce8\u610f\u673a\u5236\u6765\u6355\u83b7\u624b\u8bed\u8868\u8fbe\u8fc7\u7a0b\u4e2d\u5c40\u90e8\u8fd0\u52a8\u533a\u57df\u7684\u626d\u66f2\u53d8\u5316\uff0c\u5e76\u83b7\u5f97\u56fe\u50cf\u53d8\u5316\u7684\u52a8\u6001\u8868\u793a\u3002\u5e76\u4e14\u9996\u6b21\u5c06\u81ea\u84b8\u998f\u65b9\u6cd5\u5e94\u7528\u4e8e\u8fde\u7eed\u624b\u8bed\u7684\u5e27\u7ea7\u7279\u5f81\u63d0\u53d6\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u76f8\u90bb\u9636\u6bb5\u7684\u7279\u5f81\u5e76\u4f7f\u7528\u9ad8\u9636\u7279\u5f81\uff0c\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u4e86\u7279\u5f81\u8868\u8fbe\u4f5c\u4e3a\u6559\u5e08\u5f15\u5bfc\u4f4e\u9636\u529f\u80fd\u3002\u4e24\u8005\u7684\u7ed3\u5408\u6784\u6210\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u4e8e\u8fd0\u52a8\u6ce8\u610f\u673a\u5236\u548c\u5e27\u7ea7\u81ea\u84b8\u998f\u7684CSLR\u6574\u4f53\u6a21\u578b\uff08MAM-FSD\uff09\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u6211\u4eec\u5728\u4e09\u4e2a\u516c\u5f00\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u53d6\u89c6\u9891\u4e2d\u7684\u624b\u8bed\u8fd0\u52a8\u4fe1\u606f\uff0c\u63d0\u9ad8CSLR\u7684\u51c6\u786e\u6027\u5e76\u8fbe\u5230state-of-the-art\u6c34\u5e73\u3002|[2402.19118v1](http://arxiv.org/pdf/2402.19118v1)|null|\n", "2402.19102": "|**2024-02-29**|**FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness**|FlatNAS\uff1a\u4f18\u5316\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e2d\u7684\u5e73\u5766\u5ea6\u4ee5\u5b9e\u73b0\u5206\u5e03\u5916\u7684\u9c81\u68d2\u6027|Matteo Gambella, Fabrizio Pittorino, Manuel Roveri|Neural Architecture Search (NAS) paves the way for the automatic definition of Neural Network (NN) architectures, attracting increasing research attention and offering solutions in various scenarios. This study introduces a novel NAS solution, called Flat Neural Architecture Search (FlatNAS), which explores the interplay between a novel figure of merit based on robustness to weight perturbations and single NN optimization with Sharpness-Aware Minimization (SAM). FlatNAS is the first work in the literature to systematically explore flat regions in the loss landscape of NNs in a NAS procedure, while jointly optimizing their performance on in-distribution data, their out-of-distribution (OOD) robustness, and constraining the number of parameters in their architecture. Differently from current studies primarily concentrating on OOD algorithms, FlatNAS successfully evaluates the impact of NN architectures on OOD robustness, a crucial aspect in real-world applications of machine and deep learning. FlatNAS achieves a good trade-off between performance, OOD generalization, and the number of parameters, by using only in-distribution data in the NAS exploration. The OOD robustness of the NAS-designed models is evaluated by focusing on robustness to input data corruptions, using popular benchmark datasets in the literature.|\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e3a\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u67b6\u6784\u7684\u81ea\u52a8\u5b9a\u4e49\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5438\u5f15\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u5173\u6ce8\u5e76\u63d0\u4f9b\u4e86\u5404\u79cd\u573a\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684 NAS \u89e3\u51b3\u65b9\u6848\uff0c\u79f0\u4e3a\u5e73\u9762\u795e\u7ecf\u67b6\u6784\u641c\u7d22 (FlatNAS)\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u63a2\u7d22\u4e86\u57fa\u4e8e\u6743\u91cd\u6270\u52a8\u9c81\u68d2\u6027\u7684\u65b0\u9896\u54c1\u8d28\u56e0\u6570\u4e0e\u91c7\u7528\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316 (SAM) \u7684\u5355\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002 FlatNAS \u662f\u6587\u732e\u4e2d\u7b2c\u4e00\u4e2a\u5728 NAS \u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u5730\u63a2\u7d22 NN \u635f\u5931\u666f\u89c2\u4e2d\u5e73\u5766\u533a\u57df\u7684\u5de5\u4f5c\uff0c\u540c\u65f6\u8054\u5408\u4f18\u5316\u5b83\u4eec\u5728\u5206\u5e03\u5185\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3001\u5206\u5e03\u5916 (OOD) \u9c81\u68d2\u6027\u4ee5\u53ca\u7ea6\u675f\u6570\u91cf\u5176\u67b6\u6784\u4e2d\u7684\u53c2\u6570\u3002\u4e0e\u5f53\u524d\u4e3b\u8981\u5173\u6ce8 OOD \u7b97\u6cd5\u7684\u7814\u7a76\u4e0d\u540c\uff0cFlatNAS \u6210\u529f\u8bc4\u4f30\u4e86 NN \u67b6\u6784\u5bf9 OOD \u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u8fd9\u662f\u673a\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u3002 FlatNAS \u901a\u8fc7\u5728 NAS \u63a2\u7d22\u4e2d\u4ec5\u4f7f\u7528\u5206\u5e03\u6570\u636e\uff0c\u5728\u6027\u80fd\u3001OOD \u6cdb\u5316\u548c\u53c2\u6570\u6570\u91cf\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6743\u8861\u3002 NAS \u8bbe\u8ba1\u7684\u6a21\u578b\u7684 OOD \u9c81\u68d2\u6027\u901a\u8fc7\u5173\u6ce8\u8f93\u5165\u6570\u636e\u635f\u574f\u7684\u9c81\u68d2\u6027\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528\u6587\u732e\u4e2d\u6d41\u884c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002|[2402.19102v1](http://arxiv.org/pdf/2402.19102v1)|null|\n", "2402.18930": "|**2024-02-29**|**Variable-Rate Learned Image Compression with Multi-Objective Optimization and Quantization-Reconstruction Offsets**|\u5177\u6709\u591a\u76ee\u6807\u4f18\u5316\u548c\u91cf\u5316\u91cd\u5efa\u504f\u79fb\u7684\u53ef\u53d8\u901f\u7387\u5b66\u4e60\u56fe\u50cf\u538b\u7f29|Fatih Kamisli, Fabien Racape, Hyomin Choi|Achieving successful variable bitrate compression with computationally simple algorithms from a single end-to-end learned image or video compression model remains a challenge. Many approaches have been proposed, including conditional auto-encoders, channel-adaptive gains for the latent tensor or uniformly quantizing all elements of the latent tensor. This paper follows the traditional approach to vary a single quantization step size to perform uniform quantization of all latent tensor elements. However, three modifications are proposed to improve the variable rate compression performance. First, multi objective optimization is used for (post) training. Second, a quantization-reconstruction offset is introduced into the quantization operation. Third, variable rate quantization is used also for the hyper latent. All these modifications can be made on a pre-trained single-rate compression model by performing post training. The algorithms are implemented into three well-known image compression models and the achieved variable rate compression results indicate negligible or minimal compression performance loss compared to training multiple models. (Codes will be shared at https://github.com/InterDigitalInc/CompressAI)|\u4f7f\u7528\u8ba1\u7b97\u7b80\u5355\u7684\u7b97\u6cd5\u4ece\u5355\u4e2a\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u56fe\u50cf\u6216\u89c6\u9891\u538b\u7f29\u6a21\u578b\u6210\u529f\u5b9e\u73b0\u53ef\u53d8\u6bd4\u7279\u7387\u538b\u7f29\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u5df2\u7ecf\u63d0\u51fa\u4e86\u8bb8\u591a\u65b9\u6cd5\uff0c\u5305\u62ec\u6761\u4ef6\u81ea\u52a8\u7f16\u7801\u5668\u3001\u6f5c\u5728\u5f20\u91cf\u7684\u901a\u9053\u81ea\u9002\u5e94\u589e\u76ca\u6216\u5747\u5300\u91cf\u5316\u6f5c\u5728\u5f20\u91cf\u7684\u6240\u6709\u5143\u7d20\u3002\u672c\u6587\u9075\u5faa\u4f20\u7edf\u65b9\u6cd5\u6765\u6539\u53d8\u5355\u4e2a\u91cf\u5316\u6b65\u957f\uff0c\u4ee5\u5bf9\u6240\u6709\u6f5c\u5728\u5f20\u91cf\u5143\u7d20\u8fdb\u884c\u5747\u5300\u91cf\u5316\u3002\u7136\u800c\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u4fee\u6539\u6765\u63d0\u9ad8\u53ef\u53d8\u7387\u538b\u7f29\u6027\u80fd\u3002\u9996\u5148\uff0c\u591a\u76ee\u6807\u4f18\u5316\u7528\u4e8e\uff08\u540e\uff09\u8bad\u7ec3\u3002\u5176\u6b21\uff0c\u5c06\u91cf\u5316\u91cd\u5efa\u504f\u79fb\u5f15\u5165\u5230\u91cf\u5316\u64cd\u4f5c\u4e2d\u3002\u7b2c\u4e09\uff0c\u53ef\u53d8\u901f\u7387\u91cf\u5316\u4e5f\u7528\u4e8e\u8d85\u6f5c\u4f0f\u3002\u6240\u6709\u8fd9\u4e9b\u4fee\u6539\u90fd\u53ef\u4ee5\u901a\u8fc7\u6267\u884c\u540e\u8bad\u7ec3\u5728\u9884\u8bad\u7ec3\u7684\u5355\u901f\u7387\u538b\u7f29\u6a21\u578b\u4e0a\u8fdb\u884c\u3002\u8fd9\u4e9b\u7b97\u6cd5\u88ab\u5b9e\u65bd\u5230\u4e09\u4e2a\u4f17\u6240\u5468\u77e5\u7684\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u4e2d\uff0c\u6240\u83b7\u5f97\u7684\u53ef\u53d8\u901f\u7387\u538b\u7f29\u7ed3\u679c\u8868\u660e\u4e0e\u8bad\u7ec3\u591a\u4e2a\u6a21\u578b\u76f8\u6bd4\uff0c\u538b\u7f29\u6027\u80fd\u635f\u5931\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u6216\u6700\u5c0f\u3002 \uff08\u4ee3\u7801\u5c06\u5728 https://github.com/InterDigitalInc/CompressAI \u5171\u4eab\uff09|[2402.18930v1](http://arxiv.org/pdf/2402.18930v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.19463": "|**2024-02-29**|**SeMoLi: What Moves Together Belongs Together**|SeMoLi\uff1a\u4e00\u8d77\u79fb\u52a8\u7684\u5c31\u5c5e\u4e8e\u4e00\u8d77|Jenny Seidenschwarz, Aljo\u0161a O\u0161ep, Francesco Ferroni, Simon Lucey, Laura Leal-Taix\u00e9|We tackle semi-supervised object detection based on motion cues. Recent results suggest that heuristic-based clustering methods in conjunction with object trackers can be used to pseudo-label instances of moving objects and use these as supervisory signals to train 3D object detectors in Lidar data without manual supervision. We re-think this approach and suggest that both, object detection, as well as motion-inspired pseudo-labeling, can be tackled in a data-driven manner. We leverage recent advances in scene flow estimation to obtain point trajectories from which we extract long-term, class-agnostic motion patterns. Revisiting correlation clustering in the context of message passing networks, we learn to group those motion patterns to cluster points to object instances. By estimating the full extent of the objects, we obtain per-scan 3D bounding boxes that we use to supervise a Lidar object detection network. Our method not only outperforms prior heuristic-based approaches (57.5 AP, +14 improvement over prior work), more importantly, we show we can pseudo-label and train object detectors across datasets.|\u6211\u4eec\u89e3\u51b3\u57fa\u4e8e\u8fd0\u52a8\u7ebf\u7d22\u7684\u534a\u76d1\u7763\u5bf9\u8c61\u68c0\u6d4b\u3002\u6700\u8fd1\u7684\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u805a\u7c7b\u65b9\u6cd5\u4e0e\u5bf9\u8c61\u8ddf\u8e2a\u5668\u76f8\u7ed3\u5408\uff0c\u53ef\u7528\u4e8e\u4f2a\u6807\u8bb0\u79fb\u52a8\u5bf9\u8c61\u7684\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u7528\u4f5c\u76d1\u7763\u4fe1\u53f7\uff0c\u5728\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u4e2d\u8bad\u7ec3 3D \u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u800c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u3002\u6211\u4eec\u91cd\u65b0\u601d\u8003\u8fd9\u79cd\u65b9\u6cd5\uff0c\u5e76\u5efa\u8bae\u5bf9\u8c61\u68c0\u6d4b\u4ee5\u53ca\u8fd0\u52a8\u542f\u53d1\u7684\u4f2a\u6807\u8bb0\u90fd\u53ef\u4ee5\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u5f0f\u6765\u89e3\u51b3\u3002\u6211\u4eec\u5229\u7528\u573a\u666f\u6d41\u4f30\u8ba1\u7684\u6700\u65b0\u8fdb\u5c55\u6765\u83b7\u53d6\u70b9\u8f68\u8ff9\uff0c\u4ece\u4e2d\u63d0\u53d6\u957f\u671f\u7684\u3001\u4e0e\u7c7b\u522b\u65e0\u5173\u7684\u8fd0\u52a8\u6a21\u5f0f\u3002\u5728\u6d88\u606f\u4f20\u9012\u7f51\u7edc\u7684\u80cc\u666f\u4e0b\u91cd\u65b0\u5ba1\u89c6\u76f8\u5173\u805a\u7c7b\uff0c\u6211\u4eec\u5b66\u4e60\u5c06\u8fd9\u4e9b\u8fd0\u52a8\u6a21\u5f0f\u5206\u7ec4\u4ee5\u5c06\u70b9\u805a\u7c7b\u5230\u5bf9\u8c61\u5b9e\u4f8b\u3002\u901a\u8fc7\u4f30\u8ba1\u5bf9\u8c61\u7684\u5b8c\u6574\u8303\u56f4\uff0c\u6211\u4eec\u83b7\u5f97\u6bcf\u6b21\u626b\u63cf\u7684 3D \u8fb9\u754c\u6846\uff0c\u7528\u4e8e\u76d1\u7763\u6fc0\u5149\u96f7\u8fbe\u5bf9\u8c61\u68c0\u6d4b\u7f51\u7edc\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u4f18\u4e8e\u4e4b\u524d\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\uff0857.5 AP\uff0c\u6bd4\u4e4b\u524d\u7684\u5de5\u4f5c\u63d0\u9ad8\u4e86 14 \u500d\uff09\uff0c\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u8868\u660e\u6211\u4eec\u53ef\u4ee5\u8de8\u6570\u636e\u96c6\u4f2a\u6807\u8bb0\u548c\u8bad\u7ec3\u5bf9\u8c61\u68c0\u6d4b\u5668\u3002|[2402.19463v1](http://arxiv.org/pdf/2402.19463v1)|null|\n", "2402.19423": "|**2024-02-29**|**Leveraging AI Predicted and Expert Revised Annotations in Interactive Segmentation: Continual Tuning or Full Training?**|\u5728\u4ea4\u4e92\u5f0f\u5206\u5272\u4e2d\u5229\u7528\u4eba\u5de5\u667a\u80fd\u9884\u6d4b\u548c\u4e13\u5bb6\u4fee\u8ba2\u6ce8\u91ca\uff1a\u6301\u7eed\u8c03\u6574\u8fd8\u662f\u5168\u9762\u57f9\u8bad\uff1f|Tiezheng Zhang, Xiaoxi Chen, Chongyu Qu, Alan Yuille, Zongwei Zhou|Interactive segmentation, an integration of AI algorithms and human expertise, premises to improve the accuracy and efficiency of curating large-scale, detailed-annotated datasets in healthcare. Human experts revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from these revised annotations. This interactive process continues to enhance the quality of annotations until no major revision is needed from experts. The key challenge is how to leverage AI predicted and expert revised annotations to iteratively improve the AI. Two problems arise: (1) The risk of catastrophic forgetting--the AI tends to forget the previously learned classes if it is only retrained using the expert revised classes. (2) Computational inefficiency when retraining the AI using both AI predicted and expert revised annotations; moreover, given the dominant AI predicted annotations in the dataset, the contribution of newly revised annotations--often account for a very small fraction--to the AI training remains marginal. This paper proposes Continual Tuning to address the problems from two perspectives: network design and data reuse. Firstly, we design a shared network for all classes followed by class-specific networks dedicated to individual classes. To mitigate forgetting, we freeze the shared network for previously learned classes and only update the class-specific network for revised classes. Secondly, we reuse a small fraction of data with previous annotations to avoid over-computing. The selection of such data relies on the importance estimate of each data. The importance score is computed by combining the uncertainty and consistency of AI predictions. Our experiments demonstrate that Continual Tuning achieves a speed 16x greater than repeatedly training AI from scratch without compromising the performance.|\u4ea4\u4e92\u5f0f\u5206\u5272\u662f\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u548c\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u7684\u96c6\u6210\uff0c\u53ef\u4ee5\u63d0\u9ad8\u533b\u7597\u4fdd\u5065\u4e2d\u5927\u89c4\u6a21\u3001\u8be6\u7ec6\u6ce8\u91ca\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u4eba\u7c7b\u4e13\u5bb6\u4fee\u6539\u4eba\u5de5\u667a\u80fd\u9884\u6d4b\u7684\u6ce8\u91ca\uff0c\u53cd\u8fc7\u6765\uff0c\u4eba\u5de5\u667a\u80fd\u901a\u8fc7\u5b66\u4e60\u8fd9\u4e9b\u4fee\u6539\u540e\u7684\u6ce8\u91ca\u6765\u6539\u8fdb\u5176\u9884\u6d4b\u3002\u8fd9\u4e2a\u4ea4\u4e92\u8fc7\u7a0b\u4e0d\u65ad\u63d0\u9ad8\u6ce8\u91ca\u7684\u8d28\u91cf\uff0c\u76f4\u5230\u4e0d\u9700\u8981\u4e13\u5bb6\u8fdb\u884c\u91cd\u5927\u4fee\u6539\u3002\u5173\u952e\u7684\u6311\u6218\u662f\u5982\u4f55\u5229\u7528\u4eba\u5de5\u667a\u80fd\u9884\u6d4b\u548c\u4e13\u5bb6\u4fee\u8ba2\u7684\u6ce8\u91ca\u6765\u8fed\u4ee3\u6539\u8fdb\u4eba\u5de5\u667a\u80fd\u3002\u51fa\u73b0\u4e24\u4e2a\u95ee\u9898\uff1a\uff081\uff09\u707e\u96be\u6027\u9057\u5fd8\u7684\u98ce\u9669\u2014\u2014\u5982\u679c\u4ec5\u4f7f\u7528\u4e13\u5bb6\u4fee\u8ba2\u7684\u8bfe\u7a0b\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\uff0c\u4eba\u5de5\u667a\u80fd\u5f80\u5f80\u4f1a\u5fd8\u8bb0\u4e4b\u524d\u5b66\u4e60\u7684\u8bfe\u7a0b\u3002 (2) \u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u9884\u6d4b\u548c\u4e13\u5bb6\u4fee\u8ba2\u6ce8\u91ca\u91cd\u65b0\u8bad\u7ec3\u4eba\u5de5\u667a\u80fd\u65f6\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff1b\u6b64\u5916\uff0c\u8003\u8651\u5230\u6570\u636e\u96c6\u4e2d\u4e3b\u8981\u7684\u4eba\u5de5\u667a\u80fd\u9884\u6d4b\u6ce8\u91ca\uff0c\u65b0\u4fee\u8ba2\u7684\u6ce8\u91ca\uff08\u901a\u5e38\u53ea\u5360\u5f88\u5c0f\u7684\u4e00\u90e8\u5206\uff09\u5bf9\u4eba\u5de5\u667a\u80fd\u8bad\u7ec3\u7684\u8d21\u732e\u4ecd\u7136\u5f88\u5c0f\u3002\u672c\u6587\u63d0\u51faContinual Tuning\uff0c\u4ece\u7f51\u7edc\u8bbe\u8ba1\u548c\u6570\u636e\u590d\u7528\u4e24\u4e2a\u89d2\u5ea6\u89e3\u51b3\u8be5\u95ee\u9898\u3002\u9996\u5148\uff0c\u6211\u4eec\u4e3a\u6240\u6709\u7c7b\u522b\u8bbe\u8ba1\u4e00\u4e2a\u5171\u4eab\u7f51\u7edc\uff0c\u7136\u540e\u662f\u4e13\u7528\u4e8e\u5404\u4e2a\u7c7b\u522b\u7684\u7279\u5b9a\u7c7b\u522b\u7f51\u7edc\u3002\u4e3a\u4e86\u51cf\u5c11\u9057\u5fd8\uff0c\u6211\u4eec\u51bb\u7ed3\u4ee5\u524d\u5b66\u4e60\u8fc7\u7684\u7c7b\u7684\u5171\u4eab\u7f51\u7edc\uff0c\u53ea\u66f4\u65b0\u4fee\u8ba2\u540e\u7684\u7c7b\u7684\u7279\u5b9a\u4e8e\u7c7b\u7684\u7f51\u7edc\u3002\u5176\u6b21\uff0c\u6211\u4eec\u91cd\u7528\u4e00\u5c0f\u90e8\u5206\u5e26\u6709\u5148\u524d\u6ce8\u91ca\u7684\u6570\u636e\uff0c\u4ee5\u907f\u514d\u8fc7\u5ea6\u8ba1\u7b97\u3002\u6b64\u7c7b\u6570\u636e\u7684\u9009\u62e9\u4f9d\u8d56\u4e8e\u6bcf\u4e2a\u6570\u636e\u7684\u91cd\u8981\u6027\u4f30\u8ba1\u3002\u91cd\u8981\u6027\u5f97\u5206\u662f\u901a\u8fc7\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u4e00\u81f4\u6027\u6765\u8ba1\u7b97\u7684\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6301\u7eed\u8c03\u4f18\u7684\u901f\u5ea6\u6bd4\u4ece\u5934\u5f00\u59cb\u91cd\u590d\u8bad\u7ec3 AI \u5feb 16 \u500d\uff0c\u800c\u4e14\u4e0d\u4f1a\u5f71\u54cd\u6027\u80fd\u3002|[2402.19423v1](http://arxiv.org/pdf/2402.19423v1)|null|\n", "2402.19422": "|**2024-02-29**|**PEM: Prototype-based Efficient MaskFormer for Image Segmentation**|PEM\uff1a\u7528\u4e8e\u56fe\u50cf\u5206\u5272\u7684\u57fa\u4e8e\u539f\u578b\u7684 Efficient MaskFormer|Niccol\u00f2 Cavagnero, Gabriele Rosi, Claudia Ruttano, Francesca Pistilli, Marco Ciccone, Giuseppe Averta, Fabio Cermelli|Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high semantic content in an efficient way, thanks to the combination of deformable convolutions and context-based self-modulation. We benchmark the proposed PEM architecture on two tasks, semantic and panoptic segmentation, evaluated on two different datasets, Cityscapes and ADE20K. PEM demonstrates outstanding performance on every task and dataset, outperforming task-specific architectures while being comparable and even better than computationally-expensive baselines.|\u6700\u8fd1\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u67b6\u6784\u5728\u56fe\u50cf\u5206\u5272\u9886\u57df\u663e\u793a\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u3002\u7531\u4e8e\u5176\u7075\u6d3b\u6027\uff0c\u5b83\u4eec\u5728\u5355\u4e00\u7edf\u4e00\u6846\u67b6\u4e0b\u7684\u591a\u4e2a\u5206\u5272\u4efb\u52a1\uff08\u4f8b\u5982\u8bed\u4e49\u548c\u5168\u666f\uff09\u4e2d\u83b7\u5f97\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u5b9e\u73b0\u5982\u6b64\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u8fd9\u4e9b\u67b6\u6784\u91c7\u7528\u5bc6\u96c6\u578b\u64cd\u4f5c\u5e76\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u8fd9\u4e9b\u8d44\u6e90\u901a\u5e38\u4e0d\u53ef\u7528\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u539f\u578b\u7684 Efficient MaskFormer (PEM)\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u9ad8\u6548\u67b6\u6784\uff0c\u53ef\u4ee5\u5728\u591a\u4e2a\u5206\u5272\u4efb\u52a1\u4e2d\u8fd0\u884c\u3002 PEM\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u539f\u578b\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u5b83\u5229\u7528\u89c6\u89c9\u7279\u5f81\u7684\u5197\u4f59\u6765\u9650\u5236\u8ba1\u7b97\u5e76\u5728\u4e0d\u635f\u5bb3\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6548\u7387\u3002\u6b64\u5916\uff0cPEM \u5f15\u5165\u4e86\u9ad8\u6548\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff0c\u7531\u4e8e\u53ef\u53d8\u5f62\u5377\u79ef\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u81ea\u8c03\u5236\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u6709\u6548\u5730\u63d0\u53d6\u5177\u6709\u9ad8\u8bed\u4e49\u5185\u5bb9\u7684\u7279\u5f81\u3002\u6211\u4eec\u5728\u8bed\u4e49\u548c\u5168\u666f\u5206\u5272\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4e0a\u5bf9\u6240\u63d0\u51fa\u7684 PEM \u67b6\u6784\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6 Cityscapes \u548c ADE20K \u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002 PEM \u5728\u6bcf\u4e2a\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u51fa\u8272\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u67b6\u6784\uff0c\u540c\u65f6\u4e0e\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u57fa\u7ebf\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002|[2402.19422v1](http://arxiv.org/pdf/2402.19422v1)|null|\n", "2402.19401": "|**2024-02-29**|**Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance**|\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u76f8\u5bf9\u4e8e\u4eba\u7c7b\u8868\u73b0\u7684\u89c6\u89c9\u8fde\u7eed\u8150\u8d25\u9c81\u68d2\u6027|Huakun Shen, Boyue Caroline Hu, Krzysztof Czarnecki, Lina Marsso, Marsha Chechik|While Neural Networks (NNs) have surpassed human accuracy in image classification on ImageNet, they often lack robustness against image corruption, i.e., corruption robustness. Yet such robustness is seemingly effortless for human perception. In this paper, we propose visually-continuous corruption robustness (VCR) -- an extension of corruption robustness to allow assessing it over the wide and continuous range of changes that correspond to the human perceptive quality (i.e., from the original image to the full distortion of all perceived visual information), along with two novel human-aware metrics for NN evaluation. To compare VCR of NNs with human perception, we conducted extensive experiments on 14 commonly used image corruptions with 7,718 human participants and state-of-the-art robust NN models with different training objectives (e.g., standard, adversarial, corruption robustness), different architectures (e.g., convolution NNs, vision transformers), and different amounts of training data augmentation. Our study showed that: 1) assessing robustness against continuous corruption can reveal insufficient robustness undetected by existing benchmarks; as a result, 2) the gap between NN and human robustness is larger than previously known; and finally, 3) some image corruptions have a similar impact on human perception, offering opportunities for more cost-effective robustness assessments. Our validation set with 14 image corruptions, human robustness data, and the evaluation code is provided as a toolbox and a benchmark.|\u867d\u7136\u795e\u7ecf\u7f51\u7edc (NN) \u5728 ImageNet \u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u5df2\u7ecf\u8d85\u8d8a\u4e86\u4eba\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u7f3a\u4e4f\u9488\u5bf9\u56fe\u50cf\u635f\u574f\u7684\u9c81\u68d2\u6027\uff0c\u5373\u635f\u574f\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u4eba\u7c7b\u7684\u611f\u77e5\u6765\u8bf4\uff0c\u8fd9\u79cd\u9c81\u68d2\u6027\u4f3c\u4e4e\u6beb\u4e0d\u8d39\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u89c6\u89c9\u8fde\u7eed\u8150\u8d25\u9c81\u68d2\u6027\uff08VCR\uff09\u2014\u2014\u8150\u8d25\u9c81\u68d2\u6027\u7684\u5ef6\u4f38\uff0c\u5141\u8bb8\u5728\u4e0e\u4eba\u7c7b\u611f\u77e5\u8d28\u91cf\u76f8\u5bf9\u5e94\u7684\u5e7f\u6cdb\u4e14\u8fde\u7eed\u7684\u53d8\u5316\u8303\u56f4\u5185\u8bc4\u4f30\u8150\u8d25\u9c81\u68d2\u6027\uff08\u5373\u4ece\u539f\u59cb\u56fe\u50cf\u5230\u5b8c\u6574\u56fe\u50cf\uff09\u6240\u6709\u611f\u77e5\u89c6\u89c9\u4fe1\u606f\u7684\u5931\u771f\uff09\uff0c\u4ee5\u53ca\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\u7684\u4e24\u4e2a\u65b0\u9896\u7684\u4eba\u7c7b\u611f\u77e5\u6307\u6807\u3002\u4e3a\u4e86\u5c06\u795e\u7ecf\u7f51\u7edc\u7684 VCR \u4e0e\u4eba\u7c7b\u611f\u77e5\u8fdb\u884c\u6bd4\u8f83\uff0c\u6211\u4eec\u5bf9 7,718 \u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684 14 \u79cd\u5e38\u7528\u56fe\u50cf\u635f\u574f\u548c\u5177\u6709\u4e0d\u540c\u8bad\u7ec3\u76ee\u6807\uff08\u4f8b\u5982\uff0c\u6807\u51c6\u3001\u5bf9\u6297\u6027\u3001\u635f\u574f\u9c81\u68d2\u6027\uff09\u3001\u4e0d\u540c\u8bad\u7ec3\u76ee\u6807\u7684\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u67b6\u6784\uff08\u4f8b\u5982\uff0c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u89c6\u89c9\u53d8\u6362\u5668\uff09\u548c\u4e0d\u540c\u6570\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u589e\u5f3a\u3002\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff1a1\uff09\u8bc4\u4f30\u9488\u5bf9\u6301\u7eed\u8150\u8d25\u7684\u7a33\u5065\u6027\u53ef\u4ee5\u63ed\u793a\u73b0\u6709\u57fa\u51c6\u672a\u68c0\u6d4b\u5230\u7684\u7a33\u5065\u6027\u4e0d\u8db3\uff1b\u56e0\u6b64\uff0c2\uff09\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u7c7b\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u6bd4\u4e4b\u524d\u5df2\u77e5\u7684\u8981\u5927\uff1b\u6700\u540e\uff0c3\uff09\u4e00\u4e9b\u56fe\u50cf\u635f\u574f\u5bf9\u4eba\u7c7b\u611f\u77e5\u6709\u7c7b\u4f3c\u7684\u5f71\u54cd\uff0c\u4e3a\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002\u6211\u4eec\u7684\u9a8c\u8bc1\u96c6\u5305\u542b 14 \u4e2a\u56fe\u50cf\u635f\u574f\u3001\u4eba\u7c7b\u9c81\u68d2\u6027\u6570\u636e\u548c\u8bc4\u4f30\u4ee3\u7801\uff0c\u4f5c\u4e3a\u5de5\u5177\u7bb1\u548c\u57fa\u51c6\u63d0\u4f9b\u3002|[2402.19401v1](http://arxiv.org/pdf/2402.19401v1)|null|\n", "2402.19344": "|**2024-02-29**|**The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition**|\u7b2c\u516d\u5c4a\u91ce\u5916\u60c5\u611f\u884c\u4e3a\u5206\u6790\uff08ABAW\uff09\u5927\u8d5b|Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Stefanos Zafeiriou, Chunchang Shao, Guanyu Hu|This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW) Competition, which is part of the respective Workshop held in conjunction with IEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges in understanding human emotions and behaviors, crucial for the development of human-centered technologies. In more detail, the Competition focuses on affect related benchmarking tasks and comprises of five sub-challenges: i) Valence-Arousal Estimation (the target is to estimate two continuous affect dimensions, valence and arousal), ii) Expression Recognition (the target is to recognise between the mutually exclusive classes of the 7 basic expressions and 'other'), iii) Action Unit Detection (the target is to detect 12 action units), iv) Compound Expression Recognition (the target is to recognise between the 7 mutually exclusive compound expression classes), and v) Emotional Mimicry Intensity Estimation (the target is to estimate six continuous emotion dimensions). In the paper, we present these Challenges, describe their respective datasets and challenge protocols (we outline the evaluation metrics) and present the baseline systems as well as their obtained performance. More information for the Competition can be found in: \\url{https://affective-behavior-analysis-in-the-wild.github.io/6th}.|\u672c\u6587\u4ecb\u7ecd\u4e86\u7b2c\u516d\u5c4a\u91ce\u5916\u60c5\u611f\u884c\u4e3a\u5206\u6790 (ABAW) \u7ade\u8d5b\uff0c\u8be5\u7ade\u8d5b\u662f\u4e0e IEEE CVPR 2024 \u8054\u5408\u4e3e\u529e\u7684\u76f8\u5e94\u7814\u8ba8\u4f1a\u7684\u4e00\u90e8\u5206\u3002\u7b2c\u516d\u5c4a ABAW \u7ade\u8d5b\u89e3\u51b3\u4e86\u7406\u89e3\u4eba\u7c7b\u60c5\u611f\u548c\u884c\u4e3a\u65b9\u9762\u7684\u5f53\u4ee3\u6311\u6218\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u4eba\u7c7b\u60c5\u611f\u548c\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002\u53d1\u5c55\u4ee5\u4eba\u4e3a\u672c\u7684\u6280\u672f\u3002\u66f4\u8be6\u7ec6\u5730\u8bf4\uff0c\u6bd4\u8d5b\u4fa7\u91cd\u4e8e\u60c5\u611f\u76f8\u5173\u7684\u57fa\u51c6\u6d4b\u8bd5\u4efb\u52a1\uff0c\u5305\u62ec\u4e94\u4e2a\u5b50\u6311\u6218\uff1ai\uff09\u6548\u4ef7-\u5524\u9192\u4f30\u8ba1\uff08\u76ee\u6807\u662f\u4f30\u8ba1\u4e24\u4e2a\u8fde\u7eed\u7684\u60c5\u611f\u7ef4\u5ea6\uff0c\u6548\u4ef7\u548c\u5524\u9192\uff09\uff0cii\uff09\u8868\u60c5\u8bc6\u522b\uff08\u76ee\u6807\u662f\u8bc6\u522b 7 \u4e2a\u57fa\u672c\u8868\u8fbe\u5f0f\u548c\u201c\u5176\u4ed6\u201d\u7684\u4e92\u65a5\u7c7b\u522b\uff09\uff0ciii\uff09\u52a8\u4f5c\u5355\u5143\u68c0\u6d4b\uff08\u76ee\u6807\u662f\u68c0\u6d4b 12 \u4e2a\u52a8\u4f5c\u5355\u5143\uff09\uff0civ\uff09\u590d\u5408\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08\u76ee\u6807\u662f\u8bc6\u522b 7 \u4e2a\u4e92\u65a5\u7684\u7c7b\u522b\uff09\u590d\u5408\u8868\u8fbe\u7c7b\u522b\uff09\uff0c\u4ee5\u53ca v\uff09\u60c5\u7eea\u62df\u6001\u5f3a\u5ea6\u4f30\u8ba1\uff08\u76ee\u6807\u662f\u4f30\u8ba1\u516d\u4e2a\u8fde\u7eed\u7684\u60c5\u7eea\u7ef4\u5ea6\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fd9\u4e9b\u6311\u6218\uff0c\u63cf\u8ff0\u4e86\u5b83\u4eec\u5404\u81ea\u7684\u6570\u636e\u96c6\u548c\u6311\u6218\u534f\u8bae\uff08\u6211\u4eec\u6982\u8ff0\u4e86\u8bc4\u4f30\u6307\u6807\uff09\uff0c\u5e76\u4ecb\u7ecd\u4e86\u57fa\u7ebf\u7cfb\u7edf\u53ca\u5176\u83b7\u5f97\u7684\u6027\u80fd\u3002\u6709\u5173\u6bd4\u8d5b\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee\uff1a\\url{https://affective-behavior-analysis-in-the-wild.github.io/6th}\u3002|[2402.19344v1](http://arxiv.org/pdf/2402.19344v1)|null|\n", "2402.19340": "|**2024-02-29**|**One model to use them all: Training a segmentation model with complementary datasets**|\u4e00\u79cd\u6a21\u578b\u53ef\u4ee5\u4f7f\u7528\u6240\u6709\u8fd9\u4e9b\uff1a\u4f7f\u7528\u4e92\u8865\u6570\u636e\u96c6\u8bad\u7ec3\u5206\u5272\u6a21\u578b|Alexander C. Jenke, Sebastian Bodenstedt, Fiona R. Kolbinger, Marius Distler, J\u00fcrgen Weitz, Stefanie Speidel|Understanding a surgical scene is crucial for computer-assisted surgery systems to provide any intelligent assistance functionality. One way of achieving this scene understanding is via scene segmentation, where every pixel of a frame is classified and therefore identifies the visible structures and tissues. Progress on fully segmenting surgical scenes has been made using machine learning. However, such models require large amounts of annotated training data, containing examples of all relevant object classes. Such fully annotated datasets are hard to create, as every pixel in a frame needs to be annotated by medical experts and, therefore, are rarely available. In this work, we propose a method to combine multiple partially annotated datasets, which provide complementary annotations, into one model, enabling better scene segmentation and the use of multiple readily available datasets. Our method aims to combine available data with complementary labels by leveraging mutual exclusive properties to maximize information. Specifically, we propose to use positive annotations of other classes as negative samples and to exclude background pixels of binary annotations, as we cannot tell if they contain a class not annotated but predicted by the model. We evaluate our method by training a DeepLabV3 on the publicly available Dresden Surgical Anatomy Dataset, which provides multiple subsets of binary segmented anatomical structures. Our approach successfully combines 6 classes into one model, increasing the overall Dice Score by 4.4% compared to an ensemble of models trained on the classes individually. By including information on multiple classes, we were able to reduce confusion between stomach and colon by 24%. Our results demonstrate the feasibility of training a model on multiple datasets. This paves the way for future work further alleviating the need for one large, fully segmented datasets.|\u4e86\u89e3\u624b\u672f\u573a\u666f\u5bf9\u4e8e\u8ba1\u7b97\u673a\u8f85\u52a9\u624b\u672f\u7cfb\u7edf\u63d0\u4f9b\u667a\u80fd\u8f85\u52a9\u529f\u80fd\u81f3\u5173\u91cd\u8981\u3002\u5b9e\u73b0\u8fd9\u79cd\u573a\u666f\u7406\u89e3\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u901a\u8fc7\u573a\u666f\u5206\u5272\uff0c\u5176\u4e2d\u5bf9\u5e27\u7684\u6bcf\u4e2a\u50cf\u7d20\u8fdb\u884c\u5206\u7c7b\uff0c\u4ece\u800c\u8bc6\u522b\u53ef\u89c1\u7684\u7ed3\u6784\u548c\u7ec4\u7ec7\u3002\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5728\u5b8c\u5168\u5206\u5272\u624b\u672f\u573a\u666f\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u6a21\u578b\u9700\u8981\u5927\u91cf\u5e26\u6ce8\u91ca\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5176\u4e2d\u5305\u542b\u6240\u6709\u76f8\u5173\u5bf9\u8c61\u7c7b\u7684\u793a\u4f8b\u3002\u8fd9\u79cd\u5b8c\u5168\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u5f88\u96be\u521b\u5efa\uff0c\u56e0\u4e3a\u5e27\u4e2d\u7684\u6bcf\u4e2a\u50cf\u7d20\u90fd\u9700\u8981\u7531\u533b\u5b66\u4e13\u5bb6\u6ce8\u91ca\uff0c\u56e0\u6b64\u5f88\u5c11\u53ef\u7528\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c06\u591a\u4e2a\u90e8\u5206\u6ce8\u91ca\u7684\u6570\u636e\u96c6\uff08\u63d0\u4f9b\u8865\u5145\u6ce8\u91ca\uff09\u7ec4\u5408\u5230\u4e00\u4e2a\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u597d\u7684\u573a\u666f\u5206\u5272\u548c\u4f7f\u7528\u591a\u4e2a\u73b0\u6210\u7684\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u5229\u7528\u4e92\u65a5\u5c5e\u6027\u5c06\u53ef\u7528\u6570\u636e\u4e0e\u4e92\u8865\u6807\u7b7e\u76f8\u7ed3\u5408\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u5176\u4ed6\u7c7b\u7684\u6b63\u6ce8\u91ca\u4f5c\u4e3a\u8d1f\u6837\u672c\uff0c\u5e76\u6392\u9664\u4e8c\u8fdb\u5236\u6ce8\u91ca\u7684\u80cc\u666f\u50cf\u7d20\uff0c\u56e0\u4e3a\u6211\u4eec\u65e0\u6cd5\u5224\u65ad\u5b83\u4eec\u662f\u5426\u5305\u542b\u672a\u6ce8\u91ca\u4f46\u7531\u6a21\u578b\u9884\u6d4b\u7684\u7c7b\u3002\u6211\u4eec\u901a\u8fc7\u5728\u516c\u5f00\u7684\u5fb7\u7d2f\u65af\u987f\u5916\u79d1\u89e3\u5256\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3 DeepLabV3 \u6765\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e8c\u8fdb\u5236\u5206\u6bb5\u89e3\u5256\u7ed3\u6784\u7684\u591a\u4e2a\u5b50\u96c6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6210\u529f\u5730\u5c06 6 \u4e2a\u7c7b\u522b\u5408\u5e76\u5230\u4e00\u4e2a\u6a21\u578b\u4e2d\uff0c\u4e0e\u5355\u72ec\u8bad\u7ec3\u5404\u4e2a\u7c7b\u522b\u7684\u6a21\u578b\u96c6\u5408\u76f8\u6bd4\uff0c\u603b\u4f53 Dice \u5206\u6570\u63d0\u9ad8\u4e86 4.4%\u3002\u901a\u8fc7\u7eb3\u5165\u591a\u4e2a\u7c7b\u522b\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u80fd\u591f\u5c06\u80c3\u548c\u7ed3\u80a0\u4e4b\u95f4\u7684\u6df7\u6dc6\u51cf\u5c11 24%\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8bc1\u660e\u4e86\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002\u8fd9\u4e3a\u672a\u6765\u7684\u5de5\u4f5c\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8fdb\u4e00\u6b65\u51cf\u8f7b\u4e86\u5bf9\u5927\u578b\u3001\u5b8c\u5168\u5206\u6bb5\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002|[2402.19340v1](http://arxiv.org/pdf/2402.19340v1)|null|\n", "2402.19339": "|**2024-02-29**|**Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification**|\u7f1d\u5408\u95f4\u9699\uff1a\u5c06\u60c5\u5883\u611f\u77e5\u77e5\u8bc6\u4e0e\u89c6\u89c9\u53d8\u6362\u5668\u878d\u5408\u4ee5\u5b9e\u73b0\u9ad8\u7ea7\u56fe\u50cf\u5206\u7c7b|Delfina Sol Martinez Pandiani, Nicolas Lazzari, Valentina Presutti|The increasing demand for automatic high-level image understanding, particularly in detecting abstract concepts (AC) within images, underscores the necessity for innovative and more interpretable approaches. These approaches need to harmonize traditional deep vision methods with the nuanced, context-dependent knowledge humans employ to interpret images at intricate semantic levels. In this work, we leverage situated perceptual knowledge of cultural images to enhance performance and interpretability in AC image classification. We automatically extract perceptual semantic units from images, which we then model and integrate into the ARTstract Knowledge Graph (AKG). This resource captures situated perceptual semantics gleaned from over 14,000 cultural images labeled with ACs. Additionally, we enhance the AKG with high-level linguistic frames. We compute KG embeddings and experiment with relative representations and hybrid approaches that fuse these embeddings with visual transformer embeddings. Finally, for interpretability, we conduct posthoc qualitative analyses by examining model similarities with training instances. Our results show that our hybrid KGE-ViT methods outperform existing techniques in AC image classification. The posthoc interpretability analyses reveal the visual transformer's proficiency in capturing pixel-level visual attributes, contrasting with our method's efficacy in representing more abstract and semantic scene elements. We demonstrate the synergy and complementarity between KGE embeddings' situated perceptual knowledge and deep visual model's sensory-perceptual understanding for AC image classification. This work suggests a strong potential of neuro-symbolic methods for knowledge integration and robust image representation for use in downstream intricate visual comprehension tasks. All the materials and code are available online.|\u5bf9\u81ea\u52a8\u9ad8\u7ea7\u56fe\u50cf\u7406\u89e3\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u56fe\u50cf\u4e2d\u7684\u62bd\u8c61\u6982\u5ff5\uff08AC\uff09\u65b9\u9762\uff0c\u5f3a\u8c03\u4e86\u521b\u65b0\u548c\u66f4\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u5c06\u4f20\u7edf\u7684\u6df1\u5ea6\u89c6\u89c9\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u7528\u6765\u5728\u590d\u6742\u7684\u8bed\u4e49\u5c42\u9762\u89e3\u91ca\u56fe\u50cf\u7684\u7ec6\u81f4\u5165\u5fae\u7684\u3001\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u7684\u77e5\u8bc6\u76f8\u534f\u8c03\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u6587\u5316\u56fe\u50cf\u7684\u60c5\u5883\u611f\u77e5\u77e5\u8bc6\u6765\u589e\u5f3a AC \u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002\u6211\u4eec\u81ea\u52a8\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u611f\u77e5\u8bed\u4e49\u5355\u5143\uff0c\u7136\u540e\u5bf9\u5176\u8fdb\u884c\u5efa\u6a21\u5e76\u96c6\u6210\u5230 ARTstract \u77e5\u8bc6\u56fe (AKG) \u4e2d\u3002\u8be5\u8d44\u6e90\u6355\u83b7\u4e86\u4ece 14,000 \u591a\u4e2a\u5e26\u6709 AC \u6807\u7b7e\u7684\u6587\u5316\u56fe\u50cf\u4e2d\u6536\u96c6\u7684\u60c5\u5883\u611f\u77e5\u8bed\u4e49\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u901a\u8fc7\u9ad8\u7ea7\u8bed\u8a00\u6846\u67b6\u589e\u5f3a\u4e86 AKG\u3002\u6211\u4eec\u8ba1\u7b97 KG \u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u76f8\u5bf9\u8868\u793a\u548c\u6df7\u5408\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5c06\u8fd9\u4e9b\u5d4c\u5165\u4e0e\u89c6\u89c9\u53d8\u538b\u5668\u5d4c\u5165\u878d\u5408\u3002\u6700\u540e\uff0c\u4e3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u6211\u4eec\u901a\u8fc7\u68c0\u67e5\u6a21\u578b\u4e0e\u8bad\u7ec3\u5b9e\u4f8b\u7684\u76f8\u4f3c\u6027\u6765\u8fdb\u884c\u4e8b\u540e\u5b9a\u6027\u5206\u6790\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6df7\u5408 KGE-ViT \u65b9\u6cd5\u4f18\u4e8e AC \u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u73b0\u6709\u6280\u672f\u3002\u4e8b\u540e\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u89c6\u89c9\u8f6c\u6362\u5668\u5728\u6355\u83b7\u50cf\u7d20\u7ea7\u89c6\u89c9\u5c5e\u6027\u65b9\u9762\u7684\u719f\u7ec3\u7a0b\u5ea6\uff0c\u4e0e\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8868\u793a\u66f4\u62bd\u8c61\u548c\u8bed\u4e49\u573a\u666f\u5143\u7d20\u65b9\u9762\u7684\u529f\u6548\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002\u6211\u4eec\u8bc1\u660e\u4e86 KGE \u5d4c\u5165\u7684\u60c5\u5883\u611f\u77e5\u77e5\u8bc6\u548c\u6df1\u5ea6\u89c6\u89c9\u6a21\u578b\u7684 AC \u56fe\u50cf\u5206\u7c7b\u7684\u611f\u5b98\u77e5\u89c9\u7406\u89e3\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u548c\u4e92\u8865\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u8868\u660e\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u77e5\u8bc6\u6574\u5408\u548c\u9c81\u68d2\u56fe\u50cf\u8868\u793a\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u4e0b\u6e38\u590d\u6742\u7684\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u3002\u6240\u6709\u6750\u6599\u548c\u4ee3\u7801\u90fd\u53ef\u4ee5\u5728\u7ebf\u83b7\u53d6\u3002|[2402.19339v1](http://arxiv.org/pdf/2402.19339v1)|null|\n", "2402.19326": "|**2024-02-29**|**Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction**|\u5177\u6709\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u4e49\u4ea4\u4e92\u7684\u53ef\u6982\u62ec\u7684\u6574\u4e2a\u5e7b\u706f\u7247\u56fe\u50cf\u5206\u7c7b|Hao Li, Ying Chen, Yifei Chen, Wenxian Yang, Bowen Ding, Yuchen Han, Liansheng Wang, Rongshan Yu|Whole Slide Image (WSI) classification is often formulated as a Multiple Instance Learning (MIL) problem. Recently, Vision-Language Models (VLMs) have demonstrated remarkable performance in WSI classification. However, existing methods leverage coarse-grained pathogenetic descriptions for visual representation supervision, which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of models on diverse downstream tasks. Additionally, processing high-resolution WSIs can be computationally expensive. In this paper, we propose a novel \"Fine-grained Visual-Semantic Interaction\" (FiVE) framework for WSI classification. It is designed to enhance the model's generalizability by leveraging the interplay between localized visual patterns and fine-grained pathological semantics. Specifically, with meticulously designed queries, we start by utilizing a large language model to extract fine-grained pathological descriptions from various non-standardized raw reports. The output descriptions are then reconstructed into fine-grained labels used for training. By introducing a Task-specific Fine-grained Semantics (TFS) module, we enable prompts to capture crucial visual information in WSIs, which enhances representation learning and augments generalization capabilities significantly. Furthermore, given that pathological visual patterns are redundantly distributed across tissue slices, we sample a subset of visual instances during training. Our method demonstrates robust generalizability and strong transferability, dominantly outperforming the counterparts on the TCGA Lung Cancer dataset with at least 9.19% higher accuracy in few-shot experiments.|\u6574\u4e2a\u5e7b\u706f\u7247\u56fe\u50cf (WSI) \u5206\u7c7b\u901a\u5e38\u88ab\u8868\u8ff0\u4e3a\u591a\u5b9e\u4f8b\u5b66\u4e60 (MIL) \u95ee\u9898\u3002\u6700\u8fd1\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728 WSI \u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5229\u7528\u7c97\u7c92\u5ea6\u7684\u81f4\u75c5\u63cf\u8ff0\u8fdb\u884c\u89c6\u89c9\u8868\u793a\u76d1\u7763\uff0c\u4e0d\u8db3\u4ee5\u6355\u83b7\u81f4\u75c5\u56fe\u50cf\u7684\u590d\u6742\u89c6\u89c9\u5916\u89c2\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u901a\u7528\u6027\u3002\u6b64\u5916\uff0c\u5904\u7406\u9ad8\u5206\u8fa8\u7387 WSI \u7684\u8ba1\u7b97\u6210\u672c\u53ef\u80fd\u5f88\u9ad8\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e WSI \u5206\u7c7b\u7684\u65b0\u9896\u7684\u201c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u4e49\u4ea4\u4e92\u201d\uff08FiVE\uff09\u6846\u67b6\u3002\u5b83\u65e8\u5728\u901a\u8fc7\u5229\u7528\u5c40\u90e8\u89c6\u89c9\u6a21\u5f0f\u548c\u7ec6\u7c92\u5ea6\u75c5\u7406\u8bed\u4e49\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u6765\u589e\u5f3a\u6a21\u578b\u7684\u901a\u7528\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67e5\u8be2\uff0c\u6211\u4eec\u9996\u5148\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u5404\u79cd\u975e\u6807\u51c6\u5316\u539f\u59cb\u62a5\u544a\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7684\u75c5\u7406\u63cf\u8ff0\u3002\u7136\u540e\uff0c\u8f93\u51fa\u63cf\u8ff0\u88ab\u91cd\u5efa\u4e3a\u7528\u4e8e\u8bad\u7ec3\u7684\u7ec6\u7c92\u5ea6\u6807\u7b7e\u3002\u901a\u8fc7\u5f15\u5165\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49 (TFS) \u6a21\u5757\uff0c\u6211\u4eec\u80fd\u591f\u63d0\u793a\u6355\u83b7 WSI \u4e2d\u7684\u5173\u952e\u89c6\u89c9\u4fe1\u606f\uff0c\u4ece\u800c\u589e\u5f3a\u8868\u793a\u5b66\u4e60\u5e76\u663e\u7740\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8003\u8651\u5230\u75c5\u7406\u89c6\u89c9\u6a21\u5f0f\u5197\u4f59\u5730\u5206\u5e03\u5728\u7ec4\u7ec7\u5207\u7247\u4e0a\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u671f\u95f4\u5bf9\u89c6\u89c9\u5b9e\u4f8b\u7684\u5b50\u96c6\u8fdb\u884c\u91c7\u6837\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u6027\u548c\u53ef\u79fb\u690d\u6027\uff0c\u5728 TCGA \u80ba\u764c\u6570\u636e\u96c6\u4e0a\u660e\u663e\u4f18\u4e8e\u540c\u884c\uff0c\u5728\u51e0\u6b21\u5b9e\u9a8c\u4e2d\u51c6\u786e\u7387\u81f3\u5c11\u9ad8\u51fa 9.19%\u3002|[2402.19326v1](http://arxiv.org/pdf/2402.19326v1)|null|\n", "2402.19289": "|**2024-02-29**|**CAMixerSR: Only Details Need More \"Attention\"**|CAMixerSR\uff1a\u53ea\u6709\u7ec6\u8282\u9700\u8981\u66f4\u591a\u201c\u5173\u6ce8\u201d|Yan Wang, Shijie Zhao, Yi Liu, Junlin Li, Li Zhang|To satisfy the rapidly increasing demands on the large image (2K-8K) super-resolution (SR), prevailing methods follow two independent tracks: 1) accelerate existing networks by content-aware routing, and 2) design better super-resolution networks via token mixer refining. Despite directness, they encounter unavoidable defects (e.g., inflexible route or non-discriminative processing) limiting further improvements of quality-complexity trade-off. To erase the drawbacks, we integrate these schemes by proposing a content-aware mixer (CAMixer), which assigns convolution for simple contexts and additional deformable window-attention for sparse textures. Specifically, the CAMixer uses a learnable predictor to generate multiple bootstraps, including offsets for windows warping, a mask for classifying windows, and convolutional attentions for endowing convolution with the dynamic property, which modulates attention to include more useful textures self-adaptively and improves the representation capability of convolution. We further introduce a global classification loss to improve the accuracy of predictors. By simply stacking CAMixers, we obtain CAMixerSR which achieves superior performance on large-image SR, lightweight SR, and omnidirectional-image SR.|\u4e3a\u4e86\u6ee1\u8db3\u5bf9\u5927\u56fe\u50cf\uff082K-8K\uff09\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u5feb\u901f\u589e\u957f\u7684\u9700\u6c42\uff0c\u6d41\u884c\u7684\u65b9\u6cd5\u9075\u5faa\u4e24\u4e2a\u72ec\u7acb\u7684\u8f68\u9053\uff1a1\uff09\u901a\u8fc7\u5185\u5bb9\u611f\u77e5\u8def\u7531\u52a0\u901f\u73b0\u6709\u7f51\u7edc\uff0c2\uff09\u901a\u8fc7\u4ee4\u724c\u6df7\u5408\u5668\u7cbe\u70bc\u3002\u5c3d\u7ba1\u76f4\u63a5\uff0c\u4f46\u5b83\u4eec\u9047\u5230\u4e86\u4e0d\u53ef\u907f\u514d\u7684\u7f3a\u9677\uff08\u4f8b\u5982\uff0c\u4e0d\u7075\u6d3b\u7684\u8def\u7ebf\u6216\u975e\u6b67\u89c6\u6027\u5904\u7406\uff09\uff0c\u9650\u5236\u4e86\u8d28\u91cf\u590d\u6742\u6027\u6743\u8861\u7684\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002\u4e3a\u4e86\u6d88\u9664\u8fd9\u4e9b\u7f3a\u70b9\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u5185\u5bb9\u611f\u77e5\u6df7\u5408\u5668\uff08CAMixer\uff09\u6765\u96c6\u6210\u8fd9\u4e9b\u65b9\u6848\uff0c\u5b83\u4e3a\u7b80\u5355\u7684\u4e0a\u4e0b\u6587\u5206\u914d\u5377\u79ef\uff0c\u5e76\u4e3a\u7a00\u758f\u7eb9\u7406\u5206\u914d\u989d\u5916\u7684\u53ef\u53d8\u5f62\u7a97\u53e3\u6ce8\u610f\u3002\u5177\u4f53\u6765\u8bf4\uff0cCAMixer \u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u9884\u6d4b\u5668\u6765\u751f\u6210\u591a\u4e2a\u5f15\u5bfc\u7a0b\u5e8f\uff0c\u5305\u62ec\u7a97\u53e3\u626d\u66f2\u7684\u504f\u79fb\u91cf\u3001\u7528\u4e8e\u5206\u7c7b\u7a97\u53e3\u7684\u63a9\u7801\u4ee5\u53ca\u8d4b\u4e88\u5377\u79ef\u52a8\u6001\u5c5e\u6027\u7684\u5377\u79ef\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5730\u8c03\u8282\u6ce8\u610f\u529b\u4ee5\u5305\u542b\u66f4\u591a\u6709\u7528\u7684\u7eb9\u7406\u5e76\u63d0\u9ad8\u5377\u79ef\u7684\u8868\u793a\u80fd\u529b\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u5168\u5c40\u5206\u7c7b\u635f\u5931\u6765\u63d0\u9ad8\u9884\u6d4b\u5668\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u7b80\u5355\u5730\u5806\u53e0CAMixer\uff0c\u6211\u4eec\u83b7\u5f97\u4e86CAMixerSR\uff0c\u5b83\u5728\u5927\u56fe\u50cfSR\u3001\u8f7b\u91cf\u7ea7SR\u548c\u5168\u5411\u56fe\u50cfSR\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002|[2402.19289v1](http://arxiv.org/pdf/2402.19289v1)|null|\n", "2402.19286": "|**2024-02-29**|**PrPSeg: Universal Proposition Learning for Panoramic Renal Pathology Segmentation**|PrPSeg\uff1a\u5168\u666f\u80be\u810f\u75c5\u7406\u5206\u5272\u7684\u901a\u7528\u547d\u9898\u5b66\u4e60|Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Jialin Yue, Juming Xiong, Lining Yu, Yifei Wu, Mengmeng Yin, Yu Wang, et.al.|Understanding the anatomy of renal pathology is crucial for advancing disease diagnostics, treatment evaluation, and clinical research. The complex kidney system comprises various components across multiple levels, including regions (cortex, medulla), functional units (glomeruli, tubules), and cells (podocytes, mesangial cells in glomerulus). Prior studies have predominantly overlooked the intricate spatial interrelations among objects from clinical knowledge. In this research, we introduce a novel universal proposition learning approach, called panoramic renal pathology segmentation (PrPSeg), designed to segment comprehensively panoramic structures within kidney by integrating extensive knowledge of kidney anatomy.   In this paper, we propose (1) the design of a comprehensive universal proposition matrix for renal pathology, facilitating the incorporation of classification and spatial relationships into the segmentation process; (2) a token-based dynamic head single network architecture, with the improvement of the partial label image segmentation and capability for future data enlargement; and (3) an anatomy loss function, quantifying the inter-object relationships across the kidney.|\u4e86\u89e3\u80be\u810f\u75c5\u7406\u5b66\u7684\u89e3\u5256\u7ed3\u6784\u5bf9\u4e8e\u63a8\u8fdb\u75be\u75c5\u8bca\u65ad\u3001\u6cbb\u7597\u8bc4\u4f30\u548c\u4e34\u5e8a\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002\u590d\u6742\u7684\u80be\u810f\u7cfb\u7edf\u7531\u591a\u4e2a\u5c42\u9762\u7684\u5404\u79cd\u7ec4\u6210\u90e8\u5206\u7ec4\u6210\uff0c\u5305\u62ec\u533a\u57df\uff08\u76ae\u8d28\u3001\u9ad3\u8d28\uff09\u3001\u529f\u80fd\u5355\u4f4d\uff08\u80be\u5c0f\u7403\u3001\u80be\u5c0f\u7ba1\uff09\u548c\u7ec6\u80de\uff08\u8db3\u7ec6\u80de\u3001\u80be\u5c0f\u7403\u4e2d\u7684\u7cfb\u819c\u7ec6\u80de\uff09\u3002\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u5ffd\u89c6\u4e86\u4e34\u5e8a\u77e5\u8bc6\u4e2d\u5bf9\u8c61\u4e4b\u95f4\u590d\u6742\u7684\u7a7a\u95f4\u76f8\u4e92\u5173\u7cfb\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u901a\u7528\u547d\u9898\u5b66\u4e60\u65b9\u6cd5\uff0c\u79f0\u4e3a\u5168\u666f\u80be\u810f\u75c5\u7406\u5206\u5272\uff08PrPSeg\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u6574\u5408\u5e7f\u6cdb\u7684\u80be\u810f\u89e3\u5256\u77e5\u8bc6\u6765\u5206\u5272\u80be\u810f\u5185\u7684\u5168\u9762\u5168\u666f\u7ed3\u6784\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\uff081\uff09\u8bbe\u8ba1\u4e00\u4e2a\u5168\u9762\u7684\u80be\u810f\u75c5\u7406\u901a\u7528\u547d\u9898\u77e9\u9635\uff0c\u4fbf\u4e8e\u5c06\u5206\u7c7b\u548c\u7a7a\u95f4\u5173\u7cfb\u7eb3\u5165\u5206\u5272\u8fc7\u7a0b\uff1b \uff082\uff09\u57fa\u4e8etoken\u7684\u52a8\u6001\u5934\u5355\u7f51\u7edc\u67b6\u6784\uff0c\u6539\u8fdb\u4e86\u90e8\u5206\u6807\u7b7e\u56fe\u50cf\u5206\u5272\u548c\u672a\u6765\u6570\u636e\u653e\u5927\u7684\u80fd\u529b\uff1b (3) \u89e3\u5256\u5b66\u635f\u5931\u51fd\u6570\uff0c\u91cf\u5316\u80be\u810f\u7684\u5bf9\u8c61\u95f4\u5173\u7cfb\u3002|[2402.19286v1](http://arxiv.org/pdf/2402.19286v1)|null|\n", "2402.19263": "|**2024-02-29**|**Spinal Osteophyte Detection via Robust Patch Extraction on minimally annotated X-rays**|\u901a\u8fc7\u5728\u6700\u5c11\u6ce8\u91ca\u7684 X \u5c04\u7ebf\u4e0a\u8fdb\u884c\u7a33\u5065\u7684\u6591\u5757\u63d0\u53d6\u6765\u68c0\u6d4b\u810a\u67f1\u9aa8\u8d58|Soumya Snigdha Kundu, Yuanhan Mo, Nicharee Srikijkasemwat, Bart\u0142omiej W. Papiez|The development and progression of arthritis is strongly associated with osteophytes, which are small and elusive bone growths. This paper presents one of the first efforts towards automated spinal osteophyte detection in spinal X-rays. A novel automated patch extraction process, called SegPatch, has been proposed based on deep learning-driven vertebrae segmentation and the enlargement of mask contours. A final patch classification accuracy of 84.5\\% is secured, surpassing a baseline tiling-based patch generation technique by 9.5%. This demonstrates that even with limited annotations, SegPatch can deliver superior performance for detection of tiny structures such as osteophytes. The proposed approach has potential to assist clinicians in expediting the process of manually identifying osteophytes in spinal X-ray.|\u5173\u8282\u708e\u7684\u53d1\u751f\u548c\u8fdb\u5c55\u4e0e\u9aa8\u8d58\u5bc6\u5207\u76f8\u5173\uff0c\u9aa8\u8d58\u662f\u4e00\u79cd\u5fae\u5c0f\u4e14\u96be\u4ee5\u6349\u6478\u7684\u9aa8\u751f\u957f\u7269\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u810a\u67f1 X \u5c04\u7ebf\u4e2d\u810a\u67f1\u9aa8\u8d58\u81ea\u52a8\u68c0\u6d4b\u7684\u9996\u6279\u6210\u679c\u4e4b\u4e00\u3002\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u690e\u9aa8\u5206\u5272\u548c\u63a9\u6a21\u8f6e\u5ed3\u7684\u653e\u5927\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a SegPatch \u7684\u65b0\u578b\u81ea\u52a8\u8865\u4e01\u63d0\u53d6\u8fc7\u7a0b\u3002\u6700\u7ec8\u8865\u4e01\u5206\u7c7b\u51c6\u786e\u5ea6\u8fbe\u5230 84.5%\uff0c\u6bd4\u57fa\u4e8e\u57fa\u7ebf\u5e73\u94fa\u7684\u8865\u4e01\u751f\u6210\u6280\u672f\u9ad8\u51fa 9.5%\u3002\u8fd9\u8868\u660e\uff0c\u5373\u4f7f\u6ce8\u91ca\u6709\u9650\uff0cSegPatch \u4e5f\u53ef\u4ee5\u4e3a\u68c0\u6d4b\u9aa8\u8d58\u7b49\u5fae\u5c0f\u7ed3\u6784\u63d0\u4f9b\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u53ef\u80fd\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u52a0\u5feb\u5728\u810a\u67f1 X \u5c04\u7ebf\u4e2d\u624b\u52a8\u8bc6\u522b\u9aa8\u8d58\u7684\u8fc7\u7a0b\u3002|[2402.19263v1](http://arxiv.org/pdf/2402.19263v1)|null|\n", "2402.19231": "|**2024-02-29**|**CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition**|CricaVPR\uff1a\u7528\u4e8e\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u7684\u8de8\u56fe\u50cf\u76f8\u5173\u611f\u77e5\u8868\u793a\u5b66\u4e60|Feng Lu, Xiangyuan Lan, Lijun Zhang, Dongmei Jiang, Yaowei Wang, Chun Yuan|Over the past decade, most methods in visual place recognition (VPR) have used neural networks to produce feature representations. These networks typically produce a global representation of a place image using only this image itself and neglect the cross-image variations (e.g. viewpoint and illumination), which limits their robustness in challenging scenes. In this paper, we propose a robust global representation method with cross-image correlation awareness for VPR, named CricaVPR. Our method uses the self-attention mechanism to correlate multiple images within a batch. These images can be taken in the same place with different conditions or viewpoints, or even captured from different places. Therefore, our method can utilize the cross-image variations as a cue to guide the representation learning, which ensures more robust features are produced. To further facilitate the robustness, we propose a multi-scale convolution-enhanced adaptation method to adapt pre-trained visual foundation models to the VPR task, which introduces the multi-scale local information to further enhance the cross-image correlation-aware representation. Experimental results show that our method outperforms state-of-the-art methods by a large margin with significantly less training time. Our method achieves 94.5% R@1 on Pitts30k using 512-dim global features. The code is released at https://github.com/Lu-Feng/CricaVPR.|\u5728\u8fc7\u53bb\u7684\u5341\u5e74\u4e2d\uff0c\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\uff08VPR\uff09\u4e2d\u7684\u5927\u591a\u6570\u65b9\u6cd5\u90fd\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u4ea7\u751f\u7279\u5f81\u8868\u793a\u3002\u8fd9\u4e9b\u7f51\u7edc\u901a\u5e38\u4ec5\u4f7f\u7528\u8be5\u56fe\u50cf\u672c\u8eab\u6765\u751f\u6210\u4f4d\u7f6e\u56fe\u50cf\u7684\u5168\u5c40\u8868\u793a\uff0c\u800c\u5ffd\u7565\u8de8\u56fe\u50cf\u53d8\u5316\uff08\u4f8b\u5982\u89c6\u70b9\u548c\u7167\u660e\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u8de8\u56fe\u50cf\u76f8\u5173\u610f\u8bc6\u7684\u9c81\u68d2\u5168\u5c40\u8868\u793a\u65b9\u6cd5\uff0c\u540d\u4e3a CricaVPR\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u5173\u8054\u6279\u6b21\u4e2d\u7684\u591a\u4e2a\u56fe\u50cf\u3002\u8fd9\u4e9b\u56fe\u50cf\u53ef\u4ee5\u5728\u540c\u4e00\u5730\u70b9\u4ee5\u4e0d\u540c\u7684\u6761\u4ef6\u6216\u89c6\u89d2\u62cd\u6444\uff0c\u751a\u81f3\u53ef\u4ee5\u4ece\u4e0d\u540c\u7684\u5730\u70b9\u62cd\u6444\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5229\u7528\u8de8\u56fe\u50cf\u53d8\u5316\u4f5c\u4e3a\u6307\u5bfc\u8868\u793a\u5b66\u4e60\u7684\u7ebf\u7d22\uff0c\u4ece\u800c\u786e\u4fdd\u4ea7\u751f\u66f4\u7a33\u5065\u7684\u7279\u5f81\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u5377\u79ef\u589e\u5f3a\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u4f7f\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u9002\u5e94VPR\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u591a\u5c3a\u5ea6\u5c40\u90e8\u4fe1\u606f\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u8de8\u56fe\u50cf\u76f8\u5173\u611f\u77e5\u8868\u793a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u95f4\u663e\u7740\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\u5927\u5e45\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528 512 \u7ef4\u5168\u5c40\u7279\u5f81\u5728 Pitts30k \u4e0a\u5b9e\u73b0\u4e86 94.5% R@1\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/Lu-Feng/CricaVPR\u3002|[2402.19231v1](http://arxiv.org/pdf/2402.19231v1)|null|\n", "2402.19160": "|**2024-02-29**|**Effective Message Hiding with Order-Preserving Mechanisms**|\u901a\u8fc7\u4fdd\u5e8f\u673a\u5236\u6709\u6548\u9690\u85cf\u6d88\u606f|Gao Yu, Qiu Xuchong, Ye Zihan|Message hiding, a technique that conceals secret message bits within a cover image, aims to achieve an optimal balance among message capacity, recovery accuracy, and imperceptibility. While convolutional neural networks have notably improved message capacity and imperceptibility, achieving high recovery accuracy remains challenging. This challenge arises because convolutional operations struggle to preserve the sequential order of message bits and effectively address the discrepancy between these two modalities. To address this, we propose StegaFormer, an innovative MLP-based framework designed to preserve bit order and enable global fusion between modalities. Specifically, StegaFormer incorporates three crucial components: Order-Preserving Message Encoder (OPME), Decoder (OPMD) and Global Message-Image Fusion (GMIF). OPME and OPMD aim to preserve the order of message bits by segmenting the entire sequence into equal-length segments and incorporating sequential information during encoding and decoding. Meanwhile, GMIF employs a cross-modality fusion mechanism to effectively fuse the features from the two uncorrelated modalities. Experimental results on the COCO and DIV2K datasets demonstrate that StegaFormer surpasses existing state-of-the-art methods in terms of recovery accuracy, message capacity, and imperceptibility. We will make our code publicly available.|\u6d88\u606f\u9690\u85cf\u662f\u4e00\u79cd\u5728\u5c01\u9762\u56fe\u50cf\u4e2d\u9690\u85cf\u79d8\u5bc6\u6d88\u606f\u4f4d\u7684\u6280\u672f\uff0c\u65e8\u5728\u5b9e\u73b0\u6d88\u606f\u5bb9\u91cf\u3001\u6062\u590d\u51c6\u786e\u6027\u548c\u4e0d\u53ef\u5bdf\u89c9\u6027\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002\u867d\u7136\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u663e\u7740\u63d0\u9ad8\u4e86\u6d88\u606f\u5bb9\u91cf\u548c\u4e0d\u53ef\u611f\u77e5\u6027\uff0c\u4f46\u5b9e\u73b0\u9ad8\u6062\u590d\u7cbe\u5ea6\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u51fa\u73b0\u8fd9\u4e00\u6311\u6218\u662f\u56e0\u4e3a\u5377\u79ef\u8fd0\u7b97\u96be\u4ee5\u4fdd\u6301\u6d88\u606f\u4f4d\u7684\u987a\u5e8f\u5e76\u6709\u6548\u89e3\u51b3\u8fd9\u4e24\u79cd\u6a21\u5f0f\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 StegaFormer\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e MLP \u7684\u521b\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u4fdd\u7559\u4f4d\u987a\u5e8f\u5e76\u5b9e\u73b0\u6a21\u6001\u4e4b\u95f4\u7684\u5168\u5c40\u878d\u5408\u3002\u5177\u4f53\u6765\u8bf4\uff0cStegaFormer \u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u4fdd\u5e8f\u6d88\u606f\u7f16\u7801\u5668 (OPME)\u3001\u89e3\u7801\u5668 (OPMD) \u548c\u5168\u5c40\u6d88\u606f\u56fe\u50cf\u878d\u5408 (GMIF)\u3002 OPME \u548c OPMD \u65e8\u5728\u901a\u8fc7\u5c06\u6574\u4e2a\u5e8f\u5217\u5206\u5272\u6210\u7b49\u957f\u7684\u7247\u6bb5\u5e76\u5728\u7f16\u7801\u548c\u89e3\u7801\u671f\u95f4\u5408\u5e76\u987a\u5e8f\u4fe1\u606f\u6765\u4fdd\u7559\u6d88\u606f\u4f4d\u7684\u987a\u5e8f\u3002\u540c\u65f6\uff0cGMIF\u91c7\u7528\u8de8\u6a21\u6001\u878d\u5408\u673a\u5236\u6765\u6709\u6548\u878d\u5408\u4e24\u79cd\u4e0d\u76f8\u5173\u6a21\u6001\u7684\u7279\u5f81\u3002 COCO \u548c DIV2K \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cStegaFormer \u5728\u6062\u590d\u7cbe\u5ea6\u3001\u6d88\u606f\u5bb9\u91cf\u548c\u4e0d\u53ef\u611f\u77e5\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u6211\u4eec\u5c06\u516c\u5f00\u6211\u4eec\u7684\u4ee3\u7801\u3002|[2402.19160v1](http://arxiv.org/pdf/2402.19160v1)|null|\n", "2402.19145": "|**2024-02-29**|**A SAM-guided Two-stream Lightweight Model for Anomaly Detection**|SAM\u5f15\u5bfc\u7684\u4e24\u6d41\u8f7b\u91cf\u7ea7\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b|Chenghao Li, Lei Qi, Xin Geng|In industrial anomaly detection, model efficiency and mobile-friendliness become the primary concerns in real-world applications. Simultaneously, the impressive generalization capabilities of Segment Anything (SAM) have garnered broad academic attention, making it an ideal choice for localizing unseen anomalies and diverse real-world patterns. In this paper, considering these two critical factors, we propose a SAM-guided Two-stream Lightweight Model for unsupervised anomaly detection (STLM) that not only aligns with the two practical application requirements but also harnesses the robust generalization capabilities of SAM. We employ two lightweight image encoders, i.e., our two-stream lightweight module, guided by SAM's knowledge. To be specific, one stream is trained to generate discriminative and general feature representations in both normal and anomalous regions, while the other stream reconstructs the same images without anomalies, which effectively enhances the differentiation of two-stream representations when facing anomalous regions. Furthermore, we employ a shared mask decoder and a feature aggregation module to generate anomaly maps. Our experiments conducted on MVTec AD benchmark show that STLM, with about 16M parameters and achieving an inference time in 20ms, competes effectively with state-of-the-art methods in terms of performance, 98.26% on pixel-level AUC and 94.92% on PRO. We further experiment on more difficult datasets, e.g., VisA and DAGM, to demonstrate the effectiveness and generalizability of STLM.|\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\uff0c\u6a21\u578b\u6548\u7387\u548c\u79fb\u52a8\u53cb\u597d\u6027\u6210\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e3b\u8981\u5173\u6ce8\u70b9\u3002\u540c\u65f6\uff0cSegment Anything (SAM) \u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6cdb\u5316\u80fd\u529b\u5f15\u8d77\u4e86\u5b66\u672f\u754c\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f7f\u5176\u6210\u4e3a\u5b9a\u4f4d\u672a\u89c1\u5f02\u5e38\u548c\u591a\u6837\u5316\u73b0\u5b9e\u4e16\u754c\u6a21\u5f0f\u7684\u7406\u60f3\u9009\u62e9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u8003\u8651\u5230\u8fd9\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff08STLM\uff09\u7684 SAM \u5f15\u5bfc\u7684\u53cc\u6d41\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4e0d\u4ec5\u7b26\u5408\u4e24\u4e2a\u5b9e\u9645\u5e94\u7528\u9700\u6c42\uff0c\u800c\u4e14\u8fd8\u5229\u7528\u4e86 SAM \u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u91c7\u7528\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u5373\u6211\u4eec\u7684\u53cc\u6d41\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u4ee5 SAM \u77e5\u8bc6\u4e3a\u6307\u5bfc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e00\u4e2a\u6d41\u88ab\u8bad\u7ec3\u4e3a\u5728\u6b63\u5e38\u533a\u57df\u548c\u5f02\u5e38\u533a\u57df\u4e2d\u751f\u6210\u6709\u533a\u522b\u7684\u548c\u4e00\u822c\u7684\u7279\u5f81\u8868\u793a\uff0c\u800c\u53e6\u4e00\u4e2a\u6d41\u5219\u91cd\u5efa\u6ca1\u6709\u5f02\u5e38\u7684\u76f8\u540c\u56fe\u50cf\uff0c\u8fd9\u6709\u6548\u5730\u589e\u5f3a\u4e86\u4e24\u6d41\u8868\u793a\u5728\u9762\u5bf9\u5f02\u5e38\u533a\u57df\u65f6\u7684\u533a\u5206\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528\u5171\u4eab\u63a9\u6a21\u89e3\u7801\u5668\u548c\u7279\u5f81\u805a\u5408\u6a21\u5757\u6765\u751f\u6210\u5f02\u5e38\u56fe\u3002\u6211\u4eec\u5728 MVTec AD \u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTLM \u5177\u6709\u7ea6 16M \u53c2\u6570\u5e76\u5b9e\u73b0\u4e86 20ms \u7684\u63a8\u7406\u65f6\u95f4\uff0c\u5728\u6027\u80fd\u65b9\u9762\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u6709\u6548\u7ade\u4e89\uff0c\u50cf\u7d20\u7ea7 AUC \u4e3a 98.26%\uff0c\u50cf\u7d20\u7ea7 AUC \u4e3a 94.92%\u3002\u4e13\u4e1a\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5728\u66f4\u56f0\u96be\u7684\u6570\u636e\u96c6\uff08\u4f8b\u5982 VisA \u548c DAGM\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4ee5\u8bc1\u660e STLM \u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002|[2402.19145v1](http://arxiv.org/pdf/2402.19145v1)|null|\n", "2402.19142": "|**2024-02-29**|**ProtoP-OD: Explainable Object Detection with Prototypical Parts**|ProtoP-OD\uff1a\u4f7f\u7528\u539f\u578b\u90e8\u4ef6\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u5bf9\u8c61\u68c0\u6d4b|Pavlos Rath-Manakidis, Frederik Strothmann, Tobias Glasmachers, Laurenz Wiskott|Interpretation and visualization of the behavior of detection transformers tends to highlight the locations in the image that the model attends to, but it provides limited insight into the \\emph{semantics} that the model is focusing on. This paper introduces an extension to detection transformers that constructs prototypical local features and uses them in object detection. These custom features, which we call prototypical parts, are designed to be mutually exclusive and align with the classifications of the model. The proposed extension consists of a bottleneck module, the prototype neck, that computes a discretized representation of prototype activations and a new loss term that matches prototypes to object classes. This setup leads to interpretable representations in the prototype neck, allowing visual inspection of the image content perceived by the model and a better understanding of the model's reliability. We show experimentally that our method incurs only a limited performance penalty, and we provide examples that demonstrate the quality of the explanations provided by our method, which we argue outweighs the performance penalty.|\u68c0\u6d4b\u53d8\u538b\u5668\u884c\u4e3a\u7684\u89e3\u91ca\u548c\u53ef\u89c6\u5316\u5f80\u5f80\u4f1a\u7a81\u51fa\u663e\u793a\u6a21\u578b\u5173\u6ce8\u7684\u56fe\u50cf\u4e2d\u7684\u4f4d\u7f6e\uff0c\u4f46\u5b83\u5bf9\u6a21\u578b\u5173\u6ce8\u7684\\emph{\u8bed\u4e49}\u63d0\u4f9b\u7684\u4e86\u89e3\u6709\u9650\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u68c0\u6d4b\u53d8\u538b\u5668\u7684\u6269\u5c55\uff0c\u5b83\u6784\u9020\u5178\u578b\u7684\u5c40\u90e8\u7279\u5f81\u5e76\u5c06\u5176\u7528\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u3002\u8fd9\u4e9b\u81ea\u5b9a\u4e49\u7279\u5f81\uff08\u6211\u4eec\u79f0\u4e4b\u4e3a\u539f\u578b\u96f6\u4ef6\uff09\u88ab\u8bbe\u8ba1\u4e3a\u76f8\u4e92\u6392\u65a5\u5e76\u4e0e\u6a21\u578b\u7684\u5206\u7c7b\u4fdd\u6301\u4e00\u81f4\u3002\u6240\u63d0\u51fa\u7684\u6269\u5c55\u7531\u74f6\u9888\u6a21\u5757\uff08\u539f\u578b\u9888\u90e8\uff09\u7ec4\u6210\uff0c\u5b83\u8ba1\u7b97\u539f\u578b\u6fc0\u6d3b\u7684\u79bb\u6563\u8868\u793a\u4ee5\u53ca\u5c06\u539f\u578b\u4e0e\u5bf9\u8c61\u7c7b\u76f8\u5339\u914d\u7684\u65b0\u635f\u5931\u9879\u3002\u8fd9\u79cd\u8bbe\u7f6e\u53ef\u4ee5\u5728\u539f\u578b\u9888\u90e8\u4e2d\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u8868\u793a\uff0c\u4ece\u800c\u53ef\u4ee5\u76ee\u89c6\u68c0\u67e5\u6a21\u578b\u611f\u77e5\u7684\u56fe\u50cf\u5185\u5bb9\uff0c\u5e76\u66f4\u597d\u5730\u7406\u89e3\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ec5\u4f1a\u4ea7\u751f\u6709\u9650\u7684\u6027\u80fd\u635f\u5931\uff0c\u5e76\u4e14\u6211\u4eec\u63d0\u4f9b\u4e86\u793a\u4f8b\u6765\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u4f9b\u7684\u89e3\u91ca\u7684\u8d28\u91cf\uff0c\u6211\u4eec\u8ba4\u4e3a\u8fd9\u8d85\u8fc7\u4e86\u6027\u80fd\u635f\u5931\u3002|[2402.19142v1](http://arxiv.org/pdf/2402.19142v1)|null|\n", "2402.19122": "|**2024-02-29**|**BigGait: Learning Gait Representation You Want by Large Vision Models**|BigGait\uff1a\u901a\u8fc7\u5927\u89c6\u89c9\u6a21\u578b\u5b66\u4e60\u60a8\u60f3\u8981\u7684\u6b65\u6001\u8868\u793a|Dingqiang Ye, Chao Fan, Jingzhe Ma, Xiaoming Liu, Shiqi Yu|Gait recognition stands as one of the most pivotal remote identification technologies and progressively expands across research and industrial communities. However, existing gait recognition methods heavily rely on task-specific upstream driven by supervised learning to provide explicit gait representations, which inevitably introduce expensive annotation costs and potentially cause cumulative errors. Escaping from this trend, this work explores effective gait representations based on the all-purpose knowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a simple yet efficient gait framework, termed BigGait. Specifically, the Gait Representation Extractor (GRE) in BigGait effectively transforms all-purpose knowledge into implicit gait features in an unsupervised manner, drawing from design principles of established gait representation construction approaches. Experimental results on CCPG, CAISA-B* and SUSTech1K indicate that BigGait significantly outperforms the previous methods in both self-domain and cross-domain tasks in most cases, and provides a more practical paradigm for learning the next-generation gait representation. Eventually, we delve into prospective challenges and promising directions in LVMs-based gait recognition, aiming to inspire future work in this emerging topic. The source code will be available at https://github.com/ShiqiYu/OpenGait.|\u6b65\u6001\u8bc6\u522b\u662f\u6700\u5173\u952e\u7684\u8fdc\u7a0b\u8bc6\u522b\u6280\u672f\u4e4b\u4e00\uff0c\u5e76\u9010\u6e10\u6269\u5c55\u5230\u7814\u7a76\u548c\u5de5\u4e1a\u754c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4e8e\u76d1\u7763\u5b66\u4e60\u9a71\u52a8\u7684\u7279\u5b9a\u4efb\u52a1\u4e0a\u6e38\u6765\u63d0\u4f9b\u660e\u786e\u7684\u6b65\u6001\u8868\u793a\uff0c\u8fd9\u4e0d\u53ef\u907f\u514d\u5730\u5f15\u5165\u6602\u8d35\u7684\u6ce8\u91ca\u6210\u672c\u5e76\u53ef\u80fd\u5bfc\u81f4\u7d2f\u79ef\u9519\u8bef\u3002\u4e3a\u4e86\u6446\u8131\u8fd9\u4e00\u8d8b\u52bf\uff0c\u8fd9\u9879\u5de5\u4f5c\u57fa\u4e8e\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u5927\u89c6\u89c9\u6a21\u578b\uff08LVM\uff09\u4ea7\u751f\u7684\u901a\u7528\u77e5\u8bc6\uff0c\u63a2\u7d22\u4e86\u6709\u6548\u7684\u6b65\u6001\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u9ad8\u6548\u7684\u6b65\u6001\u6846\u67b6\uff0c\u79f0\u4e3a BigGait\u3002\u5177\u4f53\u6765\u8bf4\uff0cBigGait \u4e2d\u7684\u6b65\u6001\u8868\u793a\u63d0\u53d6\u5668 (GRE) \u501f\u9274\u5df2\u5efa\u7acb\u7684\u6b65\u6001\u8868\u793a\u6784\u5efa\u65b9\u6cd5\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u4ee5\u65e0\u76d1\u7763\u7684\u65b9\u5f0f\u6709\u6548\u5730\u5c06\u901a\u7528\u77e5\u8bc6\u8f6c\u5316\u4e3a\u9690\u5f0f\u6b65\u6001\u7279\u5f81\u3002 CCPG\u3001CAISA-B*\u548cSUSTech1K\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBigGait\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5728\u81ea\u57df\u548c\u8de8\u57df\u4efb\u52a1\u4e2d\u90fd\u663e\u7740\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3a\u5b66\u4e60\u4e0b\u4e00\u4ee3\u6b65\u6001\u8868\u793a\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u8303\u5f0f\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86\u57fa\u4e8e LVM \u7684\u6b65\u6001\u8bc6\u522b\u7684\u6f5c\u5728\u6311\u6218\u548c\u6709\u5e0c\u671b\u7684\u65b9\u5411\uff0c\u65e8\u5728\u6fc0\u53d1\u8fd9\u4e00\u65b0\u5174\u4e3b\u9898\u7684\u672a\u6765\u5de5\u4f5c\u3002\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/ShiqiYu/OpenGait \u83b7\u53d6\u3002|[2402.19122v1](http://arxiv.org/pdf/2402.19122v1)|null|\n", "2402.19091": "|**2024-02-29**|**Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection**|\u5229\u7528\u4e2d\u95f4\u7f16\u7801\u5668\u5757\u7684\u8868\u793a\u8fdb\u884c\u5408\u6210\u56fe\u50cf\u68c0\u6d4b|Christos Koutlis, Symeon Papadopoulos|The recently developed and publicly available synthetic image generation methods and services make it possible to create extremely realistic imagery on demand, raising great risks for the integrity and safety of online information. State-of-the-art Synthetic Image Detection (SID) research has led to strong evidence on the advantages of feature extraction from foundation models. However, such extracted features mostly encapsulate high-level visual semantics instead of fine-grained details, which are more important for the SID task. On the contrary, shallow layers encode low-level visual information. In this work, we leverage the image representations extracted by intermediate Transformer blocks of CLIP's image-encoder via a lightweight network that maps them to a learnable forgery-aware vector space capable of generalizing exceptionally well. We also employ a trainable module to incorporate the importance of each Transformer block to the final prediction. Our method is compared against the state-of-the-art by evaluating it on 20 test datasets and exhibits an average +10.6% absolute performance improvement. Notably, the best performing models require just a single epoch for training (~8 minutes). Code available at https://github.com/mever-team/rine.|\u6700\u8fd1\u5f00\u53d1\u5e76\u516c\u5f00\u7684\u5408\u6210\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u548c\u670d\u52a1\u4f7f\u5f97\u6309\u9700\u521b\u5efa\u6781\u5176\u903c\u771f\u7684\u56fe\u50cf\u6210\u4e3a\u53ef\u80fd\uff0c\u8fd9\u7ed9\u5728\u7ebf\u4fe1\u606f\u7684\u5b8c\u6574\u6027\u548c\u5b89\u5168\u6027\u5e26\u6765\u4e86\u5de8\u5927\u7684\u98ce\u9669\u3002\u6700\u5148\u8fdb\u7684\u5408\u6210\u56fe\u50cf\u68c0\u6d4b\uff08SID\uff09\u7814\u7a76\u4e3a\u4ece\u57fa\u7840\u6a21\u578b\u4e2d\u63d0\u53d6\u7279\u5f81\u7684\u4f18\u52bf\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u8bc1\u636e\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u63d0\u53d6\u7684\u7279\u5f81\u5927\u591a\u5c01\u88c5\u4e86\u9ad8\u7ea7\u89c6\u89c9\u8bed\u4e49\uff0c\u800c\u4e0d\u662f\u7ec6\u7c92\u5ea6\u7684\u7ec6\u8282\uff0c\u8fd9\u5bf9\u4e8e SID \u4efb\u52a1\u66f4\u4e3a\u91cd\u8981\u3002\u76f8\u53cd\uff0c\u6d45\u5c42\u7f16\u7801\u4f4e\u7ea7\u89c6\u89c9\u4fe1\u606f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5229\u7528 CLIP \u56fe\u50cf\u7f16\u7801\u5668\u7684\u4e2d\u95f4 Transformer \u5757\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7f51\u7edc\u63d0\u53d6\u7684\u56fe\u50cf\u8868\u793a\uff0c\u5c06\u5b83\u4eec\u6620\u5c04\u5230\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u7684\u53ef\u5b66\u4e60\u7684\u4f2a\u9020\u611f\u77e5\u5411\u91cf\u7a7a\u95f4\u3002\u6211\u4eec\u8fd8\u91c7\u7528\u53ef\u8bad\u7ec3\u6a21\u5757\u5c06\u6bcf\u4e2a Transformer \u5757\u7684\u91cd\u8981\u6027\u7eb3\u5165\u6700\u7ec8\u9884\u6d4b\u3002\u901a\u8fc7\u5728 20 \u4e2a\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793a\u5e73\u5747\u7edd\u5bf9\u6027\u80fd\u63d0\u9ad8\u4e86 +10.6%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6027\u80fd\u6700\u4f73\u7684\u6a21\u578b\u53ea\u9700\u8981\u4e00\u4e2a\u65f6\u671f\u7684\u8bad\u7ec3\uff08\u7ea6 8 \u5206\u949f\uff09\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/mever-team/rine \u83b7\u53d6\u3002|[2402.19091v1](http://arxiv.org/pdf/2402.19091v1)|null|\n", "2402.19082": "|**2024-02-29**|**VideoMAC: Video Masked Autoencoders Meet ConvNets**|VideoMAC\uff1a\u89c6\u9891\u5c4f\u853d\u81ea\u52a8\u7f16\u7801\u5668\u9047\u89c1\u5377\u79ef\u7f51\u7edc|Gensheng Pei, Tao Chen, Xiruo Jiang, Huafeng Liu, Zeren Sun, Yazhou Yao|Recently, the advancement of self-supervised learning techniques, like masked autoencoders (MAE), has greatly influenced visual representation learning for images and videos. Nevertheless, it is worth noting that the predominant approaches in existing masked image / video modeling rely excessively on resource-intensive vision transformers (ViTs) as the feature encoder. In this paper, we propose a new approach termed as \\textbf{VideoMAC}, which combines video masked autoencoders with resource-friendly ConvNets. Specifically, VideoMAC employs symmetric masking on randomly sampled pairs of video frames. To prevent the issue of mask pattern dissipation, we utilize ConvNets which are implemented with sparse convolutional operators as encoders. Simultaneously, we present a simple yet effective masked video modeling (MVM) approach, a dual encoder architecture comprising an online encoder and an exponential moving average target encoder, aimed to facilitate inter-frame reconstruction consistency in videos. Additionally, we demonstrate that VideoMAC, empowering classical (ResNet) / modern (ConvNeXt) convolutional encoders to harness the benefits of MVM, outperforms ViT-based approaches on downstream tasks, including video object segmentation (+\\textbf{5.2\\%} / \\textbf{6.4\\%} $\\mathcal{J}\\&\\mathcal{F}$), body part propagation (+\\textbf{6.3\\%} / \\textbf{3.1\\%} mIoU), and human pose tracking (+\\textbf{10.2\\%} / \\textbf{11.1\\%} PCK@0.1).|\u6700\u8fd1\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u5982\u63a9\u6a21\u81ea\u52a8\u7f16\u7801\u5668\uff08MAE\uff09\uff0c\u6781\u5927\u5730\u5f71\u54cd\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u7684\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u3002\u7136\u800c\uff0c\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u73b0\u6709\u8499\u7248\u56fe\u50cf/\u89c6\u9891\u5efa\u6a21\u4e2d\u7684\u4e3b\u8981\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u4f5c\u4e3a\u7279\u5f81\u7f16\u7801\u5668\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a \\textbf{VideoMAC} \u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u5c06\u89c6\u9891\u5c4f\u853d\u81ea\u52a8\u7f16\u7801\u5668\u4e0e\u8d44\u6e90\u53cb\u597d\u7684\u5377\u79ef\u7f51\u7edc\u76f8\u7ed3\u5408\u3002\u5177\u4f53\u6765\u8bf4\uff0cVideoMAC \u5bf9\u968f\u673a\u91c7\u6837\u7684\u89c6\u9891\u5e27\u5bf9\u91c7\u7528\u5bf9\u79f0\u63a9\u7801\u3002\u4e3a\u4e86\u9632\u6b62\u63a9\u6a21\u56fe\u6848\u8017\u6563\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5229\u7528\u7528\u7a00\u758f\u5377\u79ef\u7b97\u5b50\u5b9e\u73b0\u7684ConvNet\u4f5c\u4e3a\u7f16\u7801\u5668\u3002\u540c\u65f6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u63a9\u6a21\u89c6\u9891\u5efa\u6a21\uff08MVM\uff09\u65b9\u6cd5\uff0c\u4e00\u79cd\u7531\u5728\u7ebf\u7f16\u7801\u5668\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u76ee\u6807\u7f16\u7801\u5668\u7ec4\u6210\u7684\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u65e8\u5728\u4fc3\u8fdb\u89c6\u9891\u4e2d\u5e27\u95f4\u91cd\u5efa\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bc1\u660e VideoMAC \u4f7f\u7ecf\u5178 (ResNet) / \u73b0\u4ee3 (ConvNeXt) \u5377\u79ef\u7f16\u7801\u5668\u80fd\u591f\u5229\u7528 MVM \u7684\u4f18\u52bf\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u4e8e ViT \u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u89c6\u9891\u5bf9\u8c61\u5206\u5272 (+\\textbf{5.2\\%} / \\ textbf{6.4\\%} $\\mathcal{J}\\&\\mathcal{F}$)\u3001\u8eab\u4f53\u90e8\u4f4d\u4f20\u64ad (+\\textbf{6.3\\%} / \\textbf{3.1\\%} mIoU) \u548c\u4eba\u4f53\u59ff\u52bf\u8ddf\u8e2a ( +\\textbf{10.2\\%} / \\textbf{11.1\\%} PCK@0.1)\u3002|[2402.19082v1](http://arxiv.org/pdf/2402.19082v1)|null|\n", "2402.19059": "|**2024-02-29**|**VEnvision3D: A Synthetic Perception Dataset for 3D Multi-Task Model Research**|VEnvision3D\uff1a\u7528\u4e8e 3D \u591a\u4efb\u52a1\u6a21\u578b\u7814\u7a76\u7684\u7efc\u5408\u611f\u77e5\u6570\u636e\u96c6|Jiahao Zhou, Chen Long, Yue Xie, Jialiang Wang, Boheng Li, Haiping Wang, Zhe Chen, Zhen Dong|Developing a unified multi-task foundation model has become a critical challenge in computer vision research. In the current field of 3D computer vision, most datasets solely focus on a relatively limited set of tasks, which complicates the concurrent training requirements of various downstream tasks. This makes the training of multi-objective networks difficult to proceed with, which further hinders the development of foundation models in the 3D vision field. In this paper, we introduce VEnvision3D, a large 3D synthetic perception dataset for multi-task learning, including depth completion, segmentation, upsampling, place recognition, and 3D reconstruction. Since the data for each task was collected in the same scenarios, tasks are inherently aligned in terms of the utilized data. Therefore, such a unique attribute can assist in exploring the potential for the multi-task model and even the foundation model without separate training methods. Several new benchmarks based on the characteristics of the proposed dataset were presented. Extensive studies were performed on end-to-end models, revealing new observations, challenges, and opportunities for future research. In addition, we designed a straightfoward multi-task network to uncover the ability that VEnvision3D can offer for the foundation model. Our dataset and code will be open-sourced upon acceptance.|\u5f00\u53d1\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u57fa\u7840\u6a21\u578b\u5df2\u6210\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u5728\u5f53\u524d\u7684 3D \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u5927\u591a\u6570\u6570\u636e\u96c6\u4ec5\u5173\u6ce8\u76f8\u5bf9\u6709\u9650\u7684\u4e00\u7ec4\u4efb\u52a1\uff0c\u8fd9\u4f7f\u5f97\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u5e76\u53d1\u8bad\u7ec3\u8981\u6c42\u53d8\u5f97\u590d\u6742\u3002\u8fd9\u4f7f\u5f97\u591a\u76ee\u6807\u7f51\u7edc\u7684\u8bad\u7ec3\u96be\u4ee5\u8fdb\u884c\uff0c\u8fdb\u4e00\u6b65\u963b\u788d\u4e863D\u89c6\u89c9\u9886\u57df\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 VEnvision3D\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u5927\u578b 3D \u5408\u6210\u611f\u77e5\u6570\u636e\u96c6\uff0c\u5305\u62ec\u6df1\u5ea6\u8865\u5168\u3001\u5206\u5272\u3001\u4e0a\u91c7\u6837\u3001\u4f4d\u7f6e\u8bc6\u522b\u548c 3D \u91cd\u5efa\u3002\u7531\u4e8e\u6bcf\u4e2a\u4efb\u52a1\u7684\u6570\u636e\u90fd\u662f\u5728\u76f8\u540c\u7684\u573a\u666f\u4e2d\u6536\u96c6\u7684\uff0c\u56e0\u6b64\u4efb\u52a1\u5728\u6240\u4f7f\u7528\u7684\u6570\u636e\u65b9\u9762\u672c\u8d28\u4e0a\u662f\u4e00\u81f4\u7684\u3002\u56e0\u6b64\uff0c\u8fd9\u79cd\u72ec\u7279\u7684\u5c5e\u6027\u53ef\u4ee5\u5e2e\u52a9\u63a2\u7d22\u591a\u4efb\u52a1\u6a21\u578b\u751a\u81f3\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u800c\u65e0\u9700\u5355\u72ec\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002\u63d0\u51fa\u4e86\u4e00\u4e9b\u57fa\u4e8e\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u7279\u5f81\u7684\u65b0\u57fa\u51c6\u3002\u5bf9\u7aef\u5230\u7aef\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u65b0\u7684\u89c2\u5bdf\u7ed3\u679c\u3001\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u7684\u673a\u9047\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u591a\u4efb\u52a1\u7f51\u7edc\u6765\u63ed\u793a VEnvision3D \u53ef\u4ee5\u4e3a\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u7684\u529f\u80fd\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u5728\u63a5\u53d7\u540e\u5f00\u6e90\u3002|[2402.19059v1](http://arxiv.org/pdf/2402.19059v1)|null|\n", "2402.19007": "|**2024-02-29**|**DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments**|DOZE\uff1a\u52a8\u6001\u73af\u5883\u4e2d\u5f00\u653e\u8bcd\u6c47\u96f6\u6837\u672c\u5bf9\u8c61\u5bfc\u822a\u7684\u6570\u636e\u96c6|Ji Ma, Hongming Dai, Yao Mu, Pengying Wu, Hao Wang, Xiaowei Chi, Yang Fei, Shanghang Zhang, Chang Liu|Zero-Shot Object Navigation (ZSON) requires agents to autonomously locate and approach unseen objects in unfamiliar environments and has emerged as a particularly challenging task within the domain of Embodied AI. Existing datasets for developing ZSON algorithms lack consideration of dynamic obstacles, object attribute diversity, and scene texts, thus exhibiting noticeable discrepancy from real-world situations. To address these issues, we propose a Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments (DOZE) that comprises ten high-fidelity 3D scenes with over 18k tasks, aiming to mimic complex, dynamic real-world scenarios. Specifically, DOZE scenes feature multiple moving humanoid obstacles, a wide array of open-vocabulary objects, diverse distinct-attribute objects, and valuable textual hints. Besides, different from existing datasets that only provide collision checking between the agent and static obstacles, we enhance DOZE by integrating capabilities for detecting collisions between the agent and moving obstacles. This novel functionality enables evaluation of the agents' collision avoidance abilities in dynamic environments. We test four representative ZSON methods on DOZE, revealing substantial room for improvement in existing approaches concerning navigation efficiency, safety, and object recognition accuracy. Our dataset could be found at https://DOZE-Dataset.github.io/.|\u96f6\u5c04\u51fb\u5bf9\u8c61\u5bfc\u822a\uff08ZSON\uff09\u8981\u6c42\u4ee3\u7406\u5728\u964c\u751f\u7684\u73af\u5883\u4e2d\u81ea\u4e3b\u5b9a\u4f4d\u548c\u63a5\u8fd1\u770b\u4e0d\u89c1\u7684\u5bf9\u8c61\uff0c\u5e76\u4e14\u5df2\u6210\u4e3a\u5d4c\u5165\u5f0f\u4eba\u5de5\u667a\u80fd\u9886\u57df\u4e2d\u4e00\u9879\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u73b0\u6709\u7528\u4e8e\u5f00\u53d1 ZSON \u7b97\u6cd5\u7684\u6570\u636e\u96c6\u7f3a\u4e4f\u5bf9\u52a8\u6001\u969c\u788d\u7269\u3001\u5bf9\u8c61\u5c5e\u6027\u591a\u6837\u6027\u548c\u573a\u666f\u6587\u672c\u7684\u8003\u8651\uff0c\u56e0\u6b64\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u60c5\u51b5\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u5f00\u653e\u8bcd\u6c47\u96f6\u5c04\u51fb\u5bf9\u8c61\u5bfc\u822a\uff08DOZE\uff09\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5341\u4e2a\u9ad8\u4fdd\u771f 3D \u573a\u666f\u548c\u8d85\u8fc7 18k \u7684\u4efb\u52a1\uff0c\u65e8\u5728\u6a21\u62df\u590d\u6742\u3001\u52a8\u6001\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u3002\u5177\u4f53\u6765\u8bf4\uff0cDOZE \u573a\u666f\u5177\u6709\u591a\u4e2a\u79fb\u52a8\u7684\u4eba\u5f62\u969c\u788d\u3001\u5927\u91cf\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u3001\u5404\u79cd\u4e0d\u540c\u5c5e\u6027\u7684\u5bf9\u8c61\u4ee5\u53ca\u6709\u4ef7\u503c\u7684\u6587\u672c\u63d0\u793a\u3002\u6b64\u5916\uff0c\u4e0e\u4ec5\u63d0\u4f9b\u4ee3\u7406\u548c\u9759\u6001\u969c\u788d\u7269\u4e4b\u95f4\u7684\u78b0\u649e\u68c0\u67e5\u7684\u73b0\u6709\u6570\u636e\u96c6\u4e0d\u540c\uff0c\u6211\u4eec\u901a\u8fc7\u96c6\u6210\u68c0\u6d4b\u4ee3\u7406\u548c\u79fb\u52a8\u969c\u788d\u7269\u4e4b\u95f4\u7684\u78b0\u649e\u7684\u529f\u80fd\u6765\u589e\u5f3a DOZE\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u529f\u80fd\u53ef\u4ee5\u8bc4\u4f30\u4ee3\u7406\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9632\u649e\u80fd\u529b\u3002\u6211\u4eec\u5728 DOZE \u4e0a\u6d4b\u8bd5\u4e86\u56db\u79cd\u4ee3\u8868\u6027\u7684 ZSON \u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5bfc\u822a\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u7269\u4f53\u8bc6\u522b\u51c6\u786e\u6027\u65b9\u9762\u8fd8\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u5728 https://DOZE-Dataset.github.io/ \u627e\u5230\u3002|[2402.19007v1](http://arxiv.org/pdf/2402.19007v1)|null|\n", "2402.19004": "|**2024-02-29**|**RSAM-Seg: A SAM-based Approach with Prior Knowledge Integration for Remote Sensing Image Semantic Segmentation**|RSAM-Seg\uff1a\u57fa\u4e8e SAM \u7684\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u5148\u9a8c\u77e5\u8bc6\u96c6\u6210\u65b9\u6cd5|Jie Zhang, Xubing Yang, Rui Jiang, Wei Shao, Li Zhang|The development of high-resolution remote sensing satellites has provided great convenience for research work related to remote sensing. Segmentation and extraction of specific targets are essential tasks when facing the vast and complex remote sensing images. Recently, the introduction of Segment Anything Model (SAM) provides a universal pre-training model for image segmentation tasks. While the direct application of SAM to remote sensing image segmentation tasks does not yield satisfactory results, we propose RSAM-Seg, which stands for Remote Sensing SAM with Semantic Segmentation, as a tailored modification of SAM for the remote sensing field and eliminates the need for manual intervention to provide prompts. Adapter-Scale, a set of supplementary scaling modules, are proposed in the multi-head attention blocks of the encoder part of SAM. Furthermore, Adapter-Feature are inserted between the Vision Transformer (ViT) blocks. These modules aim to incorporate high-frequency image information and image embedding features to generate image-informed prompts. Experiments are conducted on four distinct remote sensing scenarios, encompassing cloud detection, field monitoring, building detection and road mapping tasks . The experimental results not only showcase the improvement over the original SAM and U-Net across cloud, buildings, fields and roads scenarios, but also highlight the capacity of RSAM-Seg to discern absent areas within the ground truth of certain datasets, affirming its potential as an auxiliary annotation method. In addition, the performance in few-shot scenarios is commendable, underscores its potential in dealing with limited datasets.|\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u536b\u661f\u7684\u53d1\u5c55\u4e3a\u9065\u611f\u76f8\u5173\u7814\u7a76\u5de5\u4f5c\u63d0\u4f9b\u4e86\u6781\u5927\u4fbf\u5229\u3002\u9762\u5bf9\u6d77\u91cf\u3001\u590d\u6742\u7684\u9065\u611f\u56fe\u50cf\uff0c\u7279\u5b9a\u76ee\u6807\u7684\u5206\u5272\u548c\u63d0\u53d6\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u4efb\u52a1\u3002\u6700\u8fd1\uff0cSegment Anything Model\uff08SAM\uff09\u7684\u63a8\u51fa\u4e3a\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u901a\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u867d\u7136\u76f4\u63a5\u5c06 SAM \u5e94\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u5e76\u4e0d\u80fd\u4ea7\u751f\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\uff0c\u4f46\u6211\u4eec\u63d0\u51fa\u4e86 RSAM-Seg\uff08\u5373\u5e26\u8bed\u4e49\u5206\u5272\u7684\u9065\u611f SAM\uff09\uff0c\u4f5c\u4e3a\u9488\u5bf9\u9065\u611f\u9886\u57df\u7684 SAM \u7684\u5b9a\u5236\u4fee\u6539\uff0c\u5e76\u4e14\u6d88\u9664\u4e86\u5bf9 SAM \u7684\u9700\u6c42\u3002\u624b\u52a8\u5e72\u9884\u4ee5\u63d0\u4f9b\u63d0\u793a\u3002 Adapter-Scale\u662f\u4e00\u7ec4\u8865\u5145\u7f29\u653e\u6a21\u5757\uff0c\u662f\u5728SAM\u7f16\u7801\u5668\u90e8\u5206\u7684\u591a\u5934\u6ce8\u610f\u5757\u4e2d\u63d0\u51fa\u7684\u3002\u6b64\u5916\uff0c\u9002\u914d\u5668\u529f\u80fd\u88ab\u63d2\u5165\u89c6\u89c9\u8f6c\u6362\u5668\uff08ViT\uff09\u5757\u4e4b\u95f4\u3002\u8fd9\u4e9b\u6a21\u5757\u65e8\u5728\u7ed3\u5408\u9ad8\u9891\u56fe\u50cf\u4fe1\u606f\u548c\u56fe\u50cf\u5d4c\u5165\u529f\u80fd\u6765\u751f\u6210\u56fe\u50cf\u63d0\u793a\u3002\u5b9e\u9a8c\u5728\u56db\u79cd\u4e0d\u540c\u7684\u9065\u611f\u573a\u666f\u4e2d\u8fdb\u884c\uff0c\u5305\u62ec\u4e91\u68c0\u6d4b\u3001\u73b0\u573a\u76d1\u6d4b\u3001\u5efa\u7b51\u7269\u68c0\u6d4b\u548c\u9053\u8def\u6d4b\u7ed8\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u4e0d\u4ec5\u5c55\u793a\u4e86\u539f\u59cb SAM \u548c U-Net \u5728\u4e91\u3001\u5efa\u7b51\u7269\u3001\u7530\u91ce\u548c\u9053\u8def\u573a\u666f\u4e2d\u7684\u6539\u8fdb\uff0c\u800c\u4e14\u8fd8\u5f3a\u8c03\u4e86 RSAM-Seg \u8bc6\u522b\u67d0\u4e9b\u6570\u636e\u96c6\u7684\u771f\u5b9e\u60c5\u51b5\u4e2d\u7f3a\u5931\u533a\u57df\u7684\u80fd\u529b\uff0c\u80af\u5b9a\u4e86\u5176\u6f5c\u529b\u4f5c\u4e3a\u8f85\u52a9\u6807\u6ce8\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5728\u5c11\u6837\u672c\u573a\u666f\u4e2d\u7684\u6027\u80fd\u503c\u5f97\u79f0\u8d5e\uff0c\u5f3a\u8c03\u4e86\u5176\u5904\u7406\u6709\u9650\u6570\u636e\u96c6\u7684\u6f5c\u529b\u3002|[2402.19004v1](http://arxiv.org/pdf/2402.19004v1)|null|\n", "2402.19001": "|**2024-02-29**|**Analysis of the Two-Step Heterogeneous Transfer Learning for Laryngeal Blood Vessel Classification: Issue and Improvement**|\u5589\u8840\u7ba1\u5206\u7c7b\u4e24\u6b65\u5f02\u6784\u8fc1\u79fb\u5b66\u4e60\u5206\u6790\uff1a\u95ee\u9898\u4e0e\u6539\u8fdb|Xinyi Fang, Chak Fong Chong, Kei Long Wong, Yapeng Wang, Tiankui Zhang, Sio-Kei Im|Transferring features learned from natural to medical images for classification is common. However, challenges arise due to the scarcity of certain medical image types and the feature disparities between natural and medical images. Two-step transfer learning has been recognized as a promising solution for this issue. However, choosing an appropriate intermediate domain would be critical in further improving the classification performance. In this work, we explore the effectiveness of using color fundus photographs of the diabetic retina dataset as an intermediate domain for two-step heterogeneous learning (THTL) to classify laryngeal vascular images with nine deep-learning models. Experiment results confirm that although the images in both the intermediate and target domains share vascularized characteristics, the accuracy is drastically reduced compared to one-step transfer learning, where only the last layer is fine-tuned (e.g., ResNet18 drops 14.7%, ResNet50 drops 14.8%). By analyzing the Layer Class Activation Maps (LayerCAM), we uncover a novel finding that the prevalent radial vascular pattern in the intermediate domain prevents learning the features of twisted and tangled vessels that distinguish the malignant class in the target domain. To address the performance drop, we propose the Step-Wise Fine-Tuning (SWFT) method on ResNet in the second step of THTL, resulting in substantial accuracy improvements. Compared to THTL's second step, where only the last layer is fine-tuned, accuracy increases by 26.1% for ResNet18 and 20.4% for ResNet50. Additionally, compared to training from scratch, using ImageNet as the source domain could slightly improve classification performance for laryngeal vascular, but the differences are insignificant.|\u5c06\u4ece\u81ea\u7136\u56fe\u50cf\u5b66\u5230\u7684\u7279\u5f81\u8f6c\u79fb\u5230\u533b\u5b66\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u662f\u5f88\u5e38\u89c1\u7684\u3002\u7136\u800c\uff0c\u7531\u4e8e\u67d0\u4e9b\u533b\u5b66\u56fe\u50cf\u7c7b\u578b\u7684\u7a00\u7f3a\u4ee5\u53ca\u81ea\u7136\u56fe\u50cf\u548c\u533b\u5b66\u56fe\u50cf\u4e4b\u95f4\u7684\u7279\u5f81\u5dee\u5f02\uff0c\u51fa\u73b0\u4e86\u6311\u6218\u3002\u4e24\u6b65\u8fc1\u79fb\u5b66\u4e60\u5df2\u88ab\u8ba4\u4e3a\u662f\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u9009\u62e9\u5408\u9002\u7684\u4e2d\u95f4\u57df\u5bf9\u4e8e\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4f7f\u7528\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u6570\u636e\u96c6\u7684\u5f69\u8272\u773c\u5e95\u7167\u7247\u4f5c\u4e3a\u4e24\u6b65\u5f02\u6784\u5b66\u4e60\uff08THTL\uff09\u7684\u4e2d\u95f4\u57df\u6765\u901a\u8fc7\u4e5d\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u5589\u90e8\u8840\u7ba1\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\uff0c\u867d\u7136\u4e2d\u95f4\u57df\u548c\u76ee\u6807\u57df\u4e2d\u7684\u56fe\u50cf\u5171\u4eab\u8840\u7ba1\u5316\u7279\u5f81\uff0c\u4f46\u4e0e\u4ec5\u5bf9\u6700\u540e\u4e00\u5c42\u8fdb\u884c\u5fae\u8c03\u7684\u4e00\u6b65\u8fc1\u79fb\u5b66\u4e60\u76f8\u6bd4\uff0c\u51c6\u786e\u7387\u5927\u5927\u964d\u4f4e\uff08\u4f8b\u5982\uff0cResNet18 \u4e0b\u964d\u4e86 14.7%\uff0cResNet50 \u4e0b\u964d\u4e8614.8%\uff09\u3002\u901a\u8fc7\u5206\u6790\u5c42\u7c7b\u6fc0\u6d3b\u56fe\uff08LayerCAM\uff09\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u4e00\u4e2a\u65b0\u53d1\u73b0\uff0c\u5373\u4e2d\u95f4\u57df\u4e2d\u666e\u904d\u5b58\u5728\u7684\u653e\u5c04\u72b6\u8840\u7ba1\u6a21\u5f0f\u963b\u6b62\u4e86\u5b66\u4e60\u533a\u5206\u76ee\u6807\u57df\u4e2d\u6076\u6027\u7c7b\u522b\u7684\u626d\u66f2\u548c\u7f20\u7ed3\u8840\u7ba1\u7684\u7279\u5f81\u3002\u4e3a\u4e86\u89e3\u51b3\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5728 THTL \u7684\u7b2c\u4e8c\u6b65\u4e2d\u63d0\u51fa\u4e86 ResNet \u4e0a\u7684\u9010\u6b65\u5fae\u8c03\uff08SWFT\uff09\u65b9\u6cd5\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002\u4e0e THTL \u7684\u7b2c\u4e8c\u6b65\uff08\u4ec5\u5bf9\u6700\u540e\u4e00\u5c42\u8fdb\u884c\u5fae\u8c03\uff09\u76f8\u6bd4\uff0cResNet18 \u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 26.1%\uff0cResNet50 \u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 20.4%\u3002\u6b64\u5916\uff0c\u4e0e\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u76f8\u6bd4\uff0c\u4f7f\u7528ImageNet\u4f5c\u4e3a\u6e90\u57df\u53ef\u4ee5\u7a0d\u5fae\u63d0\u9ad8\u5589\u90e8\u8840\u7ba1\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u5dee\u5f02\u5e76\u4e0d\u663e\u7740\u3002|[2402.19001v1](http://arxiv.org/pdf/2402.19001v1)|null|\n", "2402.18998": "|**2024-02-29**|**COFT-AD: COntrastive Fine-Tuning for Few-Shot Anomaly Detection**|COFT-AD\uff1a\u7528\u4e8e\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u5bf9\u6bd4\u5fae\u8c03|Jingyi Liao, Xun Xu, Manh Cuong Nguyen, Adam Goodge, Chuan Sheng Foo|Existing approaches towards anomaly detection~(AD) often rely on a substantial amount of anomaly-free data to train representation and density models. However, large anomaly-free datasets may not always be available before the inference stage; in which case an anomaly detection model must be trained with only a handful of normal samples, a.k.a. few-shot anomaly detection (FSAD). In this paper, we propose a novel methodology to address the challenge of FSAD which incorporates two important techniques. Firstly, we employ a model pre-trained on a large source dataset to initialize model weights. Secondly, to ameliorate the covariate shift between source and target domains, we adopt contrastive training to fine-tune on the few-shot target domain data. To learn suitable representations for the downstream AD task, we additionally incorporate cross-instance positive pairs to encourage a tight cluster of the normal samples, and negative pairs for better separation between normal and synthesized negative samples. We evaluate few-shot anomaly detection on on 3 controlled AD tasks and 4 real-world AD tasks to demonstrate the effectiveness of the proposed method.|\u73b0\u6709\u7684\u5f02\u5e38\u68c0\u6d4b\uff08AD\uff09\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5927\u91cf\u65e0\u5f02\u5e38\u6570\u636e\u6765\u8bad\u7ec3\u8868\u793a\u548c\u5bc6\u5ea6\u6a21\u578b\u3002\u7136\u800c\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u4e4b\u524d\uff0c\u5927\u578b\u65e0\u5f02\u5e38\u6570\u636e\u96c6\u53ef\u80fd\u5e76\u4e0d\u603b\u662f\u53ef\u7528\uff1b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u5fc5\u987b\u4ec5\u4f7f\u7528\u5c11\u91cf\u6b63\u5e38\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u5373\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\uff08FSAD\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9 FSAD \u7684\u6311\u6218\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e24\u79cd\u91cd\u8981\u7684\u6280\u672f\u3002\u9996\u5148\uff0c\u6211\u4eec\u91c7\u7528\u5728\u5927\u578b\u6e90\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u521d\u59cb\u5316\u6a21\u578b\u6743\u91cd\u3002\u5176\u6b21\uff0c\u4e3a\u4e86\u6539\u5584\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u6211\u4eec\u91c7\u7528\u5bf9\u6bd4\u8bad\u7ec3\u6765\u5bf9\u5c11\u6837\u672c\u76ee\u6807\u57df\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002\u4e3a\u4e86\u5b66\u4e60\u4e0b\u6e38 AD \u4efb\u52a1\u7684\u5408\u9002\u8868\u793a\uff0c\u6211\u4eec\u8fd8\u7ed3\u5408\u4e86\u8de8\u5b9e\u4f8b\u6b63\u5bf9\u4ee5\u9f13\u52b1\u6b63\u5e38\u6837\u672c\u7d27\u5bc6\u805a\u96c6\uff0c\u5e76\u7ed3\u5408\u8d1f\u5bf9\u4ee5\u66f4\u597d\u5730\u5206\u79bb\u6b63\u5e38\u6837\u672c\u548c\u5408\u6210\u8d1f\u6837\u672c\u3002\u6211\u4eec\u5728 3 \u4e2a\u53d7\u63a7 AD \u4efb\u52a1\u548c 4 \u4e2a\u73b0\u5b9e\u4e16\u754c AD \u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\uff0c\u4ee5\u8bc1\u660e\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2402.18998v1](http://arxiv.org/pdf/2402.18998v1)|null|\n", "2402.18975": "|**2024-02-29**|**Theoretically Achieving Continuous Representation of Oriented Bounding Boxes**|\u7406\u8bba\u4e0a\u5b9e\u73b0\u5b9a\u5411\u8fb9\u754c\u6846\u7684\u8fde\u7eed\u8868\u793a|Zikai Xiao, Guo-Ye Yang, Xue Yang, Tai-Jiang Mu, Junchi Yan, Shi-min Hu|Considerable efforts have been devoted to Oriented Object Detection (OOD). However, one lasting issue regarding the discontinuity in Oriented Bounding Box (OBB) representation remains unresolved, which is an inherent bottleneck for extant OOD methods. This paper endeavors to completely solve this issue in a theoretically guaranteed manner and puts an end to the ad-hoc efforts in this direction. Prior studies typically can only address one of the two cases of discontinuity: rotation and aspect ratio, and often inadvertently introduce decoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding Ambiguity (DA) as discussed in literature. Specifically, we propose a novel representation method called Continuous OBB (COBB), which can be readily integrated into existing detectors e.g. Faster-RCNN as a plugin. It can theoretically ensure continuity in bounding box regression which to our best knowledge, has not been achieved in literature for rectangle-based object representation. For fairness and transparency of experiments, we have developed a modularized benchmark based on the open-source deep learning framework Jittor's detection toolbox JDet for OOD evaluation. On the popular DOTA dataset, by integrating Faster-RCNN as the same baseline model, our new method outperforms the peer method Gliding Vertex by 1.13% mAP50 (relative improvement 1.54%), and 2.46% mAP75 (relative improvement 5.91%), without any tricks.|\u4eba\u4eec\u5728\u5b9a\u5411\u5bf9\u8c61\u68c0\u6d4b\uff08OOD\uff09\u65b9\u9762\u6295\u5165\u4e86\u5927\u91cf\u7684\u7cbe\u529b\u3002\u7136\u800c\uff0c\u5173\u4e8e\u5b9a\u5411\u8fb9\u754c\u6846 (OBB) \u8868\u793a\u7684\u4e0d\u8fde\u7eed\u6027\u7684\u4e00\u4e2a\u6301\u4e45\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\uff0c\u8fd9\u662f\u73b0\u6709 OOD \u65b9\u6cd5\u7684\u56fa\u6709\u74f6\u9888\u3002\u672c\u6587\u529b\u56fe\u4ece\u7406\u8bba\u4e0a\u4fdd\u8bc1\u5f7b\u5e95\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7ed3\u675f\u8fd9\u65b9\u9762\u7684\u4e34\u65f6\u52aa\u529b\u3002\u5148\u524d\u7684\u7814\u7a76\u901a\u5e38\u53ea\u80fd\u89e3\u51b3\u4e24\u79cd\u4e0d\u8fde\u7eed\u60c5\u51b5\u4e2d\u7684\u4e00\u79cd\uff1a\u65cb\u8f6c\u548c\u7eb5\u6a2a\u6bd4\uff0c\u5e76\u4e14\u7ecf\u5e38\u65e0\u610f\u4e2d\u5f15\u5165\u89e3\u7801\u4e0d\u8fde\u7eed\u6027\uff0c\u4f8b\u5982\u6587\u732e\u4e2d\u8ba8\u8bba\u4e86\u89e3\u7801\u4e0d\u5b8c\u6574\u6027 (DI) \u548c\u89e3\u7801\u6b67\u4e49 (DA)\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u8fde\u7eed OBB (COBB) \u7684\u65b0\u9896\u8868\u793a\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u68c0\u6d4b\u5668\u4e2d\uff0c\u4f8b\u5982Faster-RCNN \u4f5c\u4e3a\u63d2\u4ef6\u3002\u4ece\u7406\u8bba\u4e0a\u8bb2\uff0c\u5b83\u53ef\u4ee5\u786e\u4fdd\u8fb9\u754c\u6846\u56de\u5f52\u7684\u8fde\u7eed\u6027\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u57fa\u4e8e\u77e9\u5f62\u7684\u5bf9\u8c61\u8868\u793a\u7684\u6587\u732e\u5c1a\u672a\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u4e3a\u4e86\u5b9e\u9a8c\u7684\u516c\u5e73\u548c\u900f\u660e\uff0c\u6211\u4eec\u57fa\u4e8e\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6Jittor\u7684\u68c0\u6d4b\u5de5\u5177\u7bb1JDet\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u57fa\u51c6\u7528\u4e8eOOD\u8bc4\u4f30\u3002\u5728\u6d41\u884c\u7684 DOTA \u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u5c06 Faster-RCNN \u96c6\u6210\u4e3a\u76f8\u540c\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u6211\u4eec\u7684\u65b0\u65b9\u6cd5\u6bd4\u540c\u884c\u65b9\u6cd5 Gliding Vertex \u4f18\u4e8e\u540c\u884c\u65b9\u6cd5 Gliding Vertex 1.13% mAP50\uff08\u76f8\u5bf9\u6539\u8fdb 1.54%\uff09\u548c 2.46% mAP75\uff08\u76f8\u5bf9\u6539\u8fdb 5.91%\uff09\uff0c\u6ca1\u6709\u4efb\u4f55\u5f71\u54cd\u6280\u5de7\u3002|[2402.18975v1](http://arxiv.org/pdf/2402.18975v1)|null|\n", "2402.18960": "|**2024-02-29**|**Towards Out-of-Distribution Detection for breast cancer classification in Point-of-Care Ultrasound Imaging**|\u81f4\u529b\u4e8e\u5e8a\u65c1\u8d85\u58f0\u6210\u50cf\u4e2d\u4e73\u817a\u764c\u5206\u7c7b\u7684\u5206\u5e03\u5916\u68c0\u6d4b|Jennie Karlsson, Marisa Wodrich, Niels Christian Overgaard, Freja Sahlin, Kristina L\u00e5ng, Anders Heyden, Ida Arvidsson|Deep learning has shown to have great potential in medical applications. In critical domains as such, it is of high interest to have trustworthy algorithms which are able to tell when reliable assessments cannot be guaranteed. Detecting out-of-distribution (OOD) samples is a crucial step towards building a safe classifier. Following a previous study, showing that it is possible to classify breast cancer in point-of-care ultrasound images, this study investigates OOD detection using three different methods: softmax, energy score and deep ensembles. All methods are tested on three different OOD data sets. The results show that the energy score method outperforms the softmax method, performing well on two of the data sets. The ensemble method is the most robust, performing the best at detecting OOD samples for all three OOD data sets.|\u6df1\u5ea6\u5b66\u4e60\u5df2\u88ab\u8bc1\u660e\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u5728\u8fd9\u6837\u7684\u5173\u952e\u9886\u57df\uff0c\u62e5\u6709\u503c\u5f97\u4fe1\u8d56\u7684\u7b97\u6cd5\u975e\u5e38\u4ee4\u4eba\u611f\u5174\u8da3\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u80fd\u591f\u5224\u65ad\u4f55\u65f6\u65e0\u6cd5\u4fdd\u8bc1\u53ef\u9760\u7684\u8bc4\u4f30\u3002\u68c0\u6d4b\u5206\u5e03\u5916 (OOD) \u6837\u672c\u662f\u6784\u5efa\u5b89\u5168\u5206\u7c7b\u5668\u7684\u5173\u952e\u4e00\u6b65\u3002\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\u53ef\u4ee5\u5728\u62a4\u7406\u70b9\u8d85\u58f0\u56fe\u50cf\u4e2d\u5bf9\u4e73\u817a\u764c\u8fdb\u884c\u5206\u7c7b\uff0c\u672c\u7814\u7a76\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u7814\u7a76 OOD \u68c0\u6d4b\uff1asoftmax\u3001\u80fd\u91cf\u8bc4\u5206\u548c\u6df1\u5ea6\u96c6\u6210\u3002\u6240\u6709\u65b9\u6cd5\u90fd\u5728\u4e09\u4e2a\u4e0d\u540c\u7684 OOD \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u80fd\u91cf\u8bc4\u5206\u65b9\u6cd5\u4f18\u4e8e softmax \u65b9\u6cd5\uff0c\u5728\u5176\u4e2d\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002\u96c6\u6210\u65b9\u6cd5\u662f\u6700\u7a33\u5065\u7684\uff0c\u5728\u68c0\u6d4b\u6240\u6709\u4e09\u4e2a OOD \u6570\u636e\u96c6\u7684 OOD \u6837\u672c\u65b9\u9762\u8868\u73b0\u6700\u597d\u3002|[2402.18960v1](http://arxiv.org/pdf/2402.18960v1)|null|\n", "2402.18958": "|**2024-02-29**|**Boosting Semi-Supervised Object Detection in Remote Sensing Images With Active Teaching**|\u901a\u8fc7\u4e3b\u52a8\u6559\u5b66\u4fc3\u8fdb\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b|Boxuan Zhang, Zengmao Wang, Bo Du|The lack of object-level annotations poses a significant challenge for object detection in remote sensing images (RSIs). To address this issue, active learning (AL) and semi-supervised learning (SSL) techniques have been proposed to enhance the quality and quantity of annotations. AL focuses on selecting the most informative samples for annotation, while SSL leverages the knowledge from unlabeled samples. In this letter, we propose a novel AL method to boost semi-supervised object detection (SSOD) for remote sensing images with a teacher student network, called SSOD-AT. The proposed method incorporates an RoI comparison module (RoICM) to generate high-confidence pseudo-labels for regions of interest (RoIs). Meanwhile, the RoICM is utilized to identify the top-K uncertain images. To reduce redundancy in the top-K uncertain images for human labeling, a diversity criterion is introduced based on object-level prototypes of different categories using both labeled and pseudo-labeled images. Extensive experiments on DOTA and DIOR, two popular datasets, demonstrate that our proposed method outperforms state-of-the-art methods for object detection in RSIs. Compared with the best performance in the SOTA methods, the proposed method achieves 1 percent improvement in most cases in the whole AL.|\u7f3a\u4e4f\u5bf9\u8c61\u7ea7\u6ce8\u91ca\u7ed9\u9065\u611f\u56fe\u50cf\uff08RSI\uff09\u4e2d\u7684\u5bf9\u8c61\u68c0\u6d4b\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u548c\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6280\u672f\u88ab\u63d0\u51fa\u6765\u63d0\u9ad8\u6ce8\u91ca\u7684\u8d28\u91cf\u548c\u6570\u91cf\u3002 AL \u4fa7\u91cd\u4e8e\u9009\u62e9\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u6837\u672c\u8fdb\u884c\u6ce8\u91ca\uff0c\u800c SSL \u5219\u5229\u7528\u672a\u6807\u8bb0\u6837\u672c\u4e2d\u7684\u77e5\u8bc6\u3002\u5728\u8fd9\u5c01\u4fe1\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 AL \u65b9\u6cd5\uff0c\u901a\u8fc7\u5e08\u751f\u7f51\u7edc\u589e\u5f3a\u9065\u611f\u56fe\u50cf\u7684\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b (SSOD)\uff0c\u79f0\u4e3a SSOD-AT\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86 RoI \u6bd4\u8f83\u6a21\u5757 (RoICM) \u6765\u751f\u6210\u611f\u5174\u8da3\u533a\u57df (RoI) \u7684\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u3002\u540c\u65f6\uff0c\u5229\u7528RoICM\u6765\u8bc6\u522btop-K\u4e0d\u786e\u5b9a\u56fe\u50cf\u3002\u4e3a\u4e86\u51cf\u5c11\u7528\u4e8e\u4eba\u7c7b\u6807\u8bb0\u7684\u524d K \u4e2a\u4e0d\u786e\u5b9a\u56fe\u50cf\u4e2d\u7684\u5197\u4f59\uff0c\u4f7f\u7528\u6807\u8bb0\u56fe\u50cf\u548c\u4f2a\u6807\u8bb0\u56fe\u50cf\u57fa\u4e8e\u4e0d\u540c\u7c7b\u522b\u7684\u5bf9\u8c61\u7ea7\u539f\u578b\u5f15\u5165\u4e86\u591a\u6837\u6027\u6807\u51c6\u3002\u5bf9 DOTA \u548c DIOR \u8fd9\u4e24\u4e2a\u6d41\u884c\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e RSI \u4e2d\u5bf9\u8c61\u68c0\u6d4b\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u4e0e SOTA \u65b9\u6cd5\u7684\u6700\u4f73\u6027\u80fd\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6574\u4e2a AL \u7684\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86 1% \u7684\u6539\u8fdb\u3002|[2402.18958v1](http://arxiv.org/pdf/2402.18958v1)|null|\n", "2402.18929": "|**2024-02-29**|**Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable Image Super Resolution**|\u8d85\u8d8a Dropout\uff1a\u5b9e\u73b0\u901a\u7528\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u6709\u8da3\u89e3\u51b3\u65b9\u6848|Hongjun Wang, Jiyuan Chen, Yinqiang Zheng, Tieyong Zeng|Deep learning has led to a dramatic leap on Single Image Super-Resolution (SISR) performances in recent years. %Despite the substantial advancement% While most existing work assumes a simple and fixed degradation model (e.g., bicubic downsampling), the research of Blind SR seeks to improve model generalization ability with unknown degradation. Recently, Kong et al pioneer the investigation of a more suitable training strategy for Blind SR using Dropout. Although such method indeed brings substantial generalization improvements via mitigating overfitting, we argue that Dropout simultaneously introduces undesirable side-effect that compromises model's capacity to faithfully reconstruct fine details. We show both the theoretical and experimental analyses in our paper, and furthermore, we present another easy yet effective training strategy that enhances the generalization ability of the model by simply modulating its first and second-order features statistics. Experimental results have shown that our method could serve as a model-agnostic regularization and outperforms Dropout on seven benchmark datasets including both synthetic and real-world scenarios.|\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u5e26\u6765\u4e86\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SISR\uff09\u6027\u80fd\u7684\u5de8\u5927\u98de\u8dc3\u3002\u867d\u7136\u5927\u591a\u6570\u73b0\u6709\u5de5\u4f5c\u90fd\u5047\u8bbe\u4e00\u4e2a\u7b80\u5355\u4e14\u56fa\u5b9a\u7684\u9000\u5316\u6a21\u578b\uff08\u4f8b\u5982\u53cc\u4e09\u6b21\u4e0b\u91c7\u6837\uff09\uff0c\u4f46 Blind SR \u7684\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u77e5\u9000\u5316\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6700\u8fd1\uff0cKong \u7b49\u4eba\u7387\u5148\u7814\u7a76\u4e86\u4e00\u79cd\u66f4\u9002\u5408\u4f7f\u7528 Dropout \u7684 Blind SR \u8bad\u7ec3\u7b56\u7565\u3002\u5c3d\u7ba1\u8fd9\u79cd\u65b9\u6cd5\u786e\u5b9e\u901a\u8fc7\u51cf\u8f7b\u8fc7\u5ea6\u62df\u5408\u5e26\u6765\u4e86\u5b9e\u8d28\u6027\u7684\u6cdb\u5316\u6539\u8fdb\uff0c\u4f46\u6211\u4eec\u8ba4\u4e3a Dropout \u540c\u65f6\u5f15\u5165\u4e86\u4e0d\u826f\u7684\u526f\u4f5c\u7528\uff0c\u635f\u5bb3\u4e86\u6a21\u578b\u5fe0\u5b9e\u91cd\u5efa\u7cbe\u7ec6\u7ec6\u8282\u7684\u80fd\u529b\u3002\u6211\u4eec\u5728\u8bba\u6587\u4e2d\u5c55\u793a\u4e86\u7406\u8bba\u548c\u5b9e\u9a8c\u5206\u6790\uff0c\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53e6\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u7b80\u5355\u5730\u8c03\u6574\u6a21\u578b\u7684\u4e00\u9636\u548c\u4e8c\u9636\u7279\u5f81\u7edf\u8ba1\u91cf\u6765\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u4f5c\u4e3a\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u6b63\u5219\u5316\uff0c\u5e76\u4e14\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u62ec\u5408\u6210\u573a\u666f\u548c\u771f\u5b9e\u573a\u666f\uff09\u4e0a\u4f18\u4e8e Dropout\u3002|[2402.18929v1](http://arxiv.org/pdf/2402.18929v1)|null|\n", "2402.18927": "|**2024-02-29**|**Edge Computing Enabled Real-Time Video Analysis via Adaptive Spatial-Temporal Semantic Filtering**|\u8fb9\u7f18\u8ba1\u7b97\u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u7a7a\u8bed\u4e49\u8fc7\u6ee4\u5b9e\u73b0\u5b9e\u65f6\u89c6\u9891\u5206\u6790|Xiang Chen, Wenjie Zhu, Jiayuan Chen, Tong Zhang, Changyan Yi, Jun Cai|This paper proposes a novel edge computing enabled real-time video analysis system for intelligent visual devices. The proposed system consists of a tracking-assisted object detection module (TAODM) and a region of interesting module (ROIM). TAODM adaptively determines the offloading decision to process each video frame locally with a tracking algorithm or to offload it to the edge server inferred by an object detection model. ROIM determines each offloading frame's resolution and detection model configuration to ensure that the analysis results can return in time. TAODM and ROIM interact jointly to filter the repetitive spatial-temporal semantic information to maximize the processing rate while ensuring high video analysis accuracy. Unlike most existing works, this paper investigates the real-time video analysis systems where the intelligent visual device connects to the edge server through a wireless network with fluctuating network conditions. We decompose the real-time video analysis problem into the offloading decision and configurations selection sub-problems. To solve these two sub-problems, we introduce a double deep Q network (DDQN) based offloading approach and a contextual multi-armed bandit (CMAB) based adaptive configurations selection approach, respectively. A DDQN-CMAB reinforcement learning (DCRL) training framework is further developed to integrate these two approaches to improve the overall video analyzing performance. Extensive simulations are conducted to evaluate the performance of the proposed solution, and demonstrate its superiority over counterparts.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u667a\u80fd\u89c6\u89c9\u8bbe\u5907\u7684\u65b0\u578b\u8fb9\u7f18\u8ba1\u7b97\u5b9e\u65f6\u89c6\u9891\u5206\u6790\u7cfb\u7edf\u3002\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u7531\u8ddf\u8e2a\u8f85\u52a9\u76ee\u6807\u68c0\u6d4b\u6a21\u5757\uff08TAODM\uff09\u548c\u611f\u5174\u8da3\u533a\u57df\u6a21\u5757\uff08ROIM\uff09\u7ec4\u6210\u3002 TAODM \u81ea\u9002\u5e94\u5730\u786e\u5b9a\u5378\u8f7d\u51b3\u7b56\uff0c\u4f7f\u7528\u8ddf\u8e2a\u7b97\u6cd5\u5728\u672c\u5730\u5904\u7406\u6bcf\u4e2a\u89c6\u9891\u5e27\uff0c\u6216\u5c06\u5176\u5378\u8f7d\u5230\u7531\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u63a8\u65ad\u7684\u8fb9\u7f18\u670d\u52a1\u5668\u3002 ROIM\u786e\u5b9a\u6bcf\u4e2a\u5378\u8f7d\u5e27\u7684\u5206\u8fa8\u7387\u548c\u68c0\u6d4b\u6a21\u578b\u914d\u7f6e\uff0c\u4ee5\u786e\u4fdd\u5206\u6790\u7ed3\u679c\u80fd\u591f\u53ca\u65f6\u8fd4\u56de\u3002 TAODM\u548cROIM\u8054\u5408\u4ea4\u4e92\uff0c\u8fc7\u6ee4\u91cd\u590d\u7684\u65f6\u7a7a\u8bed\u4e49\u4fe1\u606f\uff0c\u5728\u4fdd\u8bc1\u8f83\u9ad8\u7684\u89c6\u9891\u5206\u6790\u7cbe\u5ea6\u7684\u540c\u65f6\u6700\u5927\u5316\u5904\u7406\u901f\u7387\u3002\u4e0e\u5927\u591a\u6570\u73b0\u6709\u5de5\u4f5c\u4e0d\u540c\uff0c\u672c\u6587\u7814\u7a76\u4e86\u5b9e\u65f6\u89c6\u9891\u5206\u6790\u7cfb\u7edf\uff0c\u5176\u4e2d\u667a\u80fd\u89c6\u89c9\u8bbe\u5907\u901a\u8fc7\u5177\u6709\u6ce2\u52a8\u7f51\u7edc\u6761\u4ef6\u7684\u65e0\u7ebf\u7f51\u7edc\u8fde\u63a5\u5230\u8fb9\u7f18\u670d\u52a1\u5668\u3002\u6211\u4eec\u5c06\u5b9e\u65f6\u89c6\u9891\u5206\u6790\u95ee\u9898\u5206\u89e3\u4e3a\u5378\u8f7d\u51b3\u7b56\u548c\u914d\u7f6e\u9009\u62e9\u5b50\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u6211\u4eec\u5206\u522b\u5f15\u5165\u4e86\u57fa\u4e8e\u53cc\u6df1 Q \u7f51\u7edc\uff08DDQN\uff09\u7684\u5378\u8f7d\u65b9\u6cd5\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\uff08CMAB\uff09\u7684\u81ea\u9002\u5e94\u914d\u7f6e\u9009\u62e9\u65b9\u6cd5\u3002\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86 DDQN-CMAB \u5f3a\u5316\u5b66\u4e60\uff08DCRL\uff09\u8bad\u7ec3\u6846\u67b6\u6765\u96c6\u6210\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6574\u4f53\u89c6\u9891\u5206\u6790\u6027\u80fd\u3002\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6a21\u62df\u6765\u8bc4\u4f30\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u5176\u76f8\u5bf9\u4e8e\u540c\u7c7b\u89e3\u51b3\u65b9\u6848\u7684\u4f18\u8d8a\u6027\u3002|[2402.18927v1](http://arxiv.org/pdf/2402.18927v1)|null|\n", "2402.18922": "|**2024-02-29**|**A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection**|\u57fa\u4e8e\u89c6\u89c9\u53d8\u538b\u5668\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u7f51\u7edc\uff0c\u7528\u4e8e\u4f2a\u88c5\u7269\u4f53\u548c\u663e\u7740\u7269\u4f53\u68c0\u6d4b|Chao Hao, Zitong Yu, Xin Liu, Jun Xu, Huanjing Yue, Jingyu Yang|Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Previous works achieved good performance by stacking various hand-designed modules and multi-scale features. However, these carefully-designed complex networks often performed well on one task but not on another. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. Furthermore, to enhance the Transformer's ability to model local information, which is important for pixel-level binary segmentation tasks, we propose a local information capture module (LICM). We also propose a dynamic weighted loss (DW loss) based on Binary Cross-Entropy (BCE) and Intersection over Union (IoU) loss, which guides the network to pay more attention to those smaller and more difficult-to-find target objects according to their size. Moreover, we explore the issue of joint training of SOD and COD, and propose a preliminary solution to the conflict in joint training, further improving the performance of SOD. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.|\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\uff08COD\uff09\u548c\u663e\u7740\u76ee\u6807\u68c0\u6d4b\uff08SOD\uff09\u662f\u8fc7\u53bb\u51e0\u5341\u5e74\u6765\u5e7f\u6cdb\u7814\u7a76\u7684\u4e24\u4e2a\u4e0d\u540c\u4f46\u5bc6\u5207\u76f8\u5173\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002\u867d\u7136\u5b83\u4eec\u7684\u76ee\u7684\u76f8\u540c\uff0c\u90fd\u662f\u5c06\u56fe\u50cf\u5206\u5272\u4e3a\u4e8c\u503c\u524d\u666f\u548c\u80cc\u666f\u533a\u57df\uff0c\u4f46\u5b83\u4eec\u7684\u533a\u522b\u5728\u4e8e COD \u4e13\u6ce8\u4e8e\u56fe\u50cf\u4e2d\u9690\u85cf\u7684\u9690\u85cf\u5bf9\u8c61\uff0c\u800c SOD \u4e13\u6ce8\u4e8e\u56fe\u50cf\u4e2d\u6700\u7a81\u51fa\u7684\u5bf9\u8c61\u3002\u4e4b\u524d\u7684\u4f5c\u54c1\u901a\u8fc7\u5806\u53e0\u5404\u79cd\u624b\u5de5\u8bbe\u8ba1\u7684\u6a21\u5757\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u590d\u6742\u7f51\u7edc\u901a\u5e38\u5728\u4e00\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u53e6\u4e00\u9879\u4efb\u52a1\u4e0a\u5374\u8868\u73b0\u4e0d\u4f73\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9 Transformer (ViT) \u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u7f51\u7edc (SENet)\uff0c\u901a\u8fc7\u91c7\u7528\u57fa\u4e8e\u975e\u5bf9\u79f0 ViT \u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684\u7b80\u5355\u8bbe\u8ba1\uff0c\u6211\u4eec\u5728\u8fd9\u4e24\u9879\u4efb\u52a1\u4e0a\u90fd\u4ea7\u751f\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u8868\u73b0\u51fa\u6bd4\u7cbe\u5fc3\u5236\u4f5c\u7684\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u589e\u5f3a Transformer \u5efa\u6a21\u5c40\u90e8\u4fe1\u606f\u7684\u80fd\u529b\uff08\u8fd9\u5bf9\u4e8e\u50cf\u7d20\u7ea7\u4e8c\u8fdb\u5236\u5206\u5272\u4efb\u52a1\u5f88\u91cd\u8981\uff09\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5c40\u90e8\u4fe1\u606f\u6355\u83b7\u6a21\u5757\uff08LICM\uff09\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u5143\u4ea4\u53c9\u71b5\uff08BCE\uff09\u548c\u4ea4\u5e76\u96c6\uff08IoU\uff09\u635f\u5931\u7684\u52a8\u6001\u52a0\u6743\u635f\u5931\uff08DW\u635f\u5931\uff09\uff0c\u5b83\u5f15\u5bfc\u7f51\u7edc\u6839\u636e\u60c5\u51b5\u66f4\u591a\u5730\u5173\u6ce8\u90a3\u4e9b\u8f83\u5c0f\u4e14\u66f4\u96be\u627e\u5230\u7684\u76ee\u6807\u5bf9\u8c61\u5230\u4ed6\u4eec\u7684\u5927\u5c0f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86SOD\u548cCOD\u7684\u8054\u5408\u8bad\u7ec3\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u8054\u5408\u8bad\u7ec3\u51b2\u7a81\u7684\u521d\u6b65\u89e3\u51b3\u65b9\u6848\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86SOD\u7684\u6027\u80fd\u3002\u5bf9\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/linuxsino/SENet \u83b7\u53d6\u3002|[2402.18922v1](http://arxiv.org/pdf/2402.18922v1)|null|\n", "2402.18919": "|**2024-02-29**|**Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation**|\u5206\u89e3\u548c\u7ec4\u5408\uff1a\u51cf\u8f7b\u6742\u6563\u76f8\u5173\u6027\u7684\u7ec4\u5408\u65b9\u6cd5|Fahimeh Hosseini Noohdani, Parsa Hosseini, Arian Yazdan Parast, Hamidreza Yaghoubi Araghi, Mahdieh Soleymani Baghshah|While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on out-of-distribution samples. One of the main sources of distribution shift for image classification is the compositional nature of images. Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input distribution between train and test environments. More importantly, these components may have spurious correlations with the label. To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images. Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especially in datapoints on which models have a high confidence). In fact, according to the amount of spurious correlation and the easiness of classification based on the causal or non-causal components, the model usually attends to one of these more (on samples with high confidence). Following this, we first try to identify the causal components of images using class activation maps of models trained with ERM. Afterward, we intervene on images by combining them and retraining the model on the augmented data, including the counterfactual ones. Along with its high interpretability, this work proposes a group-balancing method by intervening on images without requiring group labels or information regarding the spurious features during training. The method has an overall better worst group accuracy compared to previous methods with the same amount of supervision on the group labels in correlation shift.|\u867d\u7136\u6807\u51c6\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316 (ERM) \u8bad\u7ec3\u88ab\u8bc1\u660e\u5bf9\u4e8e\u5206\u5e03\u5185\u6570\u636e\u7684\u56fe\u50cf\u5206\u7c7b\u662f\u6709\u6548\u7684\uff0c\u4f46\u5b83\u5728\u5206\u5e03\u5916\u6837\u672c\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u56fe\u50cf\u5206\u7c7b\u5206\u5e03\u504f\u79fb\u7684\u4e3b\u8981\u6765\u6e90\u4e4b\u4e00\u662f\u56fe\u50cf\u7684\u7ec4\u6210\u6027\u8d28\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u9664\u4e86\u786e\u5b9a\u6807\u7b7e\u7684\u4e3b\u8981\u5bf9\u8c61\u6216\u7ec4\u4ef6\u4e4b\u5916\uff0c\u901a\u5e38\u8fd8\u5b58\u5728\u4e00\u4e9b\u5176\u4ed6\u56fe\u50cf\u7ec4\u4ef6\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u73af\u5883\u548c\u6d4b\u8bd5\u73af\u5883\u4e4b\u95f4\u7684\u8f93\u5165\u5206\u5e03\u53d1\u751f\u53d8\u5316\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u8fd9\u4e9b\u6210\u5206\u53ef\u80fd\u4e0e\u6807\u7b7e\u5b58\u5728\u865a\u5047\u76f8\u5173\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u89e3\u548c\u7ec4\u5408\uff08DaC\uff09\uff0c\u5b83\u901a\u8fc7\u57fa\u4e8e\u7ec4\u5408\u56fe\u50cf\u5143\u7d20\u7684\u7ec4\u5408\u65b9\u6cd5\u6765\u63d0\u9ad8\u5bf9\u76f8\u5173\u6027\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002\u6839\u636e\u6211\u4eec\u7684\u89c2\u5bdf\uff0c\u4f7f\u7528 ERM \u8bad\u7ec3\u7684\u6a21\u578b\u901a\u5e38\u9ad8\u5ea6\u5173\u6ce8\u56e0\u679c\u6210\u5206\u6216\u4e0e\u6807\u7b7e\u5177\u6709\u9ad8\u5ea6\u865a\u5047\u76f8\u5173\u6027\u7684\u6210\u5206\uff08\u7279\u522b\u662f\u5728\u6a21\u578b\u5177\u6709\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u6570\u636e\u70b9\u4e2d\uff09\u3002\u4e8b\u5b9e\u4e0a\uff0c\u6839\u636e\u865a\u5047\u76f8\u5173\u6027\u7684\u6570\u91cf\u4ee5\u53ca\u57fa\u4e8e\u56e0\u679c\u6216\u975e\u56e0\u679c\u6210\u5206\u8fdb\u884c\u5206\u7c7b\u7684\u96be\u6613\u7a0b\u5ea6\uff0c\u6a21\u578b\u901a\u5e38\u4f1a\u5173\u6ce8\u5176\u4e2d\u4e4b\u4e00\uff08\u5728\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u6837\u672c\u4e0a\uff09\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u9996\u5148\u5c1d\u8bd5\u4f7f\u7528 ERM \u8bad\u7ec3\u7684\u6a21\u578b\u7684\u7c7b\u6fc0\u6d3b\u56fe\u6765\u8bc6\u522b\u56fe\u50cf\u7684\u56e0\u679c\u6210\u5206\u3002\u4e4b\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u7ec4\u5408\u56fe\u50cf\u5e76\u6839\u636e\u589e\u5f3a\u6570\u636e\uff08\u5305\u62ec\u53cd\u4e8b\u5b9e\u6570\u636e\uff09\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u6765\u5bf9\u56fe\u50cf\u8fdb\u884c\u5e72\u9884\u3002\u9664\u4e86\u5176\u9ad8\u53ef\u89e3\u91ca\u6027\u4e4b\u5916\uff0c\u8fd9\u9879\u5de5\u4f5c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5e72\u9884\u56fe\u50cf\u6765\u8fdb\u884c\u7ec4\u5e73\u8861\u7684\u65b9\u6cd5\uff0c\u800c\u65e0\u9700\u7ec4\u6807\u7b7e\u6216\u6709\u5173\u8bad\u7ec3\u671f\u95f4\u865a\u5047\u7279\u5f81\u7684\u4fe1\u606f\u3002\u4e0e\u4e4b\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u5173\u504f\u79fb\u4e2d\u5bf9\u7ec4\u6807\u7b7e\u8fdb\u884c\u76f8\u540c\u91cf\u7684\u76d1\u7763\u65f6\uff0c\u603b\u4f53\u4e0a\u5177\u6709\u66f4\u597d\u7684\u6700\u5dee\u7ec4\u51c6\u786e\u5ea6\u3002|[2402.18919v1](http://arxiv.org/pdf/2402.18919v1)|null|\n", "2402.18918": "|**2024-02-29**|**SNE-RoadSegV2: Advancing Heterogeneous Feature Fusion and Fallibility Awareness for Freespace Detection**|SNE-RoadSegV2\uff1a\u63a8\u8fdb\u81ea\u7531\u7a7a\u95f4\u68c0\u6d4b\u7684\u5f02\u6784\u7279\u5f81\u878d\u5408\u548c\u6613\u9519\u610f\u8bc6|Yi Feng, Yu Ma, Qijun Chen, Ioannis Pitas, Rui Fan|Feature-fusion networks with duplex encoders have proven to be an effective technique to solve the freespace detection problem. However, despite the compelling results achieved by previous research efforts, the exploration of adequate and discriminative heterogeneous feature fusion, as well as the development of fallibility-aware loss functions remains relatively scarce. This paper makes several significant contributions to address these limitations: (1) It presents a novel heterogeneous feature fusion block, comprising a holistic attention module, a heterogeneous feature contrast descriptor, and an affinity-weighted feature recalibrator, enabling a more in-depth exploitation of the inherent characteristics of the extracted features, (2) it incorporates both inter-scale and intra-scale skip connections into the decoder architecture while eliminating redundant ones, leading to both improved accuracy and computational efficiency, and (3) it introduces two fallibility-aware loss functions that separately focus on semantic-transition and depth-inconsistent regions, collectively contributing to greater supervision during model training. Our proposed heterogeneous feature fusion network (SNE-RoadSegV2), which incorporates all these innovative components, demonstrates superior performance in comparison to all other freespace detection algorithms across multiple public datasets. Notably, it ranks the 1st on the official KITTI Road benchmark.|\u5177\u6709\u53cc\u5de5\u7f16\u7801\u5668\u7684\u7279\u5f81\u878d\u5408\u7f51\u7edc\u5df2\u88ab\u8bc1\u660e\u662f\u89e3\u51b3\u81ea\u7531\u7a7a\u95f4\u68c0\u6d4b\u95ee\u9898\u7684\u6709\u6548\u6280\u672f\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5148\u524d\u7684\u7814\u7a76\u5de5\u4f5c\u53d6\u5f97\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u6210\u679c\uff0c\u4f46\u5bf9\u5145\u5206\u4e14\u6709\u533a\u522b\u7684\u5f02\u6784\u7279\u5f81\u878d\u5408\u7684\u63a2\u7d22\u4ee5\u53ca\u6613\u9519\u610f\u8bc6\u635f\u5931\u51fd\u6570\u7684\u5f00\u53d1\u4ecd\u7136\u76f8\u5bf9\u8f83\u5c11\u3002\u672c\u6587\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u505a\u51fa\u4e86\u51e0\u4e2a\u91cd\u5927\u8d21\u732e\uff1a\uff081\uff09\u5b83\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f02\u6784\u7279\u5f81\u878d\u5408\u5757\uff0c\u5305\u62ec\u6574\u4f53\u6ce8\u610f\u6a21\u5757\u3001\u5f02\u6784\u7279\u5f81\u5bf9\u6bd4\u5ea6\u63cf\u8ff0\u7b26\u548c\u4eb2\u548c\u529b\u52a0\u6743\u7279\u5f81\u91cd\u65b0\u6821\u51c6\u5668\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6df1\u5165\u7684\u5229\u7528\u63d0\u53d6\u7279\u5f81\u7684\u56fa\u6709\u7279\u5f81\uff0c\uff082\uff09\u5b83\u5c06\u5c3a\u5ea6\u95f4\u548c\u5c3a\u5ea6\u5185\u8df3\u8dc3\u8fde\u63a5\u5408\u5e76\u5230\u89e3\u7801\u5668\u67b6\u6784\u4e2d\uff0c\u540c\u65f6\u6d88\u9664\u5197\u4f59\uff0c\u4ece\u800c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\uff083\uff09\u5b83\u5f15\u5165\u4e86\u4e24\u79cd\u9519\u8bef- \u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u5206\u522b\u5173\u6ce8\u8bed\u4e49\u8f6c\u6362\u548c\u6df1\u5ea6\u4e0d\u4e00\u81f4\u533a\u57df\uff0c\u5171\u540c\u4fc3\u8fdb\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u66f4\u597d\u7684\u76d1\u7763\u3002\u6211\u4eec\u63d0\u51fa\u7684\u5f02\u6784\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08SNE-RoadSegV2\uff09\u878d\u5408\u4e86\u6240\u6709\u8fd9\u4e9b\u521b\u65b0\u7ec4\u4ef6\uff0c\u4e0e\u8de8\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u7684\u6240\u6709\u5176\u4ed6\u81ea\u7531\u7a7a\u95f4\u68c0\u6d4b\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u5728\u5b98\u65b9 KITTI Road \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e00\u3002|[2402.18918v1](http://arxiv.org/pdf/2402.18918v1)|null|\n", "2402.18853": "|**2024-02-29**|**Rethinking Multi-domain Generalization with A General Learning Objective**|\u4ee5\u901a\u7528\u5b66\u4e60\u76ee\u6807\u91cd\u65b0\u601d\u8003\u591a\u9886\u57df\u6cdb\u5316|Zhaorui Tan, Xi Yang, Kaizhu Huang|Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \\textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous mDG endeavors actually \\textbf{optimize partially the objective} and thus lead to limited performance. As such, our study distills a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts. Extensive empirical results indicate that the proposed objective with $Y$-mapping leads to substantially better mDG performance in various downstream tasks, including regression, segmentation, and classification.|\u591a\u57df\u6cdb\u5316\uff08mDG\uff09\u7684\u666e\u904d\u76ee\u6807\u662f\u6700\u5c0f\u5316\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ee5\u589e\u5f3a\u8fb9\u7f18\u5230\u6807\u7b7e\u7684\u5206\u5e03\u6620\u5c04\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684 mDG \u6587\u732e\u7f3a\u4e4f\u901a\u7528\u7684\u5b66\u4e60\u76ee\u6807\u8303\u5f0f\uff0c\u5e76\u4e14\u7ecf\u5e38\u5bf9\u9759\u6001\u76ee\u6807\u8fb9\u7f18\u5206\u5e03\u65bd\u52a0\u7ea6\u675f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528 $Y$ \u6620\u5c04\u6765\u653e\u677e\u7ea6\u675f\u3002\u6211\u4eec\u91cd\u65b0\u601d\u8003 mDG \u7684\u5b66\u4e60\u76ee\u6807\uff0c\u5e76\u8bbe\u8ba1\u4e00\u4e2a\u65b0\u7684 \\textbf{\u4e00\u822c\u5b66\u4e60\u76ee\u6807} \u6765\u89e3\u91ca\u548c\u5206\u6790\u5927\u591a\u6570\u73b0\u6709\u7684 mDG \u667a\u6167\u3002\u8fd9\u4e2a\u603b\u4f53\u76ee\u6807\u5206\u4e3a\u4e24\u4e2a\u534f\u540c\u76ee\u7684\uff1a\u5b66\u4e60\u4e0e\u9886\u57df\u65e0\u5173\u7684\u6761\u4ef6\u7279\u5f81\u548c\u6700\u5927\u5316\u540e\u9a8c\u3002\u63a2\u7d22\u8fd8\u6269\u5c55\u5230\u4e24\u4e2a\u6709\u6548\u7684\u6b63\u5219\u5316\u9879\uff0c\u5b83\u4eec\u5305\u542b\u5148\u9a8c\u4fe1\u606f\u5e76\u6291\u5236\u65e0\u6548\u56e0\u679c\u5173\u7cfb\uff0c\u4ece\u800c\u7f13\u89e3\u5bbd\u677e\u7ea6\u675f\u5e26\u6765\u7684\u95ee\u9898\u3002\u7406\u8bba\u4e0a\uff0c\u6211\u4eec\u4e3a\u4e0e\u57df\u65e0\u5173\u7684\u6761\u4ef6\u7279\u5f81\u7684\u57df\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0a\u9650\uff0c\u63ed\u793a\u4e86\u8bb8\u591a\u4ee5\u524d\u7684 mDG \u52aa\u529b\u5b9e\u9645\u4e0a\\textbf{\u90e8\u5206\u4f18\u5316\u76ee\u6807}\uff0c\u4ece\u800c\u5bfc\u81f4\u6027\u80fd\u6709\u9650\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u7814\u7a76\u5c06\u4e00\u822c\u5b66\u4e60\u76ee\u6807\u63d0\u70bc\u4e3a\u56db\u4e2a\u5b9e\u7528\u7ec4\u6210\u90e8\u5206\uff0c\u63d0\u4f9b\u901a\u7528\u3001\u7a33\u5065\u4e14\u7075\u6d3b\u7684\u673a\u5236\u6765\u5904\u7406\u590d\u6742\u7684\u9886\u57df\u8f6c\u6362\u3002\u5927\u91cf\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5e26\u6709 $Y$ \u6620\u5c04\u7684\u76ee\u6807\u53ef\u4ee5\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\uff08\u5305\u62ec\u56de\u5f52\u3001\u5206\u5272\u548c\u5206\u7c7b\uff09\u4e2d\u5e26\u6765\u66f4\u597d\u7684 mDG \u6027\u80fd\u3002|[2402.18853v1](http://arxiv.org/pdf/2402.18853v1)|null|\n", "2402.18821": "|**2024-02-29**|**Debiased Novel Category Discovering and Localization**|\u53bb\u504f\u89c1\u7684\u5c0f\u8bf4\u7c7b\u522b\u53d1\u73b0\u548c\u672c\u5730\u5316|Juexiao Feng, Yuhong Yang, Yanchun Xie, Yaqian Li, Yandong Guo, Yuchen Guo, Yuwei He, Liuyu Xiang, Guiguang Ding|In recent years, object detection in deep learning has experienced rapid development. However, most existing object detection models perform well only on closed-set datasets, ignoring a large number of potential objects whose categories are not defined in the training set. These objects are often identified as background or incorrectly classified as pre-defined categories by the detectors. In this paper, we focus on the challenging problem of Novel Class Discovery and Localization (NCDL), aiming to train detectors that can detect the categories present in the training data, while also actively discover, localize, and cluster new categories. We analyze existing NCDL methods and identify the core issue: object detectors tend to be biased towards seen objects, and this leads to the neglect of unseen targets. To address this issue, we first propose an Debiased Region Mining (DRM) approach that combines class-agnostic Region Proposal Network (RPN) and class-aware RPN in a complementary manner. Additionally, we suggest to improve the representation network through semi-supervised contrastive learning by leveraging unlabeled data. Finally, we adopt a simple and efficient mini-batch K-means clustering method for novel class discovery. We conduct extensive experiments on the NCDL benchmark, and the results demonstrate that the proposed DRM approach significantly outperforms previous methods, establishing a new state-of-the-art.|\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u5f97\u5230\u4e86\u5feb\u901f\u53d1\u5c55\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4ec5\u5728\u5c01\u95ed\u96c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5ffd\u7565\u4e86\u8bad\u7ec3\u96c6\u4e2d\u672a\u5b9a\u4e49\u7c7b\u522b\u7684\u5927\u91cf\u6f5c\u5728\u76ee\u6807\u3002\u8fd9\u4e9b\u7269\u4f53\u901a\u5e38\u88ab\u68c0\u6d4b\u5668\u8bc6\u522b\u4e3a\u80cc\u666f\u6216\u9519\u8bef\u5730\u5206\u7c7b\u4e3a\u9884\u5b9a\u4e49\u7c7b\u522b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5173\u6ce8\u65b0\u7c7b\u522b\u53d1\u73b0\u548c\u5b9a\u4f4d\uff08NCDL\uff09\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u65e8\u5728\u8bad\u7ec3\u80fd\u591f\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u4e2d\u5b58\u5728\u7684\u7c7b\u522b\u7684\u68c0\u6d4b\u5668\uff0c\u540c\u65f6\u8fd8\u4e3b\u52a8\u53d1\u73b0\u3001\u5b9a\u4f4d\u548c\u805a\u7c7b\u65b0\u7c7b\u522b\u3002\u6211\u4eec\u5206\u6790\u4e86\u73b0\u6709\u7684 NCDL \u65b9\u6cd5\u5e76\u786e\u5b9a\u4e86\u6838\u5fc3\u95ee\u9898\uff1a\u76ee\u6807\u68c0\u6d4b\u5668\u5f80\u5f80\u504f\u5411\u4e8e\u53ef\u89c1\u7684\u76ee\u6807\uff0c\u8fd9\u5bfc\u81f4\u5ffd\u7565\u4e0d\u53ef\u89c1\u7684\u76ee\u6807\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u504f\u533a\u57df\u6316\u6398\uff08DRM\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ee5\u4e92\u8865\u7684\u65b9\u5f0f\u7ed3\u5408\u4e86\u7c7b\u65e0\u5173\u533a\u57df\u63d0\u8bae\u7f51\u7edc\uff08RPN\uff09\u548c\u7c7b\u611f\u77e5 RPN\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u7684\u534a\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6765\u6539\u8fdb\u8868\u793a\u7f51\u7edc\u3002\u6700\u540e\uff0c\u6211\u4eec\u91c7\u7528\u7b80\u5355\u9ad8\u6548\u7684\u5c0f\u6279\u91cf K \u5747\u503c\u805a\u7c7b\u65b9\u6cd5\u8fdb\u884c\u65b0\u7c7b\u53d1\u73b0\u3002\u6211\u4eec\u5bf9 NCDL \u57fa\u51c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684 DRM \u65b9\u6cd5\u663e\u7740\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002|[2402.18821v1](http://arxiv.org/pdf/2402.18821v1)|null|\n", "2402.18786": "|**2024-02-29**|**OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition**|OpticalDR\uff1a\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u6291\u90c1\u75c7\u8bc6\u522b\u7684\u6df1\u5ea6\u5149\u5b66\u6210\u50cf\u6a21\u578b|Yuchen Pan, Junjun Jiang, Kui Jiang, Zhihao Wu, Keyuan Yu, Xianming Liu|Depression Recognition (DR) poses a considerable challenge, especially in the context of the growing concerns surrounding privacy. Traditional automatic diagnosis of DR technology necessitates the use of facial images, undoubtedly expose the patient identity features and poses privacy risks. In order to mitigate the potential risks associated with the inappropriate disclosure of patient facial images, we design a new imaging system to erase the identity information of captured facial images while retain disease-relevant features. It is irreversible for identity information recovery while preserving essential disease-related characteristics necessary for accurate DR. More specifically, we try to record a de-identified facial image (erasing the identifiable features as much as possible) by a learnable lens, which is optimized in conjunction with the following DR task as well as a range of face analysis related auxiliary tasks in an end-to-end manner. These aforementioned strategies form our final Optical deep Depression Recognition network (OpticalDR). Experiments on CelebA, AVEC 2013, and AVEC 2014 datasets demonstrate that our OpticalDR has achieved state-of-the-art privacy protection performance with an average AUC of 0.51 on popular facial recognition models, and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and 7.89/8.82 on AVEC 2014, respectively.|\u6291\u90c1\u75c7\u8bc6\u522b\uff08DR\uff09\u63d0\u51fa\u4e86\u76f8\u5f53\u5927\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4eba\u4eec\u5bf9\u9690\u79c1\u65e5\u76ca\u5173\u6ce8\u7684\u80cc\u666f\u4e0b\u3002\u4f20\u7edf\u7684DR\u6280\u672f\u81ea\u52a8\u8bca\u65ad\u9700\u8981\u4f7f\u7528\u4eba\u8138\u56fe\u50cf\uff0c\u65e0\u7591\u66b4\u9732\u4e86\u60a3\u8005\u7684\u8eab\u4efd\u7279\u5f81\u5e76\u5e26\u6765\u9690\u79c1\u98ce\u9669\u3002\u4e3a\u4e86\u51cf\u8f7b\u4e0e\u4e0d\u5f53\u62ab\u9732\u60a3\u8005\u9762\u90e8\u56fe\u50cf\u76f8\u5173\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u6210\u50cf\u7cfb\u7edf\uff0c\u53ef\u4ee5\u64e6\u9664\u6355\u83b7\u7684\u9762\u90e8\u56fe\u50cf\u7684\u8eab\u4efd\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u4e0e\u75be\u75c5\u76f8\u5173\u7684\u7279\u5f81\u3002\u8eab\u4efd\u4fe1\u606f\u6062\u590d\u662f\u4e0d\u53ef\u9006\u7684\uff0c\u540c\u65f6\u4fdd\u7559\u51c6\u786e DR \u6240\u9700\u7684\u57fa\u672c\u75be\u75c5\u76f8\u5173\u7279\u5f81\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u5c1d\u8bd5\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u955c\u5934\u8bb0\u5f55\u53bb\u8bc6\u522b\u7684\u9762\u90e8\u56fe\u50cf\uff08\u5c3d\u53ef\u80fd\u64e6\u9664\u53ef\u8bc6\u522b\u7684\u7279\u5f81\uff09\uff0c\u8be5\u955c\u5934\u7ed3\u5408\u4ee5\u4e0bDR\u4efb\u52a1\u4ee5\u53ca\u4e00\u7cfb\u5217\u4eba\u8138\u5206\u6790\u76f8\u5173\u7684\u8f85\u52a9\u4efb\u52a1\u8fdb\u884c\u4e86\u4f18\u5316\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u3002\u8fd9\u4e9b\u4e0a\u8ff0\u7b56\u7565\u6784\u6210\u4e86\u6211\u4eec\u6700\u7ec8\u7684\u5149\u5b66\u6df1\u5ea6\u6291\u90c1\u8bc6\u522b\u7f51\u7edc\uff08OpticalDR\uff09\u3002\u5728 CelebA\u3001AVEC 2013 \u548c AVEC 2014 \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 OpticalDR \u5df2\u7ecf\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9690\u79c1\u4fdd\u62a4\u6027\u80fd\uff0c\u5728\u6d41\u884c\u7684\u9762\u90e8\u8bc6\u522b\u6a21\u578b\u4e0a\u5e73\u5747 AUC \u4e3a 0.51\uff0cDR \u7684 MAE/RMSE \u4e3a 7.53 \u7684\u7ade\u4e89\u7ed3\u679cAVEC 2013 \u4e0a\u7684 /8.48 \u548c AVEC 2014 \u4e0a\u7684 7.89/8.82 \u5206\u522b\u3002|[2402.18786v1](http://arxiv.org/pdf/2402.18786v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2402.19341": "|**2024-02-29**|**RoadRunner -- Learning Traversability Estimation for Autonomous Off-road Driving**|RoadRunner\u2014\u2014\u5b66\u4e60\u81ea\u4e3b\u8d8a\u91ce\u9a7e\u9a76\u7684\u53ef\u901a\u884c\u6027\u4f30\u8ba1|Jonas Frey, Shehryar Khattak, Manthan Patel, Deegan Atha, Julian Nubert, Curtis Padgett, Marco Hutter, Patrick Spieler|Autonomous navigation at high speeds in off-road environments necessitates robots to comprehensively understand their surroundings using onboard sensing only. The extreme conditions posed by the off-road setting can cause degraded camera image quality due to poor lighting and motion blur, as well as limited sparse geometric information available from LiDAR sensing when driving at high speeds. In this work, we present RoadRunner, a novel framework capable of predicting terrain traversability and an elevation map directly from camera and LiDAR sensor inputs. RoadRunner enables reliable autonomous navigation, by fusing sensory information, handling of uncertainty, and generation of contextually informed predictions about the geometry and traversability of the terrain while operating at low latency. In contrast to existing methods relying on classifying handcrafted semantic classes and using heuristics to predict traversability costs, our method is trained end-to-end in a self-supervised fashion. The RoadRunner network architecture builds upon popular sensor fusion network architectures from the autonomous driving domain, which embed LiDAR and camera information into a common Bird's Eye View perspective. Training is enabled by utilizing an existing traversability estimation stack to generate training data in hindsight in a scalable manner from real-world off-road driving datasets. Furthermore, RoadRunner improves the system latency by a factor of roughly 4, from 500 ms to 140 ms, while improving the accuracy for traversability costs and elevation map predictions. We demonstrate the effectiveness of RoadRunner in enabling safe and reliable off-road navigation at high speeds in multiple real-world driving scenarios through unstructured desert environments.|\u5728\u8d8a\u91ce\u73af\u5883\u4e2d\u9ad8\u901f\u81ea\u4e3b\u5bfc\u822a\u9700\u8981\u673a\u5668\u4eba\u4ec5\u4f7f\u7528\u673a\u8f7d\u4f20\u611f\u6765\u5168\u9762\u4e86\u89e3\u5468\u56f4\u73af\u5883\u3002\u8d8a\u91ce\u73af\u5883\u9020\u6210\u7684\u6781\u7aef\u6761\u4ef6\u53ef\u80fd\u4f1a\u5bfc\u81f4\u76f8\u673a\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\uff0c\u539f\u56e0\u662f\u7167\u660e\u4e0d\u4f73\u548c\u8fd0\u52a8\u6a21\u7cca\uff0c\u4ee5\u53ca\u9ad8\u901f\u884c\u9a76\u65f6\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u63d0\u4f9b\u7684\u7a00\u758f\u51e0\u4f55\u4fe1\u606f\u6709\u9650\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 RoadRunner\uff0c\u8fd9\u662f\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u6839\u636e\u76f8\u673a\u548c LiDAR \u4f20\u611f\u5668\u8f93\u5165\u9884\u6d4b\u5730\u5f62\u53ef\u7a7f\u8d8a\u6027\u548c\u9ad8\u7a0b\u56fe\u7684\u65b0\u9896\u6846\u67b6\u3002 RoadRunner \u901a\u8fc7\u878d\u5408\u611f\u77e5\u4fe1\u606f\u3001\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u751f\u6210\u6709\u5173\u5730\u5f62\u51e0\u4f55\u5f62\u72b6\u548c\u53ef\u901a\u884c\u6027\u7684\u4e0a\u4e0b\u6587\u9884\u6d4b\uff0c\u540c\u65f6\u4ee5\u4f4e\u5ef6\u8fdf\u8fd0\u884c\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u81ea\u4e3b\u5bfc\u822a\u3002\u4e0e\u4f9d\u8d56\u4e8e\u5bf9\u624b\u5de5\u8bed\u4e49\u7c7b\u8fdb\u884c\u5206\u7c7b\u5e76\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u9884\u6d4b\u53ef\u904d\u5386\u6027\u6210\u672c\u7684\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ee5\u81ea\u6211\u76d1\u7763\u7684\u65b9\u5f0f\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002 RoadRunner \u7f51\u7edc\u67b6\u6784\u5efa\u7acb\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u6d41\u884c\u7684\u4f20\u611f\u5668\u878d\u5408\u7f51\u7edc\u67b6\u6784\u4e4b\u4e0a\uff0c\u8be5\u67b6\u6784\u5c06 LiDAR \u548c\u6444\u50cf\u5934\u4fe1\u606f\u5d4c\u5165\u5230\u901a\u7528\u9e1f\u77b0\u89c6\u89d2\u4e2d\u3002\u8bad\u7ec3\u662f\u901a\u8fc7\u5229\u7528\u73b0\u6709\u7684\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u5806\u6808\u4ece\u73b0\u5b9e\u4e16\u754c\u7684\u8d8a\u91ce\u9a7e\u9a76\u6570\u636e\u96c6\u4ee5\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u751f\u6210\u4e8b\u540e\u8bad\u7ec3\u6570\u636e\u6765\u5b9e\u73b0\u7684\u3002\u6b64\u5916\uff0cRoadRunner \u5c06\u7cfb\u7edf\u5ef6\u8fdf\u63d0\u9ad8\u4e86\u5927\u7ea6 4 \u500d\uff0c\u4ece 500 \u6beb\u79d2\u7f29\u77ed\u5230 140 \u6beb\u79d2\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u901a\u884c\u6210\u672c\u548c\u9ad8\u7a0b\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u5c55\u793a\u4e86 RoadRunner \u5728\u7a7f\u8d8a\u975e\u7ed3\u6784\u5316\u6c99\u6f20\u73af\u5883\u7684\u591a\u4e2a\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u9ad8\u901f\u8d8a\u91ce\u5bfc\u822a\u7684\u6709\u6548\u6027\u3002|[2402.19341v1](http://arxiv.org/pdf/2402.19341v1)|null|\n", "2402.18925": "|**2024-02-29**|**PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds**|PCDepth\uff1a\u57fa\u4e8e\u6a21\u5f0f\u7684\u4e92\u8865\u5b66\u4e60\uff0c\u7528\u4e8e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4e24\u5168\u5176\u7f8e|Haotian Liu, Sanqing Qu, Fan Lu, Zongtao Bu, Florian Roehrbein, Alois Knoll, Guang Chen|Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination. Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding. However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy a few pixels. For example, event data is likely to complement contours of scene objects. In this paper, we discretize the scene into a set of high-level patterns to explore the complementarity and propose a Pattern-based Complementary learning architecture for monocular Depth estimation (PCDepth). Concretely, PCDepth comprises two primary components: a complementary visual representation learning module for discretizing the scene into high-level patterns and integrating complementary patterns across modalities and a refined depth estimator aimed at scene reconstruction and depth prediction while maintaining an efficiency-accuracy balance. Through pattern-based complementary learning, PCDepth fully exploits two modalities and achieves more accurate predictions than existing methods, especially in challenging nighttime scenarios. Extensive experiments on MVSEC and DSEC datasets verify the effectiveness and superiority of our PCDepth. Remarkably, compared with state-of-the-art, PCDepth achieves a 37.9% improvement in accuracy in MVSEC nighttime scenarios.|\u4e8b\u4ef6\u6444\u50cf\u673a\u53ef\u4ee5\u4ee5\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u8bb0\u5f55\u573a\u666f\u52a8\u6001\uff0c\u5373\u4f7f\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u4e5f\u53ef\u4ee5\u4e3a\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1 (MDE) \u63d0\u4f9b\u4e30\u5bcc\u7684\u573a\u666f\u7ec6\u8282\u3002\u56e0\u6b64\uff0c\u73b0\u6709\u7684 MDE \u8865\u5145\u5b66\u4e60\u65b9\u6cd5\u878d\u5408\u4e86\u56fe\u50cf\u4e2d\u7684\u5f3a\u5ea6\u4fe1\u606f\u548c\u4e8b\u4ef6\u6570\u636e\u4e2d\u7684\u573a\u666f\u7ec6\u8282\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u573a\u666f\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u65b9\u6cd5\u76f4\u63a5\u5728\u50cf\u7d20\u7ea7\u878d\u5408\u4e24\u79cd\u6a21\u6001\uff0c\u5ffd\u7565\u4e86\u6709\u5438\u5f15\u529b\u7684\u4e92\u8865\u6027\u4e3b\u8981\u5f71\u54cd\u4ec5\u5360\u636e\u51e0\u4e2a\u50cf\u7d20\u7684\u9ad8\u7ea7\u6a21\u5f0f\u3002\u4f8b\u5982\uff0c\u4e8b\u4ef6\u6570\u636e\u53ef\u80fd\u4f1a\u8865\u5145\u573a\u666f\u5bf9\u8c61\u7684\u8f6e\u5ed3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u573a\u666f\u79bb\u6563\u5316\u4e3a\u4e00\u7ec4\u9ad8\u7ea7\u6a21\u5f0f\u6765\u63a2\u7d22\u4e92\u8865\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff08PCDepth\uff09\u7684\u57fa\u4e8e\u6a21\u5f0f\u7684\u4e92\u8865\u5b66\u4e60\u67b6\u6784\u3002\u5177\u4f53\u6765\u8bf4\uff0cPCDepth \u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u4e00\u4e2a\u4e92\u8865\u7684\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u6a21\u5757\uff0c\u7528\u4e8e\u5c06\u573a\u666f\u79bb\u6563\u4e3a\u9ad8\u7ea7\u6a21\u5f0f\u5e76\u96c6\u6210\u8de8\u6a21\u6001\u7684\u4e92\u8865\u6a21\u5f0f\uff1b\u4ee5\u53ca\u4e00\u4e2a\u7cbe\u70bc\u7684\u6df1\u5ea6\u4f30\u8ba1\u5668\uff0c\u65e8\u5728\u573a\u666f\u91cd\u5efa\u548c\u6df1\u5ea6\u9884\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002\u901a\u8fc7\u57fa\u4e8e\u6a21\u5f0f\u7684\u4e92\u8865\u5b66\u4e60\uff0cPCDepth \u5145\u5206\u5229\u7528\u4e86\u4e24\u79cd\u6a21\u5f0f\uff0c\u5e76\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u7684\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591c\u95f4\u573a\u666f\u4e2d\u3002\u5728 MVSEC \u548c DSEC \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684 PCDepth \u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u6280\u672f\u76f8\u6bd4\uff0cPCDepth \u5728 MVSEC \u591c\u95f4\u573a\u666f\u4e2d\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e86 37.9%\u3002|[2402.18925v1](http://arxiv.org/pdf/2402.18925v1)|null|\n", "2402.18848": "|**2024-02-29**|**SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting**|SwitchLight\uff1a\u7269\u7406\u9a71\u52a8\u67b6\u6784\u548c\u4eba\u50cf\u8865\u5149\u9884\u8bad\u7ec3\u6846\u67b6\u7684\u534f\u540c\u8bbe\u8ba1|Hoon Kim, Minje Jang, Wonjun Yoon, Jisoo Lee, Donghyun Na, Sanghyun Woo|We introduce a co-designed approach for human portrait relighting that combines a physics-guided architecture with a pre-training framework. Drawing on the Cook-Torrance reflectance model, we have meticulously configured the architecture design to precisely simulate light-surface interactions. Furthermore, to overcome the limitation of scarce high-quality lightstage data, we have developed a self-supervised pre-training strategy. This novel combination of accurate physical modeling and expanded training dataset establishes a new benchmark in relighting realism.|\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u8054\u5408\u8bbe\u8ba1\u7684\u4eba\u4f53\u8096\u50cf\u91cd\u65b0\u7167\u660e\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u7269\u7406\u5f15\u5bfc\u67b6\u6784\u4e0e\u9884\u8bad\u7ec3\u6846\u67b6\u76f8\u7ed3\u5408\u3002\u501f\u9274\u5e93\u514b-\u6258\u4f26\u65af\u53cd\u5c04\u7387\u6a21\u578b\uff0c\u6211\u4eec\u7cbe\u5fc3\u914d\u7f6e\u4e86\u67b6\u6784\u8bbe\u8ba1\uff0c\u4ee5\u7cbe\u786e\u6a21\u62df\u5149\u4e0e\u8868\u9762\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u514b\u670d\u9ad8\u8d28\u91cf\u5149\u821e\u53f0\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u6211\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u3002\u8fd9\u79cd\u51c6\u786e\u7684\u7269\u7406\u5efa\u6a21\u548c\u6269\u5c55\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u65b0\u9896\u7ec4\u5408\u4e3a\u91cd\u65b0\u7167\u4eae\u73b0\u5b9e\u4e3b\u4e49\u6811\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002|[2402.18848v1](http://arxiv.org/pdf/2402.18848v1)|null|\n"}, "LLM": {}, "Transformer": {"2402.19308": "|**2024-02-29**|**Loss-Free Machine Unlearning**|\u65e0\u635f\u5931\u673a\u5668\u5fd8\u5374|Jack Foster, Stefan Schoepf, Alexandra Brintrup|We present a machine unlearning approach that is both retraining- and label-free. Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model. Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available. Thus, we present an extension to the Selective Synaptic Dampening algorithm, substituting the diagonal of the Fisher information matrix for the gradient of the l2 norm of the model output to approximate sensitivity. We evaluate our method in a range of experiments using ResNet18 and Vision Transformer. Results show our label-free method is competitive with existing state-of-the-art approaches.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u518d\u8bad\u7ec3\u548c\u65e0\u6807\u7b7e\u7684\u673a\u5668\u53d6\u6d88\u5b66\u4e60\u65b9\u6cd5\u3002\u5927\u591a\u6570\u73b0\u6709\u7684\u673a\u5668\u53bb\u5b66\u4e60\u65b9\u6cd5\u90fd\u9700\u8981\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5220\u9664\u4fe1\u606f\u3002\u8fd9\u5728\u8ba1\u7b97\u4e0a\u662f\u6602\u8d35\u7684\uff0c\u5e76\u4e14\u9700\u8981\u5728\u6a21\u578b\u7684\u751f\u547d\u5468\u671f\u5185\u5b58\u50a8\u6574\u4e2a\u6570\u636e\u96c6\u3002\u514d\u518d\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u5229\u7528\u8d39\u820d\u5c14\u4fe1\u606f\uff0c\u8be5\u4fe1\u606f\u6765\u81ea\u635f\u5931\uff0c\u5e76\u4e14\u9700\u8981\u53ef\u80fd\u65e0\u6cd5\u83b7\u5f97\u7684\u6807\u8bb0\u6570\u636e\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9009\u62e9\u6027\u7a81\u89e6\u6291\u5236\u7b97\u6cd5\u7684\u6269\u5c55\uff0c\u7528 Fisher \u4fe1\u606f\u77e9\u9635\u7684\u5bf9\u89d2\u7ebf\u66ff\u6362\u6a21\u578b\u8f93\u51fa\u7684 l2 \u8303\u6570\u7684\u68af\u5ea6\u4ee5\u8fd1\u4f3c\u7075\u654f\u5ea6\u3002\u6211\u4eec\u4f7f\u7528 ResNet18 \u548c Vision Transformer \u5728\u4e00\u7cfb\u5217\u5b9e\u9a8c\u4e2d\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65e0\u6807\u8bb0\u65b9\u6cd5\u4e0e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002|[2402.19308v1](http://arxiv.org/pdf/2402.19308v1)|null|\n", "2402.19305": "|**2024-02-29**|**HyenaPixel: Global Image Context with Convolutions**|HyenaPixel\uff1a\u5e26\u6709\u5377\u79ef\u7684\u5168\u5c40\u56fe\u50cf\u4e0a\u4e0b\u6587|Julian Spravil, Sebastian Houben, Sven Behnke|In vision tasks, a larger effective receptive field (ERF) is associated with better performance. While attention natively supports global context, convolution requires multiple stacked layers and a hierarchical structure for large context. In this work, we extend Hyena, a convolution-based attention replacement, from causal sequences to the non-causal two-dimensional image space. We scale the Hyena convolution kernels beyond the feature map size up to 191$\\times$191 to maximize the ERF while maintaining sub-quadratic complexity in the number of pixels. We integrate our two-dimensional Hyena, HyenaPixel, and bidirectional Hyena into the MetaFormer framework. For image categorization, HyenaPixel and bidirectional Hyena achieve a competitive ImageNet-1k top-1 accuracy of 83.0% and 83.5%, respectively, while outperforming other large-kernel networks. Combining HyenaPixel with attention further increases accuracy to 83.6%. We attribute the success of attention to the lack of spatial bias in later stages and support this finding with bidirectional Hyena.|\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0c\u66f4\u5927\u7684\u6709\u6548\u611f\u53d7\u91ce\uff08ERF\uff09\u4e0e\u66f4\u597d\u7684\u6027\u80fd\u76f8\u5173\u3002\u867d\u7136\u6ce8\u610f\u529b\u672c\u8eab\u652f\u6301\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u4f46\u5377\u79ef\u9700\u8981\u591a\u4e2a\u5806\u53e0\u5c42\u548c\u7528\u4e8e\u5927\u4e0a\u4e0b\u6587\u7684\u5206\u5c42\u7ed3\u6784\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06\u57fa\u4e8e\u5377\u79ef\u7684\u6ce8\u610f\u529b\u66ff\u6362 Hyena \u4ece\u56e0\u679c\u5e8f\u5217\u6269\u5c55\u5230\u975e\u56e0\u679c\u4e8c\u7ef4\u56fe\u50cf\u7a7a\u95f4\u3002\u6211\u4eec\u5c06 Hyena \u5377\u79ef\u6838\u6269\u5c55\u5230\u7279\u5f81\u56fe\u5927\u5c0f\u4e4b\u5916\uff0c\u9ad8\u8fbe 191$\\times$191\uff0c\u4ee5\u6700\u5927\u5316 ERF\uff0c\u540c\u65f6\u4fdd\u6301\u50cf\u7d20\u6570\u91cf\u7684\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u3002\u6211\u4eec\u5c06\u4e8c\u7ef4 Hyena\u3001HyenaPixel \u548c\u53cc\u5411 Hyena \u96c6\u6210\u5230 MetaFormer \u6846\u67b6\u4e2d\u3002\u5bf9\u4e8e\u56fe\u50cf\u5206\u7c7b\uff0cHyenaPixel \u548c\u53cc\u5411 Hyena \u5206\u522b\u5b9e\u73b0\u4e86 83.0% \u548c 83.5% \u7684\u5177\u6709\u7ade\u4e89\u529b\u7684 ImageNet-1k top-1 \u51c6\u786e\u7387\uff0c\u540c\u65f6\u4f18\u4e8e\u5176\u4ed6\u5927\u5185\u6838\u7f51\u7edc\u3002\u5c06 HyenaPixel \u4e0e\u6ce8\u610f\u529b\u76f8\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u5230 83.6%\u3002\u6211\u4eec\u5c06\u6ce8\u610f\u529b\u7684\u6210\u529f\u5f52\u56e0\u4e8e\u540e\u671f\u9636\u6bb5\u7f3a\u4e4f\u7a7a\u95f4\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u53cc\u5411\u9b23\u72d7\u652f\u6301\u8fd9\u4e00\u53d1\u73b0\u3002|[2402.19305v1](http://arxiv.org/pdf/2402.19305v1)|null|\n", "2402.19250": "|**2024-02-29**|**Feature boosting with efficient attention for scene parsing**|\u901a\u8fc7\u6709\u6548\u5173\u6ce8\u573a\u666f\u89e3\u6790\u6765\u589e\u5f3a\u7279\u5f81|Vivek Singh, Shailza Sharma, Fabio Cuzzolin|The complexity of scene parsing grows with the number of object and scene classes, which is higher in unrestricted open scenes. The biggest challenge is to model the spatial relation between scene elements while succeeding in identifying objects at smaller scales. This paper presents a novel feature-boosting network that gathers spatial context from multiple levels of feature extraction and computes the attention weights for each level of representation to generate the final class labels. A novel `channel attention module' is designed to compute the attention weights, ensuring that features from the relevant extraction stages are boosted while the others are attenuated. The model also learns spatial context information at low resolution to preserve the abstract spatial relationships among scene elements and reduce computation cost. Spatial attention is subsequently concatenated into a final feature set before applying feature boosting. Low-resolution spatial attention features are trained using an auxiliary task that helps learning a coarse global scene structure. The proposed model outperforms all state-of-the-art models on both the ADE20K and the Cityscapes datasets.|\u573a\u666f\u89e3\u6790\u7684\u590d\u6742\u6027\u968f\u7740\u5bf9\u8c61\u548c\u573a\u666f\u7c7b\u7684\u6570\u91cf\u800c\u589e\u52a0\uff0c\u5728\u4e0d\u53d7\u9650\u5236\u7684\u5f00\u653e\u573a\u666f\u4e2d\u66f4\u9ad8\u3002\u6700\u5927\u7684\u6311\u6218\u662f\u5bf9\u573a\u666f\u5143\u7d20\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\uff0c\u540c\u65f6\u6210\u529f\u8bc6\u522b\u8f83\u5c0f\u5c3a\u5ea6\u7684\u5bf9\u8c61\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7279\u5f81\u589e\u5f3a\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u4ece\u591a\u4e2a\u7ea7\u522b\u7684\u7279\u5f81\u63d0\u53d6\u4e2d\u6536\u96c6\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u5e76\u8ba1\u7b97\u6bcf\u4e2a\u8868\u793a\u7ea7\u522b\u7684\u6ce8\u610f\u529b\u6743\u91cd\u4ee5\u751f\u6210\u6700\u7ec8\u7684\u7c7b\u6807\u7b7e\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u201d\u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u786e\u4fdd\u76f8\u5173\u63d0\u53d6\u9636\u6bb5\u7684\u7279\u5f81\u5f97\u5230\u589e\u5f3a\uff0c\u800c\u5176\u4ed6\u9636\u6bb5\u7684\u7279\u5f81\u5f97\u5230\u589e\u5f3a\u3002\u8be5\u6a21\u578b\u8fd8\u4ee5\u4f4e\u5206\u8fa8\u7387\u5b66\u4e60\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u4fdd\u7559\u573a\u666f\u5143\u7d20\u4e4b\u95f4\u7684\u62bd\u8c61\u7a7a\u95f4\u5173\u7cfb\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u5728\u5e94\u7528\u7279\u5f81\u589e\u5f3a\u4e4b\u524d\uff0c\u7a7a\u95f4\u6ce8\u610f\u529b\u968f\u540e\u88ab\u8fde\u63a5\u5230\u6700\u7ec8\u7684\u7279\u5f81\u96c6\u4e2d\u3002\u4f7f\u7528\u6709\u52a9\u4e8e\u5b66\u4e60\u7c97\u7565\u5168\u5c40\u573a\u666f\u7ed3\u6784\u7684\u8f85\u52a9\u4efb\u52a1\u6765\u8bad\u7ec3\u4f4e\u5206\u8fa8\u7387\u7a7a\u95f4\u6ce8\u610f\u7279\u5f81\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728 ADE20K \u548c Cityscapes \u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u6240\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002|[2402.19250v1](http://arxiv.org/pdf/2402.19250v1)|null|\n", "2402.19161": "|**2024-02-29**|**MemoNav: Working Memory Model for Visual Navigation**|MemoNav\uff1a\u89c6\u89c9\u5bfc\u822a\u7684\u5de5\u4f5c\u8bb0\u5fc6\u6a21\u578b|Hongxin Li, Zeyu Wang, Xu Yang, Yuran Yang, Shuqi Mei, Zhaoxiang Zhang|Image-goal navigation is a challenging task that requires an agent to navigate to a goal indicated by an image in unfamiliar environments. Existing methods utilizing diverse scene memories suffer from inefficient exploration since they use all historical observations for decision-making without considering the goal-relevant fraction. To address this limitation, we present MemoNav, a novel memory model for image-goal navigation, which utilizes a working memory-inspired pipeline to improve navigation performance. Specifically, we employ three types of navigation memory. The node features on a map are stored in the short-term memory (STM), as these features are dynamically updated. A forgetting module then retains the informative STM fraction to increase efficiency. We also introduce long-term memory (LTM) to learn global scene representations by progressively aggregating STM features. Subsequently, a graph attention module encodes the retained STM and the LTM to generate working memory (WM) which contains the scene features essential for efficient navigation. The synergy among these three memory types boosts navigation performance by enabling the agent to learn and leverage goal-relevant scene features within a topological map. Our evaluation on multi-goal tasks demonstrates that MemoNav significantly outperforms previous methods across all difficulty levels in both Gibson and Matterport3D scenes. Qualitative results further illustrate that MemoNav plans more efficient routes.|\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u4ee3\u7406\u5728\u4e0d\u719f\u6089\u7684\u73af\u5883\u4e2d\u5bfc\u822a\u5230\u56fe\u50cf\u6307\u793a\u7684\u76ee\u6807\u3002\u5229\u7528\u4e0d\u540c\u573a\u666f\u8bb0\u5fc6\u7684\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f7f\u7528\u6240\u6709\u5386\u53f2\u89c2\u5bdf\u7ed3\u679c\u6765\u8fdb\u884c\u51b3\u7b56\uff0c\u800c\u4e0d\u8003\u8651\u4e0e\u76ee\u6807\u76f8\u5173\u7684\u90e8\u5206\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 MemoNav\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u7684\u65b0\u578b\u5185\u5b58\u6a21\u578b\uff0c\u5b83\u5229\u7528\u53d7\u5de5\u4f5c\u5185\u5b58\u542f\u53d1\u7684\u7ba1\u9053\u6765\u63d0\u9ad8\u5bfc\u822a\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u4e09\u79cd\u7c7b\u578b\u7684\u5bfc\u822a\u5b58\u50a8\u5668\u3002\u5730\u56fe\u4e0a\u7684\u8282\u70b9\u7279\u5f81\u5b58\u50a8\u5728\u77ed\u671f\u8bb0\u5fc6\uff08STM\uff09\u4e2d\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u7279\u5f81\u662f\u52a8\u6001\u66f4\u65b0\u7684\u3002\u7136\u540e\uff0c\u9057\u5fd8\u6a21\u5757\u4f1a\u4fdd\u7559\u4fe1\u606f\u4e30\u5bcc\u7684 STM \u90e8\u5206\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u957f\u671f\u8bb0\u5fc6\uff08LTM\uff09\uff0c\u901a\u8fc7\u9010\u6b65\u805a\u5408 STM \u7279\u5f81\u6765\u5b66\u4e60\u5168\u5c40\u573a\u666f\u8868\u793a\u3002\u968f\u540e\uff0c\u56fe\u6ce8\u610f\u6a21\u5757\u5bf9\u4fdd\u7559\u7684 STM \u548c LTM \u8fdb\u884c\u7f16\u7801\uff0c\u4ee5\u751f\u6210\u5de5\u4f5c\u8bb0\u5fc6 (WM)\uff0c\u5176\u4e2d\u5305\u542b\u9ad8\u6548\u5bfc\u822a\u6240\u5fc5\u9700\u7684\u573a\u666f\u7279\u5f81\u3002\u8fd9\u4e09\u79cd\u5185\u5b58\u7c7b\u578b\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u4f7f\u4ee3\u7406\u80fd\u591f\u5b66\u4e60\u548c\u5229\u7528\u62d3\u6251\u56fe\u4e2d\u4e0e\u76ee\u6807\u76f8\u5173\u7684\u573a\u666f\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5bfc\u822a\u6027\u80fd\u3002\u6211\u4eec\u5bf9\u591a\u76ee\u6807\u4efb\u52a1\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5728 Gibson \u548c Matterport3D \u573a\u666f\u4e2d\uff0cMemoNav \u5728\u6240\u6709\u96be\u5ea6\u7ea7\u522b\u4e0a\u90fd\u663e\u7740\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002\u5b9a\u6027\u7ed3\u679c\u8fdb\u4e00\u6b65\u8bf4\u660e MemoNav \u89c4\u5212\u4e86\u66f4\u9ad8\u6548\u7684\u8def\u7ebf\u3002|[2402.19161v1](http://arxiv.org/pdf/2402.19161v1)|null|\n", "2402.19026": "|**2024-02-29**|**Progressive Contrastive Learning with Multi-Prototype for Unsupervised Visible-Infrared Person Re-identification**|\u7528\u4e8e\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u4eba\u5458\u91cd\u65b0\u8bc6\u522b\u7684\u591a\u539f\u578b\u6e10\u8fdb\u5bf9\u6bd4\u5b66\u4e60|Jiangming Shi, Xiangbo Yin, Yaoxing Wang, Xiaofeng Liu, Yuan Xie, Yanyun Qu|Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match specified people in infrared images to visible images without annotation, and vice versa. USVI-ReID is a challenging yet under-explored task. Most existing methods address the USVI-ReID problem using cluster-based contrastive learning, which simply employs the cluster center as a representation of a person. However, the cluster center primarily focuses on shared information, overlooking disparity. To address the problem, we propose a Progressive Contrastive Learning with Multi-Prototype (PCLMP) method for USVI-ReID. In brief, we first generate the hard prototype by selecting the sample with the maximum distance from the cluster center. This hard prototype is used in the contrastive loss to emphasize disparity. Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster. This dynamic prototype is used to retain the natural variety of features while reducing instability in the simultaneous learning of both common and disparate information. Finally, we introduce a progressive learning strategy to gradually shift the model's attention towards hard samples, avoiding cluster deterioration. Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method. PCLMP outperforms the existing state-of-the-art method with an average mAP improvement of 3.9%. The source codes will be released.|\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u4eba\u5458\u91cd\u8bc6\u522b\uff08USVI-ReID\uff09\u65e8\u5728\u5c06\u7ea2\u5916\u56fe\u50cf\u4e2d\u7684\u6307\u5b9a\u4eba\u5458\u4e0e\u4e0d\u5e26\u6ce8\u91ca\u7684\u53ef\u89c1\u5149\u56fe\u50cf\u8fdb\u884c\u5339\u914d\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002 USVI-ReID \u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u4efb\u52a1\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u5bf9\u6bd4\u5b66\u4e60\u6765\u89e3\u51b3 USVI-ReID \u95ee\u9898\uff0c\u8be5\u5b66\u4e60\u7b80\u5355\u5730\u91c7\u7528\u805a\u7c7b\u4e2d\u5fc3\u4f5c\u4e3a\u4eba\u7684\u8868\u793a\u3002\u7136\u800c\uff0c\u96c6\u7fa4\u4e2d\u5fc3\u4e3b\u8981\u5173\u6ce8\u5171\u4eab\u4fe1\u606f\uff0c\u5ffd\u89c6\u5dee\u5f02\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e USVI-ReID \u7684\u6e10\u8fdb\u5f0f\u5bf9\u6bd4\u5b66\u4e60\u591a\u539f\u578b\uff08PCLMP\uff09\u65b9\u6cd5\u3002\u7b80\u800c\u8a00\u4e4b\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u9009\u62e9\u8ddd\u805a\u7c7b\u4e2d\u5fc3\u8ddd\u79bb\u6700\u5927\u7684\u6837\u672c\u6765\u751f\u6210\u786c\u539f\u578b\u3002\u8fd9\u4e2a\u786c\u539f\u578b\u7528\u4e8e\u5bf9\u6bd4\u635f\u5931\u4ee5\u5f3a\u8c03\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e0d\u662f\u4e25\u683c\u5730\u5c06\u67e5\u8be2\u56fe\u50cf\u4e0e\u7279\u5b9a\u539f\u578b\u5bf9\u9f50\uff0c\u800c\u662f\u901a\u8fc7\u968f\u673a\u6311\u9009\u96c6\u7fa4\u5185\u7684\u6837\u672c\u6765\u751f\u6210\u52a8\u6001\u539f\u578b\u3002\u8fd9\u79cd\u52a8\u6001\u539f\u578b\u7528\u4e8e\u4fdd\u7559\u7279\u5f81\u7684\u81ea\u7136\u591a\u6837\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u540c\u65f6\u5b66\u4e60\u5171\u540c\u4fe1\u606f\u548c\u4e0d\u540c\u4fe1\u606f\u65f6\u7684\u4e0d\u7a33\u5b9a\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6e10\u8fdb\u5f0f\u5b66\u4e60\u7b56\u7565\uff0c\u9010\u6e10\u5c06\u6a21\u578b\u7684\u6ce8\u610f\u529b\u8f6c\u79fb\u5230\u786c\u6837\u672c\u4e0a\uff0c\u907f\u514d\u96c6\u7fa4\u6076\u5316\u3002\u5728\u516c\u5f00\u53ef\u7528\u7684 SYSU-MM01 \u548c RegDB \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002 PCLMP \u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e73\u5747 mAP \u63d0\u9ad8\u4e86 3.9%\u3002\u6e90\u4ee3\u7801\u5c06\u88ab\u53d1\u5e03\u3002|[2402.19026v1](http://arxiv.org/pdf/2402.19026v1)|null|\n", "2402.18871": "|**2024-02-29**|**LoLiSRFlow: Joint Single Image Low-light Enhancement and Super-resolution via Cross-scale Transformer-based Conditional Flow**|LoLiSRFlow\uff1a\u901a\u8fc7\u57fa\u4e8e\u8de8\u5c3a\u5ea6\u53d8\u538b\u5668\u7684\u6761\u4ef6\u6d41\u8054\u5408\u5355\u56fe\u50cf\u4f4e\u5149\u589e\u5f3a\u548c\u8d85\u5206\u8fa8\u7387|Ziyu Yue, Jiaxin Gao, Sihan Xie, Yang Liu, Zhixun Su|The visibility of real-world images is often limited by both low-light and low-resolution, however, these issues are only addressed in the literature through Low-Light Enhancement (LLE) and Super- Resolution (SR) methods. Admittedly, a simple cascade of these approaches cannot work harmoniously to cope well with the highly ill-posed problem for simultaneously enhancing visibility and resolution. In this paper, we propose a normalizing flow network, dubbed LoLiSRFLow, specifically designed to consider the degradation mechanism inherent in joint LLE and SR. To break the bonds of the one-to-many mapping for low-light low-resolution images to normal-light high-resolution images, LoLiSRFLow directly learns the conditional probability distribution over a variety of feasible solutions for high-resolution well-exposed images. Specifically, a multi-resolution parallel transformer acts as a conditional encoder that extracts the Retinex-induced resolution-and-illumination invariant map as the previous one. And the invertible network maps the distribution of usually exposed high-resolution images to a latent distribution. The backward inference is equivalent to introducing an additional constrained loss for the normal training route, thus enabling the manifold of the natural exposure of the high-resolution image to be immaculately depicted. We also propose a synthetic dataset modeling the realistic low-light low-resolution degradation, named DFSR-LLE, containing 7100 low-resolution dark-light/high-resolution normal sharp pairs. Quantitative and qualitative experimental results demonstrate the effectiveness of our method on both the proposed synthetic and real datasets.|\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u7684\u53ef\u89c1\u5ea6\u901a\u5e38\u53d7\u5230\u4f4e\u5149\u548c\u4f4e\u5206\u8fa8\u7387\u7684\u9650\u5236\uff0c\u7136\u800c\uff0c\u8fd9\u4e9b\u95ee\u9898\u4ec5\u5728\u6587\u732e\u4e2d\u901a\u8fc7\u4f4e\u5149\u589e\u5f3a\uff08LLE\uff09\u548c\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u65b9\u6cd5\u6765\u89e3\u51b3\u3002\u8bda\u7136\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7684\u7b80\u5355\u7ea7\u8054\u4e0d\u80fd\u548c\u8c10\u5730\u5de5\u4f5c\uff0c\u4ee5\u5f88\u597d\u5730\u5904\u7406\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u53ef\u89c1\u6027\u548c\u5206\u8fa8\u7387\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u51c6\u5316\u6d41\u7f51\u7edc\uff0c\u79f0\u4e3a LoLiSRFLow\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8003\u8651\u8054\u5408 LLE \u548c SR \u56fa\u6709\u7684\u9000\u5316\u673a\u5236\u3002\u4e3a\u4e86\u6253\u7834\u4f4e\u5149\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u5230\u6b63\u5e38\u5149\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u4e00\u5bf9\u591a\u6620\u5c04\u7684\u675f\u7f1a\uff0cLoLiSRFLow \u76f4\u63a5\u5b66\u4e60\u9ad8\u5206\u8fa8\u7387\u66dd\u5149\u826f\u597d\u56fe\u50cf\u7684\u5404\u79cd\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u591a\u5206\u8fa8\u7387\u5e76\u884c\u53d8\u6362\u5668\u5145\u5f53\u6761\u4ef6\u7f16\u7801\u5668\uff0c\u63d0\u53d6 Retinex \u5f15\u8d77\u7684\u5206\u8fa8\u7387\u548c\u7167\u660e\u4e0d\u53d8\u56fe\u4f5c\u4e3a\u524d\u4e00\u4e2a\u56fe\u3002\u53ef\u9006\u7f51\u7edc\u5c06\u901a\u5e38\u66b4\u9732\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u5206\u5e03\u6620\u5c04\u5230\u6f5c\u5728\u5206\u5e03\u3002\u540e\u5411\u63a8\u7406\u76f8\u5f53\u4e8e\u4e3a\u6b63\u5e38\u8bad\u7ec3\u8def\u7ebf\u5f15\u5165\u989d\u5916\u7684\u7ea6\u675f\u635f\u5931\uff0c\u4ece\u800c\u80fd\u591f\u5b8c\u7f8e\u5730\u63cf\u7ed8\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u81ea\u7136\u66dd\u5149\u7684\u6d41\u5f62\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u62df\u771f\u5b9e\u4f4e\u5149\u4f4e\u5206\u8fa8\u7387\u9000\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u540d\u4e3a DFSR-LLE\uff0c\u5305\u542b 7100 \u4e2a\u4f4e\u5206\u8fa8\u7387\u6697\u5149/\u9ad8\u5206\u8fa8\u7387\u6b63\u5e38\u9510\u5229\u5bf9\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6240\u63d0\u51fa\u7684\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002|[2402.18871v1](http://arxiv.org/pdf/2402.18871v1)|null|\n", "2402.18817": "|**2024-02-29**|**Gradient Alignment for Cross-Domain Face Anti-Spoofing**|\u8de8\u57df\u4eba\u8138\u53cd\u6b3a\u9a97\u7684\u68af\u5ea6\u5bf9\u9f50|Binh M. Le, Simon S. Woo|Recent advancements in domain generalization (DG) for face anti-spoofing (FAS) have garnered considerable attention. Traditional methods have focused on designing learning objectives and additional modules to isolate domain-specific features while retaining domain-invariant characteristics in their representations. However, such approaches often lack guarantees of consistent maintenance of domain-invariant features or the complete removal of domain-specific features. Furthermore, most prior works of DG for FAS do not ensure convergence to a local flat minimum, which has been shown to be advantageous for DG. In this paper, we introduce GAC-FAS, a novel learning objective that encourages the model to converge towards an optimal flat minimum without necessitating additional learning modules. Unlike conventional sharpness-aware minimizers, GAC-FAS identifies ascending points for each domain and regulates the generalization gradient updates at these points to align coherently with empirical risk minimization (ERM) gradient updates. This unique approach specifically guides the model to be robust against domain shifts. We demonstrate the efficacy of GAC-FAS through rigorous testing on challenging cross-domain FAS datasets, where it establishes state-of-the-art performance. The code is available at https://github.com/leminhbinh0209/CVPR24-FAS.|\u4eba\u8138\u53cd\u6b3a\u9a97 (FAS) \u9886\u57df\u6cdb\u5316 (DG) \u7684\u6700\u65b0\u8fdb\u5c55\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u4f20\u7edf\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u8bbe\u8ba1\u5b66\u4e60\u76ee\u6807\u548c\u9644\u52a0\u6a21\u5757\u6765\u9694\u79bb\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u7279\u5f81\uff0c\u540c\u65f6\u5728\u5176\u8868\u793a\u4e2d\u4fdd\u7559\u9886\u57df\u4e0d\u53d8\u7684\u7279\u5f81\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u5bf9\u57df\u4e0d\u53d8\u7279\u5f81\u7684\u4e00\u81f4\u7ef4\u62a4\u6216\u57df\u7279\u5b9a\u7279\u5f81\u7684\u5b8c\u5168\u5220\u9664\u7684\u4fdd\u8bc1\u3002\u6b64\u5916\uff0cFAS \u7684 DG \u7684\u5927\u591a\u6570\u5148\u524d\u5de5\u4f5c\u5e76\u4e0d\u80fd\u786e\u4fdd\u6536\u655b\u5230\u5c40\u90e8\u5e73\u5766\u6700\u5c0f\u503c\uff0c\u8fd9\u5df2\u88ab\u8bc1\u660e\u5bf9 DG \u662f\u6709\u5229\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 GAC-FAS\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5b66\u4e60\u76ee\u6807\uff0c\u5b83\u9f13\u52b1\u6a21\u578b\u6536\u655b\u5230\u6700\u4f73\u5e73\u5766\u6700\u5c0f\u503c\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u5b66\u4e60\u6a21\u5757\u3002\u4e0e\u4f20\u7edf\u7684\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u5668\u4e0d\u540c\uff0cGAC-FAS \u8bc6\u522b\u6bcf\u4e2a\u57df\u7684\u4e0a\u5347\u70b9\uff0c\u5e76\u8c03\u8282\u8fd9\u4e9b\u70b9\u5904\u7684\u6cdb\u5316\u68af\u5ea6\u66f4\u65b0\uff0c\u4ee5\u4e0e\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316 (ERM) \u68af\u5ea6\u66f4\u65b0\u4fdd\u6301\u4e00\u81f4\u3002\u8fd9\u79cd\u72ec\u7279\u7684\u65b9\u6cd5\u4e13\u95e8\u6307\u5bfc\u6a21\u578b\u5bf9\u57df\u8f6c\u79fb\u5177\u6709\u9c81\u68d2\u6027\u3002\u6211\u4eec\u901a\u8fc7\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684\u8de8\u57df FAS \u6570\u636e\u96c6\u8fdb\u884c\u4e25\u683c\u6d4b\u8bd5\u6765\u8bc1\u660e GAC-FAS \u7684\u529f\u6548\uff0c\u5e76\u5728\u5176\u4e2d\u5efa\u7acb\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u8be5\u4ee3\u7801\u53ef\u4ece https://github.com/leminhbinh0209/CVPR24-FAS \u83b7\u53d6\u3002|[2402.18817v1](http://arxiv.org/pdf/2402.18817v1)|null|\n", "2402.18811": "|**2024-02-29**|**BFRFormer: Transformer-based generator for Real-World Blind Face Restoration**|BFRFormer\uff1a\u57fa\u4e8e Transformer \u7684\u73b0\u5b9e\u4e16\u754c\u76f2\u8138\u6062\u590d\u751f\u6210\u5668|Guojing Ge, Qi Song, Guibo Zhu, Yuting Zhang, Jinglu Chen, Miao Xin, Ming Tang, Jinqiao Wang|Blind face restoration is a challenging task due to the unknown and complex degradation. Although face prior-based methods and reference-based methods have recently demonstrated high-quality results, the restored images tend to contain over-smoothed results and lose identity-preserved details when the degradation is severe. It is observed that this is attributed to short-range dependencies, the intrinsic limitation of convolutional neural networks. To model long-range dependencies, we propose a Transformer-based blind face restoration method, named BFRFormer, to reconstruct images with more identity-preserved details in an end-to-end manner. In BFRFormer, to remove blocking artifacts, the wavelet discriminator and aggregated attention module are developed, and spectral normalization and balanced consistency regulation are adaptively applied to address the training instability and over-fitting problem, respectively. Extensive experiments show that our method outperforms state-of-the-art methods on a synthetic dataset and four real-world datasets. The source code, Casia-Test dataset, and pre-trained models are released at https://github.com/s8Znk/BFRFormer.|\u7531\u4e8e\u672a\u77e5\u4e14\u590d\u6742\u7684\u9000\u5316\uff0c\u76f2\u4eba\u8138\u6062\u590d\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u5c3d\u7ba1\u57fa\u4e8e\u4eba\u8138\u5148\u9a8c\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u53c2\u8003\u7684\u65b9\u6cd5\u6700\u8fd1\u5df2\u7ecf\u8bc1\u660e\u4e86\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u4f46\u6062\u590d\u7684\u56fe\u50cf\u5f80\u5f80\u5305\u542b\u8fc7\u5ea6\u5e73\u6ed1\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5f53\u9000\u5316\u4e25\u91cd\u65f6\u4f1a\u4e22\u5931\u8eab\u4efd\u4fdd\u7559\u7684\u7ec6\u8282\u3002\u636e\u89c2\u5bdf\uff0c\u8fd9\u5f52\u56e0\u4e8e\u77ed\u7a0b\u4f9d\u8d56\u6027\uff0c\u5373\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u5728\u9650\u5236\u3002\u4e3a\u4e86\u5bf9\u8fdc\u7a0b\u4f9d\u8d56\u6027\u8fdb\u884c\u5efa\u6a21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Transformer \u7684\u76f2\u8138\u6062\u590d\u65b9\u6cd5\uff0c\u540d\u4e3a BFRFormer\uff0c\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u91cd\u5efa\u5177\u6709\u66f4\u591a\u8eab\u4efd\u4fdd\u7559\u7ec6\u8282\u7684\u56fe\u50cf\u3002\u5728 BFRFormer \u4e2d\uff0c\u4e3a\u4e86\u6d88\u9664\u5757\u6548\u5e94\uff0c\u5f00\u53d1\u4e86\u5c0f\u6ce2\u9274\u522b\u5668\u548c\u805a\u5408\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u5e94\u7528\u8c31\u5f52\u4e00\u5316\u548c\u5e73\u8861\u4e00\u81f4\u6027\u8c03\u8282\u6765\u5206\u522b\u89e3\u51b3\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u8fc7\u62df\u5408\u95ee\u9898\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6e90\u4ee3\u7801\u3001Casia-Test \u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53d1\u5e03\u4e8e https://github.com/s8Znk/BFRFormer\u3002|[2402.18811v1](http://arxiv.org/pdf/2402.18811v1)|null|\n"}, "3D/CG": {"2402.19477": "|**2024-02-29**|**Learning a Generalized Physical Face Model From Data**|\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5e7f\u4e49\u7684\u7269\u7406\u4eba\u8138\u6a21\u578b|Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley|Physically-based simulation is a powerful approach for 3D facial animation as the resulting deformations are governed by physical constraints, allowing to easily resolve self-collisions, respond to external forces and perform realistic anatomy edits. Today's methods are data-driven, where the actuations for finite elements are inferred from captured skin geometry. Unfortunately, these approaches have not been widely adopted due to the complexity of initializing the material space and learning the deformation model for each character separately, which often requires a skilled artist followed by lengthy network training. In this work, we aim to make physics-based facial animation more accessible by proposing a generalized physical face model that we learn from a large 3D face dataset in a simulation-free manner. Once trained, our model can be quickly fit to any unseen identity and produce a ready-to-animate physical face model automatically. Fitting is as easy as providing a single 3D face scan, or even a single face image. After fitting, we offer intuitive animation controls, as well as the ability to retarget animations across characters. All the while, the resulting animations allow for physical effects like collision avoidance, gravity, paralysis, bone reshaping and more.|\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\u662f 3D \u9762\u90e8\u52a8\u753b\u7684\u4e00\u79cd\u5f3a\u5927\u65b9\u6cd5\uff0c\u56e0\u4e3a\u4ea7\u751f\u7684\u53d8\u5f62\u53d7\u7269\u7406\u7ea6\u675f\u63a7\u5236\uff0c\u53ef\u4ee5\u8f7b\u677e\u89e3\u51b3\u81ea\u78b0\u649e\u3001\u54cd\u5e94\u5916\u529b\u5e76\u6267\u884c\u903c\u771f\u7684\u89e3\u5256\u7f16\u8f91\u3002\u5982\u4eca\u7684\u65b9\u6cd5\u662f\u6570\u636e\u9a71\u52a8\u7684\uff0c\u5176\u4e2d\u6709\u9650\u5143\u7684\u9a71\u52a8\u662f\u4ece\u6355\u83b7\u7684\u76ae\u80a4\u51e0\u4f55\u5f62\u72b6\u63a8\u65ad\u51fa\u6765\u7684\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u7531\u4e8e\u521d\u59cb\u5316\u6750\u8d28\u7a7a\u95f4\u548c\u5355\u72ec\u5b66\u4e60\u6bcf\u4e2a\u89d2\u8272\u7684\u53d8\u5f62\u6a21\u578b\u7684\u590d\u6742\u6027\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5c1a\u672a\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u8fd9\u901a\u5e38\u9700\u8981\u719f\u7ec3\u7684\u827a\u672f\u5bb6\u8fdb\u884c\u957f\u65f6\u95f4\u7684\u7f51\u7edc\u8bad\u7ec3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u7269\u7406\u9762\u90e8\u6a21\u578b\uff0c\u4f7f\u57fa\u4e8e\u7269\u7406\u7684\u9762\u90e8\u52a8\u753b\u66f4\u5bb9\u6613\u5b9e\u73b0\uff0c\u8be5\u6a21\u578b\u662f\u6211\u4eec\u4ee5\u65e0\u6a21\u62df\u7684\u65b9\u5f0f\u4ece\u5927\u578b 3D \u9762\u90e8\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u7684\u3002\u7ecf\u8fc7\u8bad\u7ec3\u540e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u9002\u5e94\u4efb\u4f55\u770b\u4e0d\u89c1\u7684\u8eab\u4efd\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u51c6\u5907\u5236\u4f5c\u52a8\u753b\u7684\u7269\u7406\u9762\u90e8\u6a21\u578b\u3002\u9a8c\u914d\u5c31\u50cf\u63d0\u4f9b\u5355\u4e2a 3D \u9762\u90e8\u626b\u63cf\u751a\u81f3\u5355\u4e2a\u9762\u90e8\u56fe\u50cf\u4e00\u6837\u7b80\u5355\u3002\u9002\u914d\u540e\uff0c\u6211\u4eec\u63d0\u4f9b\u76f4\u89c2\u7684\u52a8\u753b\u63a7\u5236\uff0c\u4ee5\u53ca\u8de8\u89d2\u8272\u91cd\u65b0\u5b9a\u4f4d\u52a8\u753b\u7684\u80fd\u529b\u3002\u4e00\u76f4\u4ee5\u6765\uff0c\u751f\u6210\u7684\u52a8\u753b\u90fd\u5141\u8bb8\u7269\u7406\u6548\u679c\uff0c\u4f8b\u5982\u907f\u514d\u78b0\u649e\u3001\u91cd\u529b\u3001\u9ebb\u75f9\u3001\u9aa8\u9abc\u91cd\u5851\u7b49\u3002|[2402.19477v1](http://arxiv.org/pdf/2402.19477v1)|null|\n", "2402.18920": "|**2024-02-29**|**Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation**|\u5149\u8c31\u4e0e\u7a7a\u95f4\u7684\u7ed3\u5408\uff1a\u534f\u8c03 3D \u5f62\u72b6\u5339\u914d\u548c\u63d2\u503c|Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard|Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both pose-dominant and shape-dominant deformations. Using different challenging datasets, we demonstrate that our method outperforms previous state-of-the-art methods for both shape matching and interpolation, even compared to supervised approaches.|\u5c3d\u7ba1 3D \u5f62\u72b6\u5339\u914d\u548c\u63d2\u503c\u9ad8\u5ea6\u76f8\u5173\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u88ab\u5355\u72ec\u7814\u7a76\u5e76\u6309\u987a\u5e8f\u5e94\u7528\u4ee5\u5173\u8054\u4e0d\u540c\u7684 3D \u5f62\u72b6\uff0c\u4ece\u800c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u9884\u6d4b 3D \u5f62\u72b6\u4e4b\u95f4\u7684\u9010\u70b9\u5bf9\u5e94\u548c\u5f62\u72b6\u63d2\u503c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u6df1\u5c42\u529f\u80fd\u56fe\u6846\u67b6\u4e0e\u7ecf\u5178\u8868\u9762\u53d8\u5f62\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4ee5\u6620\u5c04\u5149\u8c31\u57df\u548c\u7a7a\u95f4\u57df\u4e2d\u7684\u5f62\u72b6\u3002\u4e00\u65b9\u9762\uff0c\u901a\u8fc7\u5408\u5e76\u7a7a\u95f4\u56fe\uff0c\u4e0e\u4e4b\u524d\u7684\u5f62\u72b6\u5339\u914d\u529f\u80fd\u56fe\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u51c6\u786e\u3001\u66f4\u5e73\u6ed1\u7684\u9010\u70b9\u5bf9\u5e94\u5173\u7cfb\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u901a\u8fc7\u5f15\u5165\u5149\u8c31\u56fe\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6446\u8131\u4e86\u5e38\u7528\u4f46\u8ba1\u7b97\u6210\u672c\u6602\u8d35\u7684\u6d4b\u5730\u8ddd\u79bb\u7ea6\u675f\uff0c\u8fd9\u4e9b\u7ea6\u675f\u4ec5\u5bf9\u8fd1\u7b49\u8ddd\u5f62\u72b6\u53d8\u5f62\u6709\u6548\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6848\u6765\u6355\u83b7\u59ff\u52bf\u4e3b\u5bfc\u548c\u5f62\u72b6\u4e3b\u5bfc\u7684\u53d8\u5f62\u3002\u4f7f\u7528\u4e0d\u540c\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5f62\u72b6\u5339\u914d\u548c\u63d2\u503c\u65b9\u9762\u90fd\u4f18\u4e8e\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u751a\u81f3\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u6bd4\u4e5f\u662f\u5982\u6b64\u3002|[2402.18920v1](http://arxiv.org/pdf/2402.18920v1)|null|\n", "2402.18844": "|**2024-02-29**|**Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey**|\u7528\u4e8e 3D \u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u548c\u7f51\u683c\u6062\u590d\u7684\u6df1\u5ea6\u5b66\u4e60\uff1a\u4e00\u9879\u8c03\u67e5|Yang Liu, Changzhen Qiu, Zhiyong Zhang|3D human pose estimation and mesh recovery have attracted widespread research interest in many areas, such as computer vision, autonomous driving, and robotics. Deep learning on 3D human pose estimation and mesh recovery has recently thrived, with numerous methods proposed to address different problems in this area. In this paper, to stimulate future research, we present a comprehensive review of recent progress over the past five years in deep learning methods for this area by delving into over 200 references. To the best of our knowledge, this survey is arguably the first to comprehensively cover deep learning methods for 3D human pose estimation, including both single-person and multi-person approaches, as well as human mesh recovery, encompassing methods based on explicit models and implicit representations. We also present comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions. A regularly updated project page can be found at https://github.com/liuyangme/SOTA-3DHPE-HMR.|3D \u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u7f51\u683c\u6062\u590d\u5438\u5f15\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u7b49\u8bb8\u591a\u9886\u57df\u7684\u5e7f\u6cdb\u7814\u7a76\u5174\u8da3\u3002 3D \u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u548c\u7f51\u683c\u6062\u590d\u7684\u6df1\u5ea6\u5b66\u4e60\u6700\u8fd1\u84ec\u52c3\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86\u8bb8\u591a\u65b9\u6cd5\u6765\u89e3\u51b3\u8be5\u9886\u57df\u7684\u4e0d\u540c\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u4e3a\u4e86\u6fc0\u53d1\u672a\u6765\u7684\u7814\u7a76\uff0c\u6211\u4eec\u901a\u8fc7\u6df1\u5165\u7814\u7a76 200 \u591a\u7bc7\u53c2\u8003\u6587\u732e\uff0c\u5168\u9762\u56de\u987e\u4e86\u8fc7\u53bb\u4e94\u5e74\u8be5\u9886\u57df\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u9879\u8c03\u67e5\u53ef\u4ee5\u8bf4\u662f\u7b2c\u4e00\u4e2a\u5168\u9762\u6db5\u76d6 3D \u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u8c03\u67e5\uff0c\u5305\u62ec\u5355\u4eba\u548c\u591a\u4eba\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4eba\u4f53\u7f51\u683c\u6062\u590d\uff0c\u5305\u62ec\u57fa\u4e8e\u663e\u5f0f\u6a21\u578b\u548c\u9690\u5f0f\u8868\u793a\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u51e0\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u7684\u6bd4\u8f83\u7ed3\u679c\uff0c\u4ee5\u53ca\u5bcc\u6709\u6d1e\u5bdf\u529b\u7684\u89c2\u5bdf\u548c\u542f\u53d1\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002\u5b9a\u671f\u66f4\u65b0\u7684\u9879\u76ee\u9875\u9762\u53ef\u4ee5\u5728 https://github.com/liuyangme/SOTA-3DHPE-HMR \u627e\u5230\u3002|[2402.18844v1](http://arxiv.org/pdf/2402.18844v1)|null|\n", "2402.18771": "|**2024-02-29**|**NARUTO: Neural Active Reconstruction from Uncertain Target Observations**|NARUTO\uff1a\u4ece\u4e0d\u786e\u5b9a\u76ee\u6807\u89c2\u5bdf\u4e2d\u8fdb\u884c\u795e\u7ecf\u4e3b\u52a8\u91cd\u5efa|Ziyue Feng, Huangying Zhan, Zheng Chen, Qingan Yan, Xiangyu Xu, Changjiang Cai, Bing Li, Qilun Zhu, Yi Xu|We present NARUTO, a neural active reconstruction system that combines a hybrid neural representation with uncertainty learning, enabling high-fidelity surface reconstruction. Our approach leverages a multi-resolution hash-grid as the mapping backbone, chosen for its exceptional convergence speed and capacity to capture high-frequency local features.The centerpiece of our work is the incorporation of an uncertainty learning module that dynamically quantifies reconstruction uncertainty while actively reconstructing the environment. By harnessing learned uncertainty, we propose a novel uncertainty aggregation strategy for goal searching and efficient path planning. Our system autonomously explores by targeting uncertain observations and reconstructs environments with remarkable completeness and fidelity. We also demonstrate the utility of this uncertainty-aware approach by enhancing SOTA neural SLAM systems through an active ray sampling strategy. Extensive evaluations of NARUTO in various environments, using an indoor scene simulator, confirm its superior performance and state-of-the-art status in active reconstruction, as evidenced by its impressive results on benchmark datasets like Replica and MP3D.|\u6211\u4eec\u63a8\u51fa\u4e86 NARUTO\uff0c\u4e00\u79cd\u795e\u7ecf\u4e3b\u52a8\u91cd\u5efa\u7cfb\u7edf\uff0c\u5b83\u5c06\u6df7\u5408\u795e\u7ecf\u8868\u793a\u4e0e\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u8868\u9762\u91cd\u5efa\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u4f5c\u4e3a\u6620\u5c04\u4e3b\u5e72\uff0c\u9009\u62e9\u5b83\u662f\u56e0\u4e3a\u5176\u5353\u8d8a\u7684\u6536\u655b\u901f\u5ea6\u548c\u6355\u83b7\u9ad8\u9891\u5c40\u90e8\u7279\u5f81\u7684\u80fd\u529b\u3002\u6211\u4eec\u5de5\u4f5c\u7684\u6838\u5fc3\u662f\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u52a8\u6001\u91cf\u5316\u91cd\u5efa\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u79ef\u6781\u5f00\u5c55\u73af\u5883\u6539\u9020\u3002\u901a\u8fc7\u5229\u7528\u5b66\u4e60\u5230\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0d\u786e\u5b9a\u6027\u805a\u5408\u7b56\u7565\uff0c\u7528\u4e8e\u76ee\u6807\u641c\u7d22\u548c\u6709\u6548\u7684\u8def\u5f84\u89c4\u5212\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u901a\u8fc7\u9488\u5bf9\u4e0d\u786e\u5b9a\u7684\u89c2\u6d4b\u8fdb\u884c\u81ea\u4e3b\u63a2\u7d22\uff0c\u5e76\u4ee5\u5353\u8d8a\u7684\u5b8c\u6574\u6027\u548c\u4fdd\u771f\u5ea6\u91cd\u5efa\u73af\u5883\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u4e3b\u52a8\u5c04\u7ebf\u91c7\u6837\u7b56\u7565\u589e\u5f3a SOTA \u795e\u7ecf SLAM \u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002\u4f7f\u7528\u5ba4\u5185\u573a\u666f\u6a21\u62df\u5668\u5bf9 NARUTO \u5728\u5404\u79cd\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u5176\u5728\u4e3b\u52a8\u91cd\u5efa\u65b9\u9762\u7684\u5353\u8d8a\u6027\u80fd\u548c\u6700\u5148\u8fdb\u7684\u72b6\u6001\uff0c\u5176\u5728 Replica \u548c MP3D \u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u5c31\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002|[2402.18771v1](http://arxiv.org/pdf/2402.18771v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.19020": "|**2024-02-29**|**Unsupervised Learning of High-resolution Light Field Imaging via Beam Splitter-based Hybrid Lenses**|\u901a\u8fc7\u57fa\u4e8e\u5206\u675f\u5668\u7684\u6df7\u5408\u955c\u5934\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u5149\u573a\u6210\u50cf\u7684\u65e0\u76d1\u7763\u5b66\u4e60|Jianxin Lei, Chengcai Xu, Langqing Shi, Junhui Hou, Ping Zhou|In this paper, we design a beam splitter-based hybrid light field imaging prototype to record 4D light field image and high-resolution 2D image simultaneously, and make a hybrid light field dataset. The 2D image could be considered as the high-resolution ground truth corresponding to the low-resolution central sub-aperture image of 4D light field image. Subsequently, we propose an unsupervised learning-based super-resolution framework with the hybrid light field dataset, which adaptively settles the light field spatial super-resolution problem with a complex degradation model. Specifically, we design two loss functions based on pre-trained models that enable the super-resolution network to learn the detailed features and light field parallax structure with only one ground truth. Extensive experiments demonstrate the same superiority of our approach with supervised learning-based state-of-the-art ones. To our knowledge, it is the first end-to-end unsupervised learning-based spatial super-resolution approach in light field imaging research, whose input is available from our beam splitter-based hybrid light field system. The hardware and software together may help promote the application of light field super-resolution to a great extent.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u675f\u5668\u7684\u6df7\u5408\u5149\u573a\u6210\u50cf\u539f\u578b\uff0c\u4ee5\u540c\u65f6\u8bb0\u5f554D\u5149\u573a\u56fe\u50cf\u548c\u9ad8\u5206\u8fa8\u73872D\u56fe\u50cf\uff0c\u5e76\u5236\u4f5c\u6df7\u5408\u5149\u573a\u6570\u636e\u96c6\u3002 2D\u56fe\u50cf\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f4D\u5149\u573a\u56fe\u50cf\u7684\u4f4e\u5206\u8fa8\u7387\u4e2d\u5fc3\u5b50\u5b54\u5f84\u56fe\u50cf\u5bf9\u5e94\u7684\u9ad8\u5206\u8fa8\u7387\u5730\u9762\u5b9e\u51b5\u3002\u968f\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u5149\u573a\u6570\u636e\u96c6\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u590d\u6742\u7684\u9000\u5316\u6a21\u578b\u81ea\u9002\u5e94\u5730\u89e3\u51b3\u4e86\u5149\u573a\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u8bbe\u8ba1\u4e86\u4e24\u79cd\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\u80fd\u591f\u4ec5\u7528\u4e00\u4e2a\u57fa\u672c\u4e8b\u5b9e\u6765\u5b66\u4e60\u8be6\u7ec6\u7279\u5f81\u548c\u5149\u573a\u89c6\u5dee\u7ed3\u6784\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u76d1\u7763\u5b66\u4e60\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u76f8\u540c\u7684\u4f18\u8d8a\u6027\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u5149\u573a\u6210\u50cf\u7814\u7a76\u4e2d\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u7aef\u5230\u7aef\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u5176\u8f93\u5165\u53ef\u4ece\u6211\u4eec\u57fa\u4e8e\u5206\u675f\u5668\u7684\u6df7\u5408\u5149\u573a\u7cfb\u7edf\u83b7\u5f97\u3002\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u7ed3\u5408\u5c06\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u6709\u52a9\u4e8e\u63a8\u52a8\u5149\u573a\u8d85\u5206\u8fa8\u7387\u7684\u5e94\u7528\u3002|[2402.19020v1](http://arxiv.org/pdf/2402.19020v1)|null|\n"}, "\u5176\u4ed6": {"2402.19473": "|**2024-02-29**|**Retrieval-Augmented Generation for AI-Generated Content: A Survey**|\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff1a\u4e00\u9879\u8c03\u67e5|Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui|The development of Artificial Intelligence Generated Content (AIGC) has been facilitated by advancements in model algorithms, scalable foundation model architectures, and the availability of ample high-quality datasets. While AIGC has achieved remarkable performance, it still faces challenges, such as the difficulty of maintaining up-to-date and long-tail knowledge, the risk of data leakage, and the high costs associated with training and inference. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances AIGC results by retrieving relevant objects from available data stores, leading to greater accuracy and robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator. We distill the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Project: https://github.com/hymie122/RAG-Survey|\u6a21\u578b\u7b97\u6cd5\u7684\u8fdb\u6b65\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u6a21\u578b\u67b6\u6784\u4ee5\u53ca\u5145\u8db3\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u53ef\u7528\u6027\u4fc3\u8fdb\u4e86\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u7684\u53d1\u5c55\u3002\u5c3d\u7ba1 AIGC \u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u8868\u73b0\uff0c\u4f46\u5b83\u4ecd\u7136\u9762\u4e34\u7740\u6311\u6218\uff0c\u4f8b\u5982\u96be\u4ee5\u7ef4\u62a4\u6700\u65b0\u7684\u957f\u5c3e\u77e5\u8bc6\u3001\u6570\u636e\u6cc4\u9732\u7684\u98ce\u9669\u4ee5\u53ca\u4e0e\u8bad\u7ec3\u548c\u63a8\u7406\u76f8\u5173\u7684\u9ad8\u6210\u672c\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6700\u8fd1\u6210\u4e3a\u89e3\u51b3\u6b64\u7c7b\u6311\u6218\u7684\u8303\u4f8b\u3002\u7279\u522b\u662f\uff0cRAG \u5f15\u5165\u4e86\u4fe1\u606f\u68c0\u7d22\u8fc7\u7a0b\uff0c\u8be5\u8fc7\u7a0b\u901a\u8fc7\u4ece\u53ef\u7528\u6570\u636e\u5b58\u50a8\u4e2d\u68c0\u7d22\u76f8\u5173\u5bf9\u8c61\u6765\u589e\u5f3a AIGC \u7ed3\u679c\uff0c\u4ece\u800c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5168\u9762\u56de\u987e\u4e86\u5c06 RAG \u6280\u672f\u96c6\u6210\u5230 AIGC \u573a\u666f\u4e2d\u7684\u73b0\u6709\u52aa\u529b\u3002\u6211\u4eec\u9996\u5148\u6839\u636e\u68c0\u7d22\u5668\u5982\u4f55\u589e\u5f3a\u751f\u6210\u5668\u5bf9 RAG \u57fa\u7840\u8fdb\u884c\u5206\u7c7b\u3002\u6211\u4eec\u63d0\u53d6\u4e86\u5404\u79cd\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u7684\u589e\u5f3a\u65b9\u6cd5\u7684\u57fa\u672c\u62bd\u8c61\u3002\u8fd9\u79cd\u7edf\u4e00\u7684\u89c6\u89d2\u6db5\u76d6\u4e86\u6240\u6709 RAG \u573a\u666f\uff0c\u9610\u660e\u4e86\u6709\u52a9\u4e8e\u672a\u6765\u6f5c\u5728\u8fdb\u6b65\u7684\u8fdb\u6b65\u548c\u5173\u952e\u6280\u672f\u3002\u6211\u4eec\u8fd8\u603b\u7ed3\u4e86 RAG \u7684\u5176\u4ed6\u589e\u5f3a\u65b9\u6cd5\uff0c\u4fc3\u8fdb RAG \u7cfb\u7edf\u7684\u6709\u6548\u5de5\u7a0b\u548c\u5b9e\u65bd\u3002\u7136\u540e\u4ece\u53e6\u4e00\u4e2a\u89d2\u5ea6\uff0c\u6211\u4eec\u8c03\u67e5\u4e86 RAG \u5728\u4e0d\u540c\u6a21\u5f0f\u548c\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 RAG \u7684\u57fa\u51c6\uff0c\u8ba8\u8bba\u4e86\u5f53\u524d RAG \u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002\u9879\u76ee\uff1ahttps://github.com/hymie122/RAG-Survey|[2402.19473v1](http://arxiv.org/pdf/2402.19473v1)|null|\n", "2402.19472": "|**2024-02-29**|**Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress**|\u7ec8\u8eab\u57fa\u51c6\uff1a\u5feb\u901f\u8fdb\u6b65\u65f6\u4ee3\u7684\u9ad8\u6548\u6a21\u578b\u8bc4\u4f30|Ameya Prabhu, Vishaal Udandarao, Philip Torr, Matthias Bethge, Adel Bibi, Samuel Albanie|Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: Sort \\& Search (S&S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across 31,000 models demonstrate that S&S achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours (1000x reduction) on a single A100 GPU, with low approximation error. As such, lifelong benchmarks offer a robust, practical solution to the \"benchmark exhaustion\" problem.|\u6807\u51c6\u5316\u57fa\u51c6\u63a8\u52a8\u673a\u5668\u5b66\u4e60\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u968f\u7740\u91cd\u590d\u6d4b\u8bd5\uff0c\u968f\u7740\u7b97\u6cd5\u8fc7\u5ea6\u5229\u7528\u57fa\u51c6\u7279\u6027\uff0c\u8fc7\u5ea6\u62df\u5408\u7684\u98ce\u9669\u4e5f\u4f1a\u589e\u52a0\u3002\u5728\u6211\u4eec\u7684\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5bfb\u6c42\u901a\u8fc7\u7f16\u5236\u4e0d\u65ad\u6269\u5927\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff08\u79f0\u4e3a\u201c\u7ec8\u8eab\u57fa\u51c6\u201d\uff09\u6765\u7f13\u89e3\u8fd9\u4e00\u6311\u6218\u3002\u4f5c\u4e3a\u6211\u4eec\u65b9\u6cd5\u7684\u793a\u4f8b\uff0c\u6211\u4eec\u521b\u5efa\u4e86 Lifelong-CIFAR10 \u548c Lifelong-ImageNet\uff0c\u5206\u522b\u5305\u542b\uff08\u76ee\u524d\uff091.69M \u548c 1.98M \u6d4b\u8bd5\u6837\u672c\u3002\u5728\u51cf\u5c11\u8fc7\u5ea6\u62df\u5408\u7684\u540c\u65f6\uff0c\u7ec8\u8eab\u57fa\u51c6\u5e26\u6765\u4e86\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5728\u4e0d\u65ad\u6269\u5927\u7684\u6837\u672c\u96c6\u4e2d\u8bc4\u4f30\u8d8a\u6765\u8d8a\u591a\u7684\u6a21\u578b\u7684\u6210\u672c\u5f88\u9ad8\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u8bc4\u4f30\u6846\u67b6\uff1a\u6392\u5e8f\\&\u641c\u7d22\uff08S&S\uff09\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u52a8\u6001\u7f16\u7a0b\u7b97\u6cd5\u6709\u9009\u62e9\u5730\u5bf9\u6d4b\u8bd5\u6837\u672c\u8fdb\u884c\u6392\u540d\u548c\u5b50\u9009\u62e9\u6765\u91cd\u7528\u5148\u524d\u8bc4\u4f30\u7684\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u7ec8\u8eab\u57fa\u51c6\u6d4b\u8bd5\u3002\u5bf9 31,000 \u4e2a\u6a21\u578b\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cS&S \u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8fd1\u4f3c\u7cbe\u5ea6\u6d4b\u91cf\uff0c\u5c06\u5355\u4e2a A100 GPU \u4e0a\u7684\u8ba1\u7b97\u6210\u672c\u4ece 180 \u4e2a GPU \u5929\u51cf\u5c11\u5230 5 \u4e2a GPU \u5c0f\u65f6\uff08\u51cf\u5c11\u4e86 1000 \u500d\uff09\uff0c\u5e76\u4e14\u8fd1\u4f3c\u8bef\u5dee\u8f83\u4f4e\u3002\u56e0\u6b64\uff0c\u7ec8\u8eab\u57fa\u51c6\u4e3a\u201c\u57fa\u51c6\u8017\u5c3d\u201d\u95ee\u9898\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002|[2402.19472v1](http://arxiv.org/pdf/2402.19472v1)|null|\n", "2402.19385": "|**2024-02-29**|**Towards Safe and Reliable Autonomous Driving: Dynamic Occupancy Set Prediction**|\u8fc8\u5411\u5b89\u5168\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\uff1a\u52a8\u6001\u5360\u7528\u96c6\u9884\u6d4b|Wenbo Shao, Jiahui Xu, Wenhao Yu, Jun Li, Hong Wang|In the rapidly evolving field of autonomous driving, accurate trajectory prediction is pivotal for vehicular safety. However, trajectory predictions often deviate from actual paths, particularly in complex and challenging environments, leading to significant errors. To address this issue, our study introduces a novel method for Dynamic Occupancy Set (DOS) prediction, enhancing trajectory prediction capabilities. This method effectively combines advanced trajectory prediction networks with a DOS prediction module, overcoming the shortcomings of existing models. It provides a comprehensive and adaptable framework for predicting the potential occupancy sets of traffic participants. The main contributions of this research include: 1) A novel DOS prediction model tailored for complex scenarios, augmenting traditional trajectory prediction; 2) The development of unique DOS representations and evaluation metrics; 3) Extensive validation through experiments, demonstrating enhanced performance and adaptability. This research contributes to the advancement of safer and more efficient intelligent vehicle and transportation systems.|\u5728\u5feb\u901f\u53d1\u5c55\u7684\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u51c6\u786e\u7684\u8f68\u8ff9\u9884\u6d4b\u5bf9\u4e8e\u8f66\u8f86\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8f68\u8ff9\u9884\u6d4b\u901a\u5e38\u4f1a\u504f\u79bb\u5b9e\u9645\u8def\u5f84\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\uff0c\u4ece\u800c\u5bfc\u81f4\u91cd\u5927\u9519\u8bef\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u52a8\u6001\u5360\u7528\u96c6\uff08DOS\uff09\u9884\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u8f68\u8ff9\u9884\u6d4b\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u5c06\u5148\u8fdb\u7684\u8f68\u8ff9\u9884\u6d4b\u7f51\u7edc\u4e0eDOS\u9884\u6d4b\u6a21\u5757\u7ed3\u5408\u8d77\u6765\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6a21\u578b\u7684\u7f3a\u70b9\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u6f5c\u5728\u5360\u7528\u96c6\u3002\u8fd9\u9879\u7814\u7a76\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a1\uff09\u9488\u5bf9\u590d\u6742\u573a\u666f\u91cf\u8eab\u5b9a\u5236\u7684\u65b0\u578b DOS \u9884\u6d4b\u6a21\u578b\uff0c\u589e\u5f3a\u4e86\u4f20\u7edf\u7684\u8f68\u8ff9\u9884\u6d4b\uff1b 2\uff09\u5f00\u53d1\u72ec\u7279\u7684DOS\u8868\u793a\u548c\u8bc4\u4f30\u6307\u6807\uff1b 3\uff09\u901a\u8fc7\u5b9e\u9a8c\u8fdb\u884c\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u5c55\u793a\u51fa\u589e\u5f3a\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002\u8fd9\u9879\u7814\u7a76\u6709\u52a9\u4e8e\u63a8\u8fdb\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u667a\u80fd\u8f66\u8f86\u548c\u4ea4\u901a\u7cfb\u7edf\u3002|[2402.19385v1](http://arxiv.org/pdf/2402.19385v1)|null|\n", "2402.19296": "|**2024-02-29**|**An AI based Digital Score of Tumour-Immune Microenvironment Predicts Benefit to Maintenance Immunotherapy in Advanced Oesophagogastric Adenocarcinoma**|\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u80bf\u7624\u514d\u75ab\u5fae\u73af\u5883\u6570\u5b57\u8bc4\u5206\u53ef\u9884\u6d4b\u665a\u671f\u98df\u7ba1\u80c3\u817a\u764c\u7ef4\u6301\u514d\u75ab\u6cbb\u7597\u7684\u76ca\u5904|Quoc Dang Vu, Caroline Fong, Anderley Gordon, Tom Lund, Tatiany L Silveira, Daniel Rodrigues, Katharina von Loga, Shan E Ahmed Raza, David Cunningham, Nasir Rajpoot|Gastric and oesophageal (OG) cancers are the leading causes of cancer mortality worldwide. In OG cancers, recent studies have showed that PDL1 immune checkpoint inhibitors (ICI) in combination with chemotherapy improves patient survival. However, our understanding of the tumour immune microenvironment in OG cancers remains limited. In this study, we interrogate multiplex immunofluorescence (mIF) images taken from patients with advanced Oesophagogastric Adenocarcinoma (OGA) who received first-line fluoropyrimidine and platinum-based chemotherapy in the PLATFORM trial (NCT02678182) to predict the efficacy of the treatment and to explore the biological basis of patients responding to maintenance durvalumab (PDL1 inhibitor). Our proposed Artificial Intelligence (AI) based marker successfully identified responder from non-responder (p < 0.05) as well as those who could potentially benefit from ICI with statistical significance (p < 0.05) for both progression free and overall survival. Our findings suggest that T cells that express FOXP3 seem to heavily influence the patient treatment response and survival outcome. We also observed that higher levels of CD8+PD1+ cells are consistently linked to poor prognosis for both OS and PFS, regardless of ICI.|\u80c3\u764c\u548c\u98df\u7ba1\u764c (OG) \u662f\u5168\u7403\u764c\u75c7\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\u3002\u5728 OG \u764c\u75c7\u4e2d\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e PDL1 \u514d\u75ab\u68c0\u67e5\u70b9\u6291\u5236\u5242 (ICI) \u4e0e\u5316\u7597\u76f8\u7ed3\u5408\u53ef\u63d0\u9ad8\u60a3\u8005\u7684\u751f\u5b58\u7387\u3002\u7136\u800c\uff0c\u6211\u4eec\u5bf9 OG \u764c\u75c7\u4e2d\u80bf\u7624\u514d\u75ab\u5fae\u73af\u5883\u7684\u4e86\u89e3\u4ecd\u7136\u6709\u9650\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5728 PLATFORM \u8bd5\u9a8c (NCT02678182) \u4e2d\u63a5\u53d7\u4e00\u7ebf\u6c1f\u5627\u5576\u548c\u94c2\u7c7b\u5316\u7597\u7684\u665a\u671f\u98df\u7ba1\u80c3\u817a\u764c (OGA) \u60a3\u8005\u7684\u591a\u91cd\u514d\u75ab\u8367\u5149 (mIF) \u56fe\u50cf\uff0c\u4ee5\u9884\u6d4b\u6cbb\u7597\u6548\u679c\u5e76\u63a2\u7d22\u60a3\u8005\u5bf9\u7ef4\u6301 durvalumab\uff08PDL1 \u6291\u5236\u5242\uff09\u6709\u53cd\u5e94\u7684\u751f\u7269\u5b66\u57fa\u7840\u3002\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd (AI) \u7684\u6807\u8bb0\u6210\u529f\u5730\u4ece\u65e0\u53cd\u5e94\u8005\u4e2d\u8bc6\u522b\u51fa\u53cd\u5e94\u8005 (p < 0.05) \u4ee5\u53ca\u90a3\u4e9b\u53ef\u80fd\u4ece ICI \u4e2d\u53d7\u76ca\u7684\u4eba\uff0c\u5bf9\u4e8e\u65e0\u8fdb\u5c55\u751f\u5b58\u671f\u548c\u603b\u751f\u5b58\u671f\u5177\u6709\u7edf\u8ba1\u663e\u7740\u6027 (p < 0.05)\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8868\u8fbe FOXP3 \u7684 T \u7ec6\u80de\u4f3c\u4e4e\u4e25\u91cd\u5f71\u54cd\u60a3\u8005\u7684\u6cbb\u7597\u53cd\u5e94\u548c\u751f\u5b58\u7ed3\u679c\u3002\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\uff0c\u65e0\u8bba ICI \u5982\u4f55\uff0c\u8f83\u9ad8\u6c34\u5e73\u7684 CD8+PD1+ \u7ec6\u80de\u59cb\u7ec8\u4e0e OS \u548c PFS \u7684\u4e0d\u826f\u9884\u540e\u76f8\u5173\u3002|[2402.19296v1](http://arxiv.org/pdf/2402.19296v1)|null|\n", "2402.19279": "|**2024-02-29**|**SIFT-Aided Rectified 2D-DIC for Displacement and Strain Measurements in Asphalt Concrete Testing**|SIFT \u8f85\u52a9\u4fee\u6b63 2D-DIC \u7528\u4e8e\u6ca5\u9752\u6df7\u51dd\u571f\u6d4b\u8bd5\u4e2d\u7684\u4f4d\u79fb\u548c\u5e94\u53d8\u6d4b\u91cf|Zehui Zhu, Imad L. Al-Qadi|Two-dimensional digital image correlation (2D-DIC) is a widely used optical technique to measure displacement and strain during asphalt concrete (AC) testing. An accurate 2-D DIC measurement can only be achieved when the camera's principal axis is perpendicular to the planar specimen surface. However, this requirement may not be met during testing due to device constraints. This paper proposes a simple and reliable method to correct errors induced by non-perpendicularity. The method is based on image feature matching and rectification. No additional equipment is needed. A theoretical error analysis was conducted to quantify the effect of a non-perpendicular camera alignment on measurement accuracy. The proposed method was validated numerically using synthetic images and experimentally in an AC fracture test. It achieved relatively high accuracy, even under considerable camera rotation angle and large deformation. As a pre-processing technique, the proposed method showed promising performance in assisting the recently developed CrackPropNet for automated crack propagation measurement under a non-perpendicular camera alignment.|\u4e8c\u7ef4\u6570\u5b57\u56fe\u50cf\u76f8\u5173 (2D-DIC) \u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u5149\u5b66\u6280\u672f\uff0c\u7528\u4e8e\u6d4b\u91cf\u6ca5\u9752\u6df7\u51dd\u571f (AC) \u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u7684\u4f4d\u79fb\u548c\u5e94\u53d8\u3002\u53ea\u6709\u5f53\u76f8\u673a\u7684\u4e3b\u8f74\u5782\u76f4\u4e8e\u5e73\u9762\u6837\u54c1\u8868\u9762\u65f6\uff0c\u624d\u80fd\u5b9e\u73b0\u7cbe\u786e\u7684 2-D DIC \u6d4b\u91cf\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8bbe\u5907\u9650\u5236\uff0c\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e00\u8981\u6c42\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u7ea0\u6b63\u975e\u5782\u76f4\u5f15\u8d77\u7684\u8bef\u5dee\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u56fe\u50cf\u7279\u5f81\u5339\u914d\u548c\u6821\u6b63\u3002\u4e0d\u9700\u8981\u989d\u5916\u7684\u8bbe\u5907\u3002\u8fdb\u884c\u4e86\u7406\u8bba\u8bef\u5dee\u5206\u6790\uff0c\u4ee5\u91cf\u5316\u975e\u5782\u76f4\u76f8\u673a\u5bf9\u51c6\u5bf9\u6d4b\u91cf\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u5bf9\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6570\u503c\u9a8c\u8bc1\uff0c\u5e76\u5728\u4ea4\u6d41\u65ad\u88c2\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u5373\u4f7f\u5728\u76f8\u5f53\u5927\u7684\u76f8\u673a\u65cb\u8f6c\u89d2\u5ea6\u548c\u5927\u53d8\u5f62\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u4e5f\u80fd\u5b9e\u73b0\u76f8\u5bf9\u8f83\u9ad8\u7684\u7cbe\u5ea6\u3002\u4f5c\u4e3a\u4e00\u79cd\u9884\u5904\u7406\u6280\u672f\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u534f\u52a9\u6700\u8fd1\u5f00\u53d1\u7684 CrackPropNet \u5728\u975e\u5782\u76f4\u76f8\u673a\u5bf9\u51c6\u4e0b\u8fdb\u884c\u81ea\u52a8\u88c2\u7eb9\u6269\u5c55\u6d4b\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002|[2402.19279v1](http://arxiv.org/pdf/2402.19279v1)|null|\n", "2402.19270": "|**2024-02-29**|**Learning Intra-view and Cross-view Geometric Knowledge for Stereo Matching**|\u5b66\u4e60\u7acb\u4f53\u5339\u914d\u7684\u89c6\u56fe\u5185\u548c\u8de8\u89c6\u56fe\u51e0\u4f55\u77e5\u8bc6|Rui Gong, Weide Liu, Zaiwang Gu, Xulei Yang, Jun Cheng|Geometric knowledge has been shown to be beneficial for the stereo matching task. However, prior attempts to integrate geometric insights into stereo matching algorithms have largely focused on geometric knowledge from single images while crucial cross-view factors such as occlusion and matching uniqueness have been overlooked. To address this gap, we propose a novel Intra-view and Cross-view Geometric knowledge learning Network (ICGNet), specifically crafted to assimilate both intra-view and cross-view geometric knowledge. ICGNet harnesses the power of interest points to serve as a channel for intra-view geometric understanding. Simultaneously, it employs the correspondences among these points to capture cross-view geometric relationships. This dual incorporation empowers the proposed ICGNet to leverage both intra-view and cross-view geometric knowledge in its learning process, substantially improving its ability to estimate disparities. Our extensive experiments demonstrate the superiority of the ICGNet over contemporary leading models.|\u51e0\u4f55\u77e5\u8bc6\u5df2\u88ab\u8bc1\u660e\u5bf9\u4e8e\u7acb\u4f53\u5339\u914d\u4efb\u52a1\u662f\u6709\u76ca\u7684\u3002\u7136\u800c\uff0c\u5148\u524d\u5c06\u51e0\u4f55\u89c1\u89e3\u96c6\u6210\u5230\u7acb\u4f53\u5339\u914d\u7b97\u6cd5\u4e2d\u7684\u5c1d\u8bd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u4e2a\u56fe\u50cf\u7684\u51e0\u4f55\u77e5\u8bc6\uff0c\u800c\u5ffd\u7565\u4e86\u5173\u952e\u7684\u4ea4\u53c9\u89c6\u56fe\u56e0\u7d20\uff0c\u4f8b\u5982\u906e\u6321\u548c\u5339\u914d\u552f\u4e00\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u56fe\u5185\u548c\u8de8\u89c6\u56fe\u51e0\u4f55\u77e5\u8bc6\u5b66\u4e60\u7f51\u7edc\uff08ICGNet\uff09\uff0c\u4e13\u95e8\u7528\u4e8e\u5438\u6536\u89c6\u56fe\u5185\u548c\u8de8\u89c6\u56fe\u51e0\u4f55\u77e5\u8bc6\u3002 ICGNet \u5229\u7528\u5174\u8da3\u70b9\u7684\u529b\u91cf\u4f5c\u4e3a\u89c6\u56fe\u5185\u51e0\u4f55\u7406\u89e3\u7684\u6e20\u9053\u3002\u540c\u65f6\uff0c\u5b83\u5229\u7528\u8fd9\u4e9b\u70b9\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u6765\u6355\u83b7\u8de8\u89c6\u56fe\u51e0\u4f55\u5173\u7cfb\u3002\u8fd9\u79cd\u53cc\u91cd\u7ed3\u5408\u4f7f\u6240\u63d0\u51fa\u7684 ICGNet \u80fd\u591f\u5728\u5176\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5229\u7528\u89c6\u56fe\u5185\u548c\u8de8\u89c6\u56fe\u51e0\u4f55\u77e5\u8bc6\uff0c\u4ece\u800c\u5927\u5927\u63d0\u9ad8\u5176\u4f30\u8ba1\u5dee\u5f02\u7684\u80fd\u529b\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86 ICGNet \u76f8\u5bf9\u4e8e\u5f53\u4ee3\u9886\u5148\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u3002|[2402.19270v1](http://arxiv.org/pdf/2402.19270v1)|null|\n", "2402.19197": "|**2024-02-29**|**Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction**|\u7cbe\u7ec6\u7ed3\u6784\u611f\u77e5\u91c7\u6837\uff1a\u5355\u89c6\u56fe\u4eba\u4f53\u91cd\u5efa\u4e2d\u50cf\u7d20\u5bf9\u9f50\u9690\u5f0f\u6a21\u578b\u7684\u65b0\u91c7\u6837\u8bad\u7ec3\u65b9\u6848|Kennard Yanting Chan, Fayao Liu, Guosheng Lin, Chuan Sheng Foo, Weisi Lin|Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to introduce this loss once a slight reworking of the pixel-aligned implicit function framework is carried out. Our results show that our methods significantly outperform SOTA methods qualitatively and quantitatively. Our code is publicly available at https://github.com/kcyt/FSS.|\u50cf\u7d20\u5bf9\u9f50\u9690\u5f0f\u6a21\u578b\uff0c\u4f8b\u5982 PIFu\u3001PIFuHD \u548c ICON\uff0c\u7528\u4e8e\u5355\u89c6\u56fe\u7a7f\u7740\u4eba\u4f53\u91cd\u5efa\u3002\u8fd9\u4e9b\u6a21\u578b\u9700\u8981\u4f7f\u7528\u62bd\u6837\u8bad\u7ec3\u65b9\u6848\u8fdb\u884c\u8bad\u7ec3\u3002\u73b0\u6709\u7684\u91c7\u6837\u8bad\u7ec3\u65b9\u6848\u8981\u4e48\u65e0\u6cd5\u6355\u83b7\u8584\u8868\u9762\uff08\u4f8b\u5982\u8033\u6735\u3001\u624b\u6307\uff09\uff0c\u8981\u4e48\u5728\u91cd\u5efa\u7684\u7f51\u683c\u4e2d\u5bfc\u81f4\u566a\u58f0\u4f2a\u5f71\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u7cbe\u7ec6\u7ed3\u6784\u611f\u77e5\u91c7\u6837\uff08FSS\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u8bad\u7ec3\u65b9\u6848\uff0c\u7528\u4e8e\u8bad\u7ec3\u7528\u4e8e\u5355\u89c6\u56fe\u4eba\u4f53\u91cd\u5efa\u7684\u50cf\u7d20\u5bf9\u9f50\u9690\u5f0f\u6a21\u578b\u3002 FSS \u901a\u8fc7\u4e3b\u52a8\u9002\u5e94\u8868\u9762\u7684\u539a\u5ea6\u548c\u590d\u6742\u6027\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u6b64\u5916\uff0c\u4e0e\u73b0\u6709\u7684\u91c7\u6837\u8bad\u7ec3\u65b9\u6848\u4e0d\u540c\uff0cFSS \u5c55\u793a\u4e86\u5982\u4f55\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5229\u7528\u6837\u672c\u70b9\u7684\u6cd5\u7ebf\u6765\u6539\u5584\u7ed3\u679c\u3002\u6700\u540e\uff0c\u4e3a\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdb\u8bad\u7ec3\u8fc7\u7a0b\uff0cFSS \u63d0\u51fa\u4e86\u7528\u4e8e\u50cf\u7d20\u5bf9\u9f50\u9690\u5f0f\u6a21\u578b\u7684\u7f51\u683c\u539a\u5ea6\u635f\u5931\u4fe1\u53f7\u3002\u4e00\u65e6\u5bf9\u50cf\u7d20\u5bf9\u9f50\u9690\u5f0f\u51fd\u6570\u6846\u67b6\u8fdb\u884c\u4e86\u8f7b\u5fae\u7684\u4fee\u6539\uff0c\u5f15\u5165\u8fd9\u79cd\u635f\u5931\u5728\u8ba1\u7b97\u4e0a\u5c31\u53d8\u5f97\u53ef\u884c\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u90fd\u663e\u7740\u4f18\u4e8e SOTA \u65b9\u6cd5\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/kcyt/FSS \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2402.19197v1](http://arxiv.org/pdf/2402.19197v1)|null|\n", "2402.19119": "|**2024-02-29**|**VIXEN: Visual Text Comparison Network for Image Difference Captioning**|VIXEN\uff1a\u7528\u4e8e\u56fe\u50cf\u5dee\u5f02\u5b57\u5e55\u7684\u89c6\u89c9\u6587\u672c\u6bd4\u8f83\u7f51\u7edc|Alexander Black, Jing Shi, Yifei Fai, Tu Bui, John Collomosse|We present VIXEN - a technique that succinctly summarizes in text the visual differences between a pair of images in order to highlight any content manipulation present. Our proposed network linearly maps image features in a pairwise manner, constructing a soft prompt for a pretrained large language model. We address the challenge of low volume of training data and lack of manipulation variety in existing image difference captioning (IDC) datasets by training on synthetically manipulated images from the recent InstructPix2Pix dataset generated via prompt-to-prompt editing framework. We augment this dataset with change summaries produced via GPT-3. We show that VIXEN produces state-of-the-art, comprehensible difference captions for diverse image contents and edit types, offering a potential mitigation against misinformation disseminated via manipulated image content. Code and data are available at http://github.com/alexblck/vixen|\u6211\u4eec\u63d0\u51fa\u4e86 VIXEN - \u4e00\u79cd\u4ee5\u6587\u672c\u5f62\u5f0f\u7b80\u6d01\u603b\u7ed3\u4e00\u5bf9\u56fe\u50cf\u4e4b\u95f4\u7684\u89c6\u89c9\u5dee\u5f02\u7684\u6280\u672f\uff0c\u4ee5\u7a81\u51fa\u663e\u793a\u4efb\u4f55\u5185\u5bb9\u64cd\u7eb5\u3002\u6211\u4eec\u63d0\u51fa\u7684\u7f51\u7edc\u4ee5\u6210\u5bf9\u7684\u65b9\u5f0f\u7ebf\u6027\u6620\u5c04\u56fe\u50cf\u7279\u5f81\uff0c\u4e3a\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u8f6f\u63d0\u793a\u3002\u6211\u4eec\u901a\u8fc7\u5bf9\u6700\u8fd1\u7684 InstructPix2Pix \u6570\u636e\u96c6\u4e2d\u901a\u8fc7\u63d0\u793a\u5230\u63d0\u793a\u7f16\u8f91\u6846\u67b6\u751f\u6210\u7684\u7efc\u5408\u64cd\u4f5c\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u50cf\u5dee\u5f02\u5b57\u5e55 (IDC) \u6570\u636e\u96c6\u4e2d\u8bad\u7ec3\u6570\u636e\u91cf\u4f4e\u548c\u7f3a\u4e4f\u64cd\u4f5c\u591a\u6837\u6027\u7684\u6311\u6218\u3002\u6211\u4eec\u4f7f\u7528 GPT-3 \u751f\u6210\u7684\u53d8\u66f4\u6458\u8981\u6765\u6269\u5145\u8be5\u6570\u636e\u96c6\u3002\u6211\u4eec\u8868\u660e\uff0cVIXEN \u53ef\u4ee5\u4e3a\u4e0d\u540c\u7684\u56fe\u50cf\u5185\u5bb9\u548c\u7f16\u8f91\u7c7b\u578b\u751f\u6210\u6700\u5148\u8fdb\u3001\u6613\u4e8e\u7406\u89e3\u7684\u5dee\u5f02\u5b57\u5e55\uff0c\u4ece\u800c\u53ef\u4ee5\u6f5c\u5728\u5730\u7f13\u89e3\u901a\u8fc7\u64cd\u7eb5\u56fe\u50cf\u5185\u5bb9\u4f20\u64ad\u7684\u9519\u8bef\u4fe1\u606f\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728 http://github.com/alexblck/vixen \u83b7\u53d6|[2402.19119v1](http://arxiv.org/pdf/2402.19119v1)|null|\n", "2402.19111": "|**2024-02-29**|**Deep Network for Image Compressed Sensing Coding Using Local Structural Sampling**|\u4f7f\u7528\u5c40\u90e8\u7ed3\u6784\u91c7\u6837\u8fdb\u884c\u56fe\u50cf\u538b\u7f29\u611f\u77e5\u7f16\u7801\u7684\u6df1\u5ea6\u7f51\u7edc|Wenxue Cui, Xingtao Wang, Xiaopeng Fan, Shaohui Liu, Xinwei Gao, Debin Zhao|Existing image compressed sensing (CS) coding frameworks usually solve an inverse problem based on measurement coding and optimization-based image reconstruction, which still exist the following two challenges: 1) The widely used random sampling matrix, such as the Gaussian Random Matrix (GRM), usually leads to low measurement coding efficiency. 2) The optimization-based reconstruction methods generally maintain a much higher computational complexity. In this paper, we propose a new CNN based image CS coding framework using local structural sampling (dubbed CSCNet) that includes three functional modules: local structural sampling, measurement coding and Laplacian pyramid reconstruction. In the proposed framework, instead of GRM, a new local structural sampling matrix is first developed, which is able to enhance the correlation between the measurements through a local perceptual sampling strategy. Besides, the designed local structural sampling matrix can be jointly optimized with the other functional modules during training process. After sampling, the measurements with high correlations are produced, which are then coded into final bitstreams by the third-party image codec. At last, a Laplacian pyramid reconstruction network is proposed to efficiently recover the target image from the measurement domain to the image domain. Extensive experimental results demonstrate that the proposed scheme outperforms the existing state-of-the-art CS coding methods, while maintaining fast computational speed.|\u73b0\u6709\u7684\u56fe\u50cf\u538b\u7f29\u611f\u77e5\uff08CS\uff09\u7f16\u7801\u6846\u67b6\u901a\u5e38\u89e3\u51b3\u57fa\u4e8e\u6d4b\u91cf\u7f16\u7801\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u56fe\u50cf\u91cd\u5efa\u7684\u9006\u95ee\u9898\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u4ee5\u4e0b\u4e24\u4e2a\u6311\u6218\uff1a1\uff09\u5e7f\u6cdb\u4f7f\u7528\u7684\u968f\u673a\u91c7\u6837\u77e9\u9635\uff0c\u4f8b\u5982\u9ad8\u65af\u968f\u673a\u77e9\u9635\uff08GRM\uff09 \uff09\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u6d4b\u91cf\u7f16\u7801\u6548\u7387\u4f4e\u4e0b\u3002 2\uff09\u57fa\u4e8e\u4f18\u5316\u7684\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u4fdd\u6301\u8f83\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e CNN \u7684\u4f7f\u7528\u5c40\u90e8\u7ed3\u6784\u91c7\u6837\u7684\u56fe\u50cf CS \u7f16\u7801\u6846\u67b6\uff08\u79f0\u4e3a CSCNet\uff09\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u529f\u80fd\u6a21\u5757\uff1a\u5c40\u90e8\u7ed3\u6784\u91c7\u6837\u3001\u6d4b\u91cf\u7f16\u7801\u548c\u62c9\u666e\u62c9\u65af\u91d1\u5b57\u5854\u91cd\u5efa\u3002\u5728\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e2d\uff0c\u9996\u5148\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5c40\u90e8\u7ed3\u6784\u91c7\u6837\u77e9\u9635\uff0c\u800c\u4e0d\u662fGRM\uff0c\u5b83\u80fd\u591f\u901a\u8fc7\u5c40\u90e8\u611f\u77e5\u91c7\u6837\u7b56\u7565\u589e\u5f3a\u6d4b\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u7684\u5c40\u90e8\u7ed3\u6784\u91c7\u6837\u77e9\u9635\u53ef\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0e\u5176\u4ed6\u529f\u80fd\u6a21\u5757\u8054\u5408\u4f18\u5316\u3002\u91c7\u6837\u540e\uff0c\u4ea7\u751f\u5177\u6709\u9ad8\u76f8\u5173\u6027\u7684\u6d4b\u91cf\u7ed3\u679c\uff0c\u7136\u540e\u7531\u7b2c\u4e09\u65b9\u56fe\u50cf\u7f16\u89e3\u7801\u5668\u5c06\u5176\u7f16\u7801\u4e3a\u6700\u7ec8\u6bd4\u7279\u6d41\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u62c9\u666e\u62c9\u65af\u91d1\u5b57\u5854\u91cd\u5efa\u7f51\u7edc\uff0c\u4ee5\u6709\u6548\u5730\u5c06\u76ee\u6807\u56fe\u50cf\u4ece\u6d4b\u91cf\u57df\u6062\u590d\u5230\u56fe\u50cf\u57df\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6848\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684 CS \u7f16\u7801\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5feb\u901f\u7684\u8ba1\u7b97\u901f\u5ea6\u3002|[2402.19111v1](http://arxiv.org/pdf/2402.19111v1)|null|\n", "2402.19108": "|**2024-02-29**|**DeepEraser: Deep Iterative Context Mining for Generic Text Eraser**|DeepEraser\uff1a\u901a\u7528\u6587\u672c\u6a61\u76ae\u64e6\u7684\u6df1\u5ea6\u8fed\u4ee3\u4e0a\u4e0b\u6587\u6316\u6398|Hao Feng, Wendi Wang, Shaokai Liu, Jiajun Deng, Wengang Zhou, Houqiang Li|In this work, we present DeepEraser, an effective deep network for generic text removal. DeepEraser utilizes a recurrent architecture that erases the text in an image via iterative operations. Our idea comes from the process of erasing pencil script, where the text area designated for removal is subject to continuous monitoring and the text is attenuated progressively, ensuring a thorough and clean erasure. Technically, at each iteration, an innovative erasing module is deployed, which not only explicitly aggregates the previous erasing progress but also mines additional semantic context to erase the target text. Through iterative refinements, the text regions are progressively replaced with more appropriate content and finally converge to a relatively accurate status. Furthermore, a custom mask generation strategy is introduced to improve the capability of DeepEraser for adaptive text removal, as opposed to indiscriminately removing all the text in an image. Our DeepEraser is notably compact with only 1.4M parameters and trained in an end-to-end manner. To verify its effectiveness, extensive experiments are conducted on several prevalent benchmarks, including SCUT-Syn, SCUT-EnsText, and Oxford Synthetic text dataset. The quantitative and qualitative results demonstrate the effectiveness of our DeepEraser over the state-of-the-art methods, as well as its strong generalization ability in custom mask text removal. The codes and pre-trained models are available at https://github.com/fh2019ustc/DeepEraser|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DeepEraser\uff0c\u4e00\u79cd\u7528\u4e8e\u901a\u7528\u6587\u672c\u5220\u9664\u7684\u6709\u6548\u6df1\u5ea6\u7f51\u7edc\u3002 DeepEraser \u5229\u7528\u5faa\u73af\u67b6\u6784\uff0c\u901a\u8fc7\u8fed\u4ee3\u64cd\u4f5c\u64e6\u9664\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u3002\u6211\u4eec\u7684\u60f3\u6cd5\u6765\u81ea\u4e8e\u64e6\u9664\u94c5\u7b14\u7a3f\u7684\u8fc7\u7a0b\uff0c\u5176\u4e2d\u6307\u5b9a\u8981\u5220\u9664\u7684\u6587\u672c\u533a\u57df\u53d7\u5230\u6301\u7eed\u76d1\u63a7\uff0c\u6587\u672c\u9010\u6e10\u51cf\u5f31\uff0c\u786e\u4fdd\u5f7b\u5e95\u3001\u5e72\u51c0\u7684\u64e6\u9664\u3002\u4ece\u6280\u672f\u4e0a\u8bb2\uff0c\u5728\u6bcf\u6b21\u8fed\u4ee3\u65f6\uff0c\u90fd\u4f1a\u90e8\u7f72\u4e00\u4e2a\u521b\u65b0\u7684\u64e6\u9664\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u4e0d\u4ec5\u663e\u5f0f\u805a\u5408\u4e4b\u524d\u7684\u64e6\u9664\u8fdb\u5ea6\uff0c\u800c\u4e14\u8fd8\u6316\u6398\u989d\u5916\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u6765\u64e6\u9664\u76ee\u6807\u6587\u672c\u3002\u901a\u8fc7\u8fed\u4ee3\u7ec6\u5316\uff0c\u6587\u672c\u533a\u57df\u9010\u6e10\u88ab\u66f4\u5408\u9002\u7684\u5185\u5bb9\u66ff\u6362\uff0c\u6700\u7ec8\u6536\u655b\u5230\u76f8\u5bf9\u51c6\u786e\u7684\u72b6\u6001\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u81ea\u5b9a\u4e49\u63a9\u6a21\u751f\u6210\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8 DeepEraser \u81ea\u9002\u5e94\u6587\u672c\u5220\u9664\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4e0d\u52a0\u533a\u522b\u5730\u5220\u9664\u56fe\u50cf\u4e2d\u7684\u6240\u6709\u6587\u672c\u3002\u6211\u4eec\u7684 DeepEraser \u975e\u5e38\u7d27\u51d1\uff0c\u53ea\u6709 140 \u4e07\u4e2a\u53c2\u6570\uff0c\u5e76\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\u3002\u4e3a\u4e86\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u5728\u51e0\u4e2a\u6d41\u884c\u7684\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5305\u62ec SCUT-Syn\u3001SCUT-EnsText \u548c Oxford Synthetic \u6587\u672c\u6570\u636e\u96c6\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684 DeepEraser \u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u5176\u5728\u81ea\u5b9a\u4e49\u8499\u7248\u6587\u672c\u5220\u9664\u65b9\u9762\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728 https://github.com/fh2019ustc/DeepEraser \u83b7\u53d6|[2402.19108v1](http://arxiv.org/pdf/2402.19108v1)|null|\n", "2402.19041": "|**2024-02-29**|**Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors**|\u4f7f\u7528\u89c6\u9891\u5e8f\u5217\u6df1\u5ea6\u89c6\u89c9\u5148\u9a8c\u53bb\u9664\u5927\u6c14\u6e4d\u6d41|P. Hill, N. Anantrasirichai, A. Achim, D. R. Bull|Atmospheric turbulence poses a challenge for the interpretation and visual perception of visual imagery due to its distortion effects. Model-based approaches have been used to address this, but such methods often suffer from artefacts associated with moving content. Conversely, deep learning based methods are dependent on large and diverse datasets that may not effectively represent any specific content. In this paper, we address these problems with a self-supervised learning method that does not require ground truth. The proposed method is not dependent on any dataset outside of the single data sequence being processed but is also able to improve the quality of any input raw sequences or pre-processed sequences. Specifically, our method is based on an accelerated Deep Image Prior (DIP), but integrates temporal information using pixel shuffling and a temporal sliding window. This efficiently learns spatio-temporal priors leading to a system that effectively mitigates atmospheric turbulence distortions. The experiments show that our method improves visual quality results qualitatively and quantitatively.|\u7531\u4e8e\u5176\u626d\u66f2\u6548\u5e94\uff0c\u5927\u6c14\u6e4d\u6d41\u5bf9\u89c6\u89c9\u56fe\u50cf\u7684\u89e3\u91ca\u548c\u89c6\u89c9\u611f\u77e5\u63d0\u51fa\u4e86\u6311\u6218\u3002\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5df2\u88ab\u7528\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u6b64\u7c7b\u65b9\u6cd5\u7ecf\u5e38\u53d7\u5230\u4e0e\u79fb\u52a8\u5185\u5bb9\u76f8\u5173\u7684\u4f2a\u5f71\u7684\u5f71\u54cd\u3002\u76f8\u53cd\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u578b\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u5730\u8868\u793a\u4efb\u4f55\u7279\u5b9a\u5185\u5bb9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7528\u4e0d\u9700\u8981\u57fa\u672c\u4e8b\u5b9e\u7684\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u6b63\u5728\u5904\u7406\u7684\u5355\u4e2a\u6570\u636e\u5e8f\u5217\u4e4b\u5916\u7684\u4efb\u4f55\u6570\u636e\u96c6\uff0c\u800c\u4e14\u8fd8\u80fd\u591f\u63d0\u9ad8\u4efb\u4f55\u8f93\u5165\u539f\u59cb\u5e8f\u5217\u6216\u9884\u5904\u7406\u5e8f\u5217\u7684\u8d28\u91cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u52a0\u901f\u7684\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\uff08DIP\uff09\uff0c\u4f46\u4f7f\u7528\u50cf\u7d20\u6539\u7ec4\u548c\u65f6\u95f4\u6ed1\u52a8\u7a97\u53e3\u96c6\u6210\u65f6\u95f4\u4fe1\u606f\u3002\u8fd9\u53ef\u4ee5\u6709\u6548\u5730\u5b66\u4e60\u65f6\u7a7a\u5148\u200b\u200b\u9a8c\uff0c\u4ece\u800c\u5f62\u6210\u4e00\u4e2a\u53ef\u4ee5\u6709\u6548\u51cf\u8f7b\u5927\u6c14\u6e4d\u6d41\u626d\u66f2\u7684\u7cfb\u7edf\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9a\u6027\u548c\u5b9a\u91cf\u5730\u63d0\u9ad8\u4e86\u89c6\u89c9\u8d28\u91cf\u7ed3\u679c\u3002|[2402.19041v1](http://arxiv.org/pdf/2402.19041v1)|null|\n", "2402.18970": "|**2024-02-29**|**PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation**|PrivatEyes\uff1a\u4f7f\u7528\u8054\u5408\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\u8fdb\u884c\u57fa\u4e8e\u5916\u89c2\u7684\u6ce8\u89c6\u4f30\u8ba1|Mayar Elfares, Pascal Reisert, Zhiming Hu, Wenwu Tang, Ralf K\u00fcsters, Andreas Bulling|Latest gaze estimation methods require large-scale training data but their collection and exchange pose significant privacy risks. We propose PrivatEyes - the first privacy-enhancing training approach for appearance-based gaze estimation based on federated learning (FL) and secure multi-party computation (MPC). PrivatEyes enables training gaze estimators on multiple local datasets across different users and server-based secure aggregation of the individual estimators' updates. PrivatEyes guarantees that individual gaze data remains private even if a majority of the aggregating servers is malicious. We also introduce a new data leakage attack DualView that shows that PrivatEyes limits the leakage of private training data more effectively than previous approaches. Evaluations on the MPIIGaze, MPIIFaceGaze, GazeCapture, and NVGaze datasets further show that the improved privacy does not lead to a lower gaze estimation accuracy or substantially higher computational costs - both of which are on par with its non-secure counterparts.|\u6700\u65b0\u7684\u6ce8\u89c6\u4f30\u8ba1\u65b9\u6cd5\u9700\u8981\u5927\u89c4\u6a21\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5b83\u4eec\u7684\u6536\u96c6\u548c\u4ea4\u6362\u4f1a\u5e26\u6765\u91cd\u5927\u7684\u9690\u79c1\u98ce\u9669\u3002\u6211\u4eec\u63d0\u51fa\u4e86 PrivatEyes\u2014\u2014\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u8054\u5408\u5b66\u4e60\uff08FL\uff09\u548c\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\uff08MPC\uff09\u7684\u57fa\u4e8e\u5916\u89c2\u7684\u6ce8\u89c6\u4f30\u8ba1\u7684\u9690\u79c1\u589e\u5f3a\u8bad\u7ec3\u65b9\u6cd5\u3002 PrivatEyes \u80fd\u591f\u5728\u4e0d\u540c\u7528\u6237\u7684\u591a\u4e2a\u672c\u5730\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6ce8\u89c6\u4f30\u8ba1\u5668\uff0c\u5e76\u57fa\u4e8e\u670d\u52a1\u5668\u5bf9\u5404\u4e2a\u4f30\u8ba1\u5668\u7684\u66f4\u65b0\u8fdb\u884c\u5b89\u5168\u805a\u5408\u3002\u5373\u4f7f\u5927\u591a\u6570\u805a\u5408\u670d\u52a1\u5668\u662f\u6076\u610f\u7684\uff0cPrivatEyes \u4e5f\u80fd\u4fdd\u8bc1\u4e2a\u4eba\u6ce8\u89c6\u6570\u636e\u4fdd\u6301\u79c1\u5bc6\u6027\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u6cc4\u6f0f\u653b\u51fb DualView\uff0c\u8be5\u653b\u51fb\u8868\u660e PrivatEyes \u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u9650\u5236\u79c1\u4eba\u8bad\u7ec3\u6570\u636e\u7684\u6cc4\u6f0f\u3002\u5bf9 MPIIGaze\u3001MPIIFaceGaze\u3001GazeCapture \u548c NVGaze \u6570\u636e\u96c6\u7684\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u6539\u8fdb\u7684\u9690\u79c1\u4e0d\u4f1a\u5bfc\u81f4\u6ce8\u89c6\u4f30\u8ba1\u7cbe\u5ea6\u964d\u4f4e\u6216\u8ba1\u7b97\u6210\u672c\u5927\u5e45\u63d0\u9ad8 - \u4e24\u8005\u4e0e\u975e\u5b89\u5168\u6570\u636e\u96c6\u76f8\u5f53\u3002|[2402.18970v1](http://arxiv.org/pdf/2402.18970v1)|null|\n", "2402.18969": "|**2024-02-29**|**OHTA: One-shot Hand Avatar via Data-driven Implicit Priors**|OHTA\uff1a\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u9690\u5f0f\u5148\u9a8c\u5b9e\u73b0\u4e00\u6b21\u6027\u624b\u90e8\u5934\u50cf|Xiaozheng Zheng, Chao Wen, Zhuo Su, Zeran Xu, Zhaohu Li, Yang Zhao, Zhou Xue|In this paper, we delve into the creation of one-shot hand avatars, attaining high-fidelity and drivable hand representations swiftly from a single image. With the burgeoning domains of the digital human, the need for quick and personalized hand avatar creation has become increasingly critical. Existing techniques typically require extensive input data and may prove cumbersome or even impractical in certain scenarios. To enhance accessibility, we present a novel method OHTA (One-shot Hand avaTAr) that enables the creation of detailed hand avatars from merely one image. OHTA tackles the inherent difficulties of this data-limited problem by learning and utilizing data-driven hand priors. Specifically, we design a hand prior model initially employed for 1) learning various hand priors with available data and subsequently for 2) the inversion and fitting of the target identity with prior knowledge. OHTA demonstrates the capability to create high-fidelity hand avatars with consistent animatable quality, solely relying on a single image. Furthermore, we illustrate the versatility of OHTA through diverse applications, encompassing text-to-avatar conversion, hand editing, and identity latent space manipulation.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86\u4e00\u6b21\u6027\u624b\u90e8\u5934\u50cf\u7684\u521b\u5efa\uff0c\u4ece\u5355\u4e2a\u56fe\u50cf\u4e2d\u5feb\u901f\u83b7\u5f97\u9ad8\u4fdd\u771f\u4e14\u53ef\u9a7e\u9a76\u7684\u624b\u90e8\u8868\u793a\u3002\u968f\u7740\u6570\u5b57\u4eba\u7c7b\u9886\u57df\u7684\u84ec\u52c3\u53d1\u5c55\uff0c\u5bf9\u5feb\u901f\u3001\u4e2a\u6027\u5316\u7684\u624b\u90e8\u5934\u50cf\u521b\u5efa\u7684\u9700\u6c42\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u73b0\u6709\u6280\u672f\u901a\u5e38\u9700\u8981\u5927\u91cf\u8f93\u5165\u6570\u636e\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u5f88\u9ebb\u70e6\u751a\u81f3\u4e0d\u5207\u5b9e\u9645\u3002\u4e3a\u4e86\u589e\u5f3a\u53ef\u8bbf\u95ee\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5 OHTA\uff08\u4e00\u6b21\u6027\u624b\u90e8\u5934\u50cf\uff09\uff0c\u53ef\u4ee5\u4ec5\u4ece\u4e00\u5f20\u56fe\u50cf\u521b\u5efa\u8be6\u7ec6\u7684\u624b\u90e8\u5934\u50cf\u3002 OHTA \u901a\u8fc7\u5b66\u4e60\u548c\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684\u624b\u5148\u9a8c\u6765\u89e3\u51b3\u8fd9\u4e2a\u6570\u636e\u6709\u9650\u95ee\u9898\u7684\u56fa\u6709\u56f0\u96be\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u624b\u90e8\u5148\u9a8c\u6a21\u578b\uff0c\u6700\u521d\u7528\u4e8e 1\uff09\u5229\u7528\u53ef\u7528\u6570\u636e\u5b66\u4e60\u5404\u79cd\u624b\u90e8\u5148\u9a8c\uff0c\u968f\u540e\u7528\u4e8e 2\uff09\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u53cd\u8f6c\u548c\u62df\u5408\u76ee\u6807\u8eab\u4efd\u3002 OHTA \u5c55\u793a\u4e86\u4ec5\u4f9d\u9760\u5355\u4e2a\u56fe\u50cf\u5373\u53ef\u521b\u5efa\u5177\u6709\u4e00\u81f4\u52a8\u753b\u8d28\u91cf\u7684\u9ad8\u4fdd\u771f\u624b\u90e8\u5934\u50cf\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5404\u79cd\u5e94\u7528\u8bf4\u660e\u4e86 OHTA \u7684\u591a\u529f\u80fd\u6027\uff0c\u5305\u62ec\u6587\u672c\u5230\u5934\u50cf\u8f6c\u6362\u3001\u624b\u52a8\u7f16\u8f91\u548c\u8eab\u4efd\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u3002|[2402.18969v1](http://arxiv.org/pdf/2402.18969v1)|null|\n", "2402.18956": "|**2024-02-29**|**WWW: A Unified Framework for Explaining What, Where and Why of Neural Networks by Interpretation of Neuron Concepts**|WWW\uff1a\u901a\u8fc7\u89e3\u91ca\u795e\u7ecf\u5143\u6982\u5ff5\u6765\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u5bb9\u3001\u4f4d\u7f6e\u548c\u539f\u56e0\u7684\u7edf\u4e00\u6846\u67b6|Yong Hyun Ahn, Hyeon Bae Kim, Seong Tae Kim|Recent advancements in neural networks have showcased their remarkable capabilities across various domains. Despite these successes, the \"black box\" problem still remains. Addressing this, we propose a novel framework, WWW, that offers the 'what', 'where', and 'why' of the neural network decisions in human-understandable terms. Specifically, WWW utilizes adaptive selection for concept discovery, employing adaptive cosine similarity and thresholding techniques to effectively explain 'what'. To address the 'where' and 'why', we proposed a novel combination of neuron activation maps (NAMs) with Shapley values, generating localized concept maps and heatmaps for individual inputs. Furthermore, WWW introduces a method for predicting uncertainty, leveraging heatmap similarities to estimate 'how' reliable the prediction is. Experimental evaluations of WWW demonstrate superior performance in both quantitative and qualitative metrics, outperforming existing methods in interpretability. WWW provides a unified solution for explaining 'what', 'where', and 'why', introducing a method for localized explanations from global interpretations and offering a plug-and-play solution adaptable to various architectures.|\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u65b0\u8fdb\u5c55\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u5404\u4e2a\u9886\u57df\u7684\u5353\u8d8a\u80fd\u529b\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fd9\u4e9b\u6210\u529f\uff0c\u201c\u9ed1\u5323\u5b50\u201d\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6 WWW\uff0c\u5b83\u4ee5\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u672f\u8bed\u63d0\u4f9b\u795e\u7ecf\u7f51\u7edc\u51b3\u7b56\u7684\u201c\u5185\u5bb9\u201d\u3001\u201c\u5730\u70b9\u201d\u548c\u201c\u539f\u56e0\u201d\u3002\u5177\u4f53\u6765\u8bf4\uff0cWWW \u5229\u7528\u81ea\u9002\u5e94\u9009\u62e9\u8fdb\u884c\u6982\u5ff5\u53d1\u73b0\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u4f59\u5f26\u76f8\u4f3c\u6027\u548c\u9608\u503c\u6280\u672f\u6765\u6709\u6548\u5730\u89e3\u91ca\u201c\u4ec0\u4e48\u201d\u3002\u4e3a\u4e86\u89e3\u51b3\u201c\u54ea\u91cc\u201d\u548c\u201c\u4e3a\u4ec0\u4e48\u201d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u5143\u6fc0\u6d3b\u56fe\uff08NAM\uff09\u4e0e\u6c99\u666e\u5229\u503c\u7684\u65b0\u9896\u7ec4\u5408\uff0c\u4e3a\u5404\u4e2a\u8f93\u5165\u751f\u6210\u5c40\u90e8\u6982\u5ff5\u56fe\u548c\u70ed\u56fe\u3002\u6b64\u5916\uff0cWWW \u5f15\u5165\u4e86\u4e00\u79cd\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u70ed\u56fe\u76f8\u4f3c\u6027\u6765\u4f30\u8ba1\u9884\u6d4b\u7684\u201c\u53ef\u9760\u6027\u201d\u3002 WWW \u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u6307\u6807\u65b9\u9762\u5747\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002 WWW\u63d0\u4f9b\u4e86\u89e3\u91ca\u201c\u4ec0\u4e48\u201d\u3001\u201c\u5728\u54ea\u91cc\u201d\u548c\u201c\u4e3a\u4ec0\u4e48\u201d\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u5f15\u5165\u4e86\u4ece\u5168\u5c40\u89e3\u91ca\u4e2d\u8fdb\u884c\u672c\u5730\u5316\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u9002\u5e94\u5404\u79cd\u4f53\u7cfb\u7ed3\u6784\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\u3002|[2402.18956v1](http://arxiv.org/pdf/2402.18956v1)|null|\n", "2402.18856": "|**2024-02-29**|**Anatomy-guided fiber trajectory distribution estimation for cranial nerves tractography**|\u8111\u795e\u7ecf\u675f\u6210\u50cf\u7684\u89e3\u5256\u5f15\u5bfc\u7ea4\u7ef4\u8f68\u8ff9\u5206\u5e03\u4f30\u8ba1|Lei Xie, Qingrun Zeng, Huajun Zhou, Guoqiang Xie, Mingchu Li, Jiahao Huang, Jianan Cui, Hao Chen, Yuanjing Feng|Diffusion MRI tractography is an important tool for identifying and analyzing the intracranial course of cranial nerves (CNs). However, the complex environment of the skull base leads to ambiguous spatial correspondence between diffusion directions and fiber geometry, and existing diffusion tractography methods of CNs identification are prone to producing erroneous trajectories and missing true positive connections. To overcome the above challenge, we propose a novel CNs identification framework with anatomy-guided fiber trajectory distribution, which incorporates anatomical shape prior knowledge during the process of CNs tracing to build diffusion tensor vector fields. We introduce higher-order streamline differential equations for continuous flow field representations to directly characterize the fiber trajectory distribution of CNs from the tract-based level. The experimental results on the vivo HCP dataset and the clinical MDM dataset demonstrate that the proposed method reduces false-positive fiber production compared to competing methods and produces reconstructed CNs (i.e. CN II, CN III, CN V, and CN VII/VIII) that are judged to better correspond to the known anatomy.|\u6269\u6563 MRI \u7ea4\u7ef4\u675f\u6210\u50cf\u662f\u8bc6\u522b\u548c\u5206\u6790\u9885\u795e\u7ecf (CN) \u9885\u5185\u8d70\u884c\u7684\u91cd\u8981\u5de5\u5177\u3002\u7136\u800c\uff0c\u590d\u6742\u7684\u9885\u5e95\u73af\u5883\u5bfc\u81f4\u6269\u6563\u65b9\u5411\u548c\u7ea4\u7ef4\u51e0\u4f55\u5f62\u72b6\u4e4b\u95f4\u7684\u7a7a\u95f4\u5bf9\u5e94\u5173\u7cfb\u4e0d\u660e\u786e\uff0c\u73b0\u6709\u7684CNs\u8bc6\u522b\u6269\u6563\u7ea4\u7ef4\u675f\u6210\u50cf\u65b9\u6cd5\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u7684\u8f68\u8ff9\u5e76\u4e22\u5931\u771f\u6b63\u7684\u6b63\u8fde\u63a5\u3002\u4e3a\u4e86\u514b\u670d\u4e0a\u8ff0\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u89e3\u5256\u5f15\u5bfc\u7ea4\u7ef4\u8f68\u8ff9\u5206\u5e03\u7684\u65b0\u578b CNs \u8bc6\u522b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728 CNs \u8ffd\u8e2a\u8fc7\u7a0b\u4e2d\u7ed3\u5408\u4e86\u89e3\u5256\u5f62\u72b6\u5148\u9a8c\u77e5\u8bc6\u6765\u6784\u5efa\u6269\u6563\u5f20\u91cf\u77e2\u91cf\u573a\u3002\u6211\u4eec\u5f15\u5165\u4e86\u8fde\u7eed\u6d41\u573a\u8868\u793a\u7684\u9ad8\u9636\u6d41\u7ebf\u5fae\u5206\u65b9\u7a0b\uff0c\u4ee5\u76f4\u63a5\u4ece\u57fa\u4e8e\u533a\u57df\u7684\u6c34\u5e73\u8868\u5f81 CN \u7684\u7ea4\u7ef4\u8f68\u8ff9\u5206\u5e03\u3002\u5728 vivo HCP \u6570\u636e\u96c6\u548c\u4e34\u5e8a MDM \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7ade\u4e89\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u51cf\u5c11\u4e86\u5047\u9633\u6027\u7ea4\u7ef4\u7684\u4ea7\u751f\uff0c\u5e76\u4ea7\u751f\u4e86\u91cd\u5efa\u7684 CN\uff08\u5373 CN II\u3001CN III\u3001CN V \u548c CN VII/VIII\uff09\uff0c\u88ab\u8ba4\u4e3a\u66f4\u597d\u5730\u5bf9\u5e94\u4e8e\u5df2\u77e5\u7684\u89e3\u5256\u7ed3\u6784\u3002|[2402.18856v1](http://arxiv.org/pdf/2402.18856v1)|null|\n", "2402.18777": "|**2024-02-29**|**GDCNet: Calibrationless geometric distortion correction of echo planar imaging data using deep learning**|GDCNet\uff1a\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5bf9\u56de\u6ce2\u5e73\u9762\u6210\u50cf\u6570\u636e\u8fdb\u884c\u65e0\u6821\u51c6\u51e0\u4f55\u5931\u771f\u6821\u6b63|Marina Manso Jimeno, Keren Bachi, George Gardner, Yasmin L. Hurd, John Thomas Vaughan Jr., Sairam Geethanath|Functional magnetic resonance imaging techniques benefit from echo-planar imaging's fast image acquisition but are susceptible to inhomogeneities in the main magnetic field, resulting in geometric distortion and signal loss artifacts in the images. Traditional methods leverage a field map or voxel displacement map for distortion correction. However, voxel displacement map estimation requires additional sequence acquisitions, and the accuracy of the estimation influences correction performance. This work implements a novel approach called GDCNet, which estimates a geometric distortion map by non-linear registration to T1-weighted anatomical images and applies it for distortion correction. GDCNet demonstrated fast distortion correction of functional images in retrospectively and prospectively acquired datasets. Among the compared models, the 2D self-supervised configuration resulted in a statistically significant improvement to normalized mutual information between distortion-corrected functional and T1-weighted images compared to the benchmark methods FUGUE and TOPUP. Furthermore, GDCNet models achieved processing speeds 14 times faster than TOPUP in the prospective dataset.|\u529f\u80fd\u6027\u78c1\u5171\u632f\u6210\u50cf\u6280\u672f\u53d7\u76ca\u4e8e\u5e73\u9762\u56de\u6ce2\u6210\u50cf\u7684\u5feb\u901f\u56fe\u50cf\u91c7\u96c6\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u4e3b\u78c1\u573a\u4e0d\u5747\u5300\u6027\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u56fe\u50cf\u4e2d\u51fa\u73b0\u51e0\u4f55\u5931\u771f\u548c\u4fe1\u53f7\u4e22\u5931\u4f2a\u5f71\u3002\u4f20\u7edf\u65b9\u6cd5\u5229\u7528\u573a\u56fe\u6216\u4f53\u7d20\u4f4d\u79fb\u56fe\u8fdb\u884c\u7578\u53d8\u6821\u6b63\u3002\u7136\u800c\uff0c\u4f53\u7d20\u4f4d\u79fb\u56fe\u4f30\u8ba1\u9700\u8981\u989d\u5916\u7684\u5e8f\u5217\u91c7\u96c6\uff0c\u5e76\u4e14\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u5f71\u54cd\u6821\u6b63\u6027\u80fd\u3002\u8fd9\u9879\u5de5\u4f5c\u5b9e\u73b0\u4e86\u4e00\u79cd\u79f0\u4e3a GDCNet \u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u975e\u7ebf\u6027\u914d\u51c6\u5230 T1 \u52a0\u6743\u89e3\u5256\u56fe\u50cf\u6765\u4f30\u8ba1\u51e0\u4f55\u7578\u53d8\u56fe\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u7578\u53d8\u6821\u6b63\u3002 GDCNet \u5728\u56de\u987e\u6027\u548c\u524d\u77bb\u6027\u83b7\u53d6\u7684\u6570\u636e\u96c6\u4e2d\u5c55\u793a\u4e86\u529f\u80fd\u56fe\u50cf\u7684\u5feb\u901f\u7578\u53d8\u6821\u6b63\u3002\u5728\u6bd4\u8f83\u6a21\u578b\u4e2d\uff0c\u4e0e\u57fa\u51c6\u65b9\u6cd5 FUGUE \u548c TOPUP \u76f8\u6bd4\uff0c2D \u81ea\u76d1\u7763\u914d\u7f6e\u5bfc\u81f4\u5931\u771f\u6821\u6b63\u529f\u80fd\u56fe\u50cf\u548c T1 \u52a0\u6743\u56fe\u50cf\u4e4b\u95f4\u7684\u5f52\u4e00\u5316\u4e92\u4fe1\u606f\u5728\u7edf\u8ba1\u4e0a\u663e\u7740\u6539\u5584\u3002\u6b64\u5916\uff0cGDCNet \u6a21\u578b\u5728\u9884\u671f\u6570\u636e\u96c6\u4e2d\u7684\u5904\u7406\u901f\u5ea6\u6bd4 TOPUP \u5feb 14 \u500d\u3002|[2402.18777v1](http://arxiv.org/pdf/2402.18777v1)|null|\n", "2402.18761": "|**2024-02-29**|**Exploration of Learned Lifting-Based Transform Structures for Fully Scalable and Accessible Wavelet-Like Image Compression**|\u63a2\u7d22\u57fa\u4e8e\u5b66\u4e60\u63d0\u5347\u7684\u53d8\u6362\u7ed3\u6784\u4ee5\u5b9e\u73b0\u5b8c\u5168\u53ef\u6269\u5c55\u4e14\u53ef\u8bbf\u95ee\u7684\u7c7b\u5c0f\u6ce2\u56fe\u50cf\u538b\u7f29|Xinyue Li, Aous Naman, David Taubman|This paper provides a comprehensive study on features and performance of different ways to incorporate neural networks into lifting-based wavelet-like transforms, within the context of fully scalable and accessible image compression. Specifically, we explore different arrangements of lifting steps, as well as various network architectures for learned lifting operators. Moreover, we examine the impact of the number of learned lifting steps, the number of channels, the number of layers and the support of kernels in each learned lifting operator. To facilitate the study, we investigate two generic training methodologies that are simultaneously appropriate to a wide variety of lifting structures considered. Experimental results ultimately suggest that retaining fixed lifting steps from the base wavelet transform is highly beneficial. Moreover, we demonstrate that employing more learned lifting steps and more layers in each learned lifting operator do not contribute strongly to the compression performance. However, benefits can be obtained by utilizing more channels in each learned lifting operator. Ultimately, the learned wavelet-like transform proposed in this paper achieves over 25% bit-rate savings compared to JPEG 2000 with compact spatial support.|\u672c\u6587\u5728\u5b8c\u5168\u53ef\u6269\u5c55\u548c\u53ef\u8bbf\u95ee\u7684\u56fe\u50cf\u538b\u7f29\u7684\u80cc\u666f\u4e0b\uff0c\u5bf9\u5c06\u795e\u7ecf\u7f51\u7edc\u7eb3\u5165\u57fa\u4e8e\u63d0\u5347\u7684\u7c7b\u5c0f\u6ce2\u53d8\u6362\u7684\u4e0d\u540c\u65b9\u6cd5\u7684\u7279\u5f81\u548c\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7814\u7a76\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u63d0\u5347\u6b65\u9aa4\u7684\u4e0d\u540c\u5b89\u6392\uff0c\u4ee5\u53ca\u5b66\u4e60\u63d0\u5347\u64cd\u4f5c\u5458\u7684\u5404\u79cd\u7f51\u7edc\u67b6\u6784\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u68c0\u67e5\u4e86\u5b66\u4e60\u63d0\u5347\u6b65\u9aa4\u6570\u91cf\u3001\u901a\u9053\u6570\u91cf\u3001\u5c42\u6570\u4ee5\u53ca\u6bcf\u4e2a\u5b66\u4e60\u63d0\u5347\u7b97\u5b50\u4e2d\u5185\u6838\u652f\u6301\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u4fc3\u8fdb\u7814\u7a76\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e24\u79cd\u901a\u7528\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b83\u4eec\u540c\u65f6\u9002\u7528\u4e8e\u6240\u8003\u8651\u7684\u5404\u79cd\u4e3e\u5347\u7ed3\u6784\u3002\u5b9e\u9a8c\u7ed3\u679c\u6700\u7ec8\u8868\u660e\uff0c\u4fdd\u7559\u57fa\u7840\u5c0f\u6ce2\u53d8\u6362\u7684\u56fa\u5b9a\u63d0\u5347\u6b65\u9aa4\u662f\u975e\u5e38\u6709\u76ca\u7684\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\uff0c\u5728\u6bcf\u4e2a\u5b66\u4e60\u7684\u63d0\u5347\u7b97\u5b50\u4e2d\u91c7\u7528\u66f4\u591a\u7684\u5b66\u4e60\u63d0\u5347\u6b65\u9aa4\u548c\u66f4\u591a\u7684\u5c42\u5bf9\u538b\u7f29\u6027\u80fd\u6ca1\u6709\u592a\u5927\u8d21\u732e\u3002\u7136\u800c\uff0c\u901a\u8fc7\u5229\u7528\u6bcf\u4e2a\u6709\u5b66\u8bc6\u7684\u8d77\u91cd\u64cd\u4f5c\u5458\u7684\u66f4\u591a\u6e20\u9053\u53ef\u4ee5\u83b7\u5f97\u597d\u5904\u3002\u6700\u7ec8\uff0c\u4e0e\u5177\u6709\u7d27\u51d1\u7a7a\u95f4\u652f\u6301\u7684 JPEG 2000 \u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u5b66\u4e60\u578b\u5c0f\u6ce2\u53d8\u6362\u5b9e\u73b0\u4e86\u8d85\u8fc7 25% \u7684\u6bd4\u7279\u7387\u8282\u7701\u3002|[2402.18761v1](http://arxiv.org/pdf/2402.18761v1)|null|\n"}}