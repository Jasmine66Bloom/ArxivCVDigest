{"\u751f\u6210\u6a21\u578b": {"2402.14009": "|**2024-02-21**|**Geometry-Informed Neural Networks**|\u51e0\u4f55\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc|Arturs Berzins, Andreas Radler, Sebastian Sanokowski, Sepp Hochreiter, Johannes Brandstetter|We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.|\u6211\u4eec\u5f15\u5165\u4e86\u51e0\u4f55\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08GINN\uff09\u7684\u6982\u5ff5\uff0c\u5176\u4e2d\u5305\u62ec\uff08i\uff09\u5728\u51e0\u4f55\u7ea6\u675f\u4e0b\u5b66\u4e60\uff0c\uff08ii\uff09\u795e\u7ecf\u573a\u4f5c\u4e3a\u5408\u9002\u7684\u8868\u793a\uff0c\u4ee5\u53ca\uff08iii\uff09\u4e3a\u51e0\u4f55\u4e2d\u7ecf\u5e38\u9047\u5230\u7684\u6b20\u5b9a\u7cfb\u7edf\u751f\u6210\u4e0d\u540c\u7684\u89e3\u51b3\u65b9\u6848\u4efb\u52a1\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cGINN \u516c\u5f0f\u4e0d\u9700\u8981\u8bad\u7ec3\u6570\u636e\uff0c\u56e0\u6b64\u53ef\u4ee5\u88ab\u89c6\u4e3a\u7eaf\u7cb9\u7531\u7ea6\u675f\u9a71\u52a8\u7684\u751f\u6210\u6a21\u578b\u3002\u6211\u4eec\u6dfb\u52a0\u4e86\u663e\u5f0f\u7684\u591a\u6837\u6027\u635f\u5931\u6765\u51cf\u8f7b\u6a21\u5f0f\u5d29\u6e83\u3002\u6211\u4eec\u8003\u8651\u4e86\u51e0\u4e2a\u7ea6\u675f\uff0c\u7279\u522b\u662f\u7ec4\u4ef6\u7684\u8fde\u901a\u6027\uff0c\u6211\u4eec\u901a\u8fc7\u83ab\u5c14\u65af\u7406\u8bba\u5c06\u5176\u8f6c\u6362\u4e3a\u53ef\u5fae\u635f\u5931\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 GINN \u5b66\u4e60\u8303\u5f0f\u5728\u590d\u6742\u7a0b\u5ea6\u4e0d\u65ad\u589e\u52a0\u7684\u4e00\u7cfb\u5217\u4e8c\u7ef4\u548c\u4e09\u7ef4\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002|[2402.14009v1](http://arxiv.org/pdf/2402.14009v1)|null|\n", "2402.13936": "|**2024-02-21**|**Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning**|\u72ec\u7279\u7684\u56fe\u50cf\u5b57\u5e55\uff1a\u5728 CLIP \u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u4e2d\u5229\u7528\u771f\u5b9e\u5b57\u5e55|Antoine Chaffin, Ewa Kijak, Vincent Claveau|Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions. Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions. However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework. We propose a new image captioning model training strategy that makes use of GT captions in different ways. Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual GAN setup extended for multimodal inputs. Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image. This objective acts as an additional learning signal grounded to the distribution of the GT captions. Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate. Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality.|\u4f7f\u7528\u6559\u5e08\u5f3a\u5236\u8bad\u7ec3\u56fe\u50cf\u5b57\u5e55\u6a21\u578b\u4f1a\u4ea7\u751f\u975e\u5e38\u901a\u7528\u7684\u6837\u672c\uff0c\u800c\u66f4\u72ec\u7279\u7684\u5b57\u5e55\u5728\u68c0\u7d22\u5e94\u7528\u7a0b\u5e8f\u4e2d\u975e\u5e38\u6709\u7528\uff0c\u6216\u8005\u751f\u6210\u63cf\u8ff0\u56fe\u50cf\u7684\u66ff\u4ee3\u6587\u672c\u4ee5\u4f9b\u53ef\u8bbf\u95ee\u6027\u3002\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5141\u8bb8\u4f7f\u7528\u751f\u6210\u7684\u6807\u9898\u548c\u8f93\u5165\u56fe\u50cf\u4e4b\u95f4\u7684\u8de8\u6a21\u5f0f\u68c0\u7d22\u76f8\u4f3c\u5ea6\u5206\u6570\u4f5c\u4e3a\u6307\u5bfc\u8bad\u7ec3\u7684\u5956\u52b1\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u72ec\u7279\u7684\u6807\u9898\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u9884\u5148\u8bad\u7ec3\u7684\u8de8\u6a21\u5f0f\u68c0\u7d22\u6a21\u578b\u53ef\u7528\u4e8e\u63d0\u4f9b\u8fd9\u79cd\u5956\u52b1\uff0c\u5b8c\u5168\u6d88\u9664\u5bf9\u53c2\u8003\u6807\u9898\u7684\u9700\u8981\u3002\u7136\u800c\uff0c\u6211\u4eec\u5728\u672c\u6587\u4e2d\u8ba4\u4e3a\uff0cGround Truth (GT) \u5b57\u5e55\u5728\u8fd9\u4e2a RL \u6846\u67b6\u4e2d\u4ecd\u7136\u6709\u7528\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5b57\u5e55\u6a21\u578b\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u5229\u7528 GT \u5b57\u5e55\u3002\u9996\u5148\uff0c\u5b83\u4eec\u53ef\u7528\u4e8e\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684 MLP \u5224\u522b\u5668\uff0c\u8be5\u5224\u522b\u5668\u53ef\u4f5c\u4e3a\u6b63\u5219\u5316\u4ee5\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u5e76\u786e\u4fdd\u751f\u6210\u7684\u5b57\u5e55\u7684\u6d41\u7545\u6027\uff0c\u4ece\u800c\u4ea7\u751f\u6269\u5c55\u7528\u4e8e\u591a\u6a21\u6001\u8f93\u5165\u7684\u6587\u672c GAN \u8bbe\u7f6e\u3002\u5176\u6b21\uff0c\u5b83\u4eec\u53ef\u4ee5\u4f5c\u4e3a RL \u7b56\u7565\u4e2d\u7684\u9644\u52a0\u8f68\u8ff9\uff0c\u4ece\u800c\u4ea7\u751f\u7531 GT \u4e0e\u56fe\u50cf\u7684\u76f8\u4f3c\u6027\u52a0\u6743\u7684\u6559\u5e08\u5f3a\u5236\u635f\u5931\u3002\u8be5\u76ee\u6807\u5145\u5f53\u57fa\u4e8e GT \u5b57\u5e55\u5206\u53d1\u7684\u9644\u52a0\u5b66\u4e60\u4fe1\u53f7\u3002\u7b2c\u4e09\uff0c\u5f53\u6dfb\u52a0\u5230\u7528\u4e8e\u8ba1\u7b97\u5efa\u8bae\u7684\u5bf9\u6bd4\u5956\u52b1\u4ee5\u51cf\u5c11\u68af\u5ea6\u4f30\u8ba1\u7684\u65b9\u5dee\u7684\u5b57\u5e55\u6c60\u4e2d\u65f6\uff0c\u5b83\u4eec\u53ef\u4ee5\u4f5c\u4e3a\u5f3a\u5927\u7684\u57fa\u7ebf\u3002 MS-COCO \u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u5728\u4fdd\u6301\u9ad8\u5199\u4f5c\u8d28\u91cf\u7684\u540c\u65f6\u751f\u6210\u9ad8\u5ea6\u72ec\u7279\u7684\u5b57\u5e55\u7684\u5174\u8da3\u3002|[2402.13936v1](http://arxiv.org/pdf/2402.13936v1)|**[link](https://github.com/nohtow/wtf-rl)**|\n", "2402.13932": "|**2024-02-21**|**Tumor segmentation on whole slide images: training or prompting?**|\u6574\u4e2a\u5e7b\u706f\u7247\u56fe\u50cf\u4e0a\u7684\u80bf\u7624\u5206\u5272\uff1a\u8bad\u7ec3\u8fd8\u662f\u63d0\u793a\uff1f|Huaqian Wu, Clara Br\u00e9mond-Martin, K\u00e9vin Bouaou, C\u00e9dric Clouchoux|Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level. However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask. Downsampling the WSI and performing semantic segmentation is another possible approach. While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss. Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself. Such approach has demonstrated promising results on many computer vision tasks. In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs. In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning.|\u80bf\u7624\u5206\u5272\u662f\u764c\u75c7\u8bca\u65ad\u7684\u5173\u952e\u4efb\u52a1\u3002\u9274\u4e8e\u7ec4\u7ec7\u5b66\u4e2d\u6574\u4e2a\u5e7b\u706f\u7247\u56fe\u50cf (WSI) \u7684\u5de8\u5927\u5c3a\u5bf8\uff0c\u7528\u4e8e WSI \u5206\u7c7b\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5728\u8865\u4e01\u7ea7\u522b\u6216\u8d85\u50cf\u7d20\u7ea7\u522b\u4e0a\u8fd0\u884c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u96be\u4ee5\u6355\u83b7\u5168\u5c40 WSI \u4fe1\u606f\uff0c\u5e76\u4e14\u65e0\u6cd5\u76f4\u63a5\u751f\u6210\u4e8c\u8fdb\u5236\u63a9\u7801\u3002\u5bf9 WSI \u8fdb\u884c\u4e0b\u91c7\u6837\u5e76\u6267\u884c\u8bed\u4e49\u5206\u5272\u662f\u53e6\u4e00\u79cd\u53ef\u80fd\u7684\u65b9\u6cd5\u3002\u867d\u7136\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u5b83\u9700\u8981\u5927\u91cf\u7684\u6ce8\u91ca\u6570\u636e\uff0c\u56e0\u4e3a\u5206\u8fa8\u7387\u964d\u4f4e\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002\u89c6\u89c9\u63d0\u793a\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8303\u4f8b\uff0c\u5b83\u5141\u8bb8\u6a21\u578b\u901a\u8fc7\u5bf9\u8f93\u5165\u7a7a\u95f4\u8fdb\u884c\u7ec6\u5fae\u4fee\u6539\u6765\u6267\u884c\u65b0\u4efb\u52a1\uff0c\u800c\u4e0d\u662f\u8c03\u6574\u6a21\u578b\u672c\u8eab\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bb8\u591a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u89c6\u89c9\u63d0\u793a\u5728\u4e09\u4e2a\u4e0d\u540c\u5668\u5b98\u80bf\u7624\u5206\u5272\u80cc\u666f\u4e0b\u7684\u529f\u6548\u3002\u4e0e\u9488\u5bf9\u6b64\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u7684\u7ecf\u5178\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u63d0\u793a\u793a\u4f8b\uff0c\u89c6\u89c9\u63d0\u793a\u53ef\u4ee5\u5b9e\u73b0\u53ef\u6bd4\u6216\u66f4\u597d\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u5927\u91cf\u5fae\u8c03\u3002|[2402.13932v1](http://arxiv.org/pdf/2402.13932v1)|null|\n", "2402.13809": "|**2024-02-21**|**NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion**|NeuralDiffuser\uff1a\u5177\u6709\u4e3b\u8981\u89c6\u89c9\u7279\u5f81\u5f15\u5bfc\u6269\u6563\u7684\u53ef\u63a7 fMRI \u91cd\u5efa|Haoyu Li, Hao Wu, Badong Chen|Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain. A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.). Moreover, LDMs would generate different image results even under the same conditions. For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details. We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details. We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a variety of results. We obtain the state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset (NSD), which offers more faithful details and consistent results.|\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b (LDM) \u4ece\u529f\u80fd\u6027\u78c1\u5171\u632f\u6210\u50cf (fMRI) \u4e2d\u91cd\u5efa\u89c6\u89c9\u523a\u6fc0\u53ef\u4ee5\u5bf9\u5927\u8111\u8fdb\u884c\u7ec6\u7c92\u5ea6\u68c0\u7d22\u3002\u91cd\u5efa\u7ec6\u8282\uff08\u4f8b\u5982\u7ed3\u6784\u3001\u80cc\u666f\u3001\u7eb9\u7406\u3001\u989c\u8272\u7b49\uff09\u7684\u51dd\u805a\u529b\u5bf9\u9f50\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\uff0cLDM \u4e5f\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u56fe\u50cf\u7ed3\u679c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u63ed\u793a\u4e86\u57fa\u4e8e LDM \u7684\u65b9\u6cd5\u7684\u795e\u7ecf\u79d1\u5b66\u89c6\u89d2\uff0c\u8be5\u65b9\u6cd5\u662f\u57fa\u4e8e\u6765\u81ea\u5927\u91cf\u56fe\u50cf\u7684\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u81ea\u4e0a\u800c\u4e0b\u7684\u521b\u5efa\uff0c\u4f46\u7f3a\u4e4f\u7ec6\u8282\u9a71\u52a8\u7684\u81ea\u4e0b\u800c\u4e0a\u7684\u611f\u77e5\uff0c\u5bfc\u81f4\u7ec6\u8282\u4e0d\u5fe0\u5b9e\u3002\u6211\u4eec\u63d0\u51fa\u4e86 NeuralDiffuser\uff0c\u5b83\u5f15\u5165\u4e86\u4e3b\u8981\u89c6\u89c9\u7279\u5f81\u6307\u5bfc\uff0c\u4ee5\u68af\u5ea6\u7684\u5f62\u5f0f\u63d0\u4f9b\u7ec6\u8282\u7ebf\u7d22\uff0c\u6269\u5c55\u4e86\u57fa\u4e8e LDM \u7684\u65b9\u6cd5\u7684\u81ea\u4e0b\u800c\u4e0a\u7684\u8fc7\u7a0b\uff0c\u4ee5\u5b9e\u73b0\u5fe0\u5b9e\u7684\u8bed\u4e49\u548c\u7ec6\u8282\u3002\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6307\u5bfc\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u91cd\u590d\u91cd\u5efa\u7684\u4e00\u81f4\u6027\uff0c\u800c\u4e0d\u662f\u5404\u79cd\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002\u6211\u4eec\u83b7\u5f97\u4e86 NeuralDiffuser \u5728\u81ea\u7136\u611f\u5b98\u6570\u636e\u96c6 (NSD) \u4e0a\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5b83\u63d0\u4f9b\u4e86\u66f4\u5fe0\u5b9e\u7684\u7ec6\u8282\u548c\u4e00\u81f4\u7684\u7ed3\u679c\u3002|[2402.13809v1](http://arxiv.org/pdf/2402.13809v1)|null|\n", "2402.13796": "|**2024-02-21**|**Scalable Methods for Brick Kiln Detection and Compliance Monitoring from Satellite Imagery: A Deployment Case Study in India**|\u5229\u7528\u536b\u661f\u56fe\u50cf\u8fdb\u884c\u7816\u7a91\u68c0\u6d4b\u548c\u5408\u89c4\u6027\u76d1\u6d4b\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff1a\u5370\u5ea6\u7684\u90e8\u7f72\u6848\u4f8b\u7814\u7a76|Rishabh Mondal, Zeel B Patel, Vannsh Jani, Nipun Batra|Air pollution kills 7 million people annually. Brick manufacturing industry is the second largest consumer of coal contributing to 8%-14% of air pollution in Indo-Gangetic plain (highly populated tract of land in the Indian subcontinent). As brick kilns are an unorganized sector and present in large numbers, detecting policy violations such as distance from habitat is non-trivial. Air quality and other domain experts rely on manual human annotation to maintain brick kiln inventory. Previous work used computer vision based machine learning methods to detect brick kilns from satellite imagery but they are limited to certain geographies and labeling the data is laborious. In this paper, we propose a framework to deploy a scalable brick kiln detection system for large countries such as India and identify 7477 new brick kilns from 28 districts in 5 states in the Indo-Gangetic plain. We then showcase efficient ways to check policy violations such as high spatial density of kilns and abnormal increase over time in a region. We show that 90% of brick kilns in Delhi-NCR violate a density-based policy. Our framework can be directly adopted by the governments across the world to automate the policy regulations around brick kilns.|\u7a7a\u6c14\u6c61\u67d3\u6bcf\u5e74\u5bfc\u81f4 700 \u4e07\u4eba\u6b7b\u4ea1\u3002\u7816\u5757\u5236\u9020\u4e1a\u662f\u7b2c\u4e8c\u5927\u7164\u70ad\u6d88\u8d39\u884c\u4e1a\uff0c\u9020\u6210\u5370\u5ea6\u6052\u6cb3\u5e73\u539f\uff08\u5370\u5ea6\u6b21\u5927\u9646\u4eba\u53e3\u7a20\u5bc6\u7684\u5730\u533a\uff098%-14%\u7684\u7a7a\u6c14\u6c61\u67d3\u3002\u7531\u4e8e\u7816\u7a91\u662f\u4e00\u4e2a\u65e0\u7ec4\u7ec7\u7684\u90e8\u95e8\u5e76\u4e14\u6570\u91cf\u5e9e\u5927\uff0c\u56e0\u6b64\u68c0\u6d4b\u8ddd\u6816\u606f\u5730\u8ddd\u79bb\u7b49\u653f\u7b56\u8fdd\u89c4\u884c\u4e3a\u5e76\u975e\u6613\u4e8b\u3002\u7a7a\u6c14\u8d28\u91cf\u548c\u5176\u4ed6\u9886\u57df\u4e13\u5bb6\u4f9d\u9760\u4eba\u5de5\u6ce8\u91ca\u6765\u7ef4\u62a4\u7816\u7a91\u5e93\u5b58\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u4f7f\u7528\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u68c0\u6d4b\u7816\u7a91\uff0c\u4f46\u5b83\u4eec\u4ec5\u9650\u4e8e\u67d0\u4e9b\u5730\u7406\u4f4d\u7f6e\uff0c\u5e76\u4e14\u6807\u8bb0\u6570\u636e\u5f88\u8d39\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u4e3a\u5370\u5ea6\u7b49\u5927\u56fd\u90e8\u7f72\u53ef\u6269\u5c55\u7684\u7816\u7a91\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5e76\u8bc6\u522b\u6765\u81ea\u5370\u5ea6\u6052\u6cb3\u5e73\u539f 5 \u4e2a\u90a6\u7684 28 \u4e2a\u5730\u533a\u7684 7477 \u4e2a\u65b0\u7816\u7a91\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u68c0\u67e5\u653f\u7b56\u8fdd\u89c4\u884c\u4e3a\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f8b\u5982\u7a91\u7089\u7684\u9ad8\u7a7a\u95f4\u5bc6\u5ea6\u4ee5\u53ca\u4e00\u4e2a\u5730\u533a\u968f\u65f6\u95f4\u7684\u5f02\u5e38\u589e\u957f\u3002\u6211\u4eec\u53d1\u73b0\u5fb7\u91cc\u56fd\u5bb6\u9996\u90fd\u533a 90% \u7684\u7816\u7a91\u8fdd\u53cd\u4e86\u57fa\u4e8e\u5bc6\u5ea6\u7684\u653f\u7b56\u3002\u6211\u4eec\u7684\u6846\u67b6\u53ef\u4ee5\u88ab\u4e16\u754c\u5404\u56fd\u653f\u5e9c\u76f4\u63a5\u91c7\u7528\uff0c\u4ee5\u5b9e\u73b0\u7816\u7a91\u76f8\u5173\u653f\u7b56\u6cd5\u89c4\u7684\u81ea\u52a8\u5316\u3002|[2402.13796v1](http://arxiv.org/pdf/2402.13796v1)|null|\n", "2402.13776": "|**2024-02-21**|**Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion**|Cas-DiffCom\uff1a\u7528\u4e8e\u5a74\u513f\u7eb5\u5411\u8d85\u5206\u8fa8\u7387 3D \u533b\u5b66\u56fe\u50cf\u8865\u5168\u7684\u7ea7\u8054\u6269\u6563\u6a21\u578b|Lianghu Guo, Tianli Tao, Xinyi Cai, Zihao Zhu, Jiawei Huang, Lixuan Zhu, Zhuoyang Gu, Haifeng Tang, Rui Zhou, Siyan Han, et.al.|Early infancy is a rapid and dynamic neurodevelopmental period for behavior and neurocognition. Longitudinal magnetic resonance imaging (MRI) is an effective tool to investigate such a crucial stage by capturing the developmental trajectories of the brain structures. However, longitudinal MRI acquisition always meets a serious data-missing problem due to participant dropout and failed scans, making longitudinal infant brain atlas construction and developmental trajectory delineation quite challenging. Thanks to the development of an AI-based generative model, neuroimage completion has become a powerful technique to retain as much available data as possible. However, current image completion methods usually suffer from inconsistency within each individual subject in the time dimension, compromising the overall quality. To solve this problem, our paper proposed a two-stage cascaded diffusion model, Cas-DiffCom, for dense and longitudinal 3D infant brain MRI completion and super-resolution. We applied our proposed method to the Baby Connectome Project (BCP) dataset. The experiment results validate that Cas-DiffCom achieves both individual consistency and high fidelity in longitudinal infant brain image completion. We further applied the generated infant brain images to two downstream tasks, brain tissue segmentation and developmental trajectory delineation, to declare its task-oriented potential in the neuroscience field.|\u5a74\u513f\u65e9\u671f\u662f\u884c\u4e3a\u548c\u795e\u7ecf\u8ba4\u77e5\u5feb\u901f\u4e14\u52a8\u6001\u7684\u795e\u7ecf\u53d1\u80b2\u65f6\u671f\u3002\u7eb5\u5411\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u662f\u901a\u8fc7\u6355\u83b7\u5927\u8111\u7ed3\u6784\u7684\u53d1\u80b2\u8f68\u8ff9\u6765\u7814\u7a76\u8fd9\u4e00\u5173\u952e\u9636\u6bb5\u7684\u6709\u6548\u5de5\u5177\u3002\u7136\u800c\uff0c\u7531\u4e8e\u53c2\u4e0e\u8005\u9000\u51fa\u548c\u626b\u63cf\u5931\u8d25\uff0c\u7eb5\u5411MRI\u91c7\u96c6\u603b\u662f\u9047\u5230\u4e25\u91cd\u7684\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u8fd9\u4f7f\u5f97\u7eb5\u5411\u5a74\u513f\u8111\u56fe\u8c31\u6784\u5efa\u548c\u53d1\u80b2\u8f68\u8ff9\u63cf\u7ed8\u76f8\u5f53\u5177\u6709\u6311\u6218\u6027\u3002\u7531\u4e8e\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u795e\u7ecf\u56fe\u50cf\u8865\u5168\u5df2\u6210\u4e3a\u4fdd\u7559\u5c3d\u53ef\u80fd\u591a\u7684\u53ef\u7528\u6570\u636e\u7684\u5f3a\u5927\u6280\u672f\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u56fe\u50cf\u8865\u5168\u65b9\u6cd5\u901a\u5e38\u4f1a\u9047\u5230\u6bcf\u4e2a\u4e2a\u4f53\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4ece\u800c\u5f71\u54cd\u6574\u4f53\u8d28\u91cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u7684\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u7ea7\u7ea7\u8054\u6269\u6563\u6a21\u578b Cas-DiffCom\uff0c\u7528\u4e8e\u5bc6\u96c6\u548c\u7eb5\u5411 3D \u5a74\u513f\u8111 MRI \u5b8c\u6210\u548c\u8d85\u5206\u8fa8\u7387\u3002\u6211\u4eec\u5c06\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u5a74\u513f\u8fde\u63a5\u7ec4\u9879\u76ee\uff08BCP\uff09\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86Cas-DiffCom\u5728\u7eb5\u5411\u5a74\u513f\u8111\u56fe\u50cf\u8865\u5168\u65b9\u9762\u5b9e\u73b0\u4e86\u4e2a\u4f53\u4e00\u81f4\u6027\u548c\u9ad8\u4fdd\u771f\u5ea6\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06\u751f\u6210\u7684\u5a74\u513f\u5927\u8111\u56fe\u50cf\u5e94\u7528\u4e8e\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff0c\u5373\u8111\u7ec4\u7ec7\u5206\u5272\u548c\u53d1\u80b2\u8f68\u8ff9\u63cf\u7ed8\uff0c\u4ee5\u5ba3\u544a\u5176\u5728\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u4ee5\u4efb\u52a1\u4e3a\u5bfc\u5411\u7684\u6f5c\u529b\u3002|[2402.13776v1](http://arxiv.org/pdf/2402.13776v1)|null|\n", "2402.13737": "|**2024-02-21**|**SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model**|SRNDiff\uff1a\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u8fdb\u884c\u77ed\u671f\u964d\u96e8\u4e34\u8fd1\u9884\u62a5|Xudong Ling, Chaorong Li, Fengqing Qin, Peng Yang, Yuanyuan Huang|Diffusion models are widely used in image generation because they can generate high-quality and realistic samples. This is in contrast to generative adversarial networks (GANs) and variational autoencoders (VAEs), which have some limitations in terms of image quality.We introduce the diffusion model to the precipitation forecasting task and propose a short-term precipitation nowcasting with condition diffusion model based on historical observational data, which is referred to as SRNDiff. By incorporating an additional conditional decoder module in the denoising process, SRNDiff achieves end-to-end conditional rainfall prediction. SRNDiff is composed of two networks: a denoising network and a conditional Encoder network. The conditional network is composed of multiple independent UNet networks. These networks extract conditional feature maps at different resolutions, providing accurate conditional information that guides the diffusion model for conditional generation.SRNDiff surpasses GANs in terms of prediction accuracy, although it requires more computational resources.The SRNDiff model exhibits higher stability and efficiency during training than GANs-based approaches, and generates high-quality precipitation distribution samples that better reflect future actual precipitation conditions. This fully validates the advantages and potential of diffusion models in precipitation forecasting, providing new insights for enhancing rainfall prediction.|\u6269\u6563\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u4e2d\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u771f\u5b9e\u7684\u6837\u672c\u3002\u8fd9\u4e0e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u548c\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\uff08VAE\uff09\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u5b83\u4eec\u5728\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u6709\u4e00\u4e9b\u5c40\u9650\u6027\u3002\u6211\u4eec\u5c06\u6269\u6563\u6a21\u578b\u5f15\u5165\u964d\u6c34\u9884\u62a5\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u77ed\u671f\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u57fa\u4e8e\u5386\u53f2\u89c2\u6d4b\u6570\u636e\uff0c\u79f0\u4e3a SRNDiff\u3002\u901a\u8fc7\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u52a0\u5165\u989d\u5916\u7684\u6761\u4ef6\u89e3\u7801\u5668\u6a21\u5757\uff0cSRNDiff \u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u6761\u4ef6\u964d\u96e8\u9884\u6d4b\u3002 SRNDiff \u7531\u4e24\u4e2a\u7f51\u7edc\u7ec4\u6210\uff1a\u53bb\u566a\u7f51\u7edc\u548c\u6761\u4ef6\u7f16\u7801\u5668\u7f51\u7edc\u3002\u6761\u4ef6\u7f51\u7edc\u7531\u591a\u4e2a\u72ec\u7acb\u7684UNet\u7f51\u7edc\u7ec4\u6210\u3002\u8fd9\u4e9b\u7f51\u7edc\u63d0\u53d6\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u6761\u4ef6\u7279\u5f81\u56fe\uff0c\u63d0\u4f9b\u51c6\u786e\u7684\u6761\u4ef6\u4fe1\u606f\uff0c\u6307\u5bfc\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u751f\u6210\u3002SRNDiff \u5728\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\u8d85\u8d8a\u4e86 GAN\uff0c\u5c3d\u7ba1\u5b83\u9700\u8981\u66f4\u591a\u7684\u8ba1\u7b97\u8d44\u6e90\u3002SRNDiff \u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002\u57fa\u4e8eGAN\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u964d\u6c34\u5206\u5e03\u6837\u672c\uff0c\u66f4\u597d\u5730\u53cd\u6620\u672a\u6765\u5b9e\u9645\u964d\u6c34\u60c5\u51b5\u3002\u8fd9\u5145\u5206\u9a8c\u8bc1\u4e86\u6269\u6563\u6a21\u578b\u5728\u964d\u6c34\u9884\u62a5\u4e2d\u7684\u4f18\u52bf\u548c\u6f5c\u529b\uff0c\u4e3a\u52a0\u5f3a\u964d\u6c34\u9884\u62a5\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002|[2402.13737v1](http://arxiv.org/pdf/2402.13737v1)|null|\n", "2402.13729": "|**2024-02-21**|**Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation**|\u5177\u6709 2D \u4e09\u5e73\u9762\u548c 3D \u5c0f\u6ce2\u8868\u793a\u7684\u6df7\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b|Kihong Kim, Haneol Lee, Jihye Park, Seyeon Kim, Kwanghee Lee, Seungryong Kim, Jaejun Yoo|Generating high-quality videos that synthesize desired realistic content is a challenging task due to their intricate high-dimensionality and complexity of videos. Several recent diffusion-based methods have shown comparable performance by compressing videos to a lower-dimensional latent space, using traditional video autoencoder architecture. However, such method that employ standard frame-wise 2D and 3D convolution fail to fully exploit the spatio-temporal nature of videos. To address this issue, we propose a novel hybrid video diffusion model, called HVDM, which can capture spatio-temporal dependencies more effectively. The HVDM is trained by a hybrid video autoencoder which extracts a disentangled representation of the video including: (i) a global context information captured by a 2D projected latent (ii) a local volume information captured by 3D convolutions with wavelet decomposition (iii) a frequency information for improving the video reconstruction. Based on this disentangled representation, our hybrid autoencoder provide a more comprehensive video latent enriching the generated videos with fine structures and details. Experiments on video generation benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed approach achieves state-of-the-art video generation quality, showing a wide range of video applications (e.g., long video generation, image-to-video, and video dynamics control).|\u7531\u4e8e\u89c6\u9891\u9519\u7efc\u590d\u6742\u7684\u9ad8\u7ef4\u6027\u548c\u590d\u6742\u6027\uff0c\u751f\u6210\u5408\u6210\u6240\u9700\u771f\u5b9e\u5185\u5bb9\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u6700\u8fd1\u7684\u51e0\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u4f20\u7edf\u7684\u89c6\u9891\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\u5c06\u89c6\u9891\u538b\u7f29\u5230\u8f83\u4f4e\u7ef4\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u8868\u73b0\u51fa\u4e86\u76f8\u5f53\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u91c7\u7528\u6807\u51c6\u9010\u5e27 2D \u548c 3D \u5377\u79ef\u7684\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u89c6\u9891\u7684\u65f6\u7a7a\u6027\u8d28\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u79f0\u4e3a HVDM\uff0c\u5b83\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u6355\u83b7\u65f6\u7a7a\u4f9d\u8d56\u6027\u3002 HVDM \u901a\u8fc7\u6df7\u5408\u89c6\u9891\u81ea\u52a8\u7f16\u7801\u5668\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u7f16\u7801\u5668\u63d0\u53d6\u89c6\u9891\u7684\u89e3\u7f20\u7ed3\u8868\u793a\uff0c\u5305\u62ec\uff1a(i) \u7531 2D \u6295\u5f71\u6f5c\u4f0f\u6355\u83b7\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f (ii) \u901a\u8fc7\u5c0f\u6ce2\u5206\u89e3\u7684 3D \u5377\u79ef\u6355\u83b7\u7684\u5c40\u90e8\u4f53\u79ef\u4fe1\u606f (iii)\u7528\u4e8e\u6539\u8fdb\u89c6\u9891\u91cd\u5efa\u7684\u9891\u7387\u4fe1\u606f\u3002\u57fa\u4e8e\u8fd9\u79cd\u89e3\u5f00\u7684\u8868\u793a\uff0c\u6211\u4eec\u7684\u6df7\u5408\u81ea\u52a8\u7f16\u7801\u5668\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89c6\u9891\u6f5c\u5728\u80fd\u529b\uff0c\u4e30\u5bcc\u4e86\u751f\u6210\u7684\u5177\u6709\u7cbe\u7ec6\u7ed3\u6784\u548c\u7ec6\u8282\u7684\u89c6\u9891\u3002\u89c6\u9891\u751f\u6210\u57fa\u51c6\uff08UCF101\u3001SkyTimelapse \u548c TaiChi\uff09\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u89c6\u9891\u5e94\u7528\uff08\u4f8b\u5982\uff0c\u957f\u89c6\u9891\u751f\u6210\u3001\u56fe\u50cf\u5230\u89c6\u9891\u3001\u548c\u89c6\u9891\u52a8\u6001\u63a7\u5236\uff09\u3002|[2402.13729v1](http://arxiv.org/pdf/2402.13729v1)|null|\n", "2402.13575": "|**2024-02-21**|**Flexible Physical Camouflage Generation Based on a Differential Approach**|\u57fa\u4e8e\u5dee\u5206\u65b9\u6cd5\u7684\u7075\u6d3b\u7269\u7406\u4f2a\u88c5\u751f\u6210|Yang Li, Wenyi Tan, Chenxing Zhao, Shuangju Zhou, Xinkai Liang, Quan Pan|This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferability. Additionally, the designed sticker-mode camouflage, coupled with a concealment constraint, adapts to the environment, yielding diverse styles of texture. Our findings highlight the versatility and efficacy of the FPA approach in adversarial camouflage applications.|\u8fd9\u9879\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u5728\u5e7f\u6cdb\u7684 3D \u6e32\u67d3\u6846\u67b6\u5185\u4e13\u95e8\u4e3a\u5bf9\u6297\u6027\u4f2a\u88c5\u91cf\u8eab\u5b9a\u5236\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u540d\u4e3a FPA\uff0c\u5b83\u8d85\u8d8a\u4e86\u4f20\u7edf\u6280\u672f\uff0c\u5fe0\u5b9e\u5730\u6a21\u62df\u5149\u7167\u6761\u4ef6\u548c\u6750\u8d28\u53d8\u5316\uff0c\u786e\u4fdd\u5728 3D \u76ee\u6807\u4e0a\u5448\u73b0\u7ec6\u81f4\u5165\u5fae\u4e14\u771f\u5b9e\u7684\u7eb9\u7406\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u91c7\u7528\u4e00\u79cd\u751f\u6210\u65b9\u6cd5\uff0c\u4ece\u6269\u6563\u6a21\u578b\u4e2d\u5b66\u4e60\u5bf9\u6297\u6a21\u5f0f\u3002\u8fd9\u6d89\u53ca\u5230\u7ed3\u5408\u4e13\u95e8\u8bbe\u8ba1\u7684\u5bf9\u6297\u6027\u635f\u5931\u548c\u9690\u853d\u7ea6\u675f\u635f\u5931\uff0c\u4ee5\u4fdd\u8bc1\u7269\u7406\u4e16\u754c\u4e2d\u4f2a\u88c5\u7684\u5bf9\u6297\u6027\u548c\u9690\u853d\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u8d34\u7eb8\u6a21\u5f0f\u4f2a\u88c5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u635f\u5bb3\u5bf9\u6297\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u8986\u76d6\u76ee\u6807\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5b9e\u8bc1\u548c\u7269\u7406\u5b9e\u9a8c\uff0cFPA\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u53ef\u8f6c\u79fb\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5f88\u5f3a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u7684\u8d34\u7eb8\u6a21\u5f0f\u8ff7\u5f69\uff0c\u52a0\u4e0a\u9690\u853d\u6027\u7ea6\u675f\uff0c\u53ef\u4ee5\u9002\u5e94\u73af\u5883\uff0c\u4ea7\u751f\u4e0d\u540c\u98ce\u683c\u7684\u7eb9\u7406\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86 FPA \u65b9\u6cd5\u5728\u5bf9\u6297\u6027\u4f2a\u88c5\u5e94\u7528\u4e2d\u7684\u591a\u529f\u80fd\u6027\u548c\u6709\u6548\u6027\u3002|[2402.13575v1](http://arxiv.org/pdf/2402.13575v1)|null|\n", "2402.13573": "|**2024-02-21**|**ToDo: Token Downsampling for Efficient Generation of High-Resolution Images**|ToDo\uff1a\u4ee4\u724c\u4e0b\u91c7\u6837\u4ee5\u9ad8\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf|Ethan Smith, Nayan Saxena, Aninda Saha|Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.|\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u4e8e\u56fe\u50cf\u6269\u6563\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u7136\u800c\uff0c\u5b83\u4eec\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u6027\u9650\u5236\u4e86\u6211\u4eec\u5728\u5408\u7406\u7684\u65f6\u95f4\u548c\u5185\u5b58\u9650\u5236\u5185\u53ef\u4ee5\u5904\u7406\u7684\u56fe\u50cf\u5927\u5c0f\u3002\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u56fe\u50cf\u6a21\u578b\u4e2d\u5bc6\u96c6\u6ce8\u610f\u529b\u7684\u91cd\u8981\u6027\uff0c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u901a\u5e38\u5305\u542b\u5197\u4f59\u7279\u5f81\uff0c\u4f7f\u5176\u9002\u5408\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5 ToDo\uff0c\u5b83\u4f9d\u8d56\u4e8e\u952e\u548c\u503c\u6807\u8bb0\u7684\u6807\u8bb0\u4e0b\u91c7\u6837\uff0c\u4ee5\u5c06\u7a33\u5b9a\u6269\u6563\u63a8\u7406\u52a0\u901f\u5230\u5e38\u89c1\u5c3a\u5bf8\u7684 2 \u500d\uff0c\u4ee5\u53ca\u9ad8\u5206\u8fa8\u7387\uff08\u5982 2048x2048\uff09\u7684 4.5 \u500d\u6216\u66f4\u591a\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5e73\u8861\u6709\u6548\u541e\u5410\u91cf\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002|[2402.13573v1](http://arxiv.org/pdf/2402.13573v1)|null|\n", "2402.13490": "|**2024-02-21**|**Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models**|\u5bf9\u6bd4\u63d0\u793a\u53ef\u6539\u5584\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u89e3\u7f20\u7ed3|Chen Wu, Fernando De la Torre|Text-to-image diffusion models have achieved remarkable performance in image synthesis, while the text interface does not always provide fine-grained control over certain image factors. For instance, changing a single token in the text can have unintended effects on the image. This paper shows a simple modification of classifier-free guidance can help disentangle image factors in text-to-image models. The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a \"baseline\" that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.|\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6027\u80fd\uff0c\u800c\u6587\u672c\u754c\u9762\u5e76\u4e0d\u603b\u662f\u63d0\u4f9b\u5bf9\u67d0\u4e9b\u56fe\u50cf\u56e0\u7d20\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u4f8b\u5982\uff0c\u66f4\u6539\u6587\u672c\u4e2d\u7684\u5355\u4e2a\u6807\u8bb0\u53ef\u80fd\u4f1a\u5bf9\u56fe\u50cf\u4ea7\u751f\u610f\u60f3\u4e0d\u5230\u7684\u5f71\u54cd\u3002\u672c\u6587\u5c55\u793a\u4e86\u5bf9\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\u7684\u7b80\u5355\u4fee\u6539\u53ef\u4ee5\u5e2e\u52a9\u7406\u6e05\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u56fe\u50cf\u56e0\u7d20\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u201c\u5bf9\u6bd4\u6307\u5bfc\u201d\u7684\u5173\u952e\u601d\u60f3\u662f\u7528\u4e24\u4e2a\u6700\u5c0f\u6807\u8bb0\u4e0d\u540c\u7684\u63d0\u793a\u6765\u8868\u5f81\u9884\u671f\u56e0\u7d20\uff1a\u6b63\u63d0\u793a\u63cf\u8ff0\u8981\u5408\u6210\u7684\u56fe\u50cf\uff0c\u57fa\u7ebf\u63d0\u793a\u5145\u5f53\u89e3\u5f00\u5176\u4ed6\u56e0\u7d20\u7684\u201c\u57fa\u7ebf\u201d\u3002\u5bf9\u6bd4\u6307\u5bfc\u662f\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u6211\u4eec\u5728\u4e09\u79cd\u60c5\u51b5\u4e0b\u8bf4\u660e\u5176\u4f18\u70b9\uff1a(1) \u6307\u5bfc\u5728\u5bf9\u8c61\u7c7b\u4e0a\u8bad\u7ec3\u7684\u7279\u5b9a\u9886\u57df\u6269\u6563\u6a21\u578b\uff0c(2) \u83b7\u5f97\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8fde\u7eed\u3001\u7c7b\u4f3c\u88c5\u5907\u7684\u63a7\u5236\uff0c\u4ee5\u53ca(3) \u63d0\u9ad8\u96f6\u6837\u672c\u56fe\u50cf\u7f16\u8f91\u5668\u7684\u6027\u80fd\u3002|[2402.13490v1](http://arxiv.org/pdf/2402.13490v1)|null|\n"}, "\u591a\u6a21\u6001": {"2402.13876": "|**2024-02-21**|**Scene Prior Filtering for Depth Map Super-Resolution**|\u7528\u4e8e\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u7684\u573a\u666f\u5148\u9a8c\u8fc7\u6ee4|Zhengxue Wang, Zhiqiang Yan, Ming-Hsuan Yang, Jinshan Pan, Jian Yang, Ying Tai, Guangwei Gao|Multi-modal fusion is vital to the success of super-resolution of depth images. However, commonly used fusion strategies, such as addition and concatenation, fall short of effectively bridging the modal gap. As a result, guided image filtering methods have been introduced to mitigate this issue. Nevertheless, it is observed that their filter kernels usually encounter significant texture interference and edge inaccuracy. To tackle these two challenges, we introduce a Scene Prior Filtering network, SPFNet, which utilizes the priors surface normal and semantic map from large-scale models. Specifically, we design an All-in-one Prior Propagation that computes the similarity between multi-modal scene priors, \\textit{i.e.}, RGB, normal, semantic, and depth, to reduce the texture interference. In addition, we present a One-to-one Prior Embedding that continuously embeds each single-modal prior into depth using Mutual Guided Filtering, further alleviating the texture interference while enhancing edges. Our SPFNet has been extensively evaluated on both real and synthetic datasets, achieving state-of-the-art performance.|\u591a\u6a21\u6001\u878d\u5408\u5bf9\u4e8e\u6df1\u5ea6\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u6210\u529f\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5e38\u7528\u7684\u878d\u5408\u7b56\u7565\uff0c\u4f8b\u5982\u52a0\u6cd5\u548c\u4e32\u8054\uff0c\u65e0\u6cd5\u6709\u6548\u5730\u5f25\u5408\u6a21\u6001\u95f4\u9699\u3002\u56e0\u6b64\uff0c\u5f15\u5165\u4e86\u5f15\u5bfc\u56fe\u50cf\u8fc7\u6ee4\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\u3002\u7136\u800c\uff0c\u636e\u89c2\u5bdf\uff0c\u5b83\u4eec\u7684\u6ee4\u6ce2\u5668\u5185\u6838\u901a\u5e38\u4f1a\u9047\u5230\u660e\u663e\u7684\u7eb9\u7406\u5e72\u6270\u548c\u8fb9\u7f18\u4e0d\u51c6\u786e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e24\u4e2a\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u573a\u666f\u5148\u9a8c\u8fc7\u6ee4\u7f51\u7edc SPFNet\uff0c\u5b83\u5229\u7528\u5927\u89c4\u6a21\u6a21\u578b\u7684\u5148\u9a8c\u8868\u9762\u6cd5\u7ebf\u548c\u8bed\u4e49\u56fe\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e00\u4f53\u5f0f\u5148\u9a8c\u4f20\u64ad\uff0c\u5b83\u8ba1\u7b97\u591a\u6a21\u6001\u573a\u666f\u5148\u9a8c\u3001\\textit{i.e.}\u3001RGB\u3001\u6cd5\u7ebf\u3001\u8bed\u4e49\u548c\u6df1\u5ea6\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u51cf\u5c11\u7eb9\u7406\u5e72\u6270\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e00\u5bf9\u4e00\u7684\u5148\u9a8c\u5d4c\u5165\uff0c\u5b83\u4f7f\u7528\u76f8\u4e92\u5f15\u5bfc\u8fc7\u6ee4\u5c06\u6bcf\u4e2a\u5355\u6a21\u6001\u5148\u9a8c\u8fde\u7eed\u5d4c\u5165\u5230\u6df1\u5ea6\u4e2d\uff0c\u8fdb\u4e00\u6b65\u51cf\u8f7b\u7eb9\u7406\u5e72\u6270\uff0c\u540c\u65f6\u589e\u5f3a\u8fb9\u7f18\u3002\u6211\u4eec\u7684 SPFNet \u5df2\u5728\u771f\u5b9e\u6570\u636e\u96c6\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2402.13876v1](http://arxiv.org/pdf/2402.13876v1)|null|\n", "2402.13851": "|**2024-02-21**|**VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models**|VL-Trojan\uff1a\u9488\u5bf9\u81ea\u56de\u5f52\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u5f0f\u6307\u4ee4\u540e\u95e8\u653b\u51fb|Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao|Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context. Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\\%) in ASR. Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios.|\u81ea\u56de\u5f52\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u51e0\u6b21\u5b66\u4e60\u80fd\u529b\u3002\u6700\u8fd1\uff0c\u63d0\u51fa\u4e86\u591a\u6a21\u5f0f\u6307\u4ee4\u8c03\u6574\u6765\u8fdb\u4e00\u6b65\u589e\u5f3a\u6307\u4ee4\u8ddf\u8e2a\u80fd\u529b\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u6307\u4ee4\u8c03\u6574\u671f\u95f4\u81ea\u56de\u5f52 VLM \u7684\u540e\u95e8\u653b\u51fb\u6240\u5e26\u6765\u7684\u6f5c\u5728\u5a01\u80c1\u3002\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u6ce8\u5165\u5e26\u6709\u5d4c\u5165\u5728\u6307\u4ee4\u6216\u56fe\u50cf\u4e2d\u7684\u89e6\u53d1\u5668\u7684\u4e2d\u6bd2\u6837\u672c\u6765\u690d\u5165\u540e\u95e8\uff0c\u4ece\u800c\u80fd\u591f\u5229\u7528\u9884\u5b9a\u4e49\u7684\u89e6\u53d1\u5668\u6076\u610f\u64cd\u7eb5\u53d7\u5bb3\u8005\u6a21\u578b\u7684\u9884\u6d4b\u3002\u7136\u800c\uff0c\u81ea\u56de\u5f52 VLM \u4e2d\u7684\u51bb\u7ed3\u89c6\u89c9\u7f16\u7801\u5668\u5bf9\u4f20\u7edf\u56fe\u50cf\u89e6\u53d1\u5668\u7684\u5b66\u4e60\u65bd\u52a0\u4e86\u9650\u5236\u3002\u6b64\u5916\uff0c\u653b\u51fb\u8005\u5728\u8bbf\u95ee\u53d7\u5bb3\u8005\u6a21\u578b\u7684\u53c2\u6570\u548c\u67b6\u6784\u65f6\u53ef\u80fd\u4f1a\u9047\u5230\u9650\u5236\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u5f0f\u6307\u4ee4\u540e\u95e8\u653b\u51fb\uff0c\u5373VL-Trojan\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u9694\u79bb\u548c\u805a\u7c7b\u7b56\u7565\u4fc3\u8fdb\u56fe\u50cf\u89e6\u53d1\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u5b57\u7b26\u7ea7\u6587\u672c\u89e6\u53d1\u751f\u6210\u65b9\u6cd5\u589e\u5f3a\u9ed1\u76d2\u653b\u51fb\u6548\u7387\u3002\u6211\u4eec\u7684\u653b\u51fb\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6210\u529f\u8bf1\u5bfc\u4e86\u76ee\u6807\u8f93\u51fa\uff0c\u663e\u7740\u8d85\u8fc7\u4e86 ASR \u7684\u57fa\u7ebf (+62.52\\%)\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5c55\u793a\u4e86\u8de8\u5404\u79cd\u6a21\u578b\u89c4\u6a21\u548c\u5c11\u91cf\u4e0a\u4e0b\u6587\u63a8\u7406\u573a\u666f\u7684\u9c81\u68d2\u6027\u3002|[2402.13851v1](http://arxiv.org/pdf/2402.13851v1)|null|\n", "2402.13607": "|**2024-02-21**|**CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models**|CODIS\uff1a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u89c6\u89c9\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5|Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, et.al.|Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a context-dependent manner. View our project website at https://thunlp-mt.github.io/CODIS.|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u968f\u7740\u8fd9\u4e9b\u6a21\u578b\u8d8a\u6765\u8d8a\u878d\u5165\u7814\u7a76\u548c\u5e94\u7528\uff0c\u5bf9\u5176\u80fd\u529b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u90fd\u6ca1\u6709\u8003\u8651\u5230\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u56fe\u50cf\u9700\u8981\u5728\u66f4\u5e7f\u6cdb\u7684\u80cc\u666f\u4e0b\u8fdb\u884c\u89e3\u91ca\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a CODIS \u7684\u65b0\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u4f7f\u7528\u81ea\u7531\u683c\u5f0f\u6587\u672c\u4e2d\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u6765\u589e\u5f3a\u89c6\u89c9\u7406\u89e3\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cMLLM \u5728\u6b64\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u59cb\u7ec8\u4f4e\u4e8e\u4eba\u7c7b\u3002\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u8bc1\u5b9e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5f88\u96be\u6709\u6548\u5730\u63d0\u53d6\u548c\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u63d0\u9ad8\u5bf9\u56fe\u50cf\u7684\u7406\u89e3\u3002\u8fd9\u5f3a\u8c03\u4e86\u589e\u5f3a MLLM \u4ee5\u4e0a\u4e0b\u6587\u76f8\u5173\u65b9\u5f0f\u7406\u89e3\u89c6\u89c9\u6548\u679c\u7684\u80fd\u529b\u7684\u8feb\u5207\u9700\u8981\u3002\u67e5\u770b\u6211\u4eec\u7684\u9879\u76ee\u7f51\u7ad9\uff1ahttps://thunlp-mt.github.io/CODIS\u3002|[2402.13607v1](http://arxiv.org/pdf/2402.13607v1)|null|\n", "2402.13587": "|**2024-02-21**|**A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation**|\u7528\u4e8e\u7535\u5b50\u5546\u52a1\u4ea7\u54c1\u63cf\u8ff0\u751f\u6210\u7684\u591a\u6a21\u5f0f\u4e0a\u4e0b\u6587\u8c03\u6574\u65b9\u6cd5|Yunxin Li, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang|In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description. During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts. This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing automatic generation of product descriptions in a wide range of applications.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u56fe\u50cf\u751f\u6210\u4ea7\u54c1\u63cf\u8ff0\u7684\u65b0\u8bbe\u7f6e\uff0c\u5e76\u901a\u8fc7\u8425\u9500\u5173\u952e\u8bcd\u8fdb\u884c\u589e\u5f3a\u3002\u5b83\u5229\u7528\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u7684\u7efc\u5408\u529b\u91cf\u6765\u521b\u5efa\u66f4\u9002\u5408\u4ea7\u54c1\u72ec\u7279\u529f\u80fd\u7684\u63cf\u8ff0\u3002\u5bf9\u4e8e\u8fd9\u79cd\u8bbe\u7f6e\uff0c\u4ee5\u524d\u7684\u65b9\u6cd5\u5229\u7528\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u5bf9\u56fe\u50cf\u548c\u5173\u952e\u5b57\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u5668\u6765\u751f\u6210\u4ea7\u54c1\u63cf\u8ff0\u3002\u7136\u800c\uff0c\u7531\u4e8e\u540c\u54c1\u7c7b\u4ea7\u54c1\u5177\u6709\u76f8\u4f3c\u7684\u6587\u6848\uff0c\u751f\u6210\u7684\u63cf\u8ff0\u5f80\u5f80\u4e0d\u51c6\u786e\u4e14\u7b3c\u7edf\uff0c\u5e76\u4e14\u5728\u5927\u89c4\u6a21\u6837\u672c\u4e0a\u4f18\u5316\u6574\u4f53\u6846\u67b6\u4f7f\u5f97\u6a21\u578b\u4e13\u6ce8\u4e8e\u5e38\u7528\u8bcd\u800c\u5ffd\u7565\u4e86\u4ea7\u54c1\u7279\u5f81\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u8c03\u4f18\u65b9\u6cd5\uff0c\u540d\u4e3a ModICT\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u7c7b\u4f3c\u7684\u4ea7\u54c1\u6837\u672c\u4f5c\u4e3a\u53c2\u8003\uff0c\u5e76\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u6765\u751f\u6210\u63cf\u8ff0\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4fdd\u6301\u89c6\u89c9\u7f16\u7801\u5668\u548c\u8bed\u8a00\u6a21\u578b\u51bb\u7ed3\uff0c\u4e13\u6ce8\u4e8e\u4f18\u5316\u8d1f\u8d23\u521b\u5efa\u591a\u6a21\u5f0f\u4e0a\u4e0b\u6587\u5f15\u7528\u548c\u52a8\u6001\u63d0\u793a\u7684\u6a21\u5757\u3002\u8fd9\u79cd\u65b9\u6cd5\u4fdd\u7559\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e86\u63cf\u8ff0\u591a\u6837\u6027\u7684\u5927\u5e45\u589e\u52a0\u3002\u4e3a\u4e86\u8bc4\u4f30 ModICT \u5728\u5404\u79cd\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u548c\u7c7b\u578b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u4ece\u7535\u5b50\u5546\u52a1\u9886\u57df\u5185\u7684\u4e09\u4e2a\u4e0d\u540c\u4ea7\u54c1\u7c7b\u522b\u6536\u96c6\u6570\u636e\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0cModICT \u663e\u7740\u63d0\u9ad8\u4e86\u751f\u6210\u7ed3\u679c\u7684\u51c6\u786e\u6027\uff08\u5728 Rouge-L \u4e0a\u63d0\u9ad8\u4e86 3.3%\uff09\u548c\u591a\u6837\u6027\uff08\u5728 D-5 \u4e0a\u63d0\u9ad8\u4e86 9.4%\uff09\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86 ModICT \u4f5c\u4e3a\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u53ef\u4ee5\u5728\u5e7f\u6cdb\u7684\u5e94\u7528\u4e2d\u589e\u5f3a\u4ea7\u54c1\u63cf\u8ff0\u7684\u81ea\u52a8\u751f\u6210\u3002|[2402.13587v1](http://arxiv.org/pdf/2402.13587v1)|null|\n", "2402.13576": "|**2024-02-21**|**Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement**|\u901a\u8fc7\u90e8\u5206\u76f8\u5173\u6027\u589e\u5f3a\u6539\u8fdb\u89c6\u9891\u8bed\u6599\u5e93\u65f6\u523b\u68c0\u7d22|Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng|Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed at retrieving a relevant moment from a large corpus of untrimmed videos using a natural language text as query. The relevance between the video and query is partial, mainly evident in two aspects: (1) Scope: The untrimmed video contains information-rich frames, and not all are relevant to the query. Strong correlation is typically observed only within the relevant moment, emphasizing the importance of capturing key content. (2) Modality: The relevance of query to different modalities varies; action descriptions align more with the visual elements, while character conversations are more related to textual information. Recognizing and addressing these modality-specific nuances is crucial for effective retrieval in VCMR. However, existing methods often treat all video contents equally, leading to sub-optimal moment retrieval. We argue that effectively capturing the partial relevance between the query and video is essential for the VCMR task. To this end, we propose a Partial Relevance Enhanced Model~(PREM) to improve VCMR. VCMR involves two sub-tasks: video retrieval and moment localization. To align with their distinct objectives, we implement specialized partial relevance enhancement strategies. For video retrieval, we introduce a multi-modal collaborative video retriever, generating distinct query representations tailored for different modalities by modality-specific pooling, ensuring a more effective match. For moment localization, we propose the focus-then-fuse moment localizer, utilizing modality-specific gates to capture essential content, followed by fusing multi-modal information for moment localization. Experimental results on TVR and DiDeMo datasets show that the proposed model outperforms the baselines, achieving a new state-of-the-art of VCMR.|\u89c6\u9891\u8bed\u6599\u5e93\u65f6\u523b\u68c0\u7d22\uff08VCMR\uff09\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\uff0c\u65e8\u5728\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u6587\u672c\u4f5c\u4e3a\u67e5\u8be2\u4ece\u5927\u91cf\u672a\u7ecf\u4fee\u526a\u7684\u89c6\u9891\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u65f6\u523b\u3002\u89c6\u9891\u4e0e\u67e5\u8be2\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u662f\u90e8\u5206\u7684\uff0c\u4e3b\u8981\u8868\u73b0\u5728\u4e24\u4e2a\u65b9\u9762\uff1a\uff081\uff09\u8303\u56f4\uff1a\u672a\u7ecf\u4fee\u526a\u7684\u89c6\u9891\u5305\u542b\u4fe1\u606f\u4e30\u5bcc\u7684\u5e27\uff0c\u5e76\u4e14\u5e76\u975e\u6240\u6709\u5e27\u90fd\u4e0e\u67e5\u8be2\u76f8\u5173\u3002\u901a\u5e38\u4ec5\u5728\u76f8\u5173\u65f6\u523b\u89c2\u5bdf\u5230\u5f3a\u76f8\u5173\u6027\uff0c\u8fd9\u5f3a\u8c03\u4e86\u6355\u83b7\u5173\u952e\u5185\u5bb9\u7684\u91cd\u8981\u6027\u3002 (2) \u6a21\u6001\uff1a\u67e5\u8be2\u4e0e\u4e0d\u540c\u6a21\u6001\u7684\u76f8\u5173\u6027\u4e0d\u540c\uff1b\u52a8\u4f5c\u63cf\u8ff0\u4e0e\u89c6\u89c9\u5143\u7d20\u66f4\u52a0\u4e00\u81f4\uff0c\u800c\u89d2\u8272\u5bf9\u8bdd\u4e0e\u6587\u672c\u4fe1\u606f\u66f4\u52a0\u76f8\u5173\u3002\u8bc6\u522b\u5e76\u89e3\u51b3\u8fd9\u4e9b\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u7ec6\u5fae\u5dee\u522b\u5bf9\u4e8e VCMR \u4e2d\u7684\u6709\u6548\u68c0\u7d22\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5e73\u7b49\u5730\u5bf9\u5f85\u6240\u6709\u89c6\u9891\u5185\u5bb9\uff0c\u5bfc\u81f4\u6b21\u4f18\u7684\u65f6\u523b\u68c0\u7d22\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u6709\u6548\u6355\u83b7\u67e5\u8be2\u548c\u89c6\u9891\u4e4b\u95f4\u7684\u90e8\u5206\u76f8\u5173\u6027\u5bf9\u4e8e VCMR \u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u90e8\u5206\u76f8\u5173\u6027\u589e\u5f3a\u6a21\u578b\uff08PREM\uff09\u6765\u63d0\u9ad8 VCMR\u3002 VCMR \u6d89\u53ca\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a\u89c6\u9891\u68c0\u7d22\u548c\u65f6\u523b\u5b9a\u4f4d\u3002\u4e3a\u4e86\u4e0e\u4ed6\u4eec\u7684\u72ec\u7279\u76ee\u6807\u4fdd\u6301\u4e00\u81f4\uff0c\u6211\u4eec\u5b9e\u65bd\u4e13\u95e8\u7684\u90e8\u5206\u76f8\u5173\u6027\u589e\u5f3a\u7b56\u7565\u3002\u5bf9\u4e8e\u89c6\u9891\u68c0\u7d22\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u591a\u6a21\u6001\u534f\u4f5c\u89c6\u9891\u68c0\u7d22\u5668\uff0c\u901a\u8fc7\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u6c60\u751f\u6210\u9488\u5bf9\u4e0d\u540c\u6a21\u6001\u5b9a\u5236\u7684\u4e0d\u540c\u67e5\u8be2\u8868\u793a\uff0c\u786e\u4fdd\u66f4\u6709\u6548\u7684\u5339\u914d\u3002\u5bf9\u4e8e\u77e9\u5b9a\u4f4d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5148\u805a\u7126\u7136\u540e\u878d\u5408\u77e9\u5b9a\u4f4d\u5668\uff0c\u5229\u7528\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u95e8\u6765\u6355\u83b7\u57fa\u672c\u5185\u5bb9\uff0c\u7136\u540e\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u4ee5\u8fdb\u884c\u77e9\u5b9a\u4f4d\u3002 TVR \u548c DiDeMo \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86 VCMR \u7684\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002|[2402.13576v1](http://arxiv.org/pdf/2402.13576v1)|null|\n", "2402.13561": "|**2024-02-21**|**Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment**|\u8ba4\u77e5\u89c6\u89c9\u8bed\u8a00\u6620\u5c04\u5668\uff1a\u901a\u8fc7\u589e\u5f3a\u7684\u89c6\u89c9\u77e5\u8bc6\u5bf9\u9f50\u4fc3\u8fdb\u591a\u6a21\u5f0f\u7406\u89e3|Yunxin Li, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang|Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.|\u8bc4\u4f30\u548c\u91cd\u65b0\u601d\u8003\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u7684\u73b0\u72b6\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5e7f\u6cdb\u4f7f\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u6295\u5f71\u65b9\u6cd5\uff08\u4f8b\u5982 Q-former \u6216 MLP\uff09\u4fa7\u91cd\u4e8e\u56fe\u50cf\u6587\u672c\u63cf\u8ff0\u7684\u5bf9\u9f50\uff0c\u4f46\u5ffd\u7565\u4e86\u89c6\u89c9\u77e5\u8bc6\u7ef4\u5ea6\u5bf9\u9f50\uff0c\u5373\u5c06\u89c6\u89c9\u6548\u679c\u4e0e\u5176\u76f8\u5173\u77e5\u8bc6\u8054\u7cfb\u8d77\u6765\u3002\u89c6\u89c9\u77e5\u8bc6\u5728\u5206\u6790\u3001\u63a8\u65ad\u548c\u89e3\u91ca\u89c6\u89c9\u4fe1\u606f\u65b9\u9762\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u57fa\u4e8e\u77e5\u8bc6\u7684\u89c6\u89c9\u95ee\u9898\u7b54\u6848\u7684\u51c6\u786e\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4e3b\u8981\u63a2\u7d22\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u77e5\u8bc6\u5bf9\u9f50\u6765\u6539\u8fdb LMM\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6311\u6218\u57fa\u4e8e\u77e5\u8bc6\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba4\u77e5\u89c6\u89c9\u8bed\u8a00\u6620\u5c04\u5668\uff08CVLM\uff09\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u77e5\u8bc6\u5bf9\u9f50\u5668\uff08VKA\uff09\u548c\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u5f0f\u6307\u4ee4\u8c03\u6574\u9636\u6bb5\u7684\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u9002\u914d\u5668\uff08FKA\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u57fa\u4e8e\u5c0f\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u7f16\u7801\u5668\u4e4b\u95f4\u7684\u4ea4\u4e92\u8bbe\u8ba1\u4e86VKA\uff0c\u5728\u6536\u96c6\u7684\u56fe\u50cf\u77e5\u8bc6\u5bf9\u4e0a\u5bf9\u5176\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u5b9e\u73b0\u89c6\u89c9\u77e5\u8bc6\u83b7\u53d6\u548c\u6295\u5f71\u3002 FKA \u7528\u4e8e\u63d0\u53d6\u56fe\u50cf\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u77e5\u8bc6\u5e76\u5c06\u5176\u6ce8\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\u3002\u6211\u4eec\u5bf9\u57fa\u4e8e\u77e5\u8bc6\u7684 VQA \u57fa\u51c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e CVLM \u663e\u7740\u63d0\u9ad8\u4e86 LMM \u5728\u57fa\u4e8e\u77e5\u8bc6\u7684 VQA \u4e0a\u7684\u6027\u80fd\uff08\u5e73\u5747\u589e\u76ca 5.0%\uff09\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u5206\u522b\u9a8c\u8bc1\u4e86 VKA \u548c FKA \u7684\u6709\u6548\u6027\u3002|[2402.13561v1](http://arxiv.org/pdf/2402.13561v1)|null|\n"}, "Nerf": {"2402.13827": "|**2024-02-21**|**Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting**|\u4f7f\u7528\u805a\u7c7b\u8bc6\u522b\u4e0d\u5fc5\u8981\u7684 3D \u9ad8\u65af\u5206\u5e03\u4ee5\u5feb\u901f\u6e32\u67d3 3D \u9ad8\u65af\u5206\u5e03|Joongho Jo, Hyeongwon Kim, Jongsun Park|3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.|3D \u9ad8\u65af\u6cfc\u6e85 (3D-GS) \u662f\u4e00\u79cd\u65b0\u7684\u6e32\u67d3\u65b9\u6cd5\uff0c\u5728\u901f\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a (NeRF)\u3002 3D-GS\u5229\u7528\u6570\u767e\u4e07\u4e2a3D\u9ad8\u65af\u6765\u8868\u793a3D\u573a\u666f\uff0c\u5e76\u5c06\u8fd9\u4e9b\u9ad8\u65af\u6295\u5f71\u52302D\u56fe\u50cf\u5e73\u9762\u4e0a\u8fdb\u884c\u6e32\u67d3\u3002\u7136\u800c\uff0c\u5728\u6e32\u67d3\u8fc7\u7a0b\u4e2d\uff0c\u5f53\u524d\u89c6\u56fe\u65b9\u5411\u5b58\u5728\u5927\u91cf\u4e0d\u5fc5\u8981\u7684 3D \u9ad8\u65af\uff0c\u5bfc\u81f4\u4e0e\u5176\u8bc6\u522b\u76f8\u5173\u7684\u5927\u91cf\u8ba1\u7b97\u6210\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u7f29\u51cf\u6280\u672f\uff0c\u53ef\u4ee5\u5b9e\u65f6\u5feb\u901f\u8bc6\u522b\u4e0d\u5fc5\u8981\u7684 3D \u9ad8\u65af\uff0c\u4ee5\u5728\u4e0d\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u6e32\u67d3\u5f53\u524d\u89c6\u56fe\u3002\u8fd9\u662f\u901a\u8fc7\u5bf9\u8ddd\u79bb\u8f83\u8fd1\u7684 3D \u9ad8\u65af\u8fdb\u884c\u79bb\u7ebf\u805a\u7c7b\uff0c\u7136\u540e\u5728\u8fd0\u884c\u65f6\u5c06\u8fd9\u4e9b\u805a\u7c7b\u6295\u5f71\u5230 2D \u56fe\u50cf\u5e73\u9762\u4e0a\u6765\u5b9e\u73b0\u7684\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5206\u6790\u4e86\u5728 GPU \u4e0a\u6267\u884c\u65f6\u4e0e\u6240\u63d0\u51fa\u7684\u6280\u672f\u76f8\u5173\u7684\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u7f1d\u652f\u6301\u6240\u63d0\u51fa\u7684\u65b9\u6848\u7684\u9ad8\u6548\u786c\u4ef6\u67b6\u6784\u3002\u5bf9\u4e8e Mip-NeRF360 \u6570\u636e\u96c6\uff0c\u6240\u63d0\u51fa\u7684\u6280\u672f\u5728 2D \u56fe\u50cf\u6295\u5f71\u4e4b\u524d\u5e73\u5747\u6392\u9664 63% \u7684 3D \u9ad8\u65af\uff0c\u8fd9\u5728\u4e0d\u727a\u7272\u5cf0\u503c\u4fe1\u566a\u6bd4 (PSNR) \u7684\u60c5\u51b5\u4e0b\u5c06\u6574\u4f53\u6e32\u67d3\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e86\u8fd1 38.3%\u3002\u4e0e GPU \u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u52a0\u901f\u5668\u8fd8\u5b9e\u73b0\u4e86 10.7 \u500d\u7684\u52a0\u901f\u3002|[2402.13827v1](http://arxiv.org/pdf/2402.13827v1)|null|\n", "2402.13510": "|**2024-02-21**|**SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural Radiance Fields**|SealD-NeRF\uff1a\u901a\u8fc7\u795e\u7ecf\u8f90\u5c04\u573a\u5bf9\u52a8\u6001\u573a\u666f\u8fdb\u884c\u4ea4\u4e92\u5f0f\u50cf\u7d20\u7ea7\u7f16\u8f91|Zhentao Huang, Yukun Shi, Neil Bruce, Minglun Gong|The widespread adoption of implicit neural representations, especially Neural Radiance Fields (NeRF), highlights a growing need for editing capabilities in implicit 3D models, essential for tasks like scene post-processing and 3D content creation. Despite previous efforts in NeRF editing, challenges remain due to limitations in editing flexibility and quality. The key issue is developing a neural representation that supports local edits for real-time updates. Current NeRF editing methods, offering pixel-level adjustments or detailed geometry and color modifications, are mostly limited to static scenes. This paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level editing in dynamic settings, specifically targeting the D-NeRF network. It allows for consistent edits across sequences by mapping editing actions to a specific timeframe, freezing the deformation network responsible for dynamic scene representation, and using a teacher-student approach to integrate changes.|\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08\u5c24\u5176\u662f\u795e\u7ecf\u8f90\u5c04\u573a (NeRF)\uff09\u7684\u5e7f\u6cdb\u91c7\u7528\u51f8\u663e\u4e86\u5bf9\u9690\u5f0f 3D \u6a21\u578b\u7f16\u8f91\u529f\u80fd\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\uff0c\u8fd9\u5bf9\u4e8e\u573a\u666f\u540e\u5904\u7406\u548c 3D \u5185\u5bb9\u521b\u5efa\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u4e4b\u524d\u5728 NeRF \u7f16\u8f91\u65b9\u9762\u505a\u51fa\u4e86\u52aa\u529b\uff0c\u4f46\u7531\u4e8e\u7f16\u8f91\u7075\u6d3b\u6027\u548c\u8d28\u91cf\u7684\u9650\u5236\uff0c\u6311\u6218\u4ecd\u7136\u5b58\u5728\u3002\u5173\u952e\u95ee\u9898\u662f\u5f00\u53d1\u4e00\u79cd\u652f\u6301\u672c\u5730\u7f16\u8f91\u4ee5\u8fdb\u884c\u5b9e\u65f6\u66f4\u65b0\u7684\u795e\u7ecf\u8868\u793a\u3002\u76ee\u524d\u7684 NeRF \u7f16\u8f91\u65b9\u6cd5\u63d0\u4f9b\u50cf\u7d20\u7ea7\u8c03\u6574\u6216\u8be6\u7ec6\u7684\u51e0\u4f55\u548c\u989c\u8272\u4fee\u6539\uff0c\u4f46\u5927\u591a\u4ec5\u9650\u4e8e\u9759\u6001\u573a\u666f\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 SealD-NeRF\uff0c\u5b83\u662f Seal-3D \u7684\u6269\u5c55\uff0c\u7528\u4e8e\u52a8\u6001\u8bbe\u7f6e\u4e2d\u7684\u50cf\u7d20\u7ea7\u7f16\u8f91\uff0c\u4e13\u95e8\u9488\u5bf9 D-NeRF \u7f51\u7edc\u3002\u5b83\u901a\u8fc7\u5c06\u7f16\u8f91\u64cd\u4f5c\u6620\u5c04\u5230\u7279\u5b9a\u65f6\u95f4\u8303\u56f4\u3001\u51bb\u7ed3\u8d1f\u8d23\u52a8\u6001\u573a\u666f\u8868\u793a\u7684\u53d8\u5f62\u7f51\u7edc\u4ee5\u53ca\u4f7f\u7528\u5e08\u751f\u65b9\u6cd5\u6765\u96c6\u6210\u66f4\u6539\uff0c\u5141\u8bb8\u8de8\u5e8f\u5217\u8fdb\u884c\u4e00\u81f4\u7684\u7f16\u8f91\u3002|[2402.13510v1](http://arxiv.org/pdf/2402.13510v1)|null|\n"}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.13929": "|**2024-02-21**|**SDXL-Lightning: Progressive Adversarial Diffusion Distillation**|SDXL-Lightning\uff1a\u6e10\u8fdb\u5f0f\u5bf9\u6297\u6269\u6563\u84b8\u998f|Shanchuan Lin, Anran Wang, Xiao Yang|We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u6563\u84b8\u998f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u4e8e SDXL \u7684\u4e00\u6b65/\u5c11\u6b65 1024px \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u6e10\u8fdb\u5f0f\u548c\u5bf9\u6297\u5f0f\u84b8\u998f\uff0c\u4ee5\u5b9e\u73b0\u8d28\u91cf\u548c\u6a21\u5f0f\u8986\u76d6\u8303\u56f4\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u7406\u8bba\u5206\u6790\u3001\u9274\u522b\u5668\u8bbe\u8ba1\u3001\u6a21\u578b\u5236\u5b9a\u548c\u8bad\u7ec3\u6280\u672f\u3002\u6211\u4eec\u5c06\u7ecf\u8fc7\u7cbe\u70bc\u7684 SDXL-Lightning \u6a21\u578b\u4f5c\u4e3a LoRA \u548c\u5b8c\u6574\u7684 UNet \u6743\u91cd\u8fdb\u884c\u5f00\u6e90\u3002|[2402.13929v1](http://arxiv.org/pdf/2402.13929v1)|null|\n", "2402.13822": "|**2024-02-21**|**MSTAR: Multi-Scale Backbone Architecture Search for Timeseries Classification**|MSTAR\uff1a\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u591a\u5c3a\u5ea6\u9aa8\u5e72\u67b6\u6784\u641c\u7d22|Tue M. Cao, Nhat H. Tran, Hieu H. Pham, Hung T. Nguyen, Le P. Nguyen|Most of the previous approaches to Time Series Classification (TSC) highlight the significance of receptive fields and frequencies while overlooking the time resolution. Hence, unavoidably suffered from scalability issues as they integrated an extensive range of receptive fields into classification models. Other methods, while having a better adaptation for large datasets, require manual design and yet not being able to reach the optimal architecture due to the uniqueness of each dataset. We overcome these challenges by proposing a novel multi-scale search space and a framework for Neural architecture search (NAS), which addresses both the problem of frequency and time resolution, discovering the suitable scale for a specific dataset. We further show that our model can serve as a backbone to employ a powerful Transformer module with both untrained and pre-trained weights. Our search space reaches the state-of-the-art performance on four datasets on four different domains while introducing more than ten highly fine-tuned models for each data.|\u4e4b\u524d\u7684\u5927\u591a\u6570\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff08TSC\uff09\u65b9\u6cd5\u90fd\u5f3a\u8c03\u611f\u53d7\u91ce\u548c\u9891\u7387\u7684\u91cd\u8981\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u65f6\u95f4\u5206\u8fa8\u7387\u3002\u56e0\u6b64\uff0c\u5f53\u4ed6\u4eec\u5c06\u5e7f\u6cdb\u7684\u611f\u53d7\u91ce\u96c6\u6210\u5230\u5206\u7c7b\u6a21\u578b\u4e2d\u65f6\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u9047\u5230\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002\u5176\u4ed6\u65b9\u6cd5\u867d\u7136\u5bf9\u5927\u578b\u6570\u636e\u96c6\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4f46\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e2a\u6570\u636e\u96c6\u7684\u72ec\u7279\u6027\u800c\u65e0\u6cd5\u8fbe\u5230\u6700\u4f73\u67b6\u6784\u3002\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u641c\u7d22\u7a7a\u95f4\u548c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u5b83\u89e3\u51b3\u4e86\u9891\u7387\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u95ee\u9898\uff0c\u53d1\u73b0\u4e86\u7279\u5b9a\u6570\u636e\u96c6\u7684\u5408\u9002\u5c3a\u5ea6\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u4f7f\u7528\u5f3a\u5927\u7684 Transformer \u6a21\u5757\u7684\u9aa8\u5e72\uff0c\u8be5\u6a21\u5757\u5177\u6709\u672a\u7ecf\u8bad\u7ec3\u548c\u9884\u8bad\u7ec3\u7684\u6743\u91cd\u3002\u6211\u4eec\u7684\u641c\u7d22\u7a7a\u95f4\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e3a\u6bcf\u4e2a\u6570\u636e\u5f15\u5165\u4e86\u5341\u591a\u4e2a\u9ad8\u5ea6\u5fae\u8c03\u7684\u6a21\u578b\u3002|[2402.13822v1](http://arxiv.org/pdf/2402.13822v1)|null|\n", "2402.13497": "|**2024-02-21**|**Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization**|\u901a\u8fc7\u4e00\u81f4\u6027\u6b63\u5219\u5316\u5c06\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u63a8\u5411\u5168\u7cbe\u5ea6\u6027\u80fd|Junbiao Pang, Tianyang Cai, Baochang Zhang, Jiaqi Wu, Ye Tao|Existing Quantization-Aware Training (QAT) methods intensively depend on the complete labeled dataset or knowledge distillation to guarantee the performances toward Full Precision (FP) accuracies. However, empirical results show that QAT still has inferior results compared to its FP counterpart. One question is how to push QAT toward or even surpass FP performances. In this paper, we address this issue from a new perspective by injecting the vicinal data distribution information to improve the generalization performances of QAT effectively. We present a simple, novel, yet powerful method introducing an Consistency Regularization (CR) for QAT. Concretely, CR assumes that augmented samples should be consistent in the latent feature space. Our method generalizes well to different network architectures and various QAT methods. Extensive experiments demonstrate that our approach significantly outperforms the current state-of-the-art QAT methods and even FP counterparts.|\u73b0\u6709\u7684\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u65b9\u6cd5\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u5b8c\u6574\u7684\u6807\u8bb0\u6570\u636e\u96c6\u6216\u77e5\u8bc6\u84b8\u998f\u6765\u4fdd\u8bc1\u5168\u7cbe\u5ea6\uff08FP\uff09\u7cbe\u5ea6\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u4e0e FP \u76f8\u6bd4\uff0cQAT \u7684\u7ed3\u679c\u4ecd\u7136\u8f83\u5dee\u3002\u4e00\u4e2a\u95ee\u9898\u662f\u5982\u4f55\u63a8\u52a8 QAT \u63a5\u8fd1\u751a\u81f3\u8d85\u8d8a FP \u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u4e00\u4e2a\u65b0\u7684\u89d2\u5ea6\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u901a\u8fc7\u6ce8\u5165\u90bb\u8fd1\u6570\u636e\u5206\u5e03\u4fe1\u606f\u6765\u6709\u6548\u63d0\u9ad8QAT\u7684\u6cdb\u5316\u6027\u80fd\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u65b0\u9896\u4f46\u529f\u80fd\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u4e3a QAT \u5f15\u5165\u4e00\u81f4\u6027\u6b63\u5219\u5316 (CR)\u3002\u5177\u4f53\u6765\u8bf4\uff0cCR \u5047\u8bbe\u589e\u5f3a\u6837\u672c\u5728\u6f5c\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5e94\u8be5\u662f\u4e00\u81f4\u7684\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5f88\u597d\u5730\u63a8\u5e7f\u5230\u4e0d\u540c\u7684\u7f51\u7edc\u67b6\u6784\u548c\u5404\u79cd QAT \u65b9\u6cd5\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684 QAT \u65b9\u6cd5\uff0c\u751a\u81f3 FP \u65b9\u6cd5\u3002|[2402.13497v1](http://arxiv.org/pdf/2402.13497v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.13955": "|**2024-02-21**|**BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions**|BEE-NET\uff1a\u4e00\u79cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u8bc6\u522b\u91ce\u5916\u8eab\u4f53\u60c5\u7eea\u8868\u8fbe|Mohammad Mahdi Dehshibi, David Masip|In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language. To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET. We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process. Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space. Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation. To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE). Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%.|\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u73af\u5883\u56e0\u7d20\uff0c\u7279\u522b\u662f\u6240\u6d89\u53ca\u7684\u573a\u666f\u548c\u7269\u4f53\uff0c\u5982\u4f55\u901a\u8fc7\u80a2\u4f53\u8bed\u8a00\u5f71\u54cd\u60c5\u7eea\u7684\u8868\u8fbe\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6d41\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u540d\u4e3a BEE-NET\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u540e\u671f\u878d\u5408\u7b56\u7565\uff0c\u5c06\u5730\u70b9\u548c\u7269\u4f53\u7684\u5143\u4fe1\u606f\u5408\u5e76\u4e3a\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6982\u7387\u6c60\u6a21\u578b\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6765\u751f\u6210\u6f5c\u5728\u7a7a\u95f4\u4e2d\u53ef\u7528\u548c\u9884\u671f\u4e0d\u53ef\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u8054\u5408\u6982\u7387\u5206\u5e03\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u878d\u5408\u7b56\u7565\u662f\u53ef\u5fae\u5206\u7684\uff0c\u5141\u8bb8\u7aef\u5230\u7aef\u8bad\u7ec3\u548c\u6355\u83b7\u6570\u636e\u70b9\u4e4b\u95f4\u9690\u85cf\u7684\u5173\u8054\uff0c\u800c\u4e0d\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u540e\u5904\u7406\u6216\u6b63\u5219\u5316\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u7684\u6df1\u5ea6\u6a21\u578b\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u8eab\u4f53\u8bed\u8a00\u6570\u636e\u5e93\uff08BoLD\uff09\uff0c\u5b83\u662f\u76ee\u524d\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u91ce\u5916\u8eab\u4f53\u60c5\u7eea\u8868\u8fbe\uff08AIBEE\uff09\u7684\u6700\u5927\u53ef\u7528\u6570\u636e\u5e93\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4 AIBEE \u76ee\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u9886\u5148 2.07%\uff0c\u60c5\u611f\u8bc6\u522b\u5f97\u5206\u8fbe\u5230 66.33%\u3002|[2402.13955v1](http://arxiv.org/pdf/2402.13955v1)|null|\n", "2402.13918": "|**2024-02-21**|**BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery**|BenchCloudVision\uff1a\u9065\u611f\u56fe\u50cf\u4e91\u68c0\u6d4b\u548c\u5206\u5272\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u57fa\u51c6\u5206\u6790|Loddo Fabio, Dario Piga, Michelucci Umberto, El Ghazouali Safouane|Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline. This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones. To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed. Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations. The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets. This benchmark can be reproduced using the material from this github link: \\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}.|\u914d\u5907\u5149\u5b66\u4f20\u611f\u5668\u7684\u536b\u661f\u53ef\u6355\u83b7\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u4e3a\u5404\u79cd\u73af\u5883\u73b0\u8c61\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002\u8fd1\u5e74\u6765\uff0c\u9488\u5bf9\u9065\u611f\u9886\u57df\u7684\u4e00\u4e9b\u6311\u6218\u7684\u7814\u7a76\u6fc0\u589e\uff0c\u4ece\u4e0d\u540c\u666f\u89c2\u4e2d\u7684\u6c34\u68c0\u6d4b\u5230\u5c71\u533a\u548c\u5730\u5f62\u7684\u5206\u5272\u3002\u6b63\u5728\u8fdb\u884c\u7684\u7814\u7a76\u76ee\u6807\u662f\u63d0\u9ad8\u536b\u661f\u56fe\u50cf\u5206\u6790\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002\u7279\u522b\u662f\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u91cd\u89c6\u5f00\u53d1\u51c6\u786e\u7684\u6c34\u4f53\u68c0\u6d4b\u3001\u96ea\u548c\u4e91\u7684\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u4e8e\u73af\u5883\u76d1\u6d4b\u3001\u8d44\u6e90\u7ba1\u7406\u548c\u707e\u5bb3\u54cd\u5e94\u975e\u5e38\u91cd\u8981\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u672c\u6587\u91cd\u70b9\u7814\u7a76\u9065\u611f\u56fe\u50cf\u7684\u4e91\u5206\u5272\u3002\u7531\u4e8e\u57fa\u4e8e\u5149\u5b66\u4f20\u611f\u5668\u7684\u5e94\u7528\u4e2d\u5b58\u5728\u4e91\uff0c\u51c6\u786e\u7684\u9065\u611f\u6570\u636e\u5206\u6790\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\u3002\u5e94\u7528\u548c\u7814\u7a76\u7b49\u6700\u7ec8\u4ea7\u54c1\u7684\u8d28\u91cf\u76f4\u63a5\u53d7\u5230\u4e91\u68c0\u6d4b\u7684\u5f71\u54cd\uff0c\u4e91\u68c0\u6d4b\u5728\u9065\u611f\u6570\u636e\u5904\u7406\u6d41\u7a0b\u4e2d\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u3002\u672c\u6587\u7814\u7a76\u4e86\u5e94\u7528\u4e8e\u4e91\u8bc6\u522b\u7684\u4e03\u79cd\u5c16\u7aef\u8bed\u4e49\u5206\u5272\u548c\u68c0\u6d4b\u7b97\u6cd5\uff0c\u8fdb\u884c\u57fa\u51c6\u5206\u6790\u4ee5\u8bc4\u4f30\u5176\u67b6\u6784\u65b9\u6cd5\u5e76\u786e\u5b9a\u6027\u80fd\u6700\u4f73\u7684\u65b9\u6cd5\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u5bf9\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684\u56fe\u50cf\u7c7b\u578b\u548c\u5149\u8c31\u5e26\u6570\u91cf\u7b49\u5173\u952e\u8981\u7d20\u8fdb\u884c\u4e86\u5206\u6790\u3002\u6b64\u5916\uff0c\u8fd9\u9879\u7814\u7a76\u8fd8\u5c1d\u8bd5\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ec5\u4f7f\u7528\u5c11\u6570\u5149\u8c31\u5e26\u200b\u200b\uff08\u5305\u62ec RGB \u548c RGBN-IR \u7ec4\u5408\uff09\u5373\u53ef\u6267\u884c\u4e91\u5206\u5272\u3002\u8be5\u6a21\u578b\u9488\u5bf9\u5404\u79cd\u5e94\u7528\u548c\u7528\u6237\u573a\u666f\u7684\u7075\u6d3b\u6027\u662f\u901a\u8fc7\u4f7f\u7528 Sentinel-2 \u548c Landsat-8 \u7684\u56fe\u50cf\u4f5c\u4e3a\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u7684\u3002\u53ef\u4ee5\u4f7f\u7528\u6b64 github \u94fe\u63a5\u4e2d\u7684\u6750\u6599\u590d\u5236\u6b64\u57fa\u51c6\uff1a\\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}\u3002|[2402.13918v1](http://arxiv.org/pdf/2402.13918v1)|**[link](https://github.com/toelt-llc/cloud_segmentation_comparative)**|\n", "2402.13848": "|**2024-02-21**|**Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps**|\u96f6 BEV\uff1a\u4efb\u4f55\u7b2c\u4e00\u4eba\u79f0\u6a21\u5f0f\u5230 BEV \u5730\u56fe\u7684\u96f6\u955c\u5934\u6295\u5f71|Gianluca Monaci, Leonid Antsfeld, Boris Chidlovskii, Christian Wolf|Bird's-eye view (BEV) maps are an important geometrically structured representation widely used in robotics, in particular self-driving vehicles and terrestrial robots. Existing algorithms either require depth information for the geometric projection, which is not always reliably available, or are trained end-to-end in a fully supervised way to map visual first-person observations to BEV representation, and are therefore restricted to the output modality they have been trained for. In contrast, we propose a new model capable of performing zero-shot projections of any modality available in a first person view to the corresponding BEV map. This is achieved by disentangling the geometric inverse perspective projection from the modality transformation, eg. RGB to occupancy. The method is general and we showcase experiments projecting to BEV three different modalities: semantic segmentation, motion vectors and object bounding boxes detected in first person. We experimentally show that the model outperforms competing methods, in particular the widely used baseline resorting to monocular depth estimation.|\u9e1f\u77b0\u56fe (BEV) \u5730\u56fe\u662f\u4e00\u79cd\u91cd\u8981\u7684\u51e0\u4f55\u7ed3\u6784\u8868\u793a\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u6280\u672f\uff0c\u7279\u522b\u662f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u5730\u9762\u673a\u5668\u4eba\u3002\u73b0\u6709\u7b97\u6cd5\u8981\u4e48\u9700\u8981\u51e0\u4f55\u6295\u5f71\u7684\u6df1\u5ea6\u4fe1\u606f\uff08\u8be5\u4fe1\u606f\u5e76\u4e0d\u603b\u662f\u53ef\u9760\u53ef\u7528\uff09\uff0c\u8981\u4e48\u4ee5\u5b8c\u5168\u76d1\u7763\u7684\u65b9\u5f0f\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4ee5\u5c06\u89c6\u89c9\u7b2c\u4e00\u4eba\u79f0\u89c2\u5bdf\u6620\u5c04\u5230 BEV \u8868\u793a\uff0c\u56e0\u6b64\u4ec5\u9650\u4e8e\u8f93\u51fa\u6a21\u6001\u4ed6\u4eec\u5df2\u7ecf\u63a5\u53d7\u8fc7\u57f9\u8bad\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6a21\u578b\uff0c\u80fd\u591f\u5c06\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4e2d\u53ef\u7528\u7684\u4efb\u4f55\u6a21\u6001\u6267\u884c\u96f6\u6837\u672c\u6295\u5f71\u5230\u76f8\u5e94\u7684 BEV \u5730\u56fe\u3002\u8fd9\u662f\u901a\u8fc7\u5c06\u51e0\u4f55\u9006\u900f\u89c6\u6295\u5f71\u4e0e\u6a21\u6001\u53d8\u6362\u5206\u5f00\u6765\u5b9e\u73b0\u7684\uff0c\u4f8b\u5982\u3002 RGB \u5360\u7528\u3002\u8be5\u65b9\u6cd5\u662f\u901a\u7528\u7684\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6295\u5c04\u5230 BEV \u4e09\u79cd\u4e0d\u540c\u6a21\u5f0f\u7684\u5b9e\u9a8c\uff1a\u8bed\u4e49\u5206\u5272\u3001\u8fd0\u52a8\u5411\u91cf\u548c\u7b2c\u4e00\u4eba\u79f0\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u8fb9\u754c\u6846\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u91c7\u7528\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u57fa\u7ebf\u3002|[2402.13848v1](http://arxiv.org/pdf/2402.13848v1)|null|\n", "2402.13778": "|**2024-02-21**|**Weakly supervised localisation of prostate cancer using reinforcement learning for bi-parametric MR images**|\u4f7f\u7528\u53cc\u53c2\u6570 MR \u56fe\u50cf\u7684\u5f3a\u5316\u5b66\u4e60\u5bf9\u524d\u5217\u817a\u764c\u8fdb\u884c\u5f31\u76d1\u7763\u5b9a\u4f4d|Martynas Pocius, Wen Yan, Dean C. Barratt, Mark Emberton, Matthew J. Clarkson, Yipeng Hu, Shaheer U. Saeed|In this paper we propose a reinforcement learning based weakly supervised system for localisation. We train a controller function to localise regions of interest within an image by introducing a novel reward definition that utilises non-binarised classification probability, generated by a pre-trained binary classifier which classifies object presence in images or image crops. The object-presence classifier may then inform the controller of its localisation quality by quantifying the likelihood of the image containing an object. Such an approach allows us to minimize any potential labelling or human bias propagated via human labelling for fully supervised localisation. We evaluate our proposed approach for a task of cancerous lesion localisation on a large dataset of real clinical bi-parametric MR images of the prostate. Comparisons to the commonly used multiple-instance learning weakly supervised localisation and to a fully supervised baseline show that our proposed method outperforms the multi-instance learning and performs comparably to fully-supervised learning, using only image-level classification labels for training.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5f31\u76d1\u7763\u5b9a\u4f4d\u7cfb\u7edf\u3002\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u5956\u52b1\u5b9a\u4e49\u6765\u8bad\u7ec3\u63a7\u5236\u5668\u51fd\u6570\u6765\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u611f\u5174\u8da3\u533a\u57df\uff0c\u8be5\u5956\u52b1\u5b9a\u4e49\u5229\u7528\u975e\u4e8c\u503c\u5316\u5206\u7c7b\u6982\u7387\uff0c\u8be5\u6982\u7387\u7531\u9884\u5148\u8bad\u7ec3\u7684\u4e8c\u5143\u5206\u7c7b\u5668\u751f\u6210\uff0c\u8be5\u5206\u7c7b\u5668\u5bf9\u56fe\u50cf\u6216\u56fe\u50cf\u4f5c\u7269\u4e2d\u7684\u5bf9\u8c61\u5b58\u5728\u8fdb\u884c\u5206\u7c7b\u3002\u7136\u540e\uff0c\u5bf9\u8c61\u5b58\u5728\u5206\u7c7b\u5668\u53ef\u4ee5\u901a\u8fc7\u91cf\u5316\u56fe\u50cf\u5305\u542b\u5bf9\u8c61\u7684\u53ef\u80fd\u6027\u6765\u901a\u77e5\u63a7\u5236\u5668\u5176\u5b9a\u4f4d\u8d28\u91cf\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6211\u4eec\u80fd\u591f\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4efb\u4f55\u6f5c\u5728\u7684\u6807\u7b7e\u6216\u901a\u8fc7\u4eba\u7c7b\u6807\u7b7e\u4f20\u64ad\u7684\u4eba\u7c7b\u504f\u89c1\uff0c\u4ee5\u5b9e\u73b0\u5b8c\u5168\u76d1\u7763\u7684\u672c\u5730\u5316\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u5728\u524d\u5217\u817a\u771f\u5b9e\u4e34\u5e8a\u53cc\u53c2\u6570 MR \u56fe\u50cf\u7684\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u764c\u6027\u75c5\u53d8\u5b9a\u4f4d\u4efb\u52a1\u7684\u65b9\u6cd5\u3002\u4e0e\u5e38\u7528\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u5f31\u76d1\u7763\u5b9a\u4f4d\u548c\u5b8c\u5168\u76d1\u7763\u57fa\u7ebf\u7684\u6bd4\u8f83\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u591a\u5b9e\u4f8b\u5b66\u4e60\uff0c\u5e76\u4e14\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u5206\u7c7b\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5176\u6027\u80fd\u4e0e\u5b8c\u5168\u76d1\u7763\u5b66\u4e60\u76f8\u5f53\u3002|[2402.13778v1](http://arxiv.org/pdf/2402.13778v1)|null|\n", "2402.13771": "|**2024-02-21**|**Mask-up: Investigating Biases in Face Re-identification for Masked Faces**|\u8499\u9762\uff1a\u8c03\u67e5\u8499\u9762\u4eba\u8138\u91cd\u65b0\u8bc6\u522b\u4e2d\u7684\u504f\u5dee|Siddharth D Jaiswal, Ankit Kr. Verma, Animesh Mukherjee|AI based Face Recognition Systems (FRSs) are now widely distributed and deployed as MLaaS solutions all over the world, moreso since the COVID-19 pandemic for tasks ranging from validating individuals' faces while buying SIM cards to surveillance of citizens. Extensive biases have been reported against marginalized groups in these systems and have led to highly discriminatory outcomes. The post-pandemic world has normalized wearing face masks but FRSs have not kept up with the changing times. As a result, these systems are susceptible to mask based face occlusion. In this study, we audit four commercial and nine open-source FRSs for the task of face re-identification between different varieties of masked and unmasked images across five benchmark datasets (total 14,722 images). These simulate a realistic validation/surveillance task as deployed in all major countries around the world. Three of the commercial and five of the open-source FRSs are highly inaccurate; they further perpetuate biases against non-White individuals, with the lowest accuracy being 0%. A survey for the same task with 85 human participants also results in a low accuracy of 40%. Thus a human-in-the-loop moderation in the pipeline does not alleviate the concerns, as has been frequently hypothesized in literature. Our large-scale study shows that developers, lawmakers and users of such services need to rethink the design principles behind FRSs, especially for the task of face re-identification, taking cognizance of observed biases.|\u57fa\u4e8e AI \u7684\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf (FRS) \u73b0\u5df2\u4f5c\u4e3a MLaaS \u89e3\u51b3\u65b9\u6848\u5728\u4e16\u754c\u5404\u5730\u5e7f\u6cdb\u5206\u5e03\u548c\u90e8\u7f72\uff0c\u81ea COVID-19 \u5927\u6d41\u884c\u4ee5\u6765\uff0c\u5176\u4efb\u52a1\u8303\u56f4\u5305\u62ec\u4ece\u8d2d\u4e70 SIM \u5361\u65f6\u9a8c\u8bc1\u4e2a\u4eba\u9762\u90e8\u5230\u76d1\u89c6\u516c\u6c11\u7b49\u4efb\u52a1\u3002\u636e\u62a5\u9053\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u4e2d\u5b58\u5728\u9488\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u5e7f\u6cdb\u504f\u89c1\uff0c\u5e76\u5bfc\u81f4\u4e86\u9ad8\u5ea6\u6b67\u89c6\u6027\u7684\u7ed3\u679c\u3002\u5927\u6d41\u884c\u540e\u7684\u4e16\u754c\u5df2\u4f7f\u6234\u53e3\u7f69\u6210\u4e3a\u5e38\u6001\uff0c\u4f46 FRS \u5374\u672a\u80fd\u8ddf\u4e0a\u65f6\u4ee3\u7684\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u57fa\u4e8e\u9762\u7f69\u7684\u9762\u90e8\u906e\u6321\u7684\u5f71\u54cd\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5ba1\u6838\u4e86 4 \u4e2a\u5546\u4e1a FRS \u548c 9 \u4e2a\u5f00\u6e90 FRS\uff0c\u4ee5\u5b8c\u6210\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u603b\u5171 14,722 \u5f20\u56fe\u50cf\uff09\u4e2d\u4e0d\u540c\u79cd\u7c7b\u7684\u8499\u7248\u548c\u672a\u8499\u7248\u56fe\u50cf\u4e4b\u95f4\u7684\u4eba\u8138\u91cd\u65b0\u8bc6\u522b\u4efb\u52a1\u3002\u8fd9\u4e9b\u6a21\u62df\u4e86\u5728\u4e16\u754c\u6240\u6709\u4e3b\u8981\u56fd\u5bb6\u90e8\u7f72\u7684\u73b0\u5b9e\u9a8c\u8bc1/\u76d1\u89c6\u4efb\u52a1\u3002\u5176\u4e2d 3 \u4e2a\u5546\u4e1a FRS \u548c 5 \u4e2a\u5f00\u6e90 FRS \u975e\u5e38\u4e0d\u51c6\u786e\uff1b\u4ed6\u4eec\u8fdb\u4e00\u6b65\u5ef6\u7eed\u4e86\u5bf9\u975e\u767d\u4eba\u7684\u504f\u89c1\uff0c\u6700\u4f4e\u51c6\u786e\u7387\u4e3a 0%\u3002\u5bf9 85 \u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u7684\u540c\u4e00\u4efb\u52a1\u7684\u8c03\u67e5\u4e5f\u5f97\u51fa 40% \u7684\u4f4e\u51c6\u786e\u5ea6\u3002\u56e0\u6b64\uff0c\u6b63\u5982\u6587\u732e\u4e2d\u7ecf\u5e38\u5047\u8bbe\u7684\u90a3\u6837\uff0c\u7ba1\u9053\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u8c03\u8282\u5e76\u4e0d\u80fd\u51cf\u8f7b\u4eba\u4eec\u7684\u62c5\u5fe7\u3002\u6211\u4eec\u7684\u5927\u89c4\u6a21\u7814\u7a76\u8868\u660e\uff0c\u6b64\u7c7b\u670d\u52a1\u7684\u5f00\u53d1\u8005\u3001\u7acb\u6cd5\u8005\u548c\u7528\u6237\u9700\u8981\u91cd\u65b0\u601d\u8003 FRS \u80cc\u540e\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9762\u90e8\u91cd\u65b0\u8bc6\u522b\u4efb\u52a1\uff0c\u5e76\u8ba4\u8bc6\u5230\u89c2\u5bdf\u5230\u7684\u504f\u5dee\u3002|[2402.13771v1](http://arxiv.org/pdf/2402.13771v1)|null|\n", "2402.13699": "|**2024-02-21**|**Explainable Classification Techniques for Quantum Dot Device Measurements**|\u91cf\u5b50\u70b9\u5668\u4ef6\u6d4b\u91cf\u7684\u53ef\u89e3\u91ca\u5206\u7c7b\u6280\u672f|Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak|In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.|\u5728\u7269\u7406\u79d1\u5b66\u4e2d\uff0c\u5bf9\u56fe\u50cf\u6570\u636e\u7684\u9c81\u68d2\u7279\u5f81\u8868\u793a\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u52a0\uff1a\u5e7f\u4e49\u4e0a\u7684\u4e8c\u7ef4\u6570\u636e\u7684\u56fe\u50cf\u91c7\u96c6\u73b0\u5728\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8bb8\u591a\u9886\u57df\uff0c\u5305\u62ec\u6211\u4eec\u5728\u8fd9\u91cc\u8003\u8651\u7684\u91cf\u5b50\u4fe1\u606f\u79d1\u5b66\u3002\u867d\u7136\u4f20\u7edf\u7684\u56fe\u50cf\u7279\u5f81\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5b83\u4eec\u7684\u4f7f\u7528\u6b63\u5728\u8fc5\u901f\u88ab\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6280\u672f\u6240\u53d6\u4ee3\uff0c\u8fd9\u4e9b\u6280\u672f\u901a\u5e38\u4f1a\u727a\u7272\u53ef\u89e3\u91ca\u6027\u6765\u6362\u53d6\u9ad8\u7cbe\u5ea6\u3002\u4e3a\u4e86\u6539\u5584\u8fd9\u79cd\u6743\u8861\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u7684\u5408\u6210\u6280\u672f\uff0c\u53ef\u4ee5\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u3002\u6211\u4eec\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u63d0\u5347\u673a\uff08EBM\uff09\u8bc1\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u53ef\u89e3\u91ca\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8868\u660e\u8be5\u6280\u672f\u5728\u91cf\u5b50\u70b9\u8c03\u8c10\u7684\u80cc\u666f\u4e0b\u5177\u6709\u6709\u610f\u4e49\u7684\u597d\u5904\uff0c\u5728\u5f53\u524d\u7684\u53d1\u5c55\u9636\u6bb5\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3002|[2402.13699v1](http://arxiv.org/pdf/2402.13699v1)|null|\n", "2402.13697": "|**2024-02-21**|**Generalizable Semantic Vision Query Generation for Zero-shot Panoptic and Semantic Segmentation**|\u7528\u4e8e\u96f6\u6837\u672c\u5168\u666f\u548c\u8bed\u4e49\u5206\u5272\u7684\u53ef\u6cdb\u5316\u8bed\u4e49\u89c6\u89c9\u67e5\u8be2\u751f\u6210|Jialei Chen, Daisuke Deguchi, Chenkai Zhang, Hiroshi Murase|Zero-shot Panoptic Segmentation (ZPS) aims to recognize foreground instances and background stuff without images containing unseen categories in training. Due to the visual data sparsity and the difficulty of generalizing from seen to unseen categories, this task remains challenging. To better generalize to unseen classes, we propose Conditional tOken aligNment and Cycle trAnsiTion (CONCAT), to produce generalizable semantic vision queries. First, a feature extractor is trained by CON to link the vision and semantics for providing target queries. Formally, CON is proposed to align the semantic queries with the CLIP visual CLS token extracted from complete and masked images. To address the lack of unseen categories, a generator is required. However, one of the gaps in synthesizing pseudo vision queries, ie, vision queries for unseen categories, is describing fine-grained visual details through semantic embeddings. Therefore, we approach CAT to train the generator in semantic-vision and vision-semantic manners. In semantic-vision, visual query contrast is proposed to model the high granularity of vision by pulling the pseudo vision queries with the corresponding targets containing segments while pushing those without segments away. To ensure the generated queries retain semantic information, in vision-semantic, the pseudo vision queries are mapped back to semantic and supervised by real semantic embeddings. Experiments on ZPS achieve a 5.2% hPQ increase surpassing SOTA. We also examine inductive ZPS and open-vocabulary semantic segmentation and obtain comparative results while being 2 times faster in testing.|\u96f6\u955c\u5934\u5168\u666f\u5206\u5272\uff08ZPS\uff09\u65e8\u5728\u8bc6\u522b\u524d\u666f\u5b9e\u4f8b\u548c\u80cc\u666f\u5185\u5bb9\uff0c\u800c\u65e0\u9700\u5728\u8bad\u7ec3\u4e2d\u5305\u542b\u5305\u542b\u672a\u89c1\u8fc7\u7c7b\u522b\u7684\u56fe\u50cf\u3002\u7531\u4e8e\u89c6\u89c9\u6570\u636e\u7684\u7a00\u758f\u6027\u4ee5\u53ca\u4ece\u53ef\u89c1\u7c7b\u522b\u63a8\u5e7f\u5230\u4e0d\u53ef\u89c1\u7c7b\u522b\u7684\u56f0\u96be\uff0c\u8fd9\u9879\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7c7b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6761\u4ef6\u6807\u8bb0\u5bf9\u9f50\u548c\u5faa\u73af\u8f6c\u6362\uff08CONCAT\uff09\uff0c\u4ee5\u4ea7\u751f\u53ef\u6cdb\u5316\u7684\u8bed\u4e49\u89c6\u89c9\u67e5\u8be2\u3002\u9996\u5148\uff0c\u7279\u5f81\u63d0\u53d6\u5668\u7ecf\u8fc7 CON \u8bad\u7ec3\uff0c\u5c06\u89c6\u89c9\u548c\u8bed\u4e49\u8054\u7cfb\u8d77\u6765\uff0c\u4ee5\u63d0\u4f9b\u76ee\u6807\u67e5\u8be2\u3002\u6b63\u5f0f\u5730\uff0cCON \u88ab\u63d0\u8bae\u5c06\u8bed\u4e49\u67e5\u8be2\u4e0e\u4ece\u5b8c\u6574\u56fe\u50cf\u548c\u63a9\u6a21\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684 CLIP \u89c6\u89c9 CLS \u6807\u8bb0\u5bf9\u9f50\u3002\u4e3a\u4e86\u89e3\u51b3\u770b\u4e0d\u89c1\u7684\u7c7b\u522b\u7684\u7f3a\u4e4f\uff0c\u9700\u8981\u4e00\u4e2a\u751f\u6210\u5668\u3002\u7136\u800c\uff0c\u5408\u6210\u4f2a\u89c6\u89c9\u67e5\u8be2\uff08\u5373\u9488\u5bf9\u672a\u89c1\u8fc7\u7684\u7c7b\u522b\u7684\u89c6\u89c9\u67e5\u8be2\uff09\u7684\u5dee\u8ddd\u4e4b\u4e00\u662f\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u63cf\u8ff0\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u7ec6\u8282\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u91c7\u7528 CAT \u4ee5\u8bed\u4e49-\u89c6\u89c9\u548c\u89c6\u89c9-\u8bed\u4e49\u7684\u65b9\u5f0f\u8bad\u7ec3\u751f\u6210\u5668\u3002\u5728\u8bed\u4e49\u89c6\u89c9\u4e2d\uff0c\u63d0\u51fa\u4e86\u89c6\u89c9\u67e5\u8be2\u5bf9\u6bd4\uff0c\u901a\u8fc7\u62c9\u52a8\u5305\u542b\u7247\u6bb5\u7684\u76f8\u5e94\u76ee\u6807\u7684\u4f2a\u89c6\u89c9\u67e5\u8be2\uff0c\u540c\u65f6\u5c06\u4e0d\u5e26\u7247\u6bb5\u7684\u76ee\u6807\u63a8\u5f00\uff0c\u6765\u5bf9\u89c6\u89c9\u7684\u9ad8\u7c92\u5ea6\u8fdb\u884c\u5efa\u6a21\u3002\u4e3a\u4e86\u786e\u4fdd\u751f\u6210\u7684\u67e5\u8be2\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\uff0c\u5728\u89c6\u89c9\u8bed\u4e49\u4e2d\uff0c\u4f2a\u89c6\u89c9\u67e5\u8be2\u88ab\u6620\u5c04\u56de\u8bed\u4e49\u5e76\u7531\u771f\u5b9e\u8bed\u4e49\u5d4c\u5165\u8fdb\u884c\u76d1\u7763\u3002 ZPS \u7684\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u8d85\u8fc7 SOTA \u7684 5.2% hPQ \u63d0\u5347\u3002\u6211\u4eec\u8fd8\u68c0\u67e5\u4e86\u5f52\u7eb3 ZPS \u548c\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff0c\u5e76\u83b7\u5f97\u4e86\u6bd4\u8f83\u7ed3\u679c\uff0c\u540c\u65f6\u6d4b\u8bd5\u901f\u5ea6\u63d0\u9ad8\u4e86 2 \u500d\u3002|[2402.13697v1](http://arxiv.org/pdf/2402.13697v1)|null|\n", "2402.13651": "|**2024-02-21**|**Robustness of Deep Neural Networks for Micro-Doppler Radar Classification**|\u5fae\u591a\u666e\u52d2\u96f7\u8fbe\u5206\u7c7b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u6027|Mikolaj Czerkawski, Carmine Clemente, Craig MichieCraig Michie, Christos Tachtatzis|With the great capabilities of deep classifiers for radar data processing come the risks of learning dataset-specific features that do not generalize well. In this work, the robustness of two deep convolutional architectures, trained and tested on the same data, is evaluated. When standard training practice is followed, both classifiers exhibit sensitivity to subtle temporal shifts of the input representation, an augmentation that carries minimal semantic content. Furthermore, the models are extremely susceptible to adversarial examples. Both small temporal shifts and adversarial examples are a result of a model overfitting on features that do not generalize well. As a remedy, it is shown that training on adversarial examples and temporally augmented samples can reduce this effect and lead to models that generalise better. Finally, models operating on cadence-velocity diagram representation rather than Doppler-time are demonstrated to be naturally more immune to adversarial examples.|\u96f7\u8fbe\u6570\u636e\u5904\u7406\u6df1\u5ea6\u5206\u7c7b\u5668\u7684\u5f3a\u5927\u529f\u80fd\u5e26\u6765\u4e86\u5b66\u4e60\u6570\u636e\u96c6\u7279\u5b9a\u7279\u5f81\u7684\u98ce\u9669\uff0c\u8fd9\u4e9b\u7279\u5f81\u4e0d\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u8bc4\u4f30\u4e86\u5728\u76f8\u540c\u6570\u636e\u4e0a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7684\u4e24\u79cd\u6df1\u5ea6\u5377\u79ef\u67b6\u6784\u7684\u9c81\u68d2\u6027\u3002\u5f53\u9075\u5faa\u6807\u51c6\u8bad\u7ec3\u5b9e\u8df5\u65f6\uff0c\u4e24\u4e2a\u5206\u7c7b\u5668\u90fd\u5bf9\u8f93\u5165\u8868\u793a\u7684\u5fae\u5999\u65f6\u95f4\u53d8\u5316\u8868\u73b0\u51fa\u654f\u611f\u6027\uff0c\u8fd9\u662f\u4e00\u79cd\u643a\u5e26\u6700\u5c11\u8bed\u4e49\u5185\u5bb9\u7684\u589e\u5f3a\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6a21\u578b\u975e\u5e38\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u4f8b\u5b50\u7684\u5f71\u54cd\u3002\u5c0f\u7684\u65f6\u95f4\u53d8\u5316\u548c\u5bf9\u6297\u6027\u4f8b\u5b50\u90fd\u662f\u6a21\u578b\u5bf9\u4e0d\u80fd\u5f88\u597d\u6982\u62ec\u7684\u7279\u5f81\u8fc7\u5ea6\u62df\u5408\u7684\u7ed3\u679c\u3002\u4f5c\u4e3a\u4e00\u79cd\u8865\u6551\u63aa\u65bd\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u5bf9\u6297\u6027\u793a\u4f8b\u548c\u65f6\u95f4\u589e\u5f3a\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u51cf\u5c11\u8fd9\u79cd\u5f71\u54cd\uff0c\u5e76\u5bfc\u81f4\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6700\u540e\uff0c\u4e8b\u5b9e\u8bc1\u660e\uff0c\u57fa\u4e8e\u8282\u594f\u901f\u5ea6\u56fe\u8868\u793a\u800c\u4e0d\u662f\u591a\u666e\u52d2\u65f6\u95f4\u8fd0\u884c\u7684\u6a21\u578b\u81ea\u7136\u66f4\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u5f71\u54cd\u3002|[2402.13651v1](http://arxiv.org/pdf/2402.13651v1)|null|\n", "2402.13643": "|**2024-02-21**|**Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition**|\u7528\u4e8e\u573a\u666f\u6587\u672c\u8bc6\u522b\u7684\u7c7b\u611f\u77e5\u63a9\u6a21\u5f15\u5bfc\u7279\u5f81\u7ec6\u5316|Mingkun Yang, Biao Yang, Minghui Liao, Yingying Zhu, Xiang Bai|Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions. In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges. Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination. Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition. By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance. We first evaluate CAM on six standard text recognition benchmarks to demonstrate its effectiveness. Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size. Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition. The code is available at https://github.com/MelosY/CAM.|\u573a\u666f\u6587\u672c\u8bc6\u522b\u662f\u4e00\u4e2a\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\uff0c\u7531\u4e8e\u573a\u666f\u6587\u672c\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff0c\u5305\u62ec\u590d\u6742\u7684\u80cc\u666f\u3001\u591a\u6837\u7684\u5b57\u4f53\u3001\u7075\u6d3b\u7684\u6392\u5217\u548c\u610f\u5916\u906e\u6321\u7b49\uff0c\u9762\u4e34\u7740\u4f17\u591a\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u7c7b\u611f\u77e5\u63a9\u6a21\u5f15\u5bfc\u7279\u5f81\u7ec6\u5316\uff08CAM\uff09\u7684\u65b0\u9896\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u4ece\u6807\u51c6\u5b57\u4f53\u751f\u6210\u7684\u89c4\u8303\u7c7b\u611f\u77e5\u5b57\u5f62\u63a9\u7801\uff0c\u4ee5\u6709\u6548\u6291\u5236\u80cc\u666f\u548c\u6587\u672c\u6837\u5f0f\u566a\u58f0\uff0c\u4ece\u800c\u589e\u5f3a\u7279\u5f81\u8fa8\u522b\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7279\u5f81\u5bf9\u9f50\u548c\u878d\u5408\u6a21\u5757\uff0c\u4ee5\u5408\u5e76\u89c4\u8303\u63a9\u6a21\u6307\u5bfc\uff0c\u4ee5\u8fdb\u4e00\u6b65\u7ec6\u5316\u6587\u672c\u8bc6\u522b\u7684\u7279\u5f81\u3002\u901a\u8fc7\u589e\u5f3a\u89c4\u8303\u63a9\u6a21\u7279\u5f81\u548c\u6587\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u8be5\u6a21\u5757\u786e\u4fdd\u66f4\u6709\u6548\u7684\u878d\u5408\uff0c\u6700\u7ec8\u63d0\u9ad8\u8bc6\u522b\u6027\u80fd\u3002\u6211\u4eec\u9996\u5148\u5728\u516d\u4e2a\u6807\u51c6\u6587\u672c\u8bc6\u522b\u57fa\u51c6\u4e0a\u8bc4\u4f30 CAM\uff0c\u4ee5\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u6a21\u578b\u5c3a\u5bf8\u8f83\u5c0f\uff0c\u4f46\u5728\u516d\u4e2a\u66f4\u5177\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\uff0cCAM \u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e86 4.1%\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u7ed3\u5408\u89c4\u8303\u63a9\u6a21\u6307\u5bfc\u548c\u5bf9\u9f50\u7279\u5f81\u7ec6\u5316\u6280\u672f\u5bf9\u4e8e\u7a33\u5065\u7684\u573a\u666f\u6587\u672c\u8bc6\u522b\u7684\u91cd\u8981\u6027\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/MelosY/CAM \u83b7\u53d6\u3002|[2402.13643v1](http://arxiv.org/pdf/2402.13643v1)|**[link](https://github.com/melosy/cam)**|\n", "2402.13631": "|**2024-02-21**|**Delving into Dark Regions for Robust Shadow Detection**|\u6df1\u5165\u7814\u7a76\u9ed1\u6697\u533a\u57df\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u9634\u5f71\u68c0\u6d4b|Huankang Guan, Ke Xu, Rynson W. H. Lau|Shadow detection is a challenging task as it requires a comprehensive understanding of shadow characteristics and global/local illumination conditions. We observe from our experiment that state-of-the-art deep methods tend to have higher error rates in differentiating shadow pixels from non-shadow pixels in dark regions (ie, regions with low-intensity values). Our key insight to this problem is that existing methods typically learn discriminative shadow features from the whole image globally, covering the full range of intensity values, and may not learn the subtle differences between shadow and non-shadow pixels in dark regions. Hence, if we can design a model to focus on a narrower range of low-intensity regions, it may be able to learn better discriminative features for shadow detection. Inspired by this insight, we propose a novel shadow detection approach that first learns global contextual cues over the entire image and then zooms into the dark regions to learn local shadow representations. To this end, we formulate an effective dark-region recommendation (DRR) module to recommend regions of low-intensity values, and a novel dark-aware shadow analysis (DASA) module to learn dark-aware shadow features from the recommended dark regions. Extensive experiments show that the proposed method outperforms the state-of-the-art methods on three popular shadow detection datasets. Code is available at https://github.com/guanhuankang/ShadowDetection2021.git.|\u9634\u5f71\u68c0\u6d4b\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5168\u9762\u4e86\u89e3\u9634\u5f71\u7279\u5f81\u548c\u5168\u5c40/\u5c40\u90e8\u7167\u660e\u6761\u4ef6\u3002\u6211\u4eec\u4ece\u5b9e\u9a8c\u4e2d\u89c2\u5bdf\u5230\uff0c\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u65b9\u6cd5\u5728\u533a\u5206\u9ed1\u6697\u533a\u57df\uff08\u5373\u5177\u6709\u4f4e\u5f3a\u5ea6\u503c\u7684\u533a\u57df\uff09\u4e2d\u7684\u9634\u5f71\u50cf\u7d20\u4e0e\u975e\u9634\u5f71\u50cf\u7d20\u65f6\u5f80\u5f80\u5177\u6709\u66f4\u9ad8\u7684\u9519\u8bef\u7387\u3002\u6211\u4eec\u5bf9\u8fd9\u4e2a\u95ee\u9898\u7684\u4e3b\u8981\u89c1\u89e3\u662f\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4ece\u6574\u4e2a\u56fe\u50cf\u5168\u5c40\u5b66\u4e60\u6709\u533a\u522b\u7684\u9634\u5f71\u7279\u5f81\uff0c\u8986\u76d6\u6574\u4e2a\u5f3a\u5ea6\u503c\u8303\u56f4\uff0c\u5e76\u4e14\u53ef\u80fd\u65e0\u6cd5\u5b66\u4e60\u9ed1\u6697\u533a\u57df\u4e2d\u9634\u5f71\u548c\u975e\u9634\u5f71\u50cf\u7d20\u4e4b\u95f4\u7684\u7ec6\u5fae\u5dee\u522b\u3002\u56e0\u6b64\uff0c\u5982\u679c\u6211\u4eec\u53ef\u4ee5\u8bbe\u8ba1\u4e00\u4e2a\u6a21\u578b\u6765\u5173\u6ce8\u66f4\u7a84\u8303\u56f4\u7684\u4f4e\u5f3a\u5ea6\u533a\u57df\uff0c\u5b83\u53ef\u80fd\u80fd\u591f\u5b66\u4e60\u66f4\u597d\u7684\u9634\u5f71\u68c0\u6d4b\u5224\u522b\u7279\u5f81\u3002\u53d7\u8fd9\u4e00\u89c1\u89e3\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9634\u5f71\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9996\u5148\u5b66\u4e60\u6574\u4e2a\u56fe\u50cf\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u7136\u540e\u653e\u5927\u9ed1\u6697\u533a\u57df\u4ee5\u5b66\u4e60\u5c40\u90e8\u9634\u5f71\u8868\u793a\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5236\u5b9a\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6697\u533a\u57df\u63a8\u8350\uff08DRR\uff09\u6a21\u5757\u6765\u63a8\u8350\u4f4e\u5f3a\u5ea6\u503c\u7684\u533a\u57df\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u9896\u7684\u6697\u611f\u77e5\u9634\u5f71\u5206\u6790\uff08DASA\uff09\u6a21\u5757\u6765\u4ece\u63a8\u8350\u7684\u6697\u533a\u57df\u4e2d\u5b66\u4e60\u6697\u611f\u77e5\u9634\u5f71\u7279\u5f81\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u9634\u5f71\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/guanhuankang/ShadowDetection2021.git \u83b7\u53d6\u3002|[2402.13631v1](http://arxiv.org/pdf/2402.13631v1)|**[link](https://github.com/guanhuankang/shadowdetection2021)**|\n", "2402.13616": "|**2024-02-21**|**YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information**|YOLOv9\uff1a\u4f7f\u7528\u53ef\u7f16\u7a0b\u68af\u5ea6\u4fe1\u606f\u5b66\u4e60\u60a8\u60f3\u5b66\u4e60\u7684\u5185\u5bb9|Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao|Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.|\u5982\u4eca\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u91cd\u70b9\u5173\u6ce8\u5982\u4f55\u8bbe\u8ba1\u6700\u5408\u9002\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4f7f\u5f97\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u80fd\u591f\u6700\u63a5\u8fd1\u771f\u5b9e\u60c5\u51b5\u3002\u540c\u65f6\uff0c\u5fc5\u987b\u8bbe\u8ba1\u4e00\u4e2a\u9002\u5f53\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u5e2e\u52a9\u83b7\u53d6\u8db3\u591f\u7684\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e00\u4e2a\u4e8b\u5b9e\uff0c\u5373\u5f53\u8f93\u5165\u6570\u636e\u7ecf\u8fc7\u9010\u5c42\u7279\u5f81\u63d0\u53d6\u548c\u7a7a\u95f4\u53d8\u6362\u65f6\uff0c\u5927\u91cf\u4fe1\u606f\u5c06\u4f1a\u4e22\u5931\u3002\u672c\u6587\u5c06\u6df1\u5165\u7814\u7a76\u6570\u636e\u901a\u8fc7\u6df1\u5ea6\u7f51\u7edc\u4f20\u8f93\u65f6\u6570\u636e\u4e22\u5931\u7684\u91cd\u8981\u95ee\u9898\uff0c\u5373\u4fe1\u606f\u74f6\u9888\u548c\u53ef\u9006\u51fd\u6570\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u53ef\u7f16\u7a0b\u68af\u5ea6\u4fe1\u606f\uff08PGI\uff09\u7684\u6982\u5ff5\u6765\u5e94\u5bf9\u6df1\u5ea6\u7f51\u7edc\u5b9e\u73b0\u591a\u4e2a\u76ee\u6807\u6240\u9700\u7684\u5404\u79cd\u53d8\u5316\u3002 PGI\u53ef\u4ee5\u4e3a\u76ee\u6807\u4efb\u52a1\u8ba1\u7b97\u76ee\u6807\u51fd\u6570\u63d0\u4f9b\u5b8c\u6574\u7684\u8f93\u5165\u4fe1\u606f\uff0c\u4ece\u800c\u83b7\u5f97\u53ef\u9760\u7684\u68af\u5ea6\u4fe1\u606f\u6765\u66f4\u65b0\u7f51\u7edc\u6743\u503c\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u8f7b\u91cf\u7ea7\u7f51\u7edc\u67b6\u6784\u2014\u2014\u57fa\u4e8e\u68af\u5ea6\u8def\u5f84\u89c4\u5212\u7684\u901a\u7528\u9ad8\u6548\u5c42\u805a\u5408\u7f51\u7edc\uff08GELAN\uff09\u3002 GELAN\u7684\u67b6\u6784\u8bc1\u5b9e\u4e86PGI\u5728\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002\u6211\u4eec\u5728\u57fa\u4e8e MS COCO \u6570\u636e\u96c6\u7684\u76ee\u6807\u68c0\u6d4b\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684 GELAN \u548c PGI\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u5f00\u53d1\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0cGELAN \u4ec5\u4f7f\u7528\u4f20\u7edf\u7684\u5377\u79ef\u7b97\u5b50\u5373\u53ef\u5b9e\u73b0\u66f4\u597d\u7684\u53c2\u6570\u5229\u7528\u7387\u3002 PGI \u53ef\u7528\u4e8e\u4ece\u8f7b\u578b\u5230\u5927\u578b\u7684\u5404\u79cd\u6a21\u578b\u3002\u5b83\u53ef\u4ee5\u7528\u6765\u83b7\u53d6\u5b8c\u6574\u7684\u4fe1\u606f\uff0c\u4f7f\u5f97\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u591f\u6bd4\u4f7f\u7528\u5927\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684state-of-the-art\u6a21\u578b\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u6bd4\u8f83\u7ed3\u679c\u5982\u56fe1\u6240\u793a\u3002\u6e90\u4ee3\u7801\u662f\u7f51\u5740\uff1ahttps://github.com/WongKinYiu/yolov9\u3002|[2402.13616v1](http://arxiv.org/pdf/2402.13616v1)|**[link](https://github.com/wongkinyiu/yolov9)**|\n", "2402.13579": "|**2024-02-21**|**Learning Pixel-wise Continuous Depth Representation via Clustering for Depth Completion**|\u901a\u8fc7\u6df1\u5ea6\u8865\u5168\u7684\u805a\u7c7b\u5b66\u4e60\u9010\u50cf\u7d20\u8fde\u7eed\u6df1\u5ea6\u8868\u793a|Chen Shenglun, Zhang Hong, Ma XinZhu, Wang Zhihui, Li Haojie|Depth completion is a long-standing challenge in computer vision, where classification-based methods have made tremendous progress in recent years. However, most existing classification-based methods rely on pre-defined pixel-shared and discrete depth values as depth categories. This representation fails to capture the continuous depth values that conform to the real depth distribution, leading to depth smearing in boundary regions. To address this issue, we revisit depth completion from the clustering perspective and propose a novel clustering-based framework called CluDe which focuses on learning the pixel-wise and continuous depth representation. The key idea of CluDe is to iteratively update the pixel-shared and discrete depth representation to its corresponding pixel-wise and continuous counterpart, driven by the real depth distribution. Specifically, CluDe first utilizes depth value clustering to learn a set of depth centers as the depth representation. While these depth centers are pixel-shared and discrete, they are more in line with the real depth distribution compared to pre-defined depth categories. Then, CluDe estimates offsets for these depth centers, enabling their dynamic adjustment along the depth axis of the depth distribution to generate the pixel-wise and continuous depth representation. Extensive experiments demonstrate that CluDe successfully reduces depth smearing around object boundaries by utilizing pixel-wise and continuous depth representation. Furthermore, CluDe achieves state-of-the-art performance on the VOID datasets and outperforms classification-based methods on the KITTI dataset.|\u6df1\u5ea6\u8865\u5168\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\uff0c\u8fd1\u5e74\u6765\u57fa\u4e8e\u5206\u7c7b\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u57fa\u4e8e\u5206\u7c7b\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u50cf\u7d20\u5171\u4eab\u548c\u79bb\u6563\u6df1\u5ea6\u503c\u4f5c\u4e3a\u6df1\u5ea6\u7c7b\u522b\u3002\u8fd9\u79cd\u8868\u793a\u65e0\u6cd5\u6355\u83b7\u7b26\u5408\u771f\u5b9e\u6df1\u5ea6\u5206\u5e03\u7684\u8fde\u7eed\u6df1\u5ea6\u503c\uff0c\u5bfc\u81f4\u8fb9\u754c\u533a\u57df\u7684\u6df1\u5ea6\u62d6\u5c3e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u4ece\u805a\u7c7b\u7684\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u6df1\u5ea6\u8865\u5168\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CluDe \u7684\u65b0\u578b\u57fa\u4e8e\u805a\u7c7b\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e13\u6ce8\u4e8e\u5b66\u4e60\u50cf\u7d20\u7ea7\u548c\u8fde\u7eed\u7684\u6df1\u5ea6\u8868\u793a\u3002 CluDe \u7684\u5173\u952e\u601d\u60f3\u662f\u5728\u771f\u5b9e\u6df1\u5ea6\u5206\u5e03\u7684\u9a71\u52a8\u4e0b\uff0c\u8fed\u4ee3\u5730\u5c06\u50cf\u7d20\u5171\u4eab\u548c\u79bb\u6563\u6df1\u5ea6\u8868\u793a\u66f4\u65b0\u4e3a\u5176\u76f8\u5e94\u7684\u50cf\u7d20\u7ea7\u548c\u8fde\u7eed\u5bf9\u5e94\u7269\u3002\u5177\u4f53\u6765\u8bf4\uff0cCluDe\u9996\u5148\u5229\u7528\u6df1\u5ea6\u503c\u805a\u7c7b\u6765\u5b66\u4e60\u4e00\u7ec4\u6df1\u5ea6\u4e2d\u5fc3\u4f5c\u4e3a\u6df1\u5ea6\u8868\u793a\u3002\u867d\u7136\u8fd9\u4e9b\u6df1\u5ea6\u4e2d\u5fc3\u662f\u50cf\u7d20\u5171\u4eab\u4e14\u79bb\u6563\u7684\uff0c\u4f46\u4e0e\u9884\u5b9a\u4e49\u7684\u6df1\u5ea6\u7c7b\u522b\u76f8\u6bd4\uff0c\u5b83\u4eec\u66f4\u7b26\u5408\u771f\u5b9e\u7684\u6df1\u5ea6\u5206\u5e03\u3002\u7136\u540e\uff0cCluDe \u4f30\u8ba1\u8fd9\u4e9b\u6df1\u5ea6\u4e2d\u5fc3\u7684\u504f\u79fb\u91cf\uff0c\u4f7f\u5176\u80fd\u591f\u6cbf\u7740\u6df1\u5ea6\u5206\u5e03\u7684\u6df1\u5ea6\u8f74\u8fdb\u884c\u52a8\u6001\u8c03\u6574\uff0c\u4ee5\u751f\u6210\u9010\u50cf\u7d20\u4e14\u8fde\u7eed\u7684\u6df1\u5ea6\u8868\u793a\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCluDe \u901a\u8fc7\u5229\u7528\u50cf\u7d20\u7ea7\u548c\u8fde\u7eed\u6df1\u5ea6\u8868\u793a\uff0c\u6210\u529f\u51cf\u5c11\u4e86\u5bf9\u8c61\u8fb9\u754c\u5468\u56f4\u7684\u6df1\u5ea6\u6a21\u7cca\u3002\u6b64\u5916\uff0cCluDe \u5728 VOID \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728 KITTI \u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u5206\u7c7b\u7684\u65b9\u6cd5\u3002|[2402.13579v1](http://arxiv.org/pdf/2402.13579v1)|null|\n", "2402.13578": "|**2024-02-21**|**TransGOP: Transformer-Based Gaze Object Prediction**|TransGOP\uff1a\u57fa\u4e8e Transformer \u7684\u6ce8\u89c6\u5bf9\u8c61\u9884\u6d4b|Binglu Wang, Chenxi Guo, Yang Jin, Haisheng Xia, Nian Liu|Gaze object prediction aims to predict the location and category of the object that is watched by a human. Previous gaze object prediction works use CNN-based object detectors to predict the object's location. However, we find that Transformer-based object detectors can predict more accurate object location for dense objects in retail scenarios. Moreover, the long-distance modeling capability of the Transformer can help to build relationships between the human head and the gaze object, which is important for the GOP task. To this end, this paper introduces Transformer into the fields of gaze object prediction and proposes an end-to-end Transformer-based gaze object prediction method named TransGOP. Specifically, TransGOP uses an off-the-shelf Transformer-based object detector to detect the location of objects and designs a Transformer-based gaze autoencoder in the gaze regressor to establish long-distance gaze relationships. Moreover, to improve gaze heatmap regression, we propose an object-to-gaze cross-attention mechanism to let the queries of the gaze autoencoder learn the global-memory position knowledge from the object detector. Finally, to make the whole framework end-to-end trained, we propose a Gaze Box loss to jointly optimize the object detector and gaze regressor by enhancing the gaze heatmap energy in the box of the gaze object. Extensive experiments on the GOO-Synth and GOO-Real datasets demonstrate that our TransGOP achieves state-of-the-art performance on all tracks, i.e., object detection, gaze estimation, and gaze object prediction. Our code will be available at https://github.com/chenxi-Guo/TransGOP.git.|\u6ce8\u89c6\u76ee\u6807\u9884\u6d4b\u65e8\u5728\u9884\u6d4b\u4eba\u7c7b\u89c2\u770b\u7684\u7269\u4f53\u7684\u4f4d\u7f6e\u548c\u7c7b\u522b\u3002\u4e4b\u524d\u7684\u51dd\u89c6\u76ee\u6807\u9884\u6d4b\u5de5\u4f5c\u4f7f\u7528\u57fa\u4e8e CNN \u7684\u76ee\u6807\u68c0\u6d4b\u5668\u6765\u9884\u6d4b\u76ee\u6807\u7684\u4f4d\u7f6e\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u57fa\u4e8e Transformer \u7684\u7269\u4f53\u68c0\u6d4b\u5668\u53ef\u4ee5\u9884\u6d4b\u96f6\u552e\u573a\u666f\u4e2d\u5bc6\u96c6\u7269\u4f53\u7684\u66f4\u51c6\u786e\u7684\u7269\u4f53\u4f4d\u7f6e\u3002\u6b64\u5916\uff0cTransformer\u7684\u8fdc\u8ddd\u79bb\u5efa\u6a21\u80fd\u529b\u53ef\u4ee5\u5e2e\u52a9\u5efa\u7acb\u4eba\u5934\u548c\u6ce8\u89c6\u5bf9\u8c61\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u5bf9\u4e8eGOP\u4efb\u52a1\u5f88\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u5c06Transformer\u5f15\u5165\u6ce8\u89c6\u76ee\u6807\u9884\u6d4b\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7aef\u5230\u7aef\u6ce8\u89c6\u76ee\u6807\u9884\u6d4b\u65b9\u6cd5TransGOP\u3002\u5177\u4f53\u6765\u8bf4\uff0cTransGOP \u4f7f\u7528\u73b0\u6210\u7684\u57fa\u4e8e Transformer \u7684\u5bf9\u8c61\u68c0\u6d4b\u5668\u6765\u68c0\u6d4b\u5bf9\u8c61\u7684\u4f4d\u7f6e\uff0c\u5e76\u5728\u6ce8\u89c6\u56de\u5f52\u5668\u4e2d\u8bbe\u8ba1\u57fa\u4e8e Transformer \u7684\u6ce8\u89c6\u81ea\u52a8\u7f16\u7801\u5668\u6765\u5efa\u7acb\u8fdc\u8ddd\u79bb\u6ce8\u89c6\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u6539\u8fdb\u6ce8\u89c6\u70ed\u56fe\u56de\u5f52\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u8c61\u5230\u6ce8\u89c6\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\uff0c\u8ba9\u6ce8\u89c6\u81ea\u52a8\u7f16\u7801\u5668\u7684\u67e5\u8be2\u4ece\u5bf9\u8c61\u68c0\u6d4b\u5668\u5b66\u4e60\u5168\u5c40\u5185\u5b58\u4f4d\u7f6e\u77e5\u8bc6\u3002\u6700\u540e\uff0c\u4e3a\u4e86\u4f7f\u6574\u4e2a\u6846\u67b6\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd Gaze Box loss\uff0c\u901a\u8fc7\u589e\u5f3a\u6ce8\u89c6\u5bf9\u8c61\u6846\u4e2d\u7684\u6ce8\u89c6\u70ed\u56fe\u80fd\u91cf\u6765\u8054\u5408\u4f18\u5316\u5bf9\u8c61\u68c0\u6d4b\u5668\u548c\u6ce8\u89c6\u56de\u5f52\u5668\u3002\u5bf9 GOO-Synth \u548c GOO-Real \u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 TransGOP \u5728\u6240\u6709\u8f68\u9053\uff08\u5373\u5bf9\u8c61\u68c0\u6d4b\u3001\u6ce8\u89c6\u4f30\u8ba1\u548c\u6ce8\u89c6\u5bf9\u8c61\u9884\u6d4b\uff09\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728 https://github.com/chenxi-Guo/TransGOP.git \u4e0a\u63d0\u4f9b\u3002|[2402.13578v1](http://arxiv.org/pdf/2402.13578v1)|null|\n", "2402.13545": "|**2024-02-21**|**A Two-Stage Dual-Path Framework for Text Tampering Detection and Recognition**|\u7528\u4e8e\u6587\u672c\u7be1\u6539\u68c0\u6d4b\u548c\u8bc6\u522b\u7684\u4e24\u9636\u6bb5\u53cc\u8def\u5f84\u6846\u67b6|Guandong Li, Xian Yang, Wenpin Ma|Document tamper detection has always been an important aspect of tamper detection. Before the advent of deep learning, document tamper detection was difficult. We have made some explorations in the field of text tamper detection based on deep learning. Our Ps tamper detection method includes three steps: feature assistance, audit point positioning, and tamper recognition. It involves hierarchical filtering and graded output (tampered/suspected tampered/untampered). By combining artificial tamper data features, we simulate and augment data samples in various scenarios (cropping with noise addition/replacement, single character/space replacement, smearing/splicing, brightness/contrast adjustment, etc.). The auxiliary features include exif/binary stream keyword retrieval/noise, which are used for branch detection based on the results. Audit point positioning uses detection frameworks and controls thresholds for high and low density detection. Tamper recognition employs a dual-path dual-stream recognition network, with RGB and ELA stream feature extraction. After dimensionality reduction through self-correlation percentile pooling, the fused output is processed through vlad, yielding an accuracy of 0.804, recall of 0.659, and precision of 0.913.|\u6587\u6863\u7be1\u6539\u68c0\u6d4b\u4e00\u76f4\u662f\u7be1\u6539\u68c0\u6d4b\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u9762\u3002\u5728\u6df1\u5ea6\u5b66\u4e60\u51fa\u73b0\u4e4b\u524d\uff0c\u6587\u6863\u7be1\u6539\u68c0\u6d4b\u5f88\u56f0\u96be\u3002\u6211\u4eec\u5728\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6587\u672c\u7be1\u6539\u68c0\u6d4b\u9886\u57df\u505a\u4e86\u4e00\u4e9b\u63a2\u7d22\u3002\u6211\u4eec\u7684Ps\u7be1\u6539\u68c0\u6d4b\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a\u7279\u5f81\u8f85\u52a9\u3001\u5ba1\u6838\u70b9\u5b9a\u4f4d\u548c\u7be1\u6539\u8bc6\u522b\u3002\u5b83\u6d89\u53ca\u5206\u7ea7\u8fc7\u6ee4\u548c\u5206\u7ea7\u8f93\u51fa\uff08\u88ab\u7be1\u6539/\u7591\u4f3c\u7be1\u6539/\u672a\u7be1\u6539\uff09\u3002\u901a\u8fc7\u7ed3\u5408\u4eba\u5de5\u7be1\u6539\u6570\u636e\u7279\u5f81\uff0c\u6211\u4eec\u6a21\u62df\u548c\u589e\u5f3a\u4e86\u5404\u79cd\u573a\u666f\u4e0b\u7684\u6570\u636e\u6837\u672c\uff08\u6dfb\u52a0\u566a\u58f0/\u66ff\u6362\u7684\u88c1\u526a\u3001\u5355\u4e2a\u5b57\u7b26/\u7a7a\u683c\u66ff\u6362\u3001\u6d82\u62b9/\u62fc\u63a5\u3001\u4eae\u5ea6/\u5bf9\u6bd4\u5ea6\u8c03\u6574\u7b49\uff09\u3002\u8f85\u52a9\u529f\u80fd\u5305\u62ecexif/\u4e8c\u8fdb\u5236\u6d41\u5173\u952e\u5b57\u68c0\u7d22/\u566a\u58f0\uff0c\u7528\u4e8e\u6839\u636e\u7ed3\u679c\u8fdb\u884c\u5206\u652f\u68c0\u6d4b\u3002\u5ba1\u6838\u70b9\u5b9a\u4f4d\u4f7f\u7528\u68c0\u6d4b\u6846\u67b6\u5e76\u63a7\u5236\u9ad8\u5bc6\u5ea6\u548c\u4f4e\u5bc6\u5ea6\u68c0\u6d4b\u7684\u9608\u503c\u3002\u7be1\u6539\u8bc6\u522b\u91c7\u7528\u53cc\u8def\u53cc\u6d41\u8bc6\u522b\u7f51\u7edc\uff0c\u5177\u6709RGB\u548cELA\u6d41\u7279\u5f81\u63d0\u53d6\u3002\u901a\u8fc7\u81ea\u76f8\u5173\u767e\u5206\u4f4d\u6570\u6c60\u5316\u964d\u7ef4\u540e\uff0c\u878d\u5408\u8f93\u51fa\u7ecf\u8fc7 vlad \u5904\u7406\uff0c\u51c6\u786e\u7387\u8fbe\u5230 0.804\uff0c\u53ec\u56de\u7387\u8fbe\u5230 0.659\uff0c\u7cbe\u5ea6\u8fbe\u5230 0.913\u3002|[2402.13545v1](http://arxiv.org/pdf/2402.13545v1)|null|\n", "2402.13465": "|**2024-02-21**|**Unsupervised learning based object detection using Contrastive Learning**|\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u7684\u57fa\u4e8e\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u5bf9\u8c61\u68c0\u6d4b|Chandan Kumar, Jansel Herrera-Gerena, John Just, Matthew Darr, Ali Jannesari|Training image-based object detectors presents formidable challenges, as it entails not only the complexities of object detection but also the added intricacies of precisely localizing objects within potentially diverse and noisy environments. However, the collection of imagery itself can often be straightforward; for instance, cameras mounted in vehicles can effortlessly capture vast amounts of data in various real-world scenarios. In light of this, we introduce a groundbreaking method for training single-stage object detectors through unsupervised/self-supervised learning.   Our state-of-the-art approach has the potential to revolutionize the labeling process, substantially reducing the time and cost associated with manual annotation. Furthermore, it paves the way for previously unattainable research opportunities, particularly for large, diverse, and challenging datasets lacking extensive labels.   In contrast to prevalent unsupervised learning methods that primarily target classification tasks, our approach takes on the unique challenge of object detection. We pioneer the concept of intra-image contrastive learning alongside inter-image counterparts, enabling the acquisition of crucial location information essential for object detection. The method adeptly learns and represents this location information, yielding informative heatmaps. Our results showcase an outstanding accuracy of \\textbf{89.2\\%}, marking a significant breakthrough of approximately \\textbf{15x} over random initialization in the realm of unsupervised object detection within the field of computer vision.|\u8bad\u7ec3\u57fa\u4e8e\u56fe\u50cf\u7684\u7269\u4f53\u68c0\u6d4b\u5668\u63d0\u51fa\u4e86\u5de8\u5927\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4e0d\u4ec5\u9700\u8981\u7269\u4f53\u68c0\u6d4b\u7684\u590d\u6742\u6027\uff0c\u800c\u4e14\u8fd8\u9700\u8981\u5728\u6f5c\u5728\u591a\u6837\u5316\u548c\u5608\u6742\u7684\u73af\u5883\u4e2d\u7cbe\u786e\u5b9a\u4f4d\u7269\u4f53\u7684\u590d\u6742\u6027\u3002\u7136\u800c\uff0c\u56fe\u50cf\u7684\u6536\u96c6\u672c\u8eab\u901a\u5e38\u5f88\u7b80\u5355\uff1b\u4f8b\u5982\uff0c\u5b89\u88c5\u5728\u8f66\u8f86\u4e0a\u7684\u6444\u50cf\u5934\u53ef\u4ee5\u8f7b\u677e\u6355\u83b7\u5404\u79cd\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5927\u91cf\u6570\u636e\u3002\u9274\u4e8e\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u901a\u8fc7\u65e0\u76d1\u7763/\u81ea\u76d1\u7763\u5b66\u4e60\u6765\u8bad\u7ec3\u5355\u7ea7\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u7a81\u7834\u6027\u65b9\u6cd5\u3002\u6211\u4eec\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u6709\u53ef\u80fd\u5f7b\u5e95\u6539\u53d8\u6807\u7b7e\u6d41\u7a0b\uff0c\u5927\u5927\u51cf\u5c11\u4e0e\u624b\u52a8\u6ce8\u91ca\u76f8\u5173\u7684\u65f6\u95f4\u548c\u6210\u672c\u3002\u6b64\u5916\uff0c\u5b83\u4e3a\u4ee5\u524d\u65e0\u6cd5\u200b\u200b\u5b9e\u73b0\u7684\u7814\u7a76\u673a\u4f1a\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7f3a\u4e4f\u5e7f\u6cdb\u6807\u7b7e\u7684\u5927\u578b\u3001\u591a\u6837\u5316\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u3002\u4e0e\u4e3b\u8981\u9488\u5bf9\u5206\u7c7b\u4efb\u52a1\u7684\u6d41\u884c\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u9762\u4e34\u7740\u5bf9\u8c61\u68c0\u6d4b\u7684\u72ec\u7279\u6311\u6218\u3002\u6211\u4eec\u7387\u5148\u63d0\u51fa\u4e86\u56fe\u50cf\u5185\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u53ca\u56fe\u50cf\u95f4\u5bf9\u6bd4\u5b66\u4e60\u7684\u6982\u5ff5\uff0c\u4ece\u800c\u80fd\u591f\u83b7\u53d6\u5bf9\u8c61\u68c0\u6d4b\u6240\u5fc5\u9700\u7684\u5173\u952e\u4f4d\u7f6e\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u719f\u7ec3\u5730\u5b66\u4e60\u5e76\u8868\u793a\u8be5\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4ece\u800c\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u70ed\u56fe\u3002\u6211\u4eec\u7684\u7ed3\u679c\u5c55\u793a\u4e86 \\textbf{89.2\\%} \u7684\u51fa\u8272\u51c6\u786e\u5ea6\uff0c\u6807\u5fd7\u7740\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u65e0\u76d1\u7763\u5bf9\u8c61\u68c0\u6d4b\u9886\u57df\u76f8\u5bf9\u4e8e\u968f\u673a\u521d\u59cb\u5316\u7684\u7ea6 \\textbf{15x} \u7684\u91cd\u5927\u7a81\u7834\u3002|[2402.13465v1](http://arxiv.org/pdf/2402.13465v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "LLM": {"2402.13602": "|**2024-02-21**|**Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving**|\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u9a7e\u9a76\u6df7\u5408\u63a8\u7406|Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg|Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by comparing their answers with human-generated ground truth inside CARLA. The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions. This formulation and answers can assist in decision-making for auto-pilot systems.|\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u56e0\u5176\u7406\u89e3\u6587\u672c\u548c\u56fe\u50cf\u3001\u751f\u6210\u7c7b\u4eba\u6587\u672c\u4ee5\u53ca\u6267\u884c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u80fd\u529b\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u4ed6\u4eec\u5c06\u8fd9\u79cd\u9ad8\u7ea7\u63a8\u7406\u4e0e\u81ea\u7136\u8bed\u8a00\u6587\u672c\u76f8\u7ed3\u5408\u4ee5\u5728\u52a8\u6001\u60c5\u51b5\u4e0b\u8fdb\u884c\u51b3\u7b56\u7684\u80fd\u529b\u8fd8\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u6cd5\u5b66\u7855\u58eb\u5982\u4f55\u9002\u5e94\u548c\u5e94\u7528\u7b97\u672f\u548c\u5e38\u8bc6\u63a8\u7406\u7684\u7ec4\u5408\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u3002\u6211\u4eec\u5047\u8bbe\u6cd5\u5b66\u7855\u58eb\u7684\u6df7\u5408\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u68c0\u6d4b\u5230\u7684\u7269\u4f53\u548c\u4f20\u611f\u5668\u6570\u636e\u3001\u4e86\u89e3\u9a7e\u9a76\u6cd5\u89c4\u548c\u7269\u7406\u5b9a\u5f8b\u5e76\u63d0\u4f9b\u989d\u5916\u7684\u80cc\u666f\u6765\u6539\u5584\u81ea\u52a8\u9a7e\u9a76\u3002\u8fd9\u89e3\u51b3\u4e86\u590d\u6742\u7684\u573a\u666f\uff0c\u4f8b\u5982\u5728\u80fd\u89c1\u5ea6\u8f83\u4f4e\uff08\u7531\u4e8e\u5929\u6c14\u6761\u4ef6\uff09\u7684\u60c5\u51b5\u4e0b\u505a\u51fa\u7684\u51b3\u7b56\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e9b\u573a\u666f\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u7b54\u6848\u4e0e CARLA \u5185\u4eba\u7c7b\u751f\u6210\u7684\u57fa\u672c\u4e8b\u5b9e\u8fdb\u884c\u6bd4\u8f83\uff0c\u6839\u636e\u51c6\u786e\u6027\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u56fe\u50cf\uff08\u68c0\u6d4b\u5230\u7684\u7269\u4f53\uff09\u548c\u4f20\u611f\u5668\u6570\u636e\u7ec4\u5408\u8f93\u5165 LLM \u65f6\uff0c\u5b83\u53ef\u4ee5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u5404\u79cd\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u5236\u52a8\u548c\u6cb9\u95e8\u63a7\u5236\u63d0\u4f9b\u7cbe\u786e\u7684\u4fe1\u606f\u3002\u8be5\u516c\u5f0f\u548c\u7b54\u6848\u53ef\u4ee5\u5e2e\u52a9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u51b3\u7b56\u3002|[2402.13602v1](http://arxiv.org/pdf/2402.13602v1)|null|\n", "2402.13546": "|**2024-02-21**|**LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs**|\u6cd5\u5b66\u7855\u58eb\u9047\u89c1\u957f\u89c6\u9891\uff1a\u5229\u7528\u6cd5\u5b66\u7855\u58eb\u4e2d\u7684\u4ea4\u4e92\u5f0f\u89c6\u89c9\u9002\u914d\u5668\u4fc3\u8fdb\u957f\u89c6\u9891\u7406\u89e3|Yunxin Li, Xinyu Chen, Baotain Hu, Min Zhang|Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of LLMs to capture instruction-aware and fine-grained visual signals. Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions. We conducted extensive experiments on nine video understanding benchmarks and experimental results show that our interactive visual adapter significantly improves the performance of video LLMs on long video QA tasks. Ablation studies further verify the effectiveness of IVA in long and short video understandings.|\u957f\u89c6\u9891\u7406\u89e3\u662f\u591a\u5a92\u4f53\u548c\u4eba\u5de5\u667a\u80fd\u4ea4\u53c9\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u5927\u4e14\u6301\u7eed\u7684\u6311\u6218\u3002\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u7406\u89e3\u89c6\u9891\u6210\u4e3a\u4e00\u79cd\u65b0\u5174\u4e14\u6709\u524d\u9014\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5927\u91cf\u7684\u89c6\u9891\u6807\u8bb0\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u4ea7\u751f\u5f88\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u7531\u4e8e\u6807\u8bb0\u805a\u5408\u800c\u5bfc\u81f4\u89c6\u89c9\u6e05\u6670\u5ea6\u964d\u4f4e\uff0c\u5e76\u4e14\u5728\u56de\u7b54\u89c6\u9891\u76f8\u5173\u95ee\u9898\u65f6\u9762\u4e34\u4e0d\u76f8\u5173\u7684\u89c6\u89c9\u6807\u8bb0\u5e26\u6765\u7684\u6311\u6218\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5728\u6cd5\u5b66\u7855\u58eb\u4e2d\u63d0\u51fa\u4e86\u4ea4\u4e92\u5f0f\u89c6\u89c9\u9002\u914d\u5668\uff08IVA\uff09\uff0c\u65e8\u5728\u589e\u5f3a\u4e0e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5143\u7d20\u7684\u4ea4\u4e92\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u5229\u7528\u89c6\u89c9\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3\u7684\u56e0\u679c\u53d8\u6362\u5668\u5c06\u957f\u89c6\u9891\u8f6c\u6362\u4e3a\u65f6\u95f4\u89c6\u9891\u6807\u8bb0\uff0c\u7136\u540e\u5c06\u5b83\u4eec\u4e0e\u89c6\u9891\u6307\u4ee4\u4e00\u8d77\u8f93\u5165\u5230\u6cd5\u5b66\u7855\u58eb\u4e2d\u3002\u968f\u540e\uff0c\u6211\u4eec\u5c06 IVA\uff08\u5305\u542b\u8f7b\u91cf\u7ea7\u65f6\u95f4\u5e27\u9009\u62e9\u5668\u548c\u7a7a\u95f4\u7279\u5f81\u4ea4\u4e92\u5668\uff09\u96c6\u6210\u5230 LLM \u7684\u5185\u90e8\u6a21\u5757\u4e2d\uff0c\u4ee5\u6355\u83b7\u6307\u4ee4\u611f\u77e5\u548c\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u4fe1\u53f7\u3002\u56e0\u6b64\uff0c\u6240\u63d0\u51fa\u7684\u89c6\u9891\u6cd5\u5b66\u7855\u58eb\u901a\u8fc7\u9002\u5f53\u7684\u957f\u89c6\u9891\u5efa\u6a21\u548c\u7cbe\u786e\u7684\u89c6\u89c9\u4ea4\u4e92\u4fc3\u8fdb\u4e86\u5bf9\u957f\u89c6\u9891\u5185\u5bb9\u7684\u5168\u9762\u7406\u89e3\u3002\u6211\u4eec\u5bf9\u4e5d\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u4ea4\u4e92\u5f0f\u89c6\u89c9\u9002\u914d\u5668\u663e\u7740\u63d0\u9ad8\u4e86\u89c6\u9891\u6cd5\u5b66\u7855\u58eb\u5728\u957f\u89c6\u9891 QA \u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86 IVA \u5728\u957f\u77ed\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6709\u6548\u6027\u3002|[2402.13546v1](http://arxiv.org/pdf/2402.13546v1)|null|\n"}, "Transformer": {"2402.13609": "|**2024-02-21**|**VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks**|VOOM\uff1a\u4f7f\u7528\u5206\u5c42\u5730\u6807\u7684\u9c81\u68d2\u89c6\u89c9\u5bf9\u8c61\u91cc\u7a0b\u8ba1\u548c\u7ed8\u56fe|Yutong Wang, Chaoyang Jiang, Xieyuanli Chen|In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency. Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment. However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points. In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment. Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects. It facilitates the creation of a 3D map that closely reflects reality. Next, we use object information to enhance the data association of feature points and consequently update the map. In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects. Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility graphs in our visual object mapping process. Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization. The implementation of our method is available at https://github.com/yutongwangBIT/VOOM.git.|\u8fd1\u5e74\u6765\uff0c\u9762\u5411\u5bf9\u8c61\u7684\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5efa\u56fe\uff08SLAM\uff09\u56e0\u5176\u80fd\u591f\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u4f9b\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u4e00\u4e9b\u7814\u7a76\u4eba\u5458\u5c1d\u8bd5\u901a\u8fc7\u5c06\u5efa\u6a21\u5bf9\u8c61\u6b8b\u5dee\u96c6\u6210\u5230\u675f\u8c03\u6574\u4e2d\u6765\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u5f88\u5c11\u6709\u7cfb\u7edf\u80fd\u591f\u8868\u73b0\u51fa\u6bd4\u57fa\u4e8e\u7279\u5f81\u7684\u89c6\u89c9 SLAM \u7cfb\u7edf\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u56e0\u4e3a\u4e00\u822c\u7684\u7c97\u7565\u5bf9\u8c61\u6a21\u578b\uff08\u4f8b\u5982\u957f\u65b9\u4f53\u6216\u692d\u7403\u4f53\uff09\u4e0d\u5982\u7279\u5f81\u70b9\u51c6\u786e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u5bf9\u8c61\u91cc\u7a0b\u8ba1\u548c\u5efa\u56fe\u6846\u67b6 VOOM\uff0c\u4ee5\u4ece\u7c97\u5230\u7ec6\u7684\u65b9\u5f0f\u4f7f\u7528\u9ad8\u7ea7\u5bf9\u8c61\u548c\u4f4e\u7ea7\u70b9\u4f5c\u4e3a\u5206\u5c42\u5730\u6807\uff0c\u800c\u4e0d\u662f\u5728\u6346\u7ed1\u8c03\u6574\u4e2d\u76f4\u63a5\u4f7f\u7528\u5bf9\u8c61\u6b8b\u5dee\u3002\u9996\u5148\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u89c2\u6d4b\u6a21\u578b\u548c\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u5076\u4e8c\u6b21\u66f2\u9762\u6570\u636e\u5173\u8054\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u793a\u7269\u7406\u5bf9\u8c61\u3002\u5b83\u6709\u52a9\u4e8e\u521b\u5efa\u5bc6\u5207\u53cd\u6620\u73b0\u5b9e\u7684 3D \u5730\u56fe\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u4f7f\u7528\u5bf9\u8c61\u4fe1\u606f\u6765\u589e\u5f3a\u7279\u5f81\u70b9\u7684\u6570\u636e\u5173\u8054\uff0c\u4ece\u800c\u66f4\u65b0\u5730\u56fe\u3002\u5728\u89c6\u89c9\u5bf9\u8c61\u6d4b\u8ddd\u540e\u7aef\u4e2d\uff0c\u66f4\u65b0\u7684\u5730\u56fe\u7528\u4e8e\u8fdb\u4e00\u6b65\u4f18\u5316\u76f8\u673a\u59ff\u52bf\u548c\u5bf9\u8c61\u3002\u540c\u65f6\uff0c\u5728\u6211\u4eec\u7684\u89c6\u89c9\u5bf9\u8c61\u6620\u5c04\u8fc7\u7a0b\u4e2d\uff0c\u5229\u7528\u5bf9\u8c61\u548c\u57fa\u4e8e\u70b9\u7684\u5171\u89c6\u56fe\u6765\u6267\u884c\u5c40\u90e8\u675f\u8c03\u6574\u3002\u5b9e\u9a8c\u8868\u660e\uff0cVOOM \u5728\u5b9a\u4f4d\u65b9\u9762\u4f18\u4e8e\u9762\u5411\u5bf9\u8c61\u7684 SLAM \u548c\u7279\u5f81\u70b9 SLAM \u7cfb\u7edf\uff08\u4f8b\u5982 ORB-SLAM2\uff09\u3002\u6211\u4eec\u65b9\u6cd5\u7684\u5b9e\u73b0\u53ef\u4ee5\u5728 https://github.com/yutongwangBIT/VOOM.git \u4e0a\u627e\u5230\u3002|[2402.13609v1](http://arxiv.org/pdf/2402.13609v1)|**[link](https://github.com/yutongwangbit/voom)**|\n", "2402.13566": "|**2024-02-21**|**Event-aware Video Corpus Moment Retrieval**|\u4e8b\u4ef6\u611f\u77e5\u89c6\u9891\u8bed\u6599\u5e93\u65f6\u523b\u68c0\u7d22|Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng|Video Corpus Moment Retrieval (VCMR) is a practical video retrieval task focused on identifying a specific moment within a vast corpus of untrimmed videos using the natural language query. Existing methods for VCMR typically rely on frame-aware video retrieval, calculating similarities between the query and video frames to rank videos based on maximum frame similarity.However, this approach overlooks the semantic structure embedded within the information between frames, namely, the event, a crucial element for human comprehension of videos. Motivated by this, we propose EventFormer, a model that explicitly utilizes events within videos as fundamental units for video retrieval. The model extracts event representations through event reasoning and hierarchical event encoding. The event reasoning module groups consecutive and visually similar frame representations into events, while the hierarchical event encoding encodes information at both the frame and event levels. We also introduce anchor multi-head self-attenion to encourage Transformer to capture the relevance of adjacent content in the video. The training of EventFormer is conducted by two-branch contrastive learning and dual optimization for two sub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMo benchmarks show the effectiveness and efficiency of EventFormer in VCMR, achieving new state-of-the-art results. Additionally, the effectiveness of EventFormer is also validated on partially relevant video retrieval task.|\u89c6\u9891\u8bed\u6599\u5e93\u65f6\u523b\u68c0\u7d22 (VCMR) \u662f\u4e00\u9879\u5b9e\u7528\u7684\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5728\u5927\u91cf\u672a\u4fee\u526a\u89c6\u9891\u7684\u8bed\u6599\u5e93\u4e2d\u8bc6\u522b\u7279\u5b9a\u65f6\u523b\u3002\u73b0\u6709\u7684 VCMR \u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5e27\u611f\u77e5\u89c6\u9891\u68c0\u7d22\uff0c\u8ba1\u7b97\u67e5\u8be2\u5e27\u548c\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u6839\u636e\u6700\u5927\u5e27\u76f8\u4f3c\u6027\u5bf9\u89c6\u9891\u8fdb\u884c\u6392\u540d\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u7565\u4e86\u5d4c\u5165\u5728\u5e27\u4e4b\u95f4\u4fe1\u606f\u4e2d\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u5373\u4e8b\u4ef6\u3001\u4eba\u7c7b\u7406\u89e3\u89c6\u9891\u7684\u5173\u952e\u8981\u7d20\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 EventFormer\uff0c\u8fd9\u662f\u4e00\u79cd\u660e\u786e\u5229\u7528\u89c6\u9891\u4e2d\u7684\u4e8b\u4ef6\u4f5c\u4e3a\u89c6\u9891\u68c0\u7d22\u7684\u57fa\u672c\u5355\u5143\u7684\u6a21\u578b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u4e8b\u4ef6\u63a8\u7406\u548c\u5206\u5c42\u4e8b\u4ef6\u7f16\u7801\u6765\u63d0\u53d6\u4e8b\u4ef6\u8868\u793a\u3002\u4e8b\u4ef6\u63a8\u7406\u6a21\u5757\u5c06\u8fde\u7eed\u4e14\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7684\u5e27\u8868\u793a\u5206\u7ec4\u4e3a\u4e8b\u4ef6\uff0c\u800c\u5206\u5c42\u4e8b\u4ef6\u7f16\u7801\u5728\u5e27\u548c\u4e8b\u4ef6\u7ea7\u522b\u5bf9\u4fe1\u606f\u8fdb\u884c\u7f16\u7801\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u951a\u70b9\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff0c\u4ee5\u9f13\u52b1 Transformer \u6355\u83b7\u89c6\u9891\u4e2d\u76f8\u90bb\u5185\u5bb9\u7684\u76f8\u5173\u6027\u3002 EventFormer\u7684\u8bad\u7ec3\u662f\u901a\u8fc7\u5bf9VCMR\u7684\u4e24\u4e2a\u5b50\u4efb\u52a1\u8fdb\u884c\u53cc\u5206\u652f\u5bf9\u6bd4\u5b66\u4e60\u548c\u5bf9\u5076\u4f18\u5316\u6765\u8fdb\u884c\u7684\u3002 TVR\u3001ANetCaps \u548c DiDeMo \u57fa\u51c6\u6d4b\u8bd5\u7684\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\u4e86 EventFormer \u5728 VCMR \u4e2d\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0cEventFormer \u7684\u6709\u6548\u6027\u4e5f\u5728\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002|[2402.13566v1](http://arxiv.org/pdf/2402.13566v1)|null|\n", "2402.13537": "|**2024-02-21**|**EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization**|EffLoc\uff1a\u7528\u4e8e\u9ad8\u6548 6 \u81ea\u7531\u5ea6\u76f8\u673a\u91cd\u5b9a\u4f4d\u7684\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8f6c\u6362\u5668|Zhendong Xiao, Changhao Chen, Shan Yang, Wu Wei|Camera relocalization is pivotal in computer vision, with applications in AR, drones, robotics, and autonomous driving. It estimates 3D camera position and orientation (6-DoF) from images. Unlike traditional methods like SLAM, recent strides use deep learning for direct end-to-end pose estimation. We propose EffLoc, a novel efficient Vision Transformer for single-image camera relocalization. EffLoc's hierarchical layout, memory-bound self-attention, and feed-forward layers boost memory efficiency and inter-channel communication. Our introduced sequential group attention (SGA) module enhances computational efficiency by diversifying input features, reducing redundancy, and expanding model capacity. EffLoc excels in efficiency and accuracy, outperforming prior methods, such as AtLoc and MapNet. It thrives on large-scale outdoor car-driving scenario, ensuring simplicity, end-to-end trainability, and eliminating handcrafted loss functions.|\u76f8\u673a\u91cd\u65b0\u5b9a\u4f4d\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5728 AR\u3001\u65e0\u4eba\u673a\u3001\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u90fd\u6709\u5e94\u7528\u3002\u5b83\u6839\u636e\u56fe\u50cf\u4f30\u8ba1 3D \u76f8\u673a\u4f4d\u7f6e\u548c\u65b9\u5411 (6-DoF)\u3002\u4e0e SLAM \u7b49\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0c\u6700\u8fd1\u7684\u8fdb\u5c55\u662f\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u76f4\u63a5\u7aef\u5230\u7aef\u59ff\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u63d0\u51fa\u4e86 EffLoc\uff0c\u4e00\u79cd\u7528\u4e8e\u5355\u56fe\u50cf\u76f8\u673a\u91cd\u5b9a\u4f4d\u7684\u65b0\u578b\u9ad8\u6548\u89c6\u89c9\u8f6c\u6362\u5668\u3002 EffLoc \u7684\u5206\u5c42\u5e03\u5c40\u3001\u5185\u5b58\u9650\u5236\u7684\u81ea\u6ce8\u610f\u529b\u548c\u524d\u9988\u5c42\u63d0\u9ad8\u4e86\u5185\u5b58\u6548\u7387\u548c\u901a\u9053\u95f4\u901a\u4fe1\u3002\u6211\u4eec\u5f15\u5165\u7684\u987a\u5e8f\u7fa4\u4f53\u6ce8\u610f\u529b\uff08SGA\uff09\u6a21\u5757\u901a\u8fc7\u591a\u6837\u5316\u8f93\u5165\u7279\u5f81\u3001\u51cf\u5c11\u5197\u4f59\u548c\u6269\u5c55\u6a21\u578b\u5bb9\u91cf\u6765\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002 EffLoc \u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e AtLoc \u548c MapNet \u7b49\u73b0\u6709\u65b9\u6cd5\u3002\u5b83\u5728\u5927\u89c4\u6a21\u6237\u5916\u6c7d\u8f66\u9a7e\u9a76\u573a\u666f\u4e2d\u84ec\u52c3\u53d1\u5c55\uff0c\u786e\u4fdd\u7b80\u5355\u6027\u3001\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u6027\uff0c\u5e76\u6d88\u9664\u624b\u5de5\u5236\u4f5c\u7684\u635f\u5931\u51fd\u6570\u3002|[2402.13537v1](http://arxiv.org/pdf/2402.13537v1)|null|\n", "2402.13475": "|**2024-02-21**|**Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images**|\u57fa\u4e8e\u591a\u5c3a\u5ea6\u65f6\u7a7a\u53d8\u6362\u5668\u7684\u4e0d\u5e73\u8861\u7eb5\u5411\u5b66\u4e60\u7528\u4e8e\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u56fe\u50cf\u7684\u9752\u5149\u773c\u9884\u6d4b|Xikai Yang, Jian Wu, Xi Wang, Yuchen Yuan, Ning Li Wang, Pheng-Ann Heng|Glaucoma is one of the major eye diseases that leads to progressive optic nerve fiber damage and irreversible blindness, afflicting millions of individuals. Glaucoma forecast is a good solution to early screening and intervention of potential patients, which is helpful to prevent further deterioration of the disease. It leverages a series of historical fundus images of an eye and forecasts the likelihood of glaucoma occurrence in the future. However, the irregular sampling nature and the imbalanced class distribution are two challenges in the development of disease forecasting approaches. To this end, we introduce the Multi-scale Spatio-temporal Transformer Network (MST-former) based on the transformer architecture tailored for sequential image inputs, which can effectively learn representative semantic information from sequential images on both temporal and spatial dimensions. Specifically, we employ a multi-scale structure to extract features at various resolutions, which can largely exploit rich spatial information encoded in each image. Besides, we design a time distance matrix to scale time attention in a non-linear manner, which could effectively deal with the irregularly sampled data. Furthermore, we introduce a temperature-controlled Balanced Softmax Cross-entropy loss to address the class imbalance issue. Extensive experiments on the Sequential fundus Images for Glaucoma Forecast (SIGF) dataset demonstrate the superiority of the proposed MST-former method, achieving an AUC of 98.6% for glaucoma forecasting. Besides, our method shows excellent generalization capability on the Alzheimer's Disease Neuroimaging Initiative (ADNI) MRI dataset, with an accuracy of 90.3% for mild cognitive impairment and Alzheimer's disease prediction, outperforming the compared method by a large margin.|\u9752\u5149\u773c\u662f\u5bfc\u81f4\u8fdb\u884c\u6027\u89c6\u795e\u7ecf\u7ea4\u7ef4\u635f\u4f24\u548c\u4e0d\u53ef\u9006\u5931\u660e\u7684\u4e3b\u8981\u773c\u75c5\u4e4b\u4e00\uff0c\u56f0\u6270\u7740\u6570\u767e\u4e07\u4eba\u3002\u9752\u5149\u773c\u9884\u6d4b\u5f88\u597d\u5730\u89e3\u51b3\u4e86\u6f5c\u5728\u60a3\u8005\u7684\u65e9\u671f\u7b5b\u67e5\u548c\u5e72\u9884\uff0c\u6709\u52a9\u4e8e\u9632\u6b62\u75c5\u60c5\u8fdb\u4e00\u6b65\u6076\u5316\u3002\u5b83\u5229\u7528\u4e00\u7cfb\u5217\u5386\u53f2\u773c\u5e95\u56fe\u50cf\u6765\u9884\u6d4b\u672a\u6765\u53d1\u751f\u9752\u5149\u773c\u7684\u53ef\u80fd\u6027\u3002\u7136\u800c\uff0c\u4e0d\u89c4\u5219\u7684\u62bd\u6837\u6027\u8d28\u548c\u4e0d\u5e73\u8861\u7684\u7c7b\u522b\u5206\u5e03\u662f\u75be\u75c5\u9884\u6d4b\u65b9\u6cd5\u53d1\u5c55\u7684\u4e24\u4e2a\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u57fa\u4e8e\u4e3a\u5e8f\u5217\u56fe\u50cf\u8f93\u5165\u91cf\u8eab\u5b9a\u5236\u7684\u53d8\u538b\u5668\u67b6\u6784\u7684\u591a\u5c3a\u5ea6\u65f6\u7a7a\u53d8\u538b\u5668\u7f51\u7edc\uff08MST-former\uff09\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u4ece\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u7684\u5e8f\u5217\u56fe\u50cf\u4e2d\u5b66\u4e60\u4ee3\u8868\u6027\u8bed\u4e49\u4fe1\u606f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u591a\u5c3a\u5ea6\u7ed3\u6784\u6765\u63d0\u53d6\u5404\u79cd\u5206\u8fa8\u7387\u7684\u7279\u5f81\uff0c\u8fd9\u53ef\u4ee5\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5229\u7528\u6bcf\u4e2a\u56fe\u50cf\u4e2d\u7f16\u7801\u7684\u4e30\u5bcc\u7a7a\u95f4\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65f6\u95f4\u8ddd\u79bb\u77e9\u9635\u4ee5\u975e\u7ebf\u6027\u65b9\u5f0f\u7f29\u653e\u65f6\u95f4\u6ce8\u610f\u529b\uff0c\u53ef\u4ee5\u6709\u6548\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u6570\u636e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6e29\u5ea6\u63a7\u5236\u7684\u5e73\u8861 Softmax \u4ea4\u53c9\u71b5\u635f\u5931\u6765\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u5bf9\u9752\u5149\u773c\u9884\u6d4b\u5e8f\u5217\u773c\u5e95\u56fe\u50cf (SIGF) \u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 MST \u524d\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u9752\u5149\u773c\u9884\u6d4b\u7684 AUC \u8fbe\u5230 98.6%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u795e\u7ecf\u5f71\u50cf\u5021\u8bae\uff08ADNI\uff09MRI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5bf9\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4b\u7684\u51c6\u786e\u7387\u8fbe\u523090.3%\uff0c\u5927\u5e45\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002|[2402.13475v1](http://arxiv.org/pdf/2402.13475v1)|null|\n"}, "3D/CG": {"2402.14000": "|**2024-02-21**|**Real-time 3D-aware Portrait Editing from a Single Image**|\u4ece\u5355\u4e2a\u56fe\u50cf\u8fdb\u884c\u5b9e\u65f6 3D \u611f\u77e5\u8096\u50cf\u7f16\u8f91|Qingyan Bai, Yinghao Xu, Zifan Shi, Hao Ouyang, Qiuyu Wang, Ceyuan Yang, Xuan Wang, Gordon Wetzstein, Yujun Shen, Qifeng Chen|This work presents 3DPE, a practical tool that can efficiently edit a face image following given prompts, like reference images or text descriptions, in the 3D-aware manner. To this end, a lightweight module is distilled from a 3D portrait generator and a text-to-image model, which provide prior knowledge of face geometry and open-vocabulary editing capability, respectively. Such a design brings two compelling advantages over existing approaches. First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor. Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min fine-tuning per case). The code, the model, and the interface will be made publicly available to facilitate future research.|\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86 3DPE\uff0c\u8fd9\u662f\u4e00\u79cd\u5b9e\u7528\u5de5\u5177\uff0c\u53ef\u4ee5\u6309\u7167\u7ed9\u5b9a\u7684\u63d0\u793a\uff08\u5982\u53c2\u8003\u56fe\u50cf\u6216\u6587\u672c\u63cf\u8ff0\uff09\u4ee5 3D \u611f\u77e5\u65b9\u5f0f\u6709\u6548\u5730\u7f16\u8f91\u9762\u90e8\u56fe\u50cf\u3002\u4e3a\u6b64\uff0c\u4ece 3D \u8096\u50cf\u751f\u6210\u5668\u548c\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u63d0\u70bc\u51fa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u5b83\u4eec\u5206\u522b\u63d0\u4f9b\u9762\u90e8\u51e0\u4f55\u5f62\u72b6\u7684\u5148\u9a8c\u77e5\u8bc6\u548c\u5f00\u653e\u8bcd\u6c47\u7f16\u8f91\u529f\u80fd\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u5e26\u6765\u4e86\u4e24\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u4f18\u52bf\u3002\u9996\u5148\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u901a\u8fc7\u524d\u9988\u7f51\u7edc\u5b9e\u73b0\u5b9e\u65f6\u7f16\u8f91\uff08\u5373\u6bcf\u4e2a\u56fe\u50cf\u7ea6 0.04 \u79d2\uff09\uff0c\u6bd4\u7b2c\u4e8c\u4e2a\u7ade\u4e89\u5bf9\u624b\u5feb 100 \u500d\u4ee5\u4e0a\u3002\u5176\u6b21\uff0c\u7531\u4e8e\u5f3a\u5927\u7684\u5148\u9a8c\uff0c\u6211\u4eec\u7684\u6a21\u5757\u53ef\u4ee5\u4e13\u6ce8\u4e8e\u7f16\u8f91\u76f8\u5173\u53d8\u4f53\u7684\u5b66\u4e60\uff0c\u4ece\u800c\u80fd\u591f\u5728\u8bad\u7ec3\u9636\u6bb5\u540c\u65f6\u5904\u7406\u5404\u79cd\u7c7b\u578b\u7684\u7f16\u8f91\uff0c\u5e76\u8fdb\u4e00\u6b65\u652f\u6301\u5728\u8bad\u7ec3\u9636\u6bb5\u5feb\u901f\u9002\u5e94\u7528\u6237\u6307\u5b9a\u7684\u65b0\u9896\u7c7b\u578b\u7684\u7f16\u8f91\u3002\u63a8\u7406\uff08\u4f8b\u5982\uff0c\u6bcf\u4e2a\u6848\u4f8b\u7ea6 5 \u5206\u949f\u7684\u5fae\u8c03\uff09\u3002\u4ee3\u7801\u3001\u6a21\u578b\u548c\u754c\u9762\u5c06\u516c\u5f00\uff0c\u4ee5\u65b9\u4fbf\u672a\u6765\u7684\u7814\u7a76\u3002|[2402.14000v1](http://arxiv.org/pdf/2402.14000v1)|null|\n", "2402.13816": "|**2024-02-21**|**A unified framework of non-local parametric methods for image denoising**|\u56fe\u50cf\u53bb\u566a\u975e\u5c40\u90e8\u53c2\u6570\u65b9\u6cd5\u7684\u7edf\u4e00\u6846\u67b6|S\u00e9bastien Herbreteau, Charles Kervrann|We propose a unified view of non-local methods for single-image denoising, for which BM3D is the most popular representative, that operate by gathering noisy patches together according to their similarities in order to process them collaboratively. Our general estimation framework is based on the minimization of the quadratic risk, which is approximated in two steps, and adapts to photon and electronic noises. Relying on unbiased risk estimation (URE) for the first step and on ``internal adaptation'', a concept borrowed from deep learning theory, for the second, we show that our approach enables to reinterpret and reconcile previous state-of-the-art non-local methods. Within this framework, we propose a novel denoiser called NL-Ridge that exploits linear combinations of patches. While conceptually simpler, we show that NL-Ridge can outperform well-established state-of-the-art single-image denoisers.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5355\u56fe\u50cf\u53bb\u566a\u7684\u975e\u5c40\u90e8\u65b9\u6cd5\u7684\u7edf\u4e00\u89c6\u56fe\uff0c\u5176\u4e2d BM3D \u662f\u6700\u6d41\u884c\u7684\u4ee3\u8868\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6839\u636e\u566a\u58f0\u5757\u7684\u76f8\u4f3c\u6027\u5c06\u566a\u58f0\u5757\u6536\u96c6\u5728\u4e00\u8d77\u4ee5\u4fbf\u534f\u4f5c\u5904\u7406\u5b83\u4eec\u6765\u8fdb\u884c\u64cd\u4f5c\u3002\u6211\u4eec\u7684\u4e00\u822c\u4f30\u8ba1\u6846\u67b6\u57fa\u4e8e\u4e8c\u6b21\u98ce\u9669\u7684\u6700\u5c0f\u5316\uff0c\u5206\u4e24\u6b65\u8fd1\u4f3c\uff0c\u5e76\u9002\u5e94\u5149\u5b50\u548c\u7535\u5b50\u566a\u58f0\u3002\u7b2c\u4e00\u6b65\u4f9d\u9760\u65e0\u504f\u98ce\u9669\u4f30\u8ba1\uff08URE\uff09\uff0c\u7b2c\u4e8c\u6b65\u4f9d\u9760\u201c\u5185\u90e8\u9002\u5e94\u201d\uff08\u4e00\u4e2a\u501f\u7528\u81ea\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba\u7684\u6982\u5ff5\uff09\uff0c\u6211\u4eec\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u91cd\u65b0\u89e3\u91ca\u548c\u534f\u8c03\u4ee5\u524d\u7684\u72b6\u6001\u827a\u672f\u975e\u672c\u5730\u65b9\u6cd5\u3002\u5728\u6b64\u6846\u67b6\u5185\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a NL-Ridge \u7684\u65b0\u578b\u964d\u566a\u5668\uff0c\u5b83\u5229\u7528\u8865\u4e01\u7684\u7ebf\u6027\u7ec4\u5408\u3002\u867d\u7136\u6982\u5ff5\u4e0a\u66f4\u7b80\u5355\uff0c\u4f46\u6211\u4eec\u8bc1\u660e NL-Ridge \u7684\u6027\u80fd\u4f18\u4e8e\u6210\u719f\u7684\u6700\u5148\u8fdb\u7684\u5355\u56fe\u50cf\u964d\u566a\u5668\u3002|[2402.13816v1](http://arxiv.org/pdf/2402.13816v1)|null|\n", "2402.13724": "|**2024-02-21**|**Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters**|\u81ea\u5e26\u89d2\u8272\uff1a\u81ea\u52a8\u751f\u6210\u81ea\u5b9a\u4e49\u89d2\u8272\u9762\u90e8\u52a8\u753b\u7684\u6574\u4f53\u89e3\u51b3\u65b9\u6848|Zechen Bai, Peng Chen, Xiaolan Peng, Lu Liu, Hui Chen, Mike Zheng Shou, Feng Tian|Animating virtual characters has always been a fundamental research problem in virtual reality (VR). Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans. However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters. In this paper, we propose a holistic solution to automatically animate virtual human faces. In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients. This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies. Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications. The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results. Furthermore, inspired by the spirit of Human-in-the-loop (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences. The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications.|\u865a\u62df\u89d2\u8272\u52a8\u753b\u4e00\u76f4\u662f\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u9886\u57df\u7684\u4e00\u4e2a\u57fa\u7840\u7814\u7a76\u95ee\u9898\u3002\u9762\u90e8\u52a8\u753b\u5728\u6709\u6548\u4f20\u8fbe\u865a\u62df\u4eba\u7684\u60c5\u611f\u548c\u6001\u5ea6\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u521b\u5efa\u6b64\u7c7b\u9762\u90e8\u52a8\u753b\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5f53\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u6d89\u53ca\u4f7f\u7528\u6602\u8d35\u7684\u8fd0\u52a8\u6355\u6349\u8bbe\u5907\u6216\u4eba\u7c7b\u52a8\u753b\u5e08\u5728\u8c03\u6574\u52a8\u753b\u53c2\u6570\u65b9\u9762\u6295\u5165\u5927\u91cf\u65f6\u95f4\u548c\u7cbe\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5236\u4f5c\u865a\u62df\u4eba\u8138\u52a8\u753b\u7684\u6574\u4f53\u89e3\u51b3\u65b9\u6848\u3002\u5728\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u4e2d\uff0c\u9996\u5148\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u4f30\u8ba1\u6df7\u5408\u5f62\u72b6\u7cfb\u6570\u5c06\u9762\u90e8\u8868\u60c5\u4ece\u8f93\u5165\u9762\u90e8\u56fe\u50cf\u91cd\u65b0\u5b9a\u4f4d\u5230\u865a\u62df\u4eba\u8138\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u751f\u6210\u5177\u6709\u4e0d\u540c\u5916\u89c2\u548c\u6df7\u5408\u5f62\u72b6\u62d3\u6251\u7684\u89d2\u8272\u7684\u52a8\u753b\u7684\u7075\u6d3b\u6027\u3002\u5176\u6b21\uff0c\u4f7f\u7528Unity 3D\u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u5305\uff0c\u4f7f\u5176\u517c\u5bb9\u6700\u6d41\u884c\u7684VR\u5e94\u7528\u7a0b\u5e8f\u3002\u8be5\u5de5\u5177\u5305\u63a5\u53d7\u56fe\u50cf\u548c\u89c6\u9891\u4f5c\u4e3a\u8f93\u5165\u6765\u4e3a\u76ee\u6807\u865a\u62df\u4eba\u8138\u5236\u4f5c\u52a8\u753b\uff0c\u5e76\u4f7f\u7528\u6237\u80fd\u591f\u64cd\u7eb5\u52a8\u753b\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u5728\u4eba\u673a\u5faa\u73af\uff08HITL\uff09\u7cbe\u795e\u7684\u542f\u53d1\u4e0b\uff0c\u6211\u4eec\u5229\u7528\u7528\u6237\u53cd\u9988\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u548c\u5de5\u5177\u5305\u7684\u6027\u80fd\uff0c\u4ece\u800c\u589e\u52a0\u5b9a\u5236\u5c5e\u6027\u4ee5\u6ee1\u8db3\u7528\u6237\u504f\u597d\u3002\u6211\u4eec\u5c06\u516c\u5f00\u6574\u4e2a\u89e3\u51b3\u65b9\u6848\u7684\u4ee3\u7801\uff0c\u5b83\u6709\u53ef\u80fd\u52a0\u901f VR \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u7684\u9762\u90e8\u52a8\u753b\u7684\u751f\u6210\u3002|[2402.13724v1](http://arxiv.org/pdf/2402.13724v1)|null|\n", "2402.13505": "|**2024-02-21**|**SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning**|SimPro\uff1a\u5b9e\u73b0\u73b0\u5b9e\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u7684\u7b80\u5355\u6982\u7387\u6846\u67b6|Chaoqun Du, Yizeng Han, Gao Huang|Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization phase, leading to the formulation of a Bayes classifier. The Bayes classifier, in turn, enhances the quality of pseudo-labels in the expectation phase. Remarkably, the SimPro framework not only comes with theoretical guarantees but also is straightforward to implement. Moreover, we introduce two novel class distributions broadening the scope of the evaluation. Our method showcases consistent state-of-the-art performance across diverse benchmarks and data distribution scenarios. Our code is available at https://github.com/LeapLabTHU/SimPro.|\u534a\u76d1\u7763\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\u96c6\u4e2d\u5728\u4e00\u4e2a\u66f4\u73b0\u5b9e\u4f46\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\uff1a\u89e3\u51b3\u6807\u8bb0\u6570\u636e\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u800c\u672a\u6807\u8bb0\u6570\u636e\u7684\u7c7b\u522b\u5206\u5e03\u4ecd\u7136\u672a\u77e5\u4e14\u53ef\u80fd\u4e0d\u5339\u914d\u3002\u8be5\u9886\u57df\u5f53\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u9884\u5148\u5047\u8bbe\u6709\u5173\u672a\u6807\u8bb0\u6570\u636e\u7684\u7c7b\u522b\u5206\u5e03\u7684\u4e25\u683c\u5047\u8bbe\uff0c\u4ece\u800c\u5c06\u6a21\u578b\u7684\u9002\u5e94\u6027\u9650\u5236\u5728\u67d0\u4e9b\u5206\u5e03\u8303\u56f4\u5185\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u9002\u5e94\u6027\u7684\u6846\u67b6\uff0c\u79f0\u4e3a SimPro\uff0c\u5b83\u4e0d\u4f9d\u8d56\u4e8e\u4efb\u4f55\u5173\u4e8e\u672a\u6807\u8bb0\u6570\u636e\u5206\u5e03\u7684\u9884\u5b9a\u4e49\u5047\u8bbe\u3002\u6211\u4eec\u7684\u6846\u67b6\u4ee5\u6982\u7387\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u901a\u8fc7\u663e\u5f0f\u89e3\u8026\u6761\u4ef6\u7c7b\u5206\u5e03\u548c\u8fb9\u7f18\u7c7b\u5206\u5e03\u7684\u5efa\u6a21\uff0c\u521b\u65b0\u5730\u6539\u8fdb\u4e86\u671f\u671b\u6700\u5927\u5316\uff08EM\uff09\u7b97\u6cd5\u3002\u8fd9\u79cd\u5206\u79bb\u6709\u52a9\u4e8e\u5728\u6700\u5927\u5316\u9636\u6bb5\u63d0\u4f9b\u7c7b\u5206\u5e03\u4f30\u8ba1\u7684\u5c01\u95ed\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u5f62\u6210\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u3002\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u53cd\u8fc7\u6765\u53c8\u63d0\u9ad8\u4e86\u671f\u671b\u9636\u6bb5\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSimPro \u6846\u67b6\u4e0d\u4ec5\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u800c\u4e14\u6613\u4e8e\u5b9e\u65bd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u7c7b\u522b\u5206\u5e03\uff0c\u6269\u5927\u4e86\u8bc4\u4f30\u8303\u56f4\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u57fa\u51c6\u548c\u6570\u636e\u5206\u5e03\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u4e00\u81f4\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/LeapLabTHU/SimPro \u83b7\u53d6\u3002|[2402.13505v1](http://arxiv.org/pdf/2402.13505v1)|**[link](https://github.com/leaplabthu/simpro)**|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {}, "\u5176\u4ed6": {"2402.14015": "|**2024-02-21**|**Corrective Machine Unlearning**|\u7ea0\u6b63\u673a\u5668\u9057\u5fd8|Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal|Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning. However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting. We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training.|\u7531\u4e8e\u4f7f\u7528\u4ece\u4e92\u8054\u7f51\u83b7\u53d6\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8d8a\u6765\u8d8a\u9762\u4e34\u6570\u636e\u5b8c\u6574\u6027\u7684\u6311\u6218\u3002\u6211\u4eec\u7814\u7a76\u6a21\u578b\u5f00\u53d1\u4eba\u5458\u5728\u68c0\u6d4b\u5230\u67d0\u4e9b\u6570\u636e\u88ab\u64cd\u7eb5\u6216\u4e0d\u6b63\u786e\u65f6\u53ef\u4ee5\u91c7\u53d6\u54ea\u4e9b\u63aa\u65bd\u3002\u8fd9\u79cd\u88ab\u64cd\u7eb5\u7684\u6570\u636e\u53ef\u80fd\u4f1a\u9020\u6210\u4e0d\u5229\u5f71\u54cd\uff0c\u4f8b\u5982\u5bb9\u6613\u53d7\u5230\u540e\u95e8\u6837\u672c\u7684\u5f71\u54cd\u3001\u7cfb\u7edf\u504f\u5dee\uff0c\u4ee5\u53ca\u901a\u5e38\u4f1a\u964d\u4f4e\u67d0\u4e9b\u8f93\u5165\u57df\u7684\u51c6\u786e\u6027\u3002\u901a\u5e38\uff0c\u6240\u6709\u88ab\u64cd\u7eb5\u7684\u8bad\u7ec3\u6837\u672c\u90fd\u662f\u672a\u77e5\u7684\uff0c\u5e76\u4e14\u53ea\u6709\u53d7\u5f71\u54cd\u6570\u636e\u7684\u4e00\u5c0f\u90e8\u5206\u4ee3\u8868\u6027\u5b50\u96c6\u88ab\u6807\u8bb0\u3002\u6211\u4eec\u5c06\u201c\u7ea0\u6b63\u6027\u673a\u5668\u9057\u5fd8\u201d\u5f62\u5f0f\u5316\u4e3a\u51cf\u8f7b\u53d7\u672a\u77e5\u64cd\u4f5c\u5f71\u54cd\u7684\u6570\u636e\u5bf9\u8bad\u7ec3\u6a21\u578b\u7684\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u53ea\u77e5\u9053\u53d7\u5f71\u54cd\u6837\u672c\u7684\u5b50\u96c6\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u7ea0\u6b63\u6027\u9057\u5fd8\u95ee\u9898\u4e0e\u4f20\u7edf\u7684\u9762\u5411\u9690\u79c1\u7684\u9057\u5fd8\u95ee\u9898\u6709\u7740\u663e\u7740\u4e0d\u540c\u7684\u8981\u6c42\u3002\u6211\u4eec\u53d1\u73b0\u5927\u591a\u6570\u73b0\u6709\u7684\u5fd8\u5374\u65b9\u6cd5\uff0c\u5305\u62ec\u4ece\u5934\u5f00\u59cb\u7684\u9ec4\u91d1\u6807\u51c6\u518d\u8bad\u7ec3\uff0c\u90fd\u9700\u8981\u8bc6\u522b\u5927\u90e8\u5206\u88ab\u64cd\u7eb5\u7684\u6570\u636e\uff0c\u4ee5\u8fdb\u884c\u6709\u6548\u7684\u7ea0\u6b63\u6027\u5fd8\u5374\u3002\u7136\u800c\uff0c\u4e00\u79cd\u65b9\u6cd5 SSD \u5728\u4ec5\u7528\u4e00\u5c0f\u90e8\u5206\u88ab\u64cd\u7eb5\u7684\u6837\u672c\u6765\u6d88\u9664\u4e0d\u5229\u5f71\u54cd\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u9650\u7684\u6210\u529f\uff0c\u8fd9\u8868\u660e\u4e86\u8fd9\u79cd\u8bbe\u7f6e\u7684\u6613\u5904\u7406\u6027\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u5de5\u4f5c\u80fd\u591f\u4fc3\u8fdb\u7814\u7a76\u5f00\u53d1\u66f4\u597d\u7684\u7ea0\u6b63\u6027\u9057\u5fd8\u65b9\u6cd5\uff0c\u5e76\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u65b0\u7684\u7b56\u7565\u6765\u5e94\u5bf9\u7f51\u7edc\u89c4\u6a21\u57f9\u8bad\u5e26\u6765\u7684\u6570\u636e\u5b8c\u6574\u6027\u6311\u6218\u3002|[2402.14015v1](http://arxiv.org/pdf/2402.14015v1)|null|\n", "2402.13756": "|**2024-02-21**|**High-throughput Visual Nano-drone to Nano-drone Relative Localization using Onboard Fully Convolutional Networks**|\u4f7f\u7528\u673a\u8f7d\u5168\u5377\u79ef\u7f51\u7edc\u8fdb\u884c\u9ad8\u901a\u91cf\u89c6\u89c9\u7eb3\u7c73\u65e0\u4eba\u673a\u5230\u7eb3\u7c73\u65e0\u4eba\u673a\u7684\u76f8\u5bf9\u5b9a\u4f4d|Luca Crupi, Alessandro Giusti, Daniele Palossi|Relative drone-to-drone localization is a fundamental building block for any swarm operations. We address this task in the context of miniaturized nano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to novel use cases enabled by their reduced form factor. The price for their versatility comes with limited onboard resources, i.e., sensors, processing units, and memory, which limits the complexity of the onboard algorithms. A traditional solution to overcome these limitations is represented by lightweight deep learning models directly deployed aboard nano-drones. This work tackles the challenging relative pose estimation between nano-drones using only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip (SoC) hosted onboard. We present a vertically integrated system based on a novel vision-based fully convolutional neural network (FCNN), which runs at 39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8 SoC. We compare our FCNN against three State-of-the-Art (SoA) systems. Considering the best-performing SoA approach, our model results in an R-squared improvement from 32 to 47% on the horizontal image coordinate and from 18 to 55% on the vertical image coordinate, on a real-world dataset of 30k images. Finally, our in-field tests show a reduction of the average tracking error of 37% compared to a previous SoA work and an endurance performance up to the entire battery lifetime of 4 minutes.|\u65e0\u4eba\u673a\u5bf9\u65e0\u4eba\u673a\u7684\u76f8\u5bf9\u5b9a\u4f4d\u662f\u4efb\u4f55\u7fa4\u4f53\u884c\u52a8\u7684\u57fa\u672c\u7ec4\u6210\u90e8\u5206\u3002\u6211\u4eec\u5728\u5fae\u578b\u7eb3\u7c73\u65e0\u4eba\u673a\uff08\u5373\u76f4\u5f84 10 \u5398\u7c73\uff09\u7684\u80cc\u666f\u4e0b\u89e3\u51b3\u4e86\u8fd9\u9879\u4efb\u52a1\uff0c\u7531\u4e8e\u5176\u5c3a\u5bf8\u51cf\u5c0f\u800c\u5e26\u6765\u7684\u65b0\u7528\u4f8b\uff0c\u4eba\u4eec\u5bf9\u8fd9\u79cd\u65e0\u4eba\u673a\u7684\u5174\u8da3\u65e5\u76ca\u589e\u957f\u3002\u5176\u591a\u529f\u80fd\u6027\u7684\u4ee3\u4ef7\u662f\u6709\u9650\u7684\u677f\u8f7d\u8d44\u6e90\uff0c\u5373\u4f20\u611f\u5668\u3001\u5904\u7406\u5355\u5143\u548c\u5185\u5b58\uff0c\u8fd9\u9650\u5236\u4e86\u677f\u8f7d\u7b97\u6cd5\u7684\u590d\u6742\u6027\u3002\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u7684\u4f20\u7edf\u89e3\u51b3\u65b9\u6848\u662f\u76f4\u63a5\u90e8\u7f72\u5728\u7eb3\u7c73\u65e0\u4eba\u673a\u4e0a\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u8fd9\u9879\u5de5\u4f5c\u4ec5\u4f7f\u7528\u7070\u5ea6\u4f4e\u5206\u8fa8\u7387\u76f8\u673a\u548c\u673a\u8f7d\u8d85\u4f4e\u529f\u8017\u7247\u4e0a\u7cfb\u7edf\uff08SoC\uff09\u6765\u89e3\u51b3\u7eb3\u7c73\u65e0\u4eba\u673a\u4e4b\u95f4\u5177\u6709\u6311\u6218\u6027\u7684\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b0\u9896\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u5168\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (FCNN) \u7684\u5782\u76f4\u96c6\u6210\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5728\u4f7f\u7528 GWT GAP8 SoC \u6269\u5c55\u7684 Crazyflie \u7eb3\u7c73\u65e0\u4eba\u673a\u4e0a\u4ee5 39Hz \u8fd0\u884c\uff0c\u529f\u8017\u4e3a 101mW\u3002\u6211\u4eec\u5c06 FCNN \u4e0e\u4e09\u4e2a\u6700\u5148\u8fdb\u7684 (SoA) \u7cfb\u7edf\u8fdb\u884c\u6bd4\u8f83\u3002\u8003\u8651\u5230\u6027\u80fd\u6700\u4f73\u7684 SoA \u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728 30k \u56fe\u50cf\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u6c34\u5e73\u56fe\u50cf\u5750\u6807\u4e0a\u7684 R \u5e73\u65b9\u4ece 32% \u63d0\u9ad8\u5230 47%\uff0c\u5782\u76f4\u56fe\u50cf\u5750\u6807\u4e0a\u4ece 18% \u63d0\u9ad8\u5230 55%\u3002\u6700\u540e\uff0c\u6211\u4eec\u7684\u73b0\u573a\u6d4b\u8bd5\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u7684 SoA \u5de5\u4f5c\u76f8\u6bd4\uff0c\u5e73\u5747\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e\u4e86 37%\uff0c\u5e76\u4e14\u6574\u4e2a\u7535\u6c60\u5bff\u547d\u957f\u8fbe 4 \u5206\u949f\u7684\u8010\u4e45\u6027\u80fd\u3002|[2402.13756v1](http://arxiv.org/pdf/2402.13756v1)|null|\n", "2402.13636": "|**2024-02-21**|**A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models**|\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6027\u522b\u504f\u89c1\u7684\u7edf\u4e00\u6846\u67b6\u548c\u6570\u636e\u96c6|Ashutosh Sathe, Prachi Jain, Sunayana Sitaram|Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of recent vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u5f97\u5230\u5e7f\u6cdb\u91c7\u7528\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u7cfb\u7edf\u5730\u8bc4\u4f30 VLM \u4e2d\u7684\u6027\u522b\u804c\u4e1a\u504f\u89c1\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u6db5\u76d6\u4e86\u6700\u65b0 VLM \u652f\u6301\u7684\u6240\u6709\u63a8\u7406\u6a21\u5f0f\uff0c\u5305\u62ec\u56fe\u50cf\u5230\u6587\u672c\u3001\u6587\u672c\u5230\u6587\u672c\u3001\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5408\u6210\u7684\u9ad8\u8d28\u91cf\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u6a21\u7cca\u4e86\u4e13\u4e1a\u884c\u4e3a\u4e2d\u7684\u6027\u522b\u5dee\u5f02\uff0c\u4ee5\u8861\u91cf\u6027\u522b\u504f\u89c1\u3002\u5728\u6211\u4eec\u5bf9\u6700\u65b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u4e0d\u540c\u7684\u8f93\u5165\u8f93\u51fa\u6a21\u5f0f\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u504f\u5dee\u5e45\u5ea6\u548c\u65b9\u5411\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u5de5\u4f5c\u80fd\u591f\u5e2e\u52a9\u6307\u5bfc\u672a\u6765\u6539\u8fdb VLM \u7684\u8fdb\u5c55\uff0c\u4ee5\u5b66\u4e60\u793e\u4f1a\u516c\u6b63\u7684\u8868\u5f81\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u6211\u4eec\u7684\u6570\u636e\u548c\u4ee3\u7801\u3002|[2402.13636v1](http://arxiv.org/pdf/2402.13636v1)|null|\n", "2402.13629": "|**2024-02-21**|**Adversarial Purification and Fine-tuning for Robust UDC Image Restoration**|\u7528\u4e8e\u9c81\u68d2 UDC \u56fe\u50cf\u6062\u590d\u7684\u5bf9\u6297\u6027\u51c0\u5316\u548c\u5fae\u8c03|Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Wenhan Luo, Zhaoxin Fan, Jianfeng Lu|This study delves into the enhancement of Under-Display Camera (UDC) image restoration models, focusing on their robustness against adversarial attacks. Despite its innovative approach to seamless display integration, UDC technology faces unique image degradation challenges exacerbated by the susceptibility to adversarial perturbations. Our research initially conducts an in-depth robustness evaluation of deep-learning-based UDC image restoration models by employing several white-box and black-box attacking methods. This evaluation is pivotal in understanding the vulnerabilities of current UDC image restoration techniques. Following the assessment, we introduce a defense framework integrating adversarial purification with subsequent fine-tuning processes. First, our approach employs diffusion-based adversarial purification, effectively neutralizing adversarial perturbations. Then, we apply the fine-tuning methodologies to refine the image restoration models further, ensuring that the quality and fidelity of the restored images are maintained. The effectiveness of our proposed approach is validated through extensive experiments, showing marked improvements in resilience against typical adversarial attacks.|\u672c\u7814\u7a76\u6df1\u5165\u7814\u7a76\u4e86\u5c4f\u4e0b\u6444\u50cf\u5934 (UDC) \u56fe\u50cf\u6062\u590d\u6a21\u578b\u7684\u589e\u5f3a\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u9488\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u5c3d\u7ba1 UDC \u6280\u672f\u91c7\u7528\u521b\u65b0\u7684\u65e0\u7f1d\u663e\u793a\u96c6\u6210\u65b9\u6cd5\uff0c\u4f46\u4ecd\u9762\u4e34\u7740\u72ec\u7279\u7684\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u6311\u6218\uff0c\u800c\u5bf9\u5bf9\u6297\u6027\u6270\u52a8\u7684\u654f\u611f\u6027\u52a0\u5267\u4e86\u8fd9\u4e00\u6311\u6218\u3002\u6211\u4eec\u7684\u7814\u7a76\u9996\u5148\u91c7\u7528\u591a\u79cd\u767d\u76d2\u548c\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\u5bf9\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684 UDC \u56fe\u50cf\u6062\u590d\u6a21\u578b\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002\u8be5\u8bc4\u4f30\u5bf9\u4e8e\u7406\u89e3\u5f53\u524d UDC \u56fe\u50cf\u6062\u590d\u6280\u672f\u7684\u6f0f\u6d1e\u81f3\u5173\u91cd\u8981\u3002\u7ecf\u8fc7\u8bc4\u4f30\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5c06\u5bf9\u6297\u6027\u51c0\u5316\u4e0e\u540e\u7eed\u5fae\u8c03\u8fc7\u7a0b\u76f8\u7ed3\u5408\u7684\u9632\u5fa1\u6846\u67b6\u3002\u9996\u5148\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u5bf9\u6297\u6027\u51c0\u5316\uff0c\u6709\u6548\u5730\u4e2d\u548c\u5bf9\u6297\u6027\u6270\u52a8\u3002\u7136\u540e\uff0c\u6211\u4eec\u5e94\u7528\u5fae\u8c03\u65b9\u6cd5\u8fdb\u4e00\u6b65\u7ec6\u5316\u56fe\u50cf\u6062\u590d\u6a21\u578b\uff0c\u786e\u4fdd\u4fdd\u6301\u6062\u590d\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u663e\u793a\u51fa\u9488\u5bf9\u5178\u578b\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5f39\u6027\u663e\u7740\u63d0\u9ad8\u3002|[2402.13629v1](http://arxiv.org/pdf/2402.13629v1)|null|\n", "2402.13536": "|**2024-02-21**|**Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel**|\u63a2\u7d22\u6bcf\u50cf\u7d20\u5fae\u6bd4\u7279\u8bed\u4e49\u56fe\u50cf\u538b\u7f29\u7684\u6781\u9650|Jordan Dotzel, Bahaa Kotb, James Dotzel, Mohamed Abdelfattah, Zhiru Zhang|Traditional methods, such as JPEG, perform image compression by operating on structural information, such as pixel values or frequency content. These methods are effective to bitrates around one bit per pixel (bpp) and higher at standard image sizes. In contrast, text-based semantic compression directly stores concepts and their relationships using natural language, which has evolved with humans to efficiently represent these salient concepts. These methods can operate at extremely low bitrates by disregarding structural information like location, size, and orientation. In this work, we use GPT-4V and DALL-E3 from OpenAI to explore the quality-compression frontier for image compression and identify the limitations of current technology. We push semantic compression as low as 100 $\\mu$bpp (up to $10,000\\times$ smaller than JPEG) by introducing an iterative reflection process to improve the decoded image. We further hypothesize this 100 $\\mu$bpp level represents a soft limit on semantic compression at standard image resolutions.|\u4f20\u7edf\u65b9\u6cd5\uff08\u4f8b\u5982 JPEG\uff09\u901a\u8fc7\u5bf9\u7ed3\u6784\u4fe1\u606f\uff08\u4f8b\u5982\u50cf\u7d20\u503c\u6216\u9891\u7387\u5185\u5bb9\uff09\u8fdb\u884c\u64cd\u4f5c\u6765\u6267\u884c\u56fe\u50cf\u538b\u7f29\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u4e8e\u6807\u51c6\u56fe\u50cf\u5c3a\u5bf8\u4e0b\u5927\u7ea6\u6bcf\u50cf\u7d20\u4e00\u4f4d (bpp) \u548c\u66f4\u9ad8\u7684\u6bd4\u7279\u7387\u6709\u6548\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u4e8e\u6587\u672c\u7684\u8bed\u4e49\u538b\u7f29\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u5b58\u50a8\u6982\u5ff5\u53ca\u5176\u5173\u7cfb\uff0c\u81ea\u7136\u8bed\u8a00\u968f\u7740\u4eba\u7c7b\u7684\u53d1\u5c55\u800c\u6709\u6548\u5730\u8868\u793a\u8fd9\u4e9b\u663e\u7740\u6982\u5ff5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u5728\u6781\u4f4e\u7684\u6bd4\u7279\u7387\u4e0b\u8fd0\u884c\uff0c\u5ffd\u7565\u4f4d\u7f6e\u3001\u5927\u5c0f\u548c\u65b9\u5411\u7b49\u7ed3\u6784\u4fe1\u606f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 OpenAI \u7684 GPT-4V \u548c DALL-E3 \u6765\u63a2\u7d22\u56fe\u50cf\u538b\u7f29\u7684\u8d28\u91cf\u538b\u7f29\u524d\u6cbf\u5e76\u786e\u5b9a\u5f53\u524d\u6280\u672f\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u5f15\u5165\u8fed\u4ee3\u53cd\u5c04\u8fc7\u7a0b\u6765\u6539\u8fdb\u89e3\u7801\u56fe\u50cf\uff0c\u6211\u4eec\u5c06\u8bed\u4e49\u538b\u7f29\u63a8\u81f3\u4f4e\u81f3 100 $\\mu$bpp\uff08\u6bd4 JPEG \u5c0f\u9ad8\u8fbe $10,000\\times$\uff09\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5047\u8bbe\u8fd9\u4e2a 100 $\\mu$bpp \u6c34\u5e73\u4ee3\u8868\u4e86\u6807\u51c6\u56fe\u50cf\u5206\u8fa8\u7387\u4e0b\u8bed\u4e49\u538b\u7f29\u7684\u8f6f\u9650\u5236\u3002|[2402.13536v1](http://arxiv.org/pdf/2402.13536v1)|null|\n", "2402.13488": "|**2024-02-21**|**A Feature Matching Method Based on Multi-Level Refinement Strategy**|\u4e00\u79cd\u57fa\u4e8e\u591a\u7ea7\u7ec6\u5316\u7b56\u7565\u7684\u7279\u5f81\u5339\u914d\u65b9\u6cd5|Shaojie Zhang, Yinghui Wang, Jiaxing Ma, Jinlong Yang, Tao Yan, Liangyi Huang, Mingfeng Wang|Feature matching is a fundamental and crucial process in visual SLAM, and precision has always been a challenging issue in feature matching. In this paper, based on a multi-level fine matching strategy, we propose a new feature matching method called KTGP-ORB. This method utilizes the similarity of local appearance in the Hamming space generated by feature descriptors to establish initial correspondences. It combines the constraint of local image motion smoothness, uses the GMS algorithm to enhance the accuracy of initial matches, and finally employs the PROSAC algorithm to optimize matches, achieving precise matching based on global grayscale information in Euclidean space. Experimental results demonstrate that the KTGP-ORB method reduces the error by an average of 29.92% compared to the ORB algorithm in complex scenes with illumination variations and blur.|\u7279\u5f81\u5339\u914d\u662f\u89c6\u89c9SLAM\u4e2d\u4e00\u4e2a\u57fa\u7840\u4e14\u5173\u952e\u7684\u8fc7\u7a0b\uff0c\u800c\u7cbe\u5ea6\u4e00\u76f4\u662f\u7279\u5f81\u5339\u914d\u4e2d\u7684\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u57fa\u4e8e\u591a\u7ea7\u7cbe\u7ec6\u5339\u914d\u7b56\u7565\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u5339\u914d\u65b9\u6cd5\uff0c\u79f0\u4e3aKTGP-ORB\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u7279\u5f81\u63cf\u8ff0\u7b26\u751f\u6210\u7684\u6c49\u660e\u7a7a\u95f4\u4e2d\u5c40\u90e8\u5916\u89c2\u7684\u76f8\u4f3c\u6027\u6765\u5efa\u7acb\u521d\u59cb\u5bf9\u5e94\u5173\u7cfb\u3002\u7ed3\u5408\u5c40\u90e8\u56fe\u50cf\u8fd0\u52a8\u5e73\u6ed1\u5ea6\u7684\u7ea6\u675f\uff0c\u5229\u7528GMS\u7b97\u6cd5\u589e\u5f3a\u521d\u59cb\u5339\u914d\u7684\u7cbe\u5ea6\uff0c\u6700\u540e\u5229\u7528PROSAC\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u5339\u914d\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u5168\u5c40\u7070\u5ea6\u4fe1\u606f\u7684\u7cbe\u786e\u5339\u914d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5149\u7167\u53d8\u5316\u548c\u6a21\u7cca\u7684\u590d\u6742\u573a\u666f\u4e2d\uff0cKTGP-ORB\u65b9\u6cd5\u6bd4ORB\u7b97\u6cd5\u5e73\u5747\u964d\u4f4e\u4e8629.92%\u7684\u8bef\u5dee\u3002|[2402.13488v1](http://arxiv.org/pdf/2402.13488v1)|null|\n"}}