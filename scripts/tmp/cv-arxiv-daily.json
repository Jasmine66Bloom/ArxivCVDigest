{"\u751f\u6210\u6a21\u578b": {"2407.04687": "|**2024-07-05**|**Embracing Massive Medical Data**|\u62e5\u62b1\u6d77\u91cf\u533b\u7597\u6570\u636e|Yu-Cheng Chou, Zongwei Zhou, Alan Yuille|As massive medical data become available with an increasing number of scans, expanding classes, and varying sources, prevalent training paradigms -- where AI is trained with multiple passes over fixed, finite datasets -- face significant challenges. First, training AI all at once on such massive data is impractical as new scans/sources/classes continuously arrive. Second, training AI continuously on new scans/sources/classes can lead to catastrophic forgetting, where AI forgets old data as it learns new data, and vice versa. To address these two challenges, we propose an online learning method that enables training AI from massive medical data. Instead of repeatedly training AI on randomly selected data samples, our method identifies the most significant samples for the current AI model based on their data uniqueness and prediction uncertainty, then trains the AI on these selective data samples. Compared with prevalent training paradigms, our method not only improves data efficiency by enabling training on continual data streams, but also mitigates catastrophic forgetting by selectively training AI on significant data samples that might otherwise be forgotten, outperforming by 15% in Dice score for multi-organ and tumor segmentation.   The code is available at https://github.com/MrGiovanni/OnlineLearning||[2407.04687v1](http://arxiv.org/pdf/2407.04687v1)|**[link](https://github.com/mrgiovanni/onlinelearning)**|\n", "2407.04461": "|**2024-07-05**|**VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing**|VCD-Texture\uff1a\u57fa\u4e8e\u65b9\u5dee\u5bf9\u9f50\u7684 3D-2D \u8054\u5408\u53bb\u566a\u6587\u672c\u5f15\u5bfc\u7eb9\u7406|Shang Liu, Chaohui Yu, Chenjie Cao, Wen Qian, Fan Wang|Recent research on texture synthesis for 3D shapes benefits a lot from dramatically developed 2D text-to-image diffusion models, including inpainting-based and optimization-based approaches. However, these methods ignore the modal gap between the 2D diffusion model and 3D objects, which primarily render 3D objects into 2D images and texture each image separately. In this paper, we revisit the texture synthesis and propose a Variance alignment based 3D-2D Collaborative Denoising framework, dubbed VCD-Texture, to address these issues. Formally, we first unify both 2D and 3D latent feature learning in diffusion self-attention modules with re-projected 3D attention receptive fields. Subsequently, the denoised multi-view 2D latent features are aggregated into 3D space and then rasterized back to formulate more consistent 2D predictions. However, the rasterization process suffers from an intractable variance bias, which is theoretically addressed by the proposed variance alignment, achieving high-fidelity texture synthesis. Moreover, we present an inpainting refinement to further improve the details with conflicting regions. Notably, there is not a publicly available benchmark to evaluate texture synthesis, which hinders its development. Thus we construct a new evaluation set built upon three open-source 3D datasets and propose to use four metrics to thoroughly validate the texturing performance. Comprehensive experiments demonstrate that VCD-Texture achieves superior performance against other counterparts.||[2407.04461v1](http://arxiv.org/pdf/2407.04461v1)|null|\n", "2407.04384": "|**2024-07-05**|**Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos**|\u4ece\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u4e2d\u8fdb\u884c\u7c7b\u522b\u7ea7 3D \u59ff\u6001\u7684\u65e0\u76d1\u7763\u5b66\u4e60|Leonhard Sommer, Artur Jesslen, Eddy Ilg, Adam Kortylewski|Category-level 3D pose estimation is a fundamentally important problem in computer vision and robotics, e.g. for embodied agents or to train 3D generative models. However, so far methods that estimate the category-level object pose require either large amounts of human annotations, CAD models or input from RGB-D sensors. In contrast, we tackle the problem of learning to estimate the category-level 3D pose only from casually taken object-centric videos without human supervision. We propose a two-step pipeline: First, we introduce a multi-view alignment procedure that determines canonical camera poses across videos with a novel and robust cyclic distance formulation for geometric and appearance matching using reconstructed coarse meshes and DINOv2 features. In a second step, the canonical poses and reconstructed meshes enable us to train a model for 3D pose estimation from a single image. In particular, our model learns to estimate dense correspondences between images and a prototypical 3D template by predicting, for each pixel in a 2D image, a feature vector of the corresponding vertex in the template mesh. We demonstrate that our method outperforms all baselines at the unsupervised alignment of object-centric videos by a large margin and provides faithful and robust predictions in-the-wild. Our code and data is available at https://github.com/GenIntel/uns-obj-pose3d.||[2407.04384v1](http://arxiv.org/pdf/2407.04384v1)|**[link](https://github.com/genintel/uns-obj-pose3d)**|\n", "2407.04247": "|**2024-07-05**|**ArAIEval Shared Task: Propagandistic Techniques Detection in Unimodal and Multimodal Arabic Content**|ArAIEval \u5171\u4eab\u4efb\u52a1\uff1a\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u963f\u62c9\u4f2f\u8bed\u5185\u5bb9\u4e2d\u7684\u5ba3\u4f20\u6280\u5de7\u68c0\u6d4b|Maram Hasanain, Md. Arid Hasan, Fatema Ahmed, Reem Suwaileh, Md. Rafiul Biswas, Wajdi Zaghouani, Firoj Alam|We present an overview of the second edition of the ArAIEval shared task, organized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In this edition, ArAIEval offers two tasks: (i) detection of propagandistic textual spans with persuasion techniques identification in tweets and news articles, and (ii) distinguishing between propagandistic and non-propagandistic memes. A total of 14 teams participated in the final evaluation phase, with 6 and 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams submitted system description papers. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further provide a brief overview of the participating systems. All datasets and evaluation scripts are released to the research community (https://araieval.gitlab.io/). We hope this will enable further research on these important tasks in Arabic.||[2407.04247v1](http://arxiv.org/pdf/2407.04247v1)|null|\n", "2407.04237": "|**2024-07-05**|**GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction**|GSD\uff1a\u7528\u4e8e 3D \u91cd\u5efa\u7684\u89c6\u56fe\u5f15\u5bfc\u9ad8\u65af\u6e85\u5c04\u6269\u6563|Yuxuan Mu, Xinxin Zuo, Chuan Guo, Yilin Wang, Juwei Lu, Xiaofeng Wu, Songcen Xu, Peng Dai, Youliang Yan, Li Cheng|We present GSD, a diffusion model approach based on Gaussian Splatting (GS) representation for 3D object reconstruction from a single view. Prior works suffer from inconsistent 3D geometry or mediocre rendering quality due to improper representations. We take a step towards resolving these shortcomings by utilizing the recent state-of-the-art 3D explicit representation, Gaussian Splatting, and an unconditional diffusion model. This model learns to generate 3D objects represented by sets of GS ellipsoids. With these strong generative 3D priors, though learning unconditionally, the diffusion model is ready for view-guided reconstruction without further model fine-tuning. This is achieved by propagating fine-grained 2D features through the efficient yet flexible splatting function and the guided denoising sampling process. In addition, a 2D diffusion model is further employed to enhance rendering fidelity, and improve reconstructed GS quality by polishing and re-using the rendered images. The final reconstructed objects explicitly come with high-quality 3D structure and texture, and can be efficiently rendered in arbitrary views. Experiments on the challenging real-world CO3D dataset demonstrate the superiority of our approach.||[2407.04237v1](http://arxiv.org/pdf/2407.04237v1)|null|\n", "2407.04231": "|**2024-07-05**|**Efficient GANs for Document Image Binarization Based on DWT and Normalization**|\u57fa\u4e8e DWT \u548c\u89c4\u8303\u5316\u7684\u6587\u6863\u56fe\u50cf\u4e8c\u503c\u5316\u9ad8\u6548 GAN|Rui-Yang Ju, KokSheik Wong, Jen-Shiun Chiang|For document image binarization task, generative adversarial networks (GANs) can generate images where shadows and noise are effectively removed, which allow for text information extraction. The current state-of-the-art (SOTA) method proposes a three-stage network architecture that utilizes six GANs. Despite its excellent model performance, the SOTA network architecture requires long training and inference times. To overcome this problem, this work introduces an efficient GAN method based on the three-stage network architecture that incorporates the Discrete Wavelet Transformation and normalization to reduce the input image size, which in turns, decrease both training and inference times. In addition, this work presents novel generators, discriminators, and loss functions to improve the model's performance. Experimental results show that the proposed method reduces the training time by 10% and the inference time by 26% when compared to the SOTA method while maintaining the model performance at 73.79 of Avg-Score. Our implementation code is available on GitHub at https://github.com/RuiyangJu/Efficient_Document_Image_Binarization.||[2407.04231v1](http://arxiv.org/pdf/2407.04231v1)|**[link](https://github.com/ruiyangju/efficient_document_image_binarization)**|\n", "2407.04215": "|**2024-07-05**|**T2IShield: Defending Against Backdoors on Text-to-Image Diffusion Models**|T2IShield\uff1a\u9632\u5fa1\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u540e\u95e8|Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen|While text-to-image diffusion models demonstrate impressive generation capabilities, they also exhibit vulnerability to backdoor attacks, which involve the manipulation of model outputs through malicious triggers. In this paper, for the first time, we propose a comprehensive defense method named T2IShield to detect, localize, and mitigate such attacks. Specifically, we find the \"Assimilation Phenomenon\" on the cross-attention maps caused by the backdoor trigger. Based on this key insight, we propose two effective backdoor detection methods: Frobenius Norm Threshold Truncation and Covariance Discriminant Analysis. Besides, we introduce a binary-search approach to localize the trigger within a backdoor sample and assess the efficacy of existing concept editing methods in mitigating backdoor attacks. Empirical evaluations on two advanced backdoor attack scenarios show the effectiveness of our proposed defense method. For backdoor sample detection, T2IShield achieves a detection F1 score of 88.9$\\%$ with low computational cost. Furthermore, T2IShield achieves a localization F1 score of 86.4$\\%$ and invalidates 99$\\%$ poisoned samples. Codes are released at https://github.com/Robin-WZQ/T2IShield.||[2407.04215v1](http://arxiv.org/pdf/2407.04215v1)|**[link](https://github.com/robin-wzq/t2ishield)**|\n"}, "\u591a\u6a21\u6001": {"2407.04697": "|**2024-07-05**|**VCoME: Verbal Video Composition with Multimodal Editing Effects**|VCoME\uff1a\u5177\u6709\u591a\u6a21\u5f0f\u7f16\u8f91\u6548\u679c\u7684\u53e3\u5934\u89c6\u9891\u5408\u6210|Weibo Gong, Xiaojie Jin, Xin Li, Dongliang He, Xinglong Wu|Verbal videos, featuring voice-overs or text overlays, provide valuable content but present significant challenges in composition, especially when incorporating editing effects to enhance clarity and visual appeal. In this paper, we introduce the novel task of verbal video composition with editing effects. This task aims to generate coherent and visually appealing verbal videos by integrating multimodal editing effects across textual, visual, and audio categories. To achieve this, we curate a large-scale dataset of video effects compositions from publicly available sources. We then formulate this task as a generative problem, involving the identification of appropriate positions in the verbal content and the recommendation of editing effects for these positions. To address this task, we propose VCoME, a general framework that employs a large multimodal model to generate editing effects for video composition. Specifically, VCoME takes in the multimodal video context and autoregressively outputs where to apply effects within the verbal content and which effects are most appropriate for each position. VCoME also supports prompt-based control of composition density and style, providing substantial flexibility for diverse applications. Through extensive quantitative and qualitative evaluations, we clearly demonstrate the effectiveness of VCoME. A comprehensive user study shows that our method produces videos of professional quality while being 85$\\times$ more efficient than professional editors.||[2407.04697v1](http://arxiv.org/pdf/2407.04697v1)|null|\n", "2407.04681": "|**2024-07-05**|**Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge**|\u91cd\u65b0\u601d\u8003\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u5b9e\u73b0\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63d0\u793a|Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, Lu Yuan|In recent years, multimodal large language models (MLLMs) have made significant strides by training on vast high-quality image-text datasets, enabling them to generally understand images well. However, the inherent difficulty in explicitly conveying fine-grained or spatially dense information in text, such as masks, poses a challenge for MLLMs, limiting their ability to answer questions requiring an understanding of detailed or localized visual elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG) concept, this paper proposes a new visual prompt approach to integrate fine-grained external knowledge, gleaned from specialized vision models (e.g., instance segmentation/OCR models), into MLLMs. This is a promising yet underexplored direction for enhancing MLLMs' performance. Our approach diverges from concurrent works, which transform external knowledge into additional text prompts, necessitating the model to indirectly learn the correspondence between visual content and text coordinates. Instead, we propose embedding fine-grained knowledge information directly into a spatial embedding map as a visual prompt. This design can be effortlessly incorporated into various MLLMs, such as LLaVA and Mipha, considerably improving their visual understanding performance. Through rigorous experiments, we demonstrate that our method can enhance MLLM performance across nine benchmarks, amplifying their fine-grained context-aware capabilities.||[2407.04681v1](http://arxiv.org/pdf/2407.04681v1)|null|\n", "2407.04619": "|**2024-07-05**|**CountGD: Multi-Modal Open-World Counting**|CountGD\uff1a\u591a\u6a21\u5f0f\u5f00\u653e\u4e16\u754c\u8ba1\u6570|Niki Amini-Naieni, Tengda Han, Andrew Zisserman|The goal of this paper is to improve the generality and accuracy of open-vocabulary object counting in images. To improve the generality, we repurpose an open-vocabulary detection foundation model (GroundingDINO) for the counting task, and also extend its capabilities by introducing modules to enable specifying the target object to count by visual exemplars. In turn, these new capabilities - being able to specify the target object by multi-modalites (text and exemplars) - lead to an improvement in counting accuracy.   We make three contributions: First, we introduce the first open-world counting model, CountGD, where the prompt can be specified by a text description or visual exemplars or both; Second, we show that the performance of the model significantly improves the state of the art on multiple counting benchmarks - when using text only, CountGD is comparable to or outperforms all previous text-only works, and when using both text and visual exemplars, we outperform all previous models; Third, we carry out a preliminary study into different interactions between the text and visual exemplar prompts, including the cases where they reinforce each other and where one restricts the other. The code and an app to test the model are available at https://www.robots.ox.ac.uk/~vgg/research/countgd/.||[2407.04619v1](http://arxiv.org/pdf/2407.04619v1)|null|\n", "2407.04603": "|**2024-07-05**|**AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation**|AWT\uff1a\u901a\u8fc7\u589e\u5f3a\u3001\u52a0\u6743\u548c\u4f20\u8f93\u6765\u4f20\u8f93\u89c6\u89c9\u8bed\u8a00\u6a21\u578b|Yuhan Zhu, Yuyang Ji, Zhiyu Zhao, Gangshan Wu, Limin Wang|Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks. However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes. To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space. AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module. We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization. AWT consistently outperforms the state-of-the-art methods in each setting. In addition, our extensive studies further demonstrate AWT's effectiveness and adaptability across different VLMs, architectures, and scales.||[2407.04603v1](http://arxiv.org/pdf/2407.04603v1)|null|\n", "2407.04587": "|**2024-07-05**|**Multimodal Classification via Modal-Aware Interactive Enhancement**|\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u4ea4\u4e92\u589e\u5f3a\u5b9e\u73b0\u591a\u6a21\u6001\u5206\u7c7b|Qing-Yuan Jiang, Zhouyang Chi, Yang Yang|Due to the notorious modality imbalance problem, multimodal learning (MML) leads to the phenomenon of optimization imbalance, thus struggling to achieve satisfactory performance. Recently, some representative methods have been proposed to boost the performance, mainly focusing on adaptive adjusting the optimization of each modality to rebalance the learning speed of dominant and non-dominant modalities. To better facilitate the interaction of model information in multimodal learning, in this paper, we propose a novel multimodal learning method, called modal-aware interactive enhancement (MIE). Specifically, we first utilize an optimization strategy based on sharpness aware minimization (SAM) to smooth the learning objective during the forward phase. Then, with the help of the geometry property of SAM, we propose a gradient modification strategy to impose the influence between different modalities during the backward phase. Therefore, we can improve the generalization ability and alleviate the modality forgetting phenomenon simultaneously for multimodal learning. Extensive experiments on widely used datasets demonstrate that our proposed method can outperform various state-of-the-art baselines to achieve the best performance.||[2407.04587v1](http://arxiv.org/pdf/2407.04587v1)|null|\n", "2407.04458": "|**2024-07-05**|**Robust Multimodal Learning via Representation Decoupling**|\u901a\u8fc7\u8868\u5f81\u89e3\u8026\u5b9e\u73b0\u7a33\u5065\u7684\u591a\u6a21\u6001\u5b66\u4e60|Shicai Wei, Yang Luo, Yuji Wang, Chunbo Luo|Multimodal learning robust to missing modality has attracted increasing attention due to its practicality. Existing methods tend to address it by learning a common subspace representation for different modality combinations. However, we reveal that they are sub-optimal due to their implicit constraint on intra-class representation. Specifically, the sample with different modalities within the same class will be forced to learn representations in the same direction. This hinders the model from capturing modality-specific information, resulting in insufficient learning. To this end, we propose a novel Decoupled Multimodal Representation Network (DMRNet) to assist robust multimodal learning. Specifically, DMRNet models the input from different modality combinations as a probabilistic distribution instead of a fixed point in the latent space, and samples embeddings from the distribution for the prediction module to calculate the task loss. As a result, the direction constraint from the loss minimization is blocked by the sampled representation. This relaxes the constraint on the inference representation and enables the model to capture the specific information for different modality combinations. Furthermore, we introduce a hard combination regularizer to prevent DMRNet from unbalanced training by guiding it to pay more attention to hard modality combinations. Finally, extensive experiments on multimodal classification and segmentation tasks demonstrate that the proposed DMRNet outperforms the state-of-the-art significantly.||[2407.04458v1](http://arxiv.org/pdf/2407.04458v1)|null|\n", "2407.04449": "|**2024-07-05**|**Multi-modal Masked Siamese Network Improves Chest X-Ray Representation Learning**|\u591a\u6a21\u6001\u8499\u7248\u66b9\u7f57\u7f51\u7edc\u6539\u5584\u80f8\u90e8 X \u5149\u7247\u8868\u5f81\u5b66\u4e60|Saeed Shurrab, Alejandro Guerra-Manzanares, Farah E. Shamout|Self-supervised learning methods for medical images primarily rely on the imaging modality during pretraining. While such approaches deliver promising results, they do not leverage associated patient or scan information collected within Electronic Health Records (EHR). Here, we propose to incorporate EHR data during self-supervised pretraining with a Masked Siamese Network (MSN) to enhance the quality of chest X-ray representations. We investigate three types of EHR data, including demographic, scan metadata, and inpatient stay information. We evaluate our approach on three publicly available chest X-ray datasets, MIMIC-CXR, CheXpert, and NIH-14, using two vision transformer (ViT) backbones, specifically ViT-Tiny and ViT-Small. In assessing the quality of the representations via linear evaluation, our proposed method demonstrates significant improvement compared to vanilla MSN and state-of-the-art self-supervised learning baselines. Our work highlights the potential of EHR-enhanced self-supervised pre-training for medical imaging. The code is publicly available at: https://github.com/nyuad-cai/CXR-EHR-MSN||[2407.04449v1](http://arxiv.org/pdf/2407.04449v1)|**[link](https://github.com/nyuad-cai/cxr-ehr-msn)**|\n", "2407.04362": "|**2024-07-05**|**Towards Context-aware Support for Color Vision Deficiency: An Approach Integrating LLM and AR**|\u9762\u5411\u8272\u89c9\u7f3a\u9677\u7684\u60c5\u5883\u611f\u77e5\u652f\u6301\uff1a\u4e00\u79cd\u878d\u5408 LLM \u548c AR \u7684\u65b9\u6cd5|Shogo Morita, Yan Zhang, Takuto Yamauchi, Sinan Chen, Jialong Li, Kenji Tei|People with color vision deficiency often face challenges in distinguishing colors such as red and green, which can complicate daily tasks and require the use of assistive tools or environmental adjustments. Current support tools mainly focus on presentation-based aids, like the color vision modes found in iPhone accessibility settings. However, offering context-aware support, like indicating the doneness of meat, remains a challenge since task-specific solutions are not cost-effective for all possible scenarios. To address this, our paper proposes an application that provides contextual and autonomous assistance. This application is mainly composed of: (i) an augmented reality interface that efficiently captures context; and (ii) a multi-modal large language model-based reasoner that serves to cognitize the context and then reason about the appropriate support contents. Preliminary user experiments with two color vision deficient users across five different scenarios have demonstrated the effectiveness and universality of our application.||[2407.04362v1](http://arxiv.org/pdf/2407.04362v1)|null|\n", "2407.04346": "|**2024-07-05**|**MobileFlow: A Multimodal LLM For Mobile GUI Agent**|MobileFlow\uff1a\u7528\u4e8e\u79fb\u52a8 GUI \u4ee3\u7406\u7684\u591a\u6a21\u5f0f LLM|Songqin Nong, Jiali Zhu, Rui Wu, Jiongchao Jin, Shuo Shan, Xiutian Huang, Wenhao Xu|Currently, the integration of mobile Graphical User Interfaces (GUIs) is ubiquitous in most people's daily lives. And the ongoing evolution of multimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly bolstered the capabilities of GUI comprehension and user action analysis, showcasing the potentiality of intelligent GUI assistants. However, current GUI Agents often need to access page layout information through calling system APIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a certain low resolution might result in the loss of fine-grained image details. At the same time, the multimodal large models built for GUI Agents currently have poor understanding and decision-making abilities for Chinese GUI interfaces, making them difficult to apply to a large number of Chinese apps. This paper introduces MobileFlow, a multimodal large language model meticulously crafted for mobile GUI agents. Transforming from the open-source model Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21 billion parameters and is equipped with novel hybrid visual encoders, making it possible for variable resolutions of image inputs and good support for multilingual GUI. By incorporating Mixture of Experts (MoE) expansions and pioneering alignment training strategies, MobileFlow has the capacity to fully interpret image data and comprehend user instructions for GUI interaction tasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task execution by GUI agents on both public and our proposed evaluation metrics, and has been successfully deployed in real-world business contexts, proving its effectiveness for practical applications.||[2407.04346v1](http://arxiv.org/pdf/2407.04346v1)|null|\n", "2407.04287": "|**2024-07-05**|**MARS: Paying more attention to visual attributes for text-based person search**|MARS\uff1a\u66f4\u52a0\u5173\u6ce8\u57fa\u4e8e\u6587\u672c\u7684\u4eba\u7269\u641c\u7d22\u7684\u89c6\u89c9\u5c5e\u6027|Alex Ergasti, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati|Text-based person search (TBPS) is a problem that gained significant interest within the research community. The task is that of retrieving one or more images of a specific individual based on a textual description. The multi-modal nature of the task requires learning representations that bridge text and image data within a shared latent space. Existing TBPS systems face two major challenges. One is defined as inter-identity noise that is due to the inherent vagueness and imprecision of text descriptions and it indicates how descriptions of visual attributes can be generally associated to different people; the other is the intra-identity variations, which are all those nuisances e.g. pose, illumination, that can alter the visual appearance of the same textual attributes for a given subject. To address these issues, this paper presents a novel TBPS architecture named MARS (Mae-Attribute-Relation-Sensitive), which enhances current state-of-the-art models by introducing two key components: a Visual Reconstruction Loss and an Attribute Loss. The former employs a Masked AutoEncoder trained to reconstruct randomly masked image patches with the aid of the textual description. In doing so the model is encouraged to learn more expressive representations and textual-visual relations in the latent space. The Attribute Loss, instead, balances the contribution of different types of attributes, defined as adjective-noun chunks of text. This loss ensures that every attribute is taken into consideration in the person retrieval process. Extensive experiments on three commonly used datasets, namely CUHK-PEDES, ICFG-PEDES, and RSTPReid, report performance improvements, with significant gains in the mean Average Precision (mAP) metric w.r.t. the current state of the art.||[2407.04287v1](http://arxiv.org/pdf/2407.04287v1)|**[link](https://github.com/ergastialex/mars)**|\n", "2407.04255": "|**2024-07-05**|**Second Place Solution of WSDM2023 Toloka Visual Question Answering Challenge**|WSDM2023 Toloka \u89c6\u89c9\u95ee\u7b54\u6311\u6218\u8d5b\u7b2c\u4e8c\u540d\u89e3\u51b3\u65b9\u6848|Xiangyu Wu, Zhouyang Chi, Yang Yang, Jianfeng Lu|In this paper, we present our solution for the WSDM2023 Toloka Visual Question Answering Challenge. Inspired by the application of multimodal pre-trained models to various downstream tasks(e.g., visual question answering, visual grounding, and cross-modal retrieval), we approached this competition as a visual grounding task, where the input is an image and a question, guiding the model to answer the question and display the answer as a bounding box on the image. We designed a three-stage solution for this task. Specifically, we used the visual-language pre-trained model OFA as the foundation. In the first stage, we constructed a large-scale synthetic dataset similar to the competition dataset and coarse-tuned the model to learn generalized semantic information. In the second stage, we treated the competition task as a visual grounding task, loaded the weights from the previous stage, and continued to fine-tune the model on the competition dataset, transferring the semantic information learned in the first stage to the competition task. Finally, we designed a bounding box matching and replacing post-processing strategy to correct the model's prediction results. Our team achieved a score of 76.342 on the final leaderboard, ranking second.||[2407.04255v1](http://arxiv.org/pdf/2407.04255v1)|null|\n", "2407.04242": "|**2024-07-05**|**Fine-grained Context and Multi-modal Alignment for Freehand 3D Ultrasound Reconstruction**|\u7528\u4e8e\u624b\u7ed8 3D \u8d85\u58f0\u91cd\u5efa\u7684\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u548c\u591a\u6a21\u6001\u5bf9\u9f50|Zhongnuo Yan, Xin Yang, Mingyuan Luo, Jiongquan Chen, Rusi Chen, Lian Liu, Dong Ni|Fine-grained spatio-temporal learning is crucial for freehand 3D ultrasound reconstruction. Previous works mainly resorted to the coarse-grained spatial features and the separated temporal dependency learning and struggles for fine-grained spatio-temporal learning. Mining spatio-temporal information in fine-grained scales is extremely challenging due to learning difficulties in long-range dependencies. In this context, we propose a novel method to exploit the long-range dependency management capabilities of the state space model (SSM) to address the above challenge. Our contribution is three-fold. First, we propose ReMamba, which mines multi-scale spatio-temporal information by devising a multi-directional SSM. Second, we propose an adaptive fusion strategy that introduces multiple inertial measurement units as auxiliary temporal information to enhance spatio-temporal perception. Last, we design an online alignment strategy that encodes the temporal information as pseudo labels for multi-modal alignment to further improve reconstruction performance. Extensive experimental validations on two large-scale datasets show remarkable improvement from our method over competitors.||[2407.04242v1](http://arxiv.org/pdf/2407.04242v1)|null|\n", "2407.04207": "|**2024-07-05**|**Elevating All Zero-Shot Sketch-Based Image Retrieval Through Multimodal Prompt Learning**|\u901a\u8fc7\u591a\u6a21\u6001\u5feb\u901f\u5b66\u4e60\u63d0\u5347\u6240\u6709\u57fa\u4e8e\u96f6\u6837\u672c\u8349\u56fe\u7684\u56fe\u50cf\u68c0\u7d22\u80fd\u529b|Mainak Singha, Ankit Jha, Divyam Gupta, Pranav Singla, Biplab Banerjee|We address the challenges inherent in sketch-based image retrieval (SBIR) across various settings, including zero-shot SBIR, generalized zero-shot SBIR, and fine-grained zero-shot SBIR, by leveraging the vision-language foundation model, CLIP. While recent endeavors have employed CLIP to enhance SBIR, these approaches predominantly follow uni-modal prompt processing and overlook to fully exploit CLIP's integrated visual and textual capabilities. To bridge this gap, we introduce SpLIP, a novel multi-modal prompt learning scheme designed to operate effectively with frozen CLIP backbones. We diverge from existing multi-modal prompting methods that either treat visual and textual prompts independently or integrate them in a limited fashion, leading to suboptimal generalization. SpLIP implements a bi-directional prompt-sharing strategy that enables mutual knowledge exchange between CLIP's visual and textual encoders, fostering a more cohesive and synergistic prompt processing mechanism that significantly reduces the semantic gap between the sketch and photo embeddings. In addition to pioneering multi-modal prompt learning, we propose two innovative strategies for further refining the embedding space. The first is an adaptive margin generation for the sketch-photo triplet loss, regulated by CLIP's class textual embeddings. The second introduces a novel task, termed conditional cross-modal jigsaw, aimed at enhancing fine-grained sketch-photo alignment, by focusing on implicitly modelling the viable patch arrangement of sketches using knowledge of unshuffled photos. Our comprehensive experimental evaluations across multiple benchmarks demonstrate the superior performance of SpLIP in all three SBIR scenarios. Code is available at https://github.com/mainaksingha01/SpLIP.||[2407.04207v1](http://arxiv.org/pdf/2407.04207v1)|null|\n"}, "Nerf": {}, "3DGS": {"2407.04545": "|**2024-07-05**|**Gaussian Eigen Models for Human Heads**|\u4eba\u4f53\u5934\u90e8\u7684\u9ad8\u65af\u7279\u5f81\u6a21\u578b|Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies|We present personalized Gaussian Eigen Models (GEMs) for human heads, a novel method that compresses dynamic 3D Gaussians into low-dimensional linear spaces. Our approach is inspired by the seminal work of Blanz and Vetter, where a mesh-based 3D morphable model (3DMM) is constructed from registered meshes. Based on dynamic 3D Gaussians, we create a lower-dimensional representation of primitives that applies to most 3DGS head avatars. Specifically, we propose a universal method to distill the appearance of a mesh-controlled UNet Gaussian avatar using an ensemble of linear eigenbasis. We replace heavy CNN-based architectures with a single linear layer improving speed and enabling a range of real-time downstream applications. To create a particular facial expression, one simply needs to perform a dot product between the eigen coefficients and the distilled basis. This efficient method removes the requirement for an input mesh during testing, enhancing simplicity and speed in expression generation. This process is highly efficient and supports real-time rendering on everyday devices, leveraging the effectiveness of standard Gaussian Splatting. In addition, we demonstrate how the GEM can be controlled using a ResNet-based regression architecture. We show and compare self-reenactment and cross-person reenactment to state-of-the-art 3D avatar methods, demonstrating higher quality and better control. A real-time demo showcases the applicability of the GEM representation.||[2407.04545v1](http://arxiv.org/pdf/2407.04545v1)|null|\n", "2407.04504": "|**2024-07-05**|**Segment Any 4D Gaussians**|\u5206\u5272\u4efb\u610f 4D \u9ad8\u65af|Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu Liu, Qi Tian, Xinggang Wang|Modeling, understanding, and reconstructing the real world are crucial in XR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable success in modeling and understanding 3D scenes. Similarly, various 4D representations have demonstrated the ability to capture the dynamics of the 4D world. However, there is a dearth of research focusing on segmentation within 4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D), one of the first frameworks to segment anything in the 4D digital world based on 4D Gaussians. In SA4D, an efficient temporal identity feature field is introduced to handle Gaussian drifting, with the potential to learn precise identity features from noisy and sparse input. Additionally, a 4D segmentation refinement process is proposed to remove artifacts. Our SA4D achieves precise, high-quality segmentation within seconds in 4D Gaussians and shows the ability to remove, recolor, compose, and render high-quality anything masks. More demos are available at: https://jsxzs.github.io/sa4d/.||[2407.04504v1](http://arxiv.org/pdf/2407.04504v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2407.04616": "|**2024-07-05**|**Isomorphic Pruning for Vision Models**|\u89c6\u89c9\u6a21\u578b\u7684\u540c\u6784\u526a\u679d|Gongfan Fang, Xinyin Ma, Michael Bi Mi, Xinchao Wang|Structured pruning reduces the computational overhead of deep neural networks by removing redundant sub-structures. However, assessing the relative importance of different sub-structures remains a significant challenge, particularly in advanced vision models featuring novel mechanisms and architectures like self-attention, depth-wise convolutions, or residual connections. These heterogeneous substructures usually exhibit diverged parameter scales, weight distributions, and computational topology, introducing considerable difficulty to importance comparison. To overcome this, we present Isomorphic Pruning, a simple approach that demonstrates effectiveness across a range of network architectures such as Vision Transformers and CNNs, and delivers competitive performance across different model sizes. Isomorphic Pruning originates from an observation that, when evaluated under a pre-defined importance criterion, heterogeneous sub-structures demonstrate significant divergence in their importance distribution, as opposed to isomorphic structures that present similar importance patterns. This inspires us to perform isolated ranking and comparison on different types of sub-structures for more reliable pruning. Our empirical results on ImageNet-1K demonstrate that Isomorphic Pruning surpasses several pruning baselines dedicatedly designed for Transformers or CNNs. For instance, we improve the accuracy of DeiT-Tiny from 74.52% to 77.50% by pruning an off-the-shelf DeiT-Base model. And for ConvNext-Tiny, we enhanced performance from 82.06% to 82.18%, while reducing the number of parameters and memory usage. Code is available at \\url{https://github.com/VainF/Isomorphic-Pruning}.||[2407.04616v1](http://arxiv.org/pdf/2407.04616v1)|**[link](https://github.com/vainf/isomorphic-pruning)**|\n", "2407.04513": "|**2024-07-05**|**LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing Layer Execution Order**|LayerShuffle\uff1a\u901a\u8fc7\u968f\u673a\u5316\u5c42\u6267\u884c\u987a\u5e8f\u6765\u589e\u5f3a Vision Transformers \u7684\u9c81\u68d2\u6027|Matthias Freiberger, Peter Kun, Anders Sundnes L\u00f8vlie, Sebastian Risi|Due to their architecture and how they are trained, artificial neural networks are typically not robust toward pruning, replacing, or shuffling layers at test time. However, such properties would be desirable for different applications, such as distributed neural network architectures where the order of execution cannot be guaranteed or parts of the network can fail during inference. In this work, we address these issues through a number of proposed training approaches for vision transformers whose most important component is randomizing the execution order of attention modules at training time. We show that with our proposed approaches, vision transformers are indeed capable to adapt to arbitrary layer execution orders at test time assuming one tolerates a reduction (about 20\\%) in accuracy at the same model size. We also find that our trained models can be randomly merged with each other resulting in functional (\"Frankenstein\") models without loss of performance compared to the source models. Finally, we layer-prune our models at test time and find that their performance declines gracefully.||[2407.04513v1](http://arxiv.org/pdf/2407.04513v1)|null|\n", "2407.04208": "|**2024-07-05**|**AMD: Automatic Multi-step Distillation of Large-scale Vision Models**|AMD\uff1a\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u7684\u81ea\u52a8\u591a\u6b65\u84b8\u998f|Cheng Han, Qifan Wang, Sohail A. Dianat, Majid Rabbani, Raghuveer M. Rao, Yi Fang, Qiang Guan, Lifu Huang, Dongfang Liu|Transformer-based architectures have become the de-facto standard models for diverse vision tasks owing to their superior performance. As the size of the models continues to scale up, model distillation becomes extremely important in various real applications, particularly on devices limited by computational resources. However, prevailing knowledge distillation methods exhibit diminished efficacy when confronted with a large capacity gap between the teacher and the student, e.g, 10x compression rate. In this paper, we present a novel approach named Automatic Multi-step Distillation (AMD) for large-scale vision model compression. In particular, our distillation process unfolds across multiple steps. Initially, the teacher undergoes distillation to form an intermediate teacher-assistant model, which is subsequently distilled further to the student. An efficient and effective optimization framework is introduced to automatically identify the optimal teacher-assistant that leads to the maximal student performance. We conduct extensive experiments on multiple image classification datasets, including CIFAR-10, CIFAR-100, and ImageNet. The findings consistently reveal that our approach outperforms several established baselines, paving a path for future knowledge distillation methods on large-scale vision models.||[2407.04208v1](http://arxiv.org/pdf/2407.04208v1)|null|\n", "2407.04203": "|**2024-07-05**|**HCS-TNAS: Hybrid Constraint-driven Semi-supervised Transformer-NAS for Ultrasound Image Segmentation**|HCS-TNAS\uff1a\u7528\u4e8e\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u7684\u6df7\u5408\u7ea6\u675f\u9a71\u52a8\u534a\u76d1\u7763 Transformer-NAS|Renqi Chen|Accurate ultrasound segmentation is pursued because it aids clinicians in achieving a comprehensive diagnosis. Due to the presence of low image quality and high costs associated with annotation, two primary concerns arise: (1) enhancing the understanding of multi-scale features, and (2) improving the resistance to data dependency. To mitigate these concerns, we propose HCS-TNAS, a novel neural architecture search (NAS) method that automatically designs the network. For the first concern, we employ multi-level searching encompassing cellular, layer, and module levels. Specifically, we design an Efficient NAS-ViT module that searches for multi-scale tokens in the vision Transformer (ViT) to capture context and local information, rather than relying solely on simple combinations of operations. For the second concern, we propose a hybrid constraint-driven semi-supervised learning method that considers additional network independence and incorporates contrastive loss in a NAS formulation. By further developing a stage-wise optimization strategy, a rational network structure can be identified. Extensive experiments on three publicly available ultrasound image datasets demonstrate that HCS-TNAS effectively improves segmentation accuracy and outperforms state-of-the-art methods.||[2407.04203v1](http://arxiv.org/pdf/2407.04203v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2407.04683": "|**2024-07-05**|**Efficient Betti Matching Enables Topology-Aware 3D Segmentation via Persistent Homology**|\u9ad8\u6548 Betti \u5339\u914d\u901a\u8fc7\u6301\u4e45\u540c\u6e90\u6027\u5b9e\u73b0\u62d3\u6251\u611f\u77e5\u7684 3D \u5206\u5272|Nico Stucki, Vincent B\u00fcrgin, Johannes C. Paetzold, Ulrich Bauer|In this work, we propose an efficient algorithm for the calculation of the Betti matching, which can be used as a loss function to train topology aware segmentation networks. Betti matching loss builds on techniques from topological data analysis, specifically persistent homology. A major challenge is the computational cost of computing persistence barcodes. In response to this challenge, we propose a new, highly optimized implementation of Betti matching, implemented in C++ together with a python interface, which achieves significant speedups compared to the state-of-the-art implementation Cubical Ripser. We use Betti matching 3D to train segmentation networks with the Betti matching loss and demonstrate improved topological correctness of predicted segmentations across several datasets. The source code is available at https://github.com/nstucki/Betti-Matching-3D.||[2407.04683v1](http://arxiv.org/pdf/2407.04683v1)|null|\n", "2407.04676": "|**2024-07-05**|**Is plantar thermography a valid digital biomarker for characterising diabetic foot ulceration risk?**|\u8db3\u5e95\u70ed\u6210\u50cf\u662f\u5426\u662f\u8868\u5f81\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1\u98ce\u9669\u7684\u6709\u6548\u6570\u5b57\u751f\u7269\u6807\u5fd7\u7269\uff1f|Akshay Jagadeesh, Chanchanok Aramrat, Aqsha Nur, Poppy Mallinson, Sanjay Kinra|Background: In the absence of prospective data on diabetic foot ulcers (DFU), cross-sectional associations with causal risk factors (peripheral neuropathy, and peripheral arterial disease (PAD)) could be used to establish the validity of plantar thermography for DFU risk stratification.   Methods: First, we investigated the associations between the intrinsic clusters of plantar thermographic images with several DFU risk factors using an unsupervised deep-learning framework. We then studied associations between obtained thermography clusters and DFU risk factors. Second, to identify those associations with predictive power, we used supervised learning to train Convolutional Neural Network (CNN) regression/classification models that predicted the risk factor based on the thermograph (and visual) input.   Findings: Our dataset comprised 282 thermographs from type 2 diabetes mellitus patients (aged 56.31 +- 9.18 years, 51.42 % males). On clustering, we found two overlapping clusters (silhouette score = 0.10, indicating weak separation). There was strong evidence for associations between assigned clusters and several factors related to diabetic foot ulceration such as peripheral neuropathy, PAD, number of diabetes complications, and composite DFU risk prediction scores such as Martins-Mendes, PODUS-2020, and SIGN. However, models predicting said risk factors had poor performances.   Interpretation: The strong associations between intrinsic thermography clusters and several DFU risk factors support the validity of using thermography for characterising DFU risk. However, obtained associations did not prove to be predictive, likely due to, spectrum bias, or because thermography and classical risk factors characterise incompletely overlapping portions of the DFU risk construct. Our findings highlight the challenges in standardising ground truths when defining novel digital biomarkers.||[2407.04676v1](http://arxiv.org/pdf/2407.04676v1)|null|\n", "2407.04651": "|**2024-07-05**|**SAM Fewshot Finetuning for Anatomical Segmentation in Medical Images**|SAM Fewshot \u5fae\u8c03\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u89e3\u5256\u5206\u5272|Weiyi Xie, Nathalie Willems, Shubham Patil, Yang Li, Mayank Kumar|We propose a straightforward yet highly effective few-shot fine-tuning strategy for adapting the Segment Anything (SAM) to anatomical segmentation tasks in medical images. Our novel approach revolves around reformulating the mask decoder within SAM, leveraging few-shot embeddings derived from a limited set of labeled images (few-shot collection) as prompts for querying anatomical objects captured in image embeddings. This innovative reformulation greatly reduces the need for time-consuming online user interactions for labeling volumetric images, such as exhaustively marking points and bounding boxes to provide prompts slice by slice. With our method, users can manually segment a few 2D slices offline, and the embeddings of these annotated image regions serve as effective prompts for online segmentation tasks. Our method prioritizes the efficiency of the fine-tuning process by exclusively training the mask decoder through caching mechanisms while keeping the image encoder frozen. Importantly, this approach is not limited to volumetric medical images, but can generically be applied to any 2D/3D segmentation task. To thoroughly evaluate our method, we conducted extensive validation on four datasets, covering six anatomical segmentation tasks across two modalities. Furthermore, we conducted a comparative analysis of different prompting options within SAM and the fully-supervised nnU-Net. The results demonstrate the superior performance of our method compared to SAM employing only point prompts (approximately 50% improvement in IoU) and performs on-par with fully supervised methods whilst reducing the requirement of labeled data by at least an order of magnitude.||[2407.04651v1](http://arxiv.org/pdf/2407.04651v1)|null|\n", "2407.04638": "|**2024-07-05**|**Semi-Supervised Segmentation via Embedding Matching**|\u901a\u8fc7\u5d4c\u5165\u5339\u914d\u8fdb\u884c\u534a\u76d1\u7763\u5206\u5272|Weiyi Xie, Nathalie Willems, Nikolas Lessmann, Tom Gibbons, Daniele De Massari|Deep convolutional neural networks are widely used in medical image segmentation but require many labeled images for training. Annotating three-dimensional medical images is a time-consuming and costly process. To overcome this limitation, we propose a novel semi-supervised segmentation method that leverages mostly unlabeled images and a small set of labeled images in training. Our approach involves assessing prediction uncertainty to identify reliable predictions on unlabeled voxels from the teacher model. These voxels serve as pseudo-labels for training the student model. In voxels where the teacher model produces unreliable predictions, pseudo-labeling is carried out based on voxel-wise embedding correspondence using reference voxels from labeled images. We applied this method to automate hip bone segmentation in CT images, achieving notable results with just 4 CT scans. The proposed approach yielded a Hausdorff distance with 95th percentile (HD95) of 3.30 and IoU of 0.929, surpassing existing methods achieving HD95 (4.07) and IoU (0.927) at their best.||[2407.04638v1](http://arxiv.org/pdf/2407.04638v1)|null|\n", "2407.04597": "|**2024-07-05**|**Feature Attenuation of Defective Representation Can Resolve Incomplete Masking on Anomaly Detection**|\u7f3a\u9677\u8868\u793a\u7684\u7279\u5f81\u8870\u51cf\u53ef\u4ee5\u89e3\u51b3\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u4e0d\u5b8c\u5168\u63a9\u76d6\u95ee\u9898|YeongHyeon Park, Sungho Kang, Myung Jin Kim, Hyeong Seok Kim, Juneho Yi|In unsupervised anomaly detection (UAD) research, while state-of-the-art models have reached a saturation point with extensive studies on public benchmark datasets, they adopt large-scale tailor-made neural networks (NN) for detection performance or pursued unified models for various tasks. Towards edge computing, it is necessary to develop a computationally efficient and scalable solution that avoids large-scale complex NNs. Motivated by this, we aim to optimize the UAD performance with minimal changes to NN settings. Thus, we revisit the reconstruction-by-inpainting approach and rethink to improve it by analyzing strengths and weaknesses. The strength of the SOTA methods is a single deterministic masking approach that addresses the challenges of random multiple masking that is inference latency and output inconsistency. Nevertheless, the issue of failure to provide a mask to completely cover anomalous regions is a remaining weakness. To mitigate this issue, we propose Feature Attenuation of Defective Representation (FADeR) that only employs two MLP layers which attenuates feature information of anomaly reconstruction during decoding. By leveraging FADeR, features of unseen anomaly patterns are reconstructed into seen normal patterns, reducing false alarms. Experimental results demonstrate that FADeR achieves enhanced performance compared to similar-scale NNs. Furthermore, our approach exhibits scalability in performance enhancement when integrated with other single deterministic masking methods in a plug-and-play manner.||[2407.04597v1](http://arxiv.org/pdf/2407.04597v1)|null|\n", "2407.04590": "|**2024-07-05**|**SH17: A Dataset for Human Safety and Personal Protective Equipment Detection in Manufacturing Industry**|SH17\uff1a\u5236\u9020\u4e1a\u4eba\u7c7b\u5b89\u5168\u548c\u4e2a\u4eba\u9632\u62a4\u8bbe\u5907\u68c0\u6d4b\u6570\u636e\u96c6|Hafiz Mughees Ahmad, Afshin Rahimi|Workplace accidents continue to pose significant risks for human safety, particularly in industries such as construction and manufacturing, and the necessity for effective Personal Protective Equipment (PPE) compliance has become increasingly paramount. Our research focuses on the development of non-invasive techniques based on the Object Detection (OD) and Convolutional Neural Network (CNN) to detect and verify the proper use of various types of PPE such as helmets, safety glasses, masks, and protective clothing. This study proposes the SH17 Dataset, consisting of 8,099 annotated images containing 75,994 instances of 17 classes collected from diverse industrial environments, to train and validate the OD models. We have trained state-of-the-art OD models for benchmarking, and initial results demonstrate promising accuracy levels with You Only Look Once (YOLO)v9-e model variant exceeding 70.9% in PPE detection. The performance of the model validation on cross-domain datasets suggests that integrating these technologies can significantly improve safety management systems, providing a scalable and efficient solution for industries striving to meet human safety regulations and protect their workforce. The dataset is available at https://github.com/ahmadmughees/sh17dataset.||[2407.04590v1](http://arxiv.org/pdf/2407.04590v1)|**[link](https://github.com/ahmadmughees/sh17dataset)**|\n", "2407.04560": "|**2024-07-05**|**Real Time Emotion Analysis Using Deep Learning for Education, Entertainment, and Beyond**|\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u5b9e\u65f6\u60c5\u7eea\u5206\u6790\uff0c\u7528\u4e8e\u6559\u80b2\u3001\u5a31\u4e50\u7b49|Abhilash Khuntia, Shubham Kale|The significance of emotion detection is increasing in education, entertainment, and various other domains. We are developing a system that can identify and transform facial expressions into emojis to provide immediate feedback.The project consists of two components. Initially, we will employ sophisticated image processing techniques and neural networks to construct a deep learning model capable of precisely categorising facial expressions. Next, we will develop a basic application that records live video using the camera on your device. The app will utilise a sophisticated model to promptly analyse facial expressions and promptly exhibit corresponding emojis.Our objective is to develop a dynamic tool that integrates deep learning and real-time video processing for the purposes of online education, virtual events, gaming, and enhancing user experience. This tool enhances interactions and introduces novel emotional intelligence technologies.||[2407.04560v1](http://arxiv.org/pdf/2407.04560v1)|null|\n", "2407.04538": "|**2024-07-05**|**PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers**|PDiscoFormer\uff1a\u5229\u7528 Vision Transformers \u653e\u5bbd\u96f6\u4ef6\u53d1\u73b0\u7ea6\u675f|Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos|Computer vision methods that explicitly detect object parts and reason on them are a step towards inherently interpretable models. Existing approaches that perform part discovery driven by a fine-grained classification task make very restrictive assumptions on the geometric properties of the discovered parts; they should be small and compact. Although this prior is useful in some cases, in this paper we show that pre-trained transformer-based vision models, such as self-supervised DINOv2 ViT, enable the relaxation of these constraints. In particular, we find that a total variation (TV) prior, which allows for multiple connected components of any size, substantially outperforms previous work. We test our approach on three fine-grained classification benchmarks: CUB, PartImageNet and Oxford Flowers, and compare our results to previously published methods as well as a re-implementation of the state-of-the-art method PDiscoNet with a transformer-based backbone. We consistently obtain substantial improvements across the board, both on part discovery metrics and the downstream classification task, showing that the strong inductive biases in self-supervised ViT models require to rethink the geometric priors that can be used for unsupervised part discovery.||[2407.04538v1](http://arxiv.org/pdf/2407.04538v1)|**[link](https://github.com/ananthu-aniraj/pdiscoformer)**|\n", "2407.04519": "|**2024-07-05**|**Success or Failure? Analyzing Segmentation Refinement with Few-Shot Segmentation**|\u6210\u529f\u8fd8\u662f\u5931\u8d25\uff1f\u4f7f\u7528\u5c0f\u6837\u672c\u5206\u5272\u5206\u6790\u5206\u5272\u7ec6\u5316|Seonghyeon Moon, Haein Kong, Muhammad Haris Khan|The purpose of segmentation refinement is to enhance the initial coarse masks generated by segmentation algorithms. The refined masks are expected to capture the details and contours of the target objects. Research on segmentation refinement has developed as a response to the need for high-quality initial masks. However, to our knowledge, no method has been developed that can determine the success of segmentation refinement. Such a method could ensure the reliability of segmentation in applications where the outcome of the segmentation is important, and fosters innovation in image processing technologies. To address this research gap, we propose JFS~(Judging From Support-set), a method to identify the success of segmentation refinement leveraging a few-shot segmentation (FSS) model. The traditional goal of the problem in FSS is to find a target object in a query image utilizing target information given by a support set. However, in our proposed method, we use the FSS network in a novel way to assess the segmentation refinement. When there are two masks, a coarse mask and a refined mask from segmentation refinement, these two masks become support masks. The existing support mask works as a ground truth mask to judge whether the quality of the refined segmentation is more accurate than the coarse mask. We first obtained a coarse mask and refined it using SEPL (SAM Enhanced Pseduo-Labels) to get the two masks. Then, these become input to FSS model to judge whether the post-processing was successful. JFS is evaluated on the best and worst cases from SEPL to validate its effectiveness. The results showed that JFS can determine whether the SEPL is a success or not.||[2407.04519v1](http://arxiv.org/pdf/2407.04519v1)|null|\n", "2407.04507": "|**2024-07-05**|**Few-Shot Airway-Tree Modeling using Data-Driven Sparse Priors**|\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u7684\u7a00\u758f\u5148\u9a8c\u8fdb\u884c\u5c0f\u6837\u672c\u6c14\u9053\u6811\u5efa\u6a21|Ali Keshavarzi, Elsa Angelini|The lack of large annotated datasets in medical imaging is an intrinsic burden for supervised Deep Learning (DL) segmentation models. Few-shot learning approaches are cost-effective solutions to transfer pre-trained models using only limited annotated data. However, such methods can be prone to overfitting due to limited data diversity especially when segmenting complex, diverse, and sparse tubular structures like airways. Furthermore, crafting informative image representations has played a crucial role in medical imaging, enabling discriminative enhancement of anatomical details. In this paper, we initially train a data-driven sparsification module to enhance airways efficiently in lung CT scans. We then incorporate these sparse representations in a standard supervised segmentation pipeline as a pretraining step to enhance the performance of the DL models. Results presented on the ATM public challenge cohort show the effectiveness of using sparse priors in pre-training, leading to segmentation Dice score increase by 1% to 10% in full-scale and few-shot learning scenarios, respectively.||[2407.04507v1](http://arxiv.org/pdf/2407.04507v1)|null|\n", "2407.04505": "|**2024-07-05**|**Hyperspectral Dataset and Deep Learning methods for Waste from Electric and Electronic Equipment Identification (WEEE)**|\u7535\u6c14\u548c\u7535\u5b50\u8bbe\u5907\u5e9f\u5f03\u7269\u8bc6\u522b\uff08WEEE\uff09\u7684\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5|Artzai Picon, Pablo Galan, Arantza Bereciartua-Perez, Leire Benito-del-Valle|Hyperspectral imaging, a rapidly evolving field, has witnessed the ascendancy of deep learning techniques, supplanting classical feature extraction and classification methods in various applications. However, many researchers employ arbitrary architectures for hyperspectral image processing, often without rigorous analysis of the interplay between spectral and spatial information. This oversight neglects the implications of combining these two modalities on model performance.   In this paper, we evaluate the performance of diverse deep learning architectures for hyperspectral image segmentation. Our analysis disentangles the impact of different architectures, spanning various spectral and spatial granularities. Specifically, we investigate the effects of spectral resolution (capturing spectral information) and spatial texture (conveying spatial details) on segmentation outcomes. Additionally, we explore the transferability of knowledge from large pre-trained image foundation models, originally designed for RGB images, to the hyperspectral domain.   Results show that incorporating spatial information alongside spectral data leads to improved segmentation results, and that it is essential to further work on novel architectures comprising spectral and spatial information and on the adaption of RGB foundation models into the hyperspectral domain.   Furthermore, we contribute to the field by cleaning and publicly releasing the Tecnalia WEEE Hyperspectral dataset. This dataset contains different non-ferrous fractions of Waste Electrical and Electronic Equipment (WEEE), including Copper, Brass, Aluminum, Stainless Steel, and White Copper, spanning the range of 400 to 1000 nm.   We expect these conclusions can guide novel researchers in the field of hyperspectral imaging.||[2407.04505v1](http://arxiv.org/pdf/2407.04505v1)|**[link](https://github.com/samtzai/tecnalia_weee_hyperspectral_dataset)**|\n", "2407.04490": "|**2024-07-05**|**Micro-gesture Online Recognition using Learnable Query Points**|\u4f7f\u7528\u53ef\u5b66\u4e60\u67e5\u8be2\u70b9\u7684\u5fae\u624b\u52bf\u5728\u7ebf\u8bc6\u522b|Pengyu Liu, Fei Wang, Kun Li, Guoliang Chen, Yanyan Wei, Shengeng Tang, Zhiliang Wu, Dan Guo|In this paper, we briefly introduce the solution developed by our team, HFUT-VUT, for the Micro-gesture Online Recognition track in the MiGA challenge at IJCAI 2024. The Micro-gesture Online Recognition task involves identifying the category and locating the start and end times of micro-gestures in video clips. Compared to the typical Temporal Action Detection task, the Micro-gesture Online Recognition task focuses more on distinguishing between micro-gestures and pinpointing the start and end times of actions. Our solution ranks 2nd in the Micro-gesture Online Recognition track.||[2407.04490v1](http://arxiv.org/pdf/2407.04490v1)|null|\n", "2407.04489": "|**2024-07-05**|**Dude: Dual Distribution-Aware Context Prompt Learning For Large Vision-Language Model**|Dude\uff1a\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u91cd\u5206\u5e03\u611f\u77e5\u4e0a\u4e0b\u6587\u63d0\u793a\u5b66\u4e60|Duy M. H. Nguyen, An T. Le, Trung Q. Nguyen, Nghiem T. Diep, Tai Nguyen, Duy Duong-Tran, Jan Peters, Li Shen, Mathias Niepert, Daniel Sonntag|Prompt learning methods are gaining increasing attention due to their ability to customize large vision-language models to new domains using pre-trained contextual knowledge and minimal training data. However, existing works typically rely on optimizing unified prompt inputs, often struggling with fine-grained classification tasks due to insufficient discriminative attributes. To tackle this, we consider a new framework based on a dual context of both domain-shared and class-specific contexts, where the latter is generated by Large Language Models (LLMs) such as GPTs. Such dual prompt methods enhance the model's feature representation by joining implicit and explicit factors encoded in LLM knowledge. Moreover, we formulate the Unbalanced Optimal Transport (UOT) theory to quantify the relationships between constructed prompts and visual tokens. Through partial matching, UOT can properly align discrete sets of visual tokens and prompt embeddings under different mass distributions, which is particularly valuable for handling irrelevant or noisy elements, ensuring that the preservation of mass does not restrict transport solutions. Furthermore, UOT's characteristics integrate seamlessly with image augmentation, expanding the training sample pool while maintaining a reasonable distance between perturbed images and prompt inputs. Extensive experiments across few-shot classification and adapter settings substantiate the superiority of our model over current state-of-the-art baselines.||[2407.04489v1](http://arxiv.org/pdf/2407.04489v1)|null|\n", "2407.04484": "|**2024-07-05**|**Optimizing the image correction pipeline for pedestrian detection in the thermal-infrared domain**|\u4f18\u5316\u70ed\u7ea2\u5916\u9886\u57df\u884c\u4eba\u68c0\u6d4b\u7684\u56fe\u50cf\u6821\u6b63\u6d41\u7a0b|Christophe Karam, Jessy Matias, Xavier Breniere, Jocelyn Chanussot|Infrared imagery can help in low-visibility situations such as fog and low-light scenarios, but it is prone to thermal noise and requires further processing and correction. This work studies the effect of different infrared processing pipelines on the performance of a pedestrian detection in an urban environment, similar to autonomous driving scenarios. Detection on infrared images is shown to outperform that on visible images, but the infrared correction pipeline is crucial since the models cannot extract information from raw infrared images. Two thermal correction pipelines are studied, the shutter and the shutterless pipes. Experiments show that some correction algorithms like spatial denoising are detrimental to performance even if they increase visual quality for a human observer. Other algorithms like destriping and, to a lesser extent, temporal denoising, increase computational time, but have some role to play in increasing detection accuracy. As it stands, the optimal trade-off for speed and accuracy is simply to use the shutterless pipe with a tonemapping algorithm only, for autonomous driving applications within varied environments.||[2407.04484v1](http://arxiv.org/pdf/2407.04484v1)|null|\n", "2407.04476": "|**2024-07-05**|**Rethinking Data Input for Point Cloud Upsampling**|\u91cd\u65b0\u601d\u8003\u70b9\u4e91\u4e0a\u91c7\u6837\u7684\u6570\u636e\u8f93\u5165|Tongxu Zhang|In recent years, point cloud upsampling has been widely applied in fields such as 3D reconstruction and surface generation. However, existing point cloud upsampling inputs are all patch based, and there is no research discussing the differences and principles between point cloud model full input and patch based input. In order to compare with patch based point cloud input, this article proposes a new data input method, which divides the full point cloud model to ensure shape integrity while training PU-GCN. This article was validated on the PU1K and ABC datasets, but the results showed that Patch based performance is better than model based full input i.e. Average Segment input. Therefore, this article explores the data input factors and model modules that affect the upsampling results of point clouds.||[2407.04476v1](http://arxiv.org/pdf/2407.04476v1)|null|\n", "2407.04400": "|**2024-07-05**|**Hard-Attention Gates with Gradient Routing for Endoscopic Image Computing**|\u7528\u4e8e\u5185\u7aa5\u955c\u56fe\u50cf\u8ba1\u7b97\u7684\u5177\u6709\u68af\u5ea6\u8def\u7531\u7684\u786c\u6ce8\u610f\u95e8|Giorgio Roffo, Carlo Biffi, Pietro Salvagnini, Andrea Cherubini|To address overfitting and enhance model generalization in gastroenterological polyp size assessment, our study introduces Feature-Selection Gates (FSG) or Hard-Attention Gates (HAG) alongside Gradient Routing (GR) for dynamic feature selection. This technique aims to boost Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) by promoting sparse connectivity, thereby reducing overfitting and enhancing generalization. HAG achieves this through sparsification with learnable weights, serving as a regularization strategy. GR further refines this process by optimizing HAG parameters via dual forward passes, independently from the main model, to improve feature re-weighting. Our evaluation spanned multiple datasets, including CIFAR-100 for a broad impact assessment and specialized endoscopic datasets (REAL-Colon, Misawa, and SUN) focusing on polyp size estimation, covering over 200 polyps in more than 370,000 frames. The findings indicate that our HAG-enhanced networks substantially enhance performance in both binary and triclass classification tasks related to polyp sizing. Specifically, CNNs experienced an F1 Score improvement to 87.8% in binary classification, while in triclass classification, the ViT-T model reached an F1 Score of 76.5%, outperforming traditional CNNs and ViT-T models. To facilitate further research, we are releasing our codebase, which includes implementations for CNNs, multistream CNNs, ViT, and HAG-augmented variants. This resource aims to standardize the use of endoscopic datasets, providing public training-validation-testing splits for reliable and comparable research in gastroenterological polyp size estimation. The codebase is available at github.com/cosmoimd/feature-selection-gates.||[2407.04400v1](http://arxiv.org/pdf/2407.04400v1)|null|\n", "2407.04382": "|**2024-07-05**|**Self-Supervised Representation Learning for Adversarial Attack Detection**|\u7528\u4e8e\u5bf9\u6297\u6027\u653b\u51fb\u68c0\u6d4b\u7684\u81ea\u76d1\u7763\u8868\u5f81\u5b66\u4e60|Yi Li, Plamen Angelov, Neeraj Suri|Supervised learning-based adversarial attack detection methods rely on a large number of labeled data and suffer significant performance degradation when applying the trained model to new domains. In this paper, we propose a self-supervised representation learning framework for the adversarial attack detection task to address this drawback. Firstly, we map the pixels of augmented input images into an embedding space. Then, we employ the prototype-wise contrastive estimation loss to cluster prototypes as latent variables. Additionally, drawing inspiration from the concept of memory banks, we introduce a discrimination bank to distinguish and learn representations for each individual instance that shares the same or a similar prototype, establishing a connection between instances and their associated prototypes. We propose a parallel axial-attention (PAA)-based encoder to facilitate the training process by parallel training over height- and width-axis of attention maps. Experimental results show that, compared to various benchmark self-supervised vision learning models and supervised adversarial attack detection methods, the proposed model achieves state-of-the-art performance on the adversarial attack detection task across a wide range of images.||[2407.04382v1](http://arxiv.org/pdf/2407.04382v1)|null|\n", "2407.04381": "|**2024-07-05**|**Multi-Branch Auxiliary Fusion YOLO with Re-parameterization Heterogeneous Convolutional for accurate object detection**|\u591a\u5206\u652f\u8f85\u52a9\u878d\u5408 YOLO \u4e0e\u91cd\u65b0\u53c2\u6570\u5316\u5f02\u6784\u5377\u79ef\u5b9e\u73b0\u7cbe\u786e\u7684\u7269\u4f53\u68c0\u6d4b|Zhiqiang Yang, Qiu Guan, Keer Zhao, Jianmin Yang, Xinli Xu, Haixia Long, Ying Tang|Due to the effective performance of multi-scale feature fusion, Path Aggregation FPN (PAFPN) is widely employed in YOLO detectors. However, it cannot efficiently and adaptively integrate high-level semantic information with low-level spatial information simultaneously. We propose a new model named MAF-YOLO in this paper, which is a novel object detection framework with a versatile neck named Multi-Branch Auxiliary FPN (MAFPN). Within MAFPN, the Superficial Assisted Fusion (SAF) module is designed to combine the output of the backbone with the neck, preserving an optimal level of shallow information to facilitate subsequent learning. Meanwhile, the Advanced Assisted Fusion (AAF) module deeply embedded within the neck conveys a more diverse range of gradient information to the output layer.   Furthermore, our proposed Re-parameterized Heterogeneous Efficient Layer Aggregation Network (RepHELAN) module ensures that both the overall model architecture and convolutional design embrace the utilization of heterogeneous large convolution kernels. Therefore, this guarantees the preservation of information related to small targets while simultaneously achieving the multi-scale receptive field. Finally, taking the nano version of MAF-YOLO for example, it can achieve 42.4% AP on COCO with only 3.76M learnable parameters and 10.51G FLOPs, and approximately outperforms YOLOv8n by about 5.1%. The source code of this work is available at: https://github.com/yang-0201/MAF-YOLO.||[2407.04381v1](http://arxiv.org/pdf/2407.04381v1)|**[link](https://github.com/yang-0201/MAF-YOLO)**|\n", "2407.04360": "|**2024-07-05**|**Shape Prior Segmentation Guided by Harmonic Beltrami Signature**|\u8c10\u6ce2 Beltrami \u7279\u5f81\u5f15\u5bfc\u7684\u5f62\u72b6\u4f18\u5148\u5206\u5272|Chenran Lin, Lok Ming Lui|This paper presents a novel shape prior segmentation method guided by the Harmonic Beltrami Signature (HBS). The HBS is a shape representation fully capturing 2D simply connected shapes, exhibiting resilience against perturbations and invariance to translation, rotation, and scaling. The proposed method integrates the HBS within a quasi-conformal topology preserving segmentation framework, leveraging shape prior knowledge to significantly enhance segmentation performance, especially for low-quality or occluded images. The key innovation lies in the bifurcation of the optimization process into two iterative stages: 1) The computation of a quasi-conformal deformation map, which transforms the unit disk into the targeted segmentation area, driven by image data and other regularization terms; 2) The subsequent refinement of this map is contingent upon minimizing the $L_2$ distance between its Beltrami coefficient and the reference HBS. This shape-constrained refinement ensures that the segmentation adheres to the reference shape(s) by exploiting the inherent invariance, robustness, and discerning shape discriminative capabilities afforded by the HBS. Extensive experiments on synthetic and real-world images validate the method's ability to improve segmentation accuracy over baselines, eliminate preprocessing requirements, resist noise corruption, and flexibly acquire and apply shape priors. Overall, the HBS segmentation framework offers an efficient strategy to robustly incorporate the shape prior knowledge, thereby advancing critical low-level vision tasks.||[2407.04360v1](http://arxiv.org/pdf/2407.04360v1)|null|\n", "2407.04353": "|**2024-07-05**|**Segmenting Medical Images: From UNet to Res-UNet and nnUNet**|\u5206\u5272\u533b\u5b66\u56fe\u50cf\uff1a\u4ece UNet \u5230 Res-UNet \u548c nnUNet|Lina Huang, Alina Miron, Kate Hone, Yongmin Li|This study provides a comparative analysis of deep learning models including UNet, Res-UNet, Attention Res-UNet, and nnUNet, and evaluates their performance in brain tumour, polyp, and multi-class heart segmentation tasks. The analysis focuses on precision, accuracy, recall, Dice Similarity Coefficient (DSC), and Intersection over Union (IoU) to assess their clinical applicability. In brain tumour segmentation, Res-UNet and nnUNet significantly outperformed UNet, with Res-UNet leading in DSC and IoU scores, indicating superior accuracy in tumour delineation. Meanwhile, nnUNet excelled in recall and accuracy, which are crucial for reliable tumour detection in clinical diagnosis and planning. In polyp detection, nnUNet was the most effective, achieving the highest metrics across all categories and proving itself as a reliable diagnostic tool in endoscopy. In the complex task of heart segmentation, Res-UNet and Attention Res-UNet were outstanding in delineating the left ventricle, with Res-UNet also leading in right ventricle segmentation. nnUNet was unmatched in myocardium segmentation, achieving top scores in precision, recall, DSC, and IoU. The conclusion notes that although Res-UNet occasionally outperforms nnUNet in specific metrics, the differences are quite small. Moreover, nnUNet consistently shows superior overall performance across the experiments. Particularly noted for its high recall and accuracy, which are crucial in clinical settings to minimize misdiagnosis and ensure timely treatment, nnUNet's robust performance in crucial metrics across all tested categories establishes it as the most effective model for these varied and complex segmentation tasks.||[2407.04353v1](http://arxiv.org/pdf/2407.04353v1)|null|\n", "2407.04334": "|**2024-07-05**|**Learning Geometric Invariant Features for Classification of Vector Polygons with Graph Message-passing Neural Network**|\u5229\u7528\u56fe\u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u51e0\u4f55\u4e0d\u53d8\u7279\u5f81\u8fdb\u884c\u77e2\u91cf\u591a\u8fb9\u5f62\u5206\u7c7b|Zexian Huang, Kourosh Khoshelham, Martin Tomko|Geometric shape classification of vector polygons remains a non-trivial learning task in spatial analysis. Previous studies mainly focus on devising deep learning approaches for representation learning of rasterized vector polygons, whereas the study of discrete representations of polygons and subsequent deep learning approaches have not been fully investigated. In this study, we investigate a graph representation of vector polygons and propose a novel graph message-passing neural network (PolyMP) to learn the geometric-invariant features for shape classification of polygons. Through extensive experiments, we show that the graph representation of polygons combined with a permutation-invariant graph message-passing neural network achieves highly robust performances on benchmark datasets (i.e., synthetic glyph and real-world building footprint datasets) as compared to baseline methods. We demonstrate that the proposed graph-based PolyMP network enables the learning of expressive geometric features invariant to geometric transformations of polygons (i.e., translation, rotation, scaling and shearing) and is robust to trivial vertex removals of polygons. We further show the strong generalizability of PolyMP, which enables generalizing the learned geometric features from the synthetic glyph polygons to the real-world building footprints.||[2407.04334v1](http://arxiv.org/pdf/2407.04334v1)|null|\n", "2407.04327": "|**2024-07-05**|**TF-SASM: Training-free Spatial-aware Sparse Memory for Multi-object Tracking**|TF-SASM\uff1a\u7528\u4e8e\u591a\u76ee\u6807\u8ddf\u8e2a\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u7a7a\u95f4\u611f\u77e5\u7a00\u758f\u5185\u5b58|Thuc Nguyen-Quang, Minh-Triet Tran|Multi-object tracking (MOT) in computer vision remains a significant challenge, requiring precise localization and continuous tracking of multiple objects in video sequences. This task is crucial for various applications, including action recognition and behavior analysis. Key challenges include occlusion, reidentification, tracking fast-moving objects, and handling camera motion artifacts. Past research has explored tracking-by-detection methods and end-to-end models, with recent attention on tracking-by-attention approaches leveraging transformer architectures. The emergence of data sets that emphasize robust reidentification, such as DanceTrack, has highlighted the need for effective solutions. While memory-based approaches have shown promise, they often suffer from high computational complexity and memory usage. We propose a novel sparse memory approach that selectively stores critical features based on object motion and overlapping awareness, aiming to enhance efficiency while minimizing redundancy. Building upon the MOTRv2 model, a hybrid of tracking-by-attention and tracking-by-detection, we introduce a training-free memory designed to bolster reidentification capabilities and preserve the model's flexibility. Our memory approach achieves significant improvements over MOTRv2 in the DanceTrack test set, demonstrating a gain of 1.1\\% in HOTA metrics and 2.1\\% in IDF1 score.||[2407.04327v1](http://arxiv.org/pdf/2407.04327v1)|null|\n", "2407.04326": "|**2024-07-05**|**LMSeg: A deep graph message-passing network for efficient and accurate semantic segmentation of large-scale 3D landscape meshes**|LMSeg\uff1a\u4e00\u79cd\u6df1\u5ea6\u56fe\u6d88\u606f\u4f20\u9012\u7f51\u7edc\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u5bf9\u5927\u89c4\u6a21 3D \u666f\u89c2\u7f51\u683c\u8fdb\u884c\u8bed\u4e49\u5206\u5272|Zexian Huang, Kourosh Khoshelham, Gunditj Mirring Traditional Owners Corporation, Martin Tomko|Semantic segmentation of large-scale 3D landscape meshes is pivotal for various geospatial applications, including spatial analysis, automatic mapping and localization of target objects, and urban planning and development. This requires an efficient and accurate 3D perception system to understand and analyze real-world environments. However, traditional mesh segmentation methods face challenges in accurately segmenting small objects and maintaining computational efficiency due to the complexity and large size of 3D landscape mesh datasets. This paper presents an end-to-end deep graph message-passing network, LMSeg, designed to efficiently and accurately perform semantic segmentation on large-scale 3D landscape meshes. The proposed approach takes the barycentric dual graph of meshes as inputs and applies deep message-passing neural networks to hierarchically capture the geometric and spatial features from the barycentric graph structures and learn intricate semantic information from textured meshes. The hierarchical and local pooling of the barycentric graph, along with the effective geometry aggregation modules of LMSeg, enable fast inference and accurate segmentation of small-sized and irregular mesh objects in various complex landscapes. Extensive experiments on two benchmark datasets (natural and urban landscapes) demonstrate that LMSeg significantly outperforms existing learning-based segmentation methods in terms of object segmentation accuracy and computational efficiency. Furthermore, our method exhibits strong generalization capabilities across diverse landscapes and demonstrates robust resilience against varying mesh densities and landscape topologies.||[2407.04326v1](http://arxiv.org/pdf/2407.04326v1)|null|\n", "2407.04308": "|**2024-07-05**|**SSP-GNN: Learning to Track via Bilevel Optimization**|SSP-GNN\uff1a\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u5b66\u4e60\u8ddf\u8e2a|Griffin Golias, Masa Nakura-Fan, Vitaly Ablavsky|We propose a graph-based tracking formulation for multi-object tracking (MOT) where target detections contain kinematic information and re-identification features (attributes). Our method applies a successive shortest paths (SSP) algorithm to a tracking graph defined over a batch of frames. The edge costs in this tracking graph are computed via a message-passing network, a graph neural network (GNN) variant. The parameters of the GNN, and hence, the tracker, are learned end-to-end on a training set of example ground-truth tracks and detections. Specifically, learning takes the form of bilevel optimization guided by our novel loss function. We evaluate our algorithm on simulated scenarios to understand its sensitivity to scenario aspects and model hyperparameters. Across varied scenario complexities, our method compares favorably to a strong baseline.||[2407.04308v1](http://arxiv.org/pdf/2407.04308v1)|null|\n", "2407.04305": "|**2024-07-05**|**Towards Stable 3D Object Detection**|\u5b9e\u73b0\u7a33\u5b9a\u7684 3D \u7269\u4f53\u68c0\u6d4b|Jiabao Wang, Qiang Meng, Guochao Liu, Liujiang Yan, Ke Wang, Ming-Ming Cheng, Qibin Hou|In autonomous driving, the temporal stability of 3D object detection greatly impacts the driving safety. However, the detection stability cannot be accessed by existing metrics such as mAP and MOTA, and consequently is less explored by the community. To bridge this gap, this work proposes Stability Index (SI), a new metric that can comprehensively evaluate the stability of 3D detectors in terms of confidence, box localization, extent, and heading. By benchmarking state-of-the-art object detectors on the Waymo Open Dataset, SI reveals interesting properties of object stability that have not been previously discovered by other metrics. To help models improve their stability, we further introduce a general and effective training strategy, called Prediction Consistency Learning (PCL). PCL essentially encourages the prediction consistency of the same objects under different timestamps and augmentations, leading to enhanced detection stability. Furthermore, we examine the effectiveness of PCL with the widely-used CenterPoint, and achieve a remarkable SI of 86.00 for vehicle class, surpassing the baseline by 5.48. We hope our work could serve as a reliable baseline and draw the community's attention to this crucial issue in 3D object detection. Codes will be made publicly available.||[2407.04305v1](http://arxiv.org/pdf/2407.04305v1)|null|\n", "2407.04277": "|**2024-07-05**|**Research, Applications and Prospects of Event-Based Pedestrian Detection: A Survey**|\u57fa\u4e8e\u4e8b\u4ef6\u7684\u884c\u4eba\u68c0\u6d4b\u7814\u7a76\u3001\u5e94\u7528\u53ca\u524d\u666f\uff1a\u7efc\u8ff0|Han Wang, Yuman Nie, Yun Li, Hongjie Liu, Min Liu, Wen Cheng, Yaoxiong Wang|Event-based cameras, inspired by the biological retina, have evolved into cutting-edge sensors distinguished by their minimal power requirements, negligible latency, superior temporal resolution, and expansive dynamic range. At present, cameras used for pedestrian detection are mainly frame-based imaging sensors, which have suffered from lethargic response times and hefty data redundancy. In contrast, event-based cameras address these limitations by eschewing extraneous data transmissions and obviating motion blur in high-speed imaging scenarios. On pedestrian detection via event-based cameras, this paper offers an exhaustive review of research and applications particularly in the autonomous driving context. Through methodically scrutinizing relevant literature, the paper outlines the foundational principles, developmental trajectory, and the comparative merits and demerits of eventbased detection relative to traditional frame-based methodologies. This review conducts thorough analyses of various event stream inputs and their corresponding network models to evaluate their applicability across diverse operational environments. It also delves into pivotal elements such as crucial datasets and data acquisition techniques essential for advancing this technology, as well as advanced algorithms for processing event stream data. Culminating with a synthesis of the extant landscape, the review accentuates the unique advantages and persistent challenges inherent in event-based pedestrian detection, offering a prognostic view on potential future developments in this fast-progressing field.||[2407.04277v1](http://arxiv.org/pdf/2407.04277v1)|null|\n", "2407.04274": "|**2024-07-05**|**Fine-grained Dynamic Network for Generic Event Boundary Detection**|\u7528\u4e8e\u4e00\u822c\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u7684\u7ec6\u7c92\u5ea6\u52a8\u6001\u7f51\u7edc|Ziwei Zheng, Lijun He, Le Yang, Fan Li|Generic event boundary detection (GEBD) aims at pinpointing event boundaries naturally perceived by humans, playing a crucial role in understanding long-form videos. Given the diverse nature of generic boundaries, spanning different video appearances, objects, and actions, this task remains challenging. Existing methods usually detect various boundaries by the same protocol, regardless of their distinctive characteristics and detection difficulties, resulting in suboptimal performance. Intuitively, a more intelligent and reasonable way is to adaptively detect boundaries by considering their special properties. In light of this, we propose a novel dynamic pipeline for generic event boundaries named DyBDet. By introducing a multi-exit network architecture, DyBDet automatically learns the subnet allocation to different video snippets, enabling fine-grained detection for various boundaries. Besides, a multi-order difference detector is also proposed to ensure generic boundaries can be effectively identified and adaptively processed. Extensive experiments on the challenging Kinetics-GEBD and TAPOS datasets demonstrate that adopting the dynamic strategy significantly benefits GEBD tasks, leading to obvious improvements in both performance and efficiency compared to the current state-of-the-art.||[2407.04274v1](http://arxiv.org/pdf/2407.04274v1)|null|\n", "2407.04265": "|**2024-07-05**|**Parametric Curve Segment Extraction by Support Regions**|\u6839\u636e\u652f\u6491\u533a\u57df\u63d0\u53d6\u53c2\u6570\u66f2\u7ebf\u6bb5|Cem \u00dcnsalan|We introduce a method to extract curve segments in parametric form from the image directly using the Laplacian of Gaussian (LoG) filter response. Our segmentation gives convex and concave curves. To do so, we form curve support regions by grouping pixels of the thresholded filter response. Then, we model each support region boundary by Fourier series and extract the corresponding parametric curve segment.||[2407.04265v1](http://arxiv.org/pdf/2407.04265v1)|null|\n", "2407.04260": "|**2024-07-05**|**Efficient Detection of Long Consistent Cycles and its Application to Distributed Synchronization**|\u957f\u4e00\u81f4\u5faa\u73af\u7684\u6709\u6548\u68c0\u6d4b\u53ca\u5176\u5728\u5206\u5e03\u5f0f\u540c\u6b65\u4e2d\u7684\u5e94\u7528|Shaohan Li, Yunpeng Shi, Gilad Lerman|Group synchronization plays a crucial role in global pipelines for Structure from Motion (SfM). Its formulation is nonconvex and it is faced with highly corrupted measurements. Cycle consistency has been effective in addressing these challenges. However, computationally efficient solutions are needed for cycles longer than three, especially in practical scenarios where 3-cycles are unavailable. To overcome this computational bottleneck, we propose an algorithm for group synchronization that leverages information from cycles of lengths ranging from three to six with a time complexity of order $O(n^3)$ (or $O(n^{2.373})$ when using a faster matrix multiplication algorithm). We establish non-trivial theory for this and related methods that achieves competitive sample complexity, assuming the uniform corruption model. To advocate the practical need for our method, we consider distributed group synchronization, which requires at least 4-cycles, and we illustrate state-of-the-art performance by our method in this context.||[2407.04260v1](http://arxiv.org/pdf/2407.04260v1)|null|\n", "2407.04249": "|**2024-07-05**|**FeatureSORT: Essential Features for Effective Tracking**|FeatureSORT\uff1a\u6709\u6548\u8ddf\u8e2a\u7684\u57fa\u672c\u529f\u80fd|Hamidreza Hashempoor, Rosemary Koikara, Yu Dong Hwang|In this work, we introduce a novel tracker designed for online multiple object tracking with a focus on being simple, while being effective. we provide multiple feature modules each of which stands for a particular appearance information. By integrating distinct appearance features, including clothing color, style, and target direction, alongside a ReID network for robust embedding extraction, our tracker significantly enhances online tracking accuracy. Additionally, we propose the incorporation of a stronger detector and also provide an advanced post processing methods that further elevate the tracker's performance. During real time operation, we establish measurement to track associated distance function which includes the IoU, direction, color, style, and ReID features similarity information, where each metric is calculated separately. With the design of our feature related distance function, it is possible to track objects through longer period of occlusions, while keeping the number of identity switches comparatively low. Extensive experimental evaluation demonstrates notable improvement in tracking accuracy and reliability, as evidenced by reduced identity switches and enhanced occlusion handling. These advancements not only contribute to the state of the art in object tracking but also open new avenues for future research and practical applications demanding high precision and reliability.||[2407.04249v1](http://arxiv.org/pdf/2407.04249v1)|null|\n", "2407.04243": "|**2024-07-05**|**Exploration of Class Center for Fine-Grained Visual Classification**|\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u7684\u7c7b\u522b\u4e2d\u5fc3\u63a2\u7d22|Hang Yao, Qiguang Miao, Peipei Zhao, Chaoneng Li, Xin Li, Guanwen Feng, Ruyi Liu|Different from large-scale classification tasks, fine-grained visual classification is a challenging task due to two critical problems: 1) evident intra-class variances and subtle inter-class differences, and 2) overfitting owing to fewer training samples in datasets. Most existing methods extract key features to reduce intra-class variances, but pay no attention to subtle inter-class differences in fine-grained visual classification. To address this issue, we propose a loss function named exploration of class center, which consists of a multiple class-center constraint and a class-center label generation. This loss function fully utilizes the information of the class center from the perspective of features and labels. From the feature perspective, the multiple class-center constraint pulls samples closer to the target class center, and pushes samples away from the most similar nontarget class center. Thus, the constraint reduces intra-class variances and enlarges inter-class differences. From the label perspective, the class-center label generation utilizes classcenter distributions to generate soft labels to alleviate overfitting. Our method can be easily integrated with existing fine-grained visual classification approaches as a loss function, to further boost excellent performance with only slight training costs. Extensive experiments are conducted to demonstrate consistent improvements achieved by our method on four widely-used fine-grained visual classification datasets. In particular, our method achieves state-of-the-art performance on the FGVC-Aircraft and CUB-200-2011 datasets.||[2407.04243v1](http://arxiv.org/pdf/2407.04243v1)|**[link](https://github.com/hyao1/ecc)**|\n", "2407.04218": "|**2024-07-05**|**Batch Transformer: Look for Attention in Batch**|Batch Transformer\uff1a\u5728\u6279\u91cf\u4e2d\u5bfb\u627e\u6ce8\u610f\u529b|Myung Beom Her, Jisu Jeong, Hojoon Song, Ji-Hyeong Han|Facial expression recognition (FER) has received considerable attention in computer vision, with \"in-the-wild\" environments such as human-computer interaction. However, FER images contain uncertainties such as occlusion, low resolution, pose variation, illumination variation, and subjectivity, which includes some expressions that do not match the target label. Consequently, little information is obtained from a noisy single image and it is not trusted. This could significantly degrade the performance of the FER task. To address this issue, we propose a batch transformer (BT), which consists of the proposed class batch attention (CBA) module, to prevent overfitting in noisy data and extract trustworthy information by training on features reflected from several images in a batch, rather than information from a single image. We also propose multi-level attention (MLA) to prevent overfitting the specific features by capturing correlations between each level. In this paper, we present a batch transformer network (BTN) that combines the above proposals. Experimental results on various FER benchmark datasets show that the proposed BTN consistently outperforms the state-ofthe-art in FER datasets. Representative results demonstrate the promise of the proposed BTN for FER.||[2407.04218v1](http://arxiv.org/pdf/2407.04218v1)|null|\n", "2407.04190": "|**2024-07-05**|**Computer Vision for Clinical Gait Analysis: A Gait Abnormality Video Dataset**|\u7528\u4e8e\u4e34\u5e8a\u6b65\u6001\u5206\u6790\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\uff1a\u6b65\u6001\u5f02\u5e38\u89c6\u9891\u6570\u636e\u96c6|Rahm Ranjan, David Ahmedt-Aristizabal, Mohammad Ali Armin, Juno Kim|Clinical gait analysis (CGA) using computer vision is an emerging field in artificial intelligence that faces barriers of accessible, real-world data, and clear task objectives. This paper lays the foundation for current developments in CGA as well as vision-based methods and datasets suitable for gait analysis. We introduce The Gait Abnormality in Video Dataset (GAVD) in response to our review of over 150 current gait-related computer vision datasets, which highlighted the need for a large and accessible gait dataset clinically annotated for CGA. GAVD stands out as the largest video gait dataset, comprising 1874 sequences of normal, abnormal and pathological gaits. Additionally, GAVD includes clinically annotated RGB data sourced from publicly available content on online platforms. It also encompasses over 400 subjects who have undergone clinical grade visual screening to represent a diverse range of abnormal gait patterns, captured in various settings, including hospital clinics and urban uncontrolled outdoor environments. We demonstrate the validity of the dataset and utility of action recognition models for CGA using pretrained models Temporal Segment Networks(TSN) and SlowFast network to achieve video abnormality detection of 94% and 92% respectively when tested on GAVD dataset. A GitHub repository https://github.com/Rahmyyy/GAVD consisting of convenient URL links, and clinically relevant annotation for CGA is provided for over 450 online videos, featuring diverse subjects performing a range of normal, pathological, and abnormal gait patterns.||[2407.04190v1](http://arxiv.org/pdf/2407.04190v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2407.04230": "|**2024-07-05**|**A Physical Model-Guided Framework for Underwater Image Enhancement and Depth Estimation**|\u7528\u4e8e\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u548c\u6df1\u5ea6\u4f30\u8ba1\u7684\u7269\u7406\u6a21\u578b\u5f15\u5bfc\u6846\u67b6|Dazhao Du, Enhan Li, Lingyu Si, Fanjiang Xu, Jianwei Niu, Fuchun Sun|Due to the selective absorption and scattering of light by diverse aquatic media, underwater images usually suffer from various visual degradations. Existing underwater image enhancement (UIE) approaches that combine underwater physical imaging models with neural networks often fail to accurately estimate imaging model parameters such as depth and veiling light, resulting in poor performance in certain scenarios. To address this issue, we propose a physical model-guided framework for jointly training a Deep Degradation Model (DDM) with any advanced UIE model. DDM includes three well-designed sub-networks to accurately estimate various imaging parameters: a veiling light estimation sub-network, a factors estimation sub-network, and a depth estimation sub-network. Based on the estimated parameters and the underwater physical imaging model, we impose physical constraints on the enhancement process by modeling the relationship between underwater images and desired clean images, i.e., outputs of the UIE model. Moreover, while our framework is compatible with any UIE model, we design a simple yet effective fully convolutional UIE model, termed UIEConv. UIEConv utilizes both global and local features for image enhancement through a dual-branch structure. UIEConv trained within our framework achieves remarkable enhancement results across diverse underwater scenes. Furthermore, as a byproduct of UIE, the trained depth estimation sub-network enables accurate underwater scene depth estimation. Extensive experiments conducted in various real underwater imaging scenarios, including deep-sea environments with artificial light sources, validate the effectiveness of our framework and the UIEConv model.||[2407.04230v1](http://arxiv.org/pdf/2407.04230v1)|null|\n"}, "LLM": {"2407.04689": "|**2024-07-05**|**RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation**|RAM\uff1a\u57fa\u4e8e\u68c0\u7d22\u7684\u53ef\u901a\u7528\u96f6\u6837\u672c\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u4f9b\u6027\u8f6c\u79fb|Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, Yue Wang|This work proposes a retrieve-and-transfer framework for zero-shot robotic manipulation, dubbed RAM, featuring generalizability across various objects, environments, and embodiments. Unlike existing approaches that learn manipulation from expensive in-domain demonstrations, RAM capitalizes on a retrieval-based affordance transfer paradigm to acquire versatile manipulation capabilities from abundant out-of-domain data. First, RAM extracts unified affordance at scale from diverse sources of demonstrations including robotic data, human-object interaction (HOI) data, and custom data to construct a comprehensive affordance memory. Then given a language instruction, RAM hierarchically retrieves the most similar demonstration from the affordance memory and transfers such out-of-domain 2D affordance to in-domain 3D executable affordance in a zero-shot and embodiment-agnostic manner. Extensive simulation and real-world evaluations demonstrate that our RAM consistently outperforms existing works in diverse daily tasks. Additionally, RAM shows significant potential for downstream applications such as automatic and efficient data collection, one-shot visual imitation, and LLM/VLM-integrated long-horizon manipulation. For more details, please check our website at https://yxkryptonite.github.io/RAM/.||[2407.04689v1](http://arxiv.org/pdf/2407.04689v1)|**[link](https://github.com/yxKryptonite/RAM_code)**|\n"}, "Transformer": {"2407.04699": "|**2024-07-05**|**LaRa: Efficient Large-Baseline Radiance Fields**|LaRa\uff1a\u9ad8\u6548\u7684\u5927\u57fa\u7ebf\u8f90\u5c04\u573a|Anpei Chen, Haofei Xu, Stefano Esposito, Siyu Tang, Andreas Geiger|Radiance field methods have achieved photorealistic novel view synthesis and geometry reconstruction. But they are mostly applied in per-scene optimization or small-baseline settings. While several recent works investigate feed-forward reconstruction with large baselines by utilizing transformers, they all operate with a standard global attention mechanism and hence ignore the local nature of 3D reconstruction. We propose a method that unifies local and global reasoning in transformer layers, resulting in improved quality and faster convergence. Our model represents scenes as Gaussian Volumes and combines this with an image encoder and Group Attention Layers for efficient feed-forward reconstruction. Experimental results demonstrate that our model, trained for two days on four GPUs, demonstrates high fidelity in reconstructing 360&deg radiance fields, and robustness to zero-shot and out-of-domain testing.||[2407.04699v1](http://arxiv.org/pdf/2407.04699v1)|null|\n", "2407.04621": "|**2024-07-05**|**OneRestore: A Universal Restoration Framework for Composite Degradation**|OneRestore\uff1a\u590d\u5408\u6750\u6599\u9000\u5316\u7684\u901a\u7528\u4fee\u590d\u6846\u67b6|Yu Guo, Yuan Gao, Yuxu Lu, Huilin Zhu, Ryan Wen Liu, Shengfeng He|In real-world scenarios, image impairments often manifest as composite degradations, presenting a complex interplay of elements such as low light, haze, rain, and snow. Despite this reality, existing restoration methods typically target isolated degradation types, thereby falling short in environments where multiple degrading factors coexist. To bridge this gap, our study proposes a versatile imaging model that consolidates four physical corruption paradigms to accurately represent complex, composite degradation scenarios. In this context, we propose OneRestore, a novel transformer-based framework designed for adaptive, controllable scene restoration. The proposed framework leverages a unique cross-attention mechanism, merging degraded scene descriptors with image features, allowing for nuanced restoration. Our model allows versatile input scene descriptors, ranging from manual text embeddings to automatic extractions based on visual attributes. Our methodology is further enhanced through a composite degradation restoration loss, using extra degraded images as negative samples to fortify model constraints. Comparative results on synthetic and real-world datasets demonstrate OneRestore as a superior solution, significantly advancing the state-of-the-art in addressing complex, composite degradations.||[2407.04621v1](http://arxiv.org/pdf/2407.04621v1)|**[link](https://github.com/gy65896/onerestore)**|\n", "2407.04604": "|**2024-07-05**|**PartCraft: Crafting Creative Objects by Parts**|PartCraft\uff1a\u5229\u7528\u96f6\u4ef6\u5236\u4f5c\u521b\u610f\u7269\u54c1|Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, Tao Xiang|This paper propels creative control in generative visual AI by allowing users to \"select\". Departing from traditional text or sketch-based methods, we for the first time allow users to choose visual concepts by parts for their creative endeavors. The outcome is fine-grained generation that precisely captures selected visual concepts, ensuring a holistically faithful and plausible result. To achieve this, we first parse objects into parts through unsupervised feature clustering. Then, we encode parts into text tokens and introduce an entropy-based normalized attention loss that operates on them. This loss design enables our model to learn generic prior topology knowledge about object's part composition, and further generalize to novel part compositions to ensure the generation looks holistically faithful. Lastly, we employ a bottleneck encoder to project the part tokens. This not only enhances fidelity but also accelerates learning, by leveraging shared knowledge and facilitating information exchange among instances. Visual results in the paper and supplementary material showcase the compelling power of PartCraft in crafting highly customized, innovative creations, exemplified by the \"charming\" and creative birds. Code is released at https://github.com/kamwoh/partcraft.||[2407.04604v1](http://arxiv.org/pdf/2407.04604v1)|**[link](https://github.com/kamwoh/partcraft)**|\n", "2407.04369": "|**2024-07-05**|**ZARRIO @ Ego4D Short Term Object Interaction Anticipation Challenge: Leveraging Affordances and Attention-based models for STA**|ZARRIO @ Ego4D \u77ed\u671f\u5bf9\u8c61\u4ea4\u4e92\u9884\u671f\u6311\u6218\uff1a\u5229\u7528\u53ef\u4f9b\u6027\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u8fdb\u884c STA|Lorenzo Mur-Labadia, Ruben Martinez-Cantin, Josechu Guerrero-Campo, Giovanni Maria Farinella|Short-Term object-interaction Anticipation (STA) consists of detecting the location of the next-active objects, the noun and verb categories of the interaction, and the time to contact from the observation of egocentric video. We propose STAformer, a novel attention-based architecture integrating frame-guided temporal pooling, dual image-video attention, and multi-scale feature fusion to support STA predictions from an image-input video pair. Moreover, we introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. On the test set, our results obtain a final 33.5 N mAP, 17.25 N+V mAP, 11.77 N+{\\delta} mAP and 6.75 Overall top-5 mAP metric when trained on the v2 training dataset.||[2407.04369v1](http://arxiv.org/pdf/2407.04369v1)|null|\n"}, "3D/CG": {"2407.04355": "|**2024-07-05**|**Data-Driven Tissue- and Subject-Specific Elastic Regularization for Medical Image Registration**|\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u6570\u636e\u9a71\u52a8\u7ec4\u7ec7\u548c\u53d7\u8bd5\u8005\u7279\u5b9a\u5f39\u6027\u6b63\u5219\u5316|Anna Reithmeir, Lina Felsner, Rickmer Braren, Julia A. Schnabel, Veronika A. Zimmer|Physics-inspired regularization is desired for intra-patient image registration since it can effectively capture the biomechanical characteristics of anatomical structures. However, a major challenge lies in the reliance on physical parameters: Parameter estimations vary widely across the literature, and the physical properties themselves are inherently subject-specific. In this work, we introduce a novel data-driven method that leverages hypernetworks to learn the tissue-dependent elasticity parameters of an elastic regularizer. Notably, our approach facilitates the estimation of patient-specific parameters without the need to retrain the network. We evaluate our method on three publicly available 2D and 3D lung CT and cardiac MR datasets. We find that with our proposed subject-specific tissue-dependent regularization, a higher registration quality is achieved across all datasets compared to using a global regularizer. The code is available at https://github.com/compai-lab/2024-miccai-reithmeir.||[2407.04355v1](http://arxiv.org/pdf/2407.04355v1)|**[link](https://github.com/compai-lab/2024-miccai-reithmeir)**|\n", "2407.04345": "|**2024-07-05**|**CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images**|CanonicalFusion\uff1a\u4ece\u591a\u5e45\u56fe\u50cf\u751f\u6210\u53ef\u9a7e\u9a76\u7684 3D \u4eba\u4f53\u5934\u50cf|Jisu Shin, Junmyeong Lee, Seongmin Lee, Min-Gyu Park, Ju-Mi Kang, Ju Hong Yoon, Hae-Gon Jeon|We present a novel framework for reconstructing animatable human avatars from multiple images, termed CanonicalFusion. Our central concept involves integrating individual reconstruction results into the canonical space. To be specific, we first predict Linear Blend Skinning (LBS) weight maps and depth maps using a shared-encoder-dual-decoder network, enabling direct canonicalization of the 3D mesh from the predicted depth maps. Here, instead of predicting high-dimensional skinning weights, we infer compressed skinning weights, i.e., 3-dimensional vector, with the aid of pre-trained MLP networks. We also introduce a forward skinning-based differentiable rendering scheme to merge the reconstructed results from multiple images. This scheme refines the initial mesh by reposing the canonical mesh via the forward skinning and by minimizing photometric and geometric errors between the rendered and the predicted results. Our optimization scheme considers the position and color of vertices as well as the joint angles for each image, thereby mitigating the negative effects of pose errors. We conduct extensive experiments to demonstrate the effectiveness of our method and compare our CanonicalFusion with state-of-the-art methods. Our source codes are available at https://github.com/jsshin98/CanonicalFusion.||[2407.04345v1](http://arxiv.org/pdf/2407.04345v1)|**[link](https://github.com/jsshin98/canonicalfusion)**|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2407.04663": "|**2024-07-05**|**Unsupervised 4D Cardiac Motion Tracking with Spatiotemporal Optical Flow Networks**|\u5229\u7528\u65f6\u7a7a\u5149\u6d41\u7f51\u7edc\u8fdb\u884c\u65e0\u76d1\u7763 4D \u5fc3\u810f\u8fd0\u52a8\u8ddf\u8e2a|Long Teng, Wei Feng, Menglong Zhu, Xinchao Li|Cardiac motion tracking from echocardiography can be used to estimate and quantify myocardial motion within a cardiac cycle. It is a cost-efficient and effective approach for assessing myocardial function. However, ultrasound imaging has the inherent characteristics of spatially low resolution and temporally random noise, which leads to difficulties in obtaining reliable annotation. Thus it is difficult to perform supervised learning for motion tracking. In addition, there is no end-to-end unsupervised method currently in the literature. This paper presents a motion tracking method where unsupervised optical flow networks are designed with spatial reconstruction loss and temporal-consistency loss. Our proposed loss functions make use of the pair-wise and temporal correlation to estimate cardiac motion from noisy background. Experiments using a synthetic 4D echocardiography dataset has shown the effectiveness of our approach, and its superiority over existing methods on both accuracy and running speed. To the best of our knowledge, this is the first work performed that uses unsupervised end-to-end deep learning optical flow network for 4D cardiac motion tracking.||[2407.04663v1](http://arxiv.org/pdf/2407.04663v1)|null|\n", "2407.04258": "|**2024-07-05**|**Unsupervised Video Summarization via Reinforcement Learning and a Trained Evaluator**|\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u8bad\u7ec3\u6709\u7d20\u7684\u8bc4\u4f30\u8005\u8fdb\u884c\u65e0\u76d1\u7763\u89c6\u9891\u6458\u8981|Mehryar Abbasi, Hadi Hadizadeh, Parvaneh Saeedi|This paper presents a novel approach for unsupervised video summarization using reinforcement learning. It aims to address the existing limitations of current unsupervised methods, including unstable training of adversarial generator-discriminator architectures and reliance on hand-crafted reward functions for quality evaluation. The proposed method is based on the concept that a concise and informative summary should result in a reconstructed video that closely resembles the original. The summarizer model assigns an importance score to each frame and generates a video summary. In the proposed scheme, reinforcement learning, coupled with a unique reward generation pipeline, is employed to train the summarizer model. The reward generation pipeline trains the summarizer to create summaries that lead to improved reconstructions. It comprises a generator model capable of reconstructing masked frames from a partially masked video, along with a reward mechanism that compares the reconstructed video from the summary against the original. The video generator is trained in a self-supervised manner to reconstruct randomly masked frames, enhancing its ability to generate accurate summaries. This training pipeline results in a summarizer model that better mimics human-generated video summaries compared to methods relying on hand-crafted rewards. The training process consists of two stable and isolated training steps, unlike adversarial architectures. Experimental results demonstrate promising performance, with F-scores of 62.3 and 54.5 on TVSum and SumMe datasets, respectively. Additionally, the inference stage is 300 times faster than our previously reported state-of-the-art method.||[2407.04258v1](http://arxiv.org/pdf/2407.04258v1)|null|\n"}, "\u5176\u4ed6": {"2407.04688": "|**2024-07-05**|**Enhancing Vehicle Re-identification and Matching for Weaving Analysis**|\u589e\u5f3a\u8f66\u8f86\u91cd\u65b0\u8bc6\u522b\u548c\u5339\u914d\u4ee5\u8fdb\u884c\u4ea4\u7ec7\u5206\u6790|Mei Qiu, Wei Lin, Stanley Chien, Lauren Christopher, Yaobin Chen, Shu Hu|Vehicle weaving on highways contributes to traffic congestion, raises safety issues, and underscores the need for sophisticated traffic management systems. Current tools are inadequate in offering precise and comprehensive data on lane-specific weaving patterns. This paper introduces an innovative method for collecting non-overlapping video data in weaving zones, enabling the generation of quantitative insights into lane-specific weaving behaviors. Our experimental results confirm the efficacy of this approach, delivering critical data that can assist transportation authorities in enhancing traffic control and roadway infrastructure.||[2407.04688v1](http://arxiv.org/pdf/2407.04688v1)|null|\n", "2407.04592": "|**2024-07-05**|**Smell and Emotion: Recognising emotions in smell-related artworks**|\u55c5\u89c9\u4e0e\u60c5\u611f\uff1a\u4ece\u4e0e\u55c5\u89c9\u76f8\u5173\u7684\u827a\u672f\u4f5c\u54c1\u4e2d\u8bc6\u522b\u60c5\u611f|Vishal Patoliya, Mathias Zinnen, Andreas Maier, Vincent Christlein|Emotions and smell are underrepresented in digital art history. In this exploratory work, we show that recognising emotions from smell-related artworks is technically feasible but has room for improvement. Using style transfer and hyperparameter optimization we achieve a minor performance boost and open up the field for future extensions.||[2407.04592v1](http://arxiv.org/pdf/2407.04592v1)|null|\n", "2407.04559": "|**2024-07-05**|**Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition**|\u5c1a\u672a\u5b8c\u5168\u4e86\u89e3\uff1a\u8bc4\u4f30\u89c6\u89c9\u53d9\u4e8b\u9700\u8981\u7684\u4e0d\u4ec5\u4ec5\u662f\u8861\u91cf\u8fde\u8d2f\u6027\u3001\u57fa\u7840\u6027\u548c\u91cd\u590d\u6027|Aditya K Surikuchi, Raquel Fern\u00e1ndez, Sandro Pezzelle|Visual storytelling consists in generating a natural language story given a temporally ordered sequence of images. This task is not only challenging for models, but also very difficult to evaluate with automatic metrics since there is no consensus about what makes a story 'good'. In this paper, we introduce a novel method that measures story quality in terms of human likeness regarding three key aspects highlighted in previous work: visual grounding, coherence, and repetitiveness. We then use this method to evaluate the stories generated by several models, showing that the foundation model LLaVA obtains the best result, but only slightly so compared to TAPM, a 50-times smaller visual storytelling model. Upgrading the visual and language components of TAPM results in a model that yields competitive performance with a relatively low number of parameters. Finally, we carry out a human evaluation study, whose results suggest that a 'good' story may require more than a human-like level of visual grounding, coherence, and repetition.||[2407.04559v1](http://arxiv.org/pdf/2407.04559v1)|null|\n", "2407.04542": "|**2024-07-05**|**Rethinking Image Compression on the Web with Generative AI**|\u4f7f\u7528\u751f\u6210\u5f0f AI \u91cd\u65b0\u601d\u8003 Web \u4e0a\u7684\u56fe\u50cf\u538b\u7f29|Shayan Ali Hassan, Danish Humair, Ihsan Ayyub Qazi, Zafar Ayyub Qazi|The rapid growth of the Internet, driven by social media, web browsing, and video streaming, has made images central to the Web experience, resulting in significant data transfer and increased webpage sizes. Traditional image compression methods, while reducing bandwidth, often degrade image quality. This paper explores a novel approach using generative AI to reconstruct images at the edge or client-side. We develop a framework that leverages text prompts and provides additional conditioning inputs like Canny edges and color palettes to a text-to-image model, achieving up to 99.8% bandwidth savings in the best cases and 92.6% on average, while maintaining high perceptual similarity. Empirical analysis and a user study show that our method preserves image meaning and structure more effectively than traditional compression methods, offering a promising solution for reducing bandwidth usage and improving Internet affordability with minimal degradation in image quality.||[2407.04542v1](http://arxiv.org/pdf/2407.04542v1)|null|\n", "2407.04396": "|**2024-07-05**|**Graph-Guided Test-Time Adaptation for Glaucoma Diagnosis using Fundus Photography**|\u4f7f\u7528\u773c\u5e95\u7167\u76f8\u8fdb\u884c\u9752\u5149\u773c\u8bca\u65ad\u7684\u56fe\u5f62\u5f15\u5bfc\u6d4b\u8bd5\u65f6\u95f4\u8c03\u6574|Qian Zeng, Fan Zhang|Glaucoma is a leading cause of irreversible blindness worldwide. While deep learning approaches using fundus images have largely improved early diagnosis of glaucoma, variations in images from different devices and locations (known as domain shifts) challenge the use of pre-trained models in real-world settings. To address this, we propose a novel Graph-guided Test-Time Adaptation (GTTA) framework to generalize glaucoma diagnosis models to unseen test environments. GTTA integrates the topological information of fundus images into the model training, enhancing the model's transferability and reducing the risk of learning spurious correlation. During inference, GTTA introduces a novel test-time training objective to make the source-trained classifier progressively adapt to target patterns with reliable class conditional estimation and consistency regularization. Experiments on cross-domain glaucoma diagnosis benchmarks demonstrate the superiority of the overall framework and individual components under different backbone networks.||[2407.04396v1](http://arxiv.org/pdf/2407.04396v1)|null|\n", "2407.04271": "|**2024-07-05**|**Variational Partial Group Convolutions for Input-Aware Partial Equivariance of Rotations and Color-Shifts**|\u7528\u4e8e\u8f93\u5165\u611f\u77e5\u65cb\u8f6c\u548c\u989c\u8272\u504f\u79fb\u90e8\u5206\u7b49\u53d8\u5206\u7684\u53d8\u5206\u90e8\u5206\u7ec4\u5377\u79ef|Hyunsu Kim, Yegon Kim, Hongseok Yang, Juho Lee|Group Equivariant CNNs (G-CNNs) have shown promising efficacy in various tasks, owing to their ability to capture hierarchical features in an equivariant manner. However, their equivariance is fixed to the symmetry of the whole group, limiting adaptability to diverse partial symmetries in real-world datasets, such as limited rotation symmetry of handwritten digit images and limited color-shift symmetry of flower images. Recent efforts address this limitation, one example being Partial G-CNN which restricts the output group space of convolution layers to break full equivariance. However, such an approach still fails to adjust equivariance levels across data. In this paper, we propose a novel approach, Variational Partial G-CNN (VP G-CNN), to capture varying levels of partial equivariance specific to each data instance. VP G-CNN redesigns the distribution of the output group elements to be conditioned on input data, leveraging variational inference to avoid overfitting. This enables the model to adjust its equivariance levels according to the needs of individual data points. Additionally, we address training instability inherent in discrete group equivariance models by redesigning the reparametrizable distribution. We demonstrate the effectiveness of VP G-CNN on both toy and real-world datasets, including MNIST67-180, CIFAR10, ColorMNIST, and Flowers102. Our results show robust performance, even in uncertainty metrics.||[2407.04271v1](http://arxiv.org/pdf/2407.04271v1)|null|\n", "2407.04245": "|**2024-07-05**|**Every Pixel Has its Moments: Ultra-High-Resolution Unpaired Image-to-Image Translation via Dense Normalization**|\u6bcf\u4e2a\u50cf\u7d20\u90fd\u6709\u5176\u72ec\u7279\u4e4b\u5904\uff1a\u901a\u8fc7\u5bc6\u96c6\u89c4\u8303\u5316\u5b9e\u73b0\u8d85\u9ad8\u5206\u8fa8\u7387\u975e\u914d\u5bf9\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362|Ming-Yang Ho, Che-Ming Wu, Min-Sheng Wu, Yufeng Jane Tseng|Recent advancements in ultra-high-resolution unpaired image-to-image translation have aimed to mitigate the constraints imposed by limited GPU memory through patch-wise inference. Nonetheless, existing methods often compromise between the reduction of noticeable tiling artifacts and the preservation of color and hue contrast, attributed to the reliance on global image- or patch-level statistics in the instance normalization layers. In this study, we introduce a Dense Normalization (DN) layer designed to estimate pixel-level statistical moments. This approach effectively diminishes tiling artifacts while concurrently preserving local color and hue contrasts. To address the computational demands of pixel-level estimation, we further propose an efficient interpolation algorithm. Moreover, we invent a parallelism strategy that enables the DN layer to operate in a single pass. Through extensive experiments, we demonstrate that our method surpasses all existing approaches in performance. Notably, our DN layer is hyperparameter-free and can be seamlessly integrated into most unpaired image-to-image translation frameworks without necessitating retraining. Overall, our work paves the way for future exploration in handling images of arbitrary resolutions within the realm of unpaired image-to-image translation. Code is available at: https://github.com/Kaminyou/Dense-Normalization.||[2407.04245v1](http://arxiv.org/pdf/2407.04245v1)|null|\n", "2407.04241": "|**2024-07-05**|**AnySR: Realizing Image Super-Resolution as Any-Scale, Any-Resource**|AnySR\uff1a\u5b9e\u73b0\u4efb\u610f\u5c3a\u5ea6\u3001\u4efb\u610f\u8d44\u6e90\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387|Wengyi Zhan, Mingbao Lin, Chia-Wen Lin, Rongrong Ji|In an effort to improve the efficiency and scalability of single-image super-resolution (SISR) applications, we introduce AnySR, to rebuild existing arbitrary-scale SR methods into any-scale, any-resource implementation. As a contrast to off-the-shelf methods that solve SR tasks across various scales with the same computing costs, our AnySR innovates in: 1) building arbitrary-scale tasks as any-resource implementation, reducing resource requirements for smaller scales without additional parameters; 2) enhancing any-scale performance in a feature-interweaving fashion, inserting scale pairs into features at regular intervals and ensuring correct feature/scale processing. The efficacy of our AnySR is fully demonstrated by rebuilding most existing arbitrary-scale SISR methods and validating on five popular SISR test datasets. The results show that our AnySR implements SISR tasks in a computing-more-efficient fashion, and performs on par with existing arbitrary-scale SISR methods. For the first time, we realize SISR tasks as not only any-scale in literature, but also as any-resource. Code is available at https://github.com/CrispyFeSo4/AnySR.||[2407.04241v1](http://arxiv.org/pdf/2407.04241v1)|**[link](https://github.com/crispyfeso4/anysr)**|\n"}}