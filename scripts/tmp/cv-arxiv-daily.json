{"\u751f\u6210\u6a21\u578b": {"2405.02700": "|**2024-05-04**|**Towards a Scalable Identification of Novel Modes in Generative Models**|\u8fc8\u5411\u751f\u6210\u6a21\u578b\u4e2d\u65b0\u6a21\u5f0f\u7684\u53ef\u6269\u5c55\u8bc6\u522b|Jingwei Zhang, Mohammad Jalali, Cheuk Ting Li, Farzan Farnia|An interpretable comparison of generative models requires the identification of sample types produced more frequently by each of the involved models. While several quantitative scores have been proposed in the literature to rank different generative models, such score-based evaluations do not reveal the nuanced differences between the generative models in capturing various sample types. In this work, we propose a method called Fourier-based Identification of Novel Clusters (FINC) to identify modes produced by a generative model with a higher frequency in comparison to a reference distribution. FINC provides a scalable stochastic algorithm based on random Fourier features to estimate the eigenspace of kernel covariance matrices of two generative models and utilize the principal eigendirections to detect the sample types present more dominantly in each model. We demonstrate the application of the FINC method to standard computer vision datasets and generative model frameworks. Our numerical results suggest the scalability and efficiency of the developed Fourier-based method in highlighting the sample types captured with different frequencies by widely-used generative models.||[2405.02700v1](http://arxiv.org/pdf/2405.02700v1)|null|\n", "2405.02595": "|**2024-05-04**|**Vision-based 3D occupancy prediction in autonomous driving: a review and outlook**|\u81ea\u52a8\u9a7e\u9a76\u4e2d\u57fa\u4e8e\u89c6\u89c9\u7684 3D \u5360\u7528\u9884\u6d4b\uff1a\u56de\u987e\u4e0e\u5c55\u671b|Yanan Zhang, Jinqing Zhang, Zengran Wang, Junhao Xu, Di Huang|In recent years, autonomous driving has garnered escalating attention for its potential to relieve drivers' burdens and improve driving safety. Vision-based 3D occupancy prediction, which predicts the spatial occupancy status and semantics of 3D voxel grids around the autonomous vehicle from image inputs, is an emerging perception task suitable for cost-effective perception system of autonomous driving. Although numerous studies have demonstrated the greater advantages of 3D occupancy prediction over object-centric perception tasks, there is still a lack of a dedicated review focusing on this rapidly developing field. In this paper, we first introduce the background of vision-based 3D occupancy prediction and discuss the challenges in this task. Secondly, we conduct a comprehensive survey of the progress in vision-based 3D occupancy prediction from three aspects: feature enhancement, deployment friendliness and label efficiency, and provide an in-depth analysis of the potentials and challenges of each category of methods. Finally, we present a summary of prevailing research trends and propose some inspiring future outlooks. To provide a valuable reference for researchers, a regularly updated collection of related papers, datasets, and codes is organized at https://github.com/zya3d/Awesome-3D-Occupancy-Prediction.||[2405.02595v1](http://arxiv.org/pdf/2405.02595v1)|**[link](https://github.com/zya3d/awesome-3d-occupancy-prediction)**|\n"}, "\u591a\u6a21\u6001": {"2405.02771": "|**2024-05-04**|**MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning**|MMEarth\uff1a\u63a2\u7d22\u5730\u7406\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u7684\u591a\u6a21\u6001\u501f\u53e3\u4efb\u52a1|Vishal Nedungadi, Ankit Kariryaa, Stefan Oehmcke, Serge Belongie, Christian Igel, Nico Lang|The volume of unlabelled Earth observation (EO) data is huge, but many important applications lack labelled training data. However, EO data offers the unique opportunity to pair data from different modalities and sensors automatically based on geographic location and time, at virtually no human labor cost. We seize this opportunity to create a diverse multi-modal pretraining dataset at global scale. Using this new corpus of 1.2 million locations, we propose a Multi-Pretext Masked Autoencoder (MP-MAE) approach to learn general-purpose representations for optical satellite images. Our approach builds on the ConvNeXt V2 architecture, a fully convolutional masked autoencoder (MAE). Drawing upon a suite of multi-modal pretext tasks, we demonstrate that our MP-MAE approach outperforms both MAEs pretrained on ImageNet and MAEs pretrained on domain-specific satellite images. This is shown on several downstream tasks including image classification and semantic segmentation. We find that multi-modal pretraining notably improves the linear probing performance, e.g. 4pp on BigEarthNet and 16pp on So2Sat, compared to pretraining on optical satellite images only. We show that this also leads to better label and parameter efficiency which are crucial aspects in global scale applications.||[2405.02771v1](http://arxiv.org/pdf/2405.02771v1)|null|\n", "2405.02766": "|**2024-05-04**|**Beyond Unimodal Learning: The Importance of Integrating Multiple Modalities for Lifelong Learning**|\u8d85\u8d8a\u5355\u6a21\u6001\u5b66\u4e60\uff1a\u6574\u5408\u591a\u79cd\u6a21\u6001\u5bf9\u4e8e\u7ec8\u8eab\u5b66\u4e60\u7684\u91cd\u8981\u6027|Fahad Sarfraz, Bahram Zonooz, Elahe Arani|While humans excel at continual learning (CL), deep neural networks (DNNs) exhibit catastrophic forgetting. A salient feature of the brain that allows effective CL is that it utilizes multiple modalities for learning and inference, which is underexplored in DNNs. Therefore, we study the role and interactions of multiple modalities in mitigating forgetting and introduce a benchmark for multimodal continual learning. Our findings demonstrate that leveraging multiple views and complementary information from multiple modalities enables the model to learn more accurate and robust representations. This makes the model less vulnerable to modality-specific regularities and considerably mitigates forgetting. Furthermore, we observe that individual modalities exhibit varying degrees of robustness to distribution shift. Finally, we propose a method for integrating and aligning the information from different modalities by utilizing the relational structural similarities between the data points in each modality. Our method sets a strong baseline that enables both single- and multimodal inference. Our study provides a promising case for further exploring the role of multiple modalities in enabling CL and provides a standard benchmark for future research.||[2405.02766v1](http://arxiv.org/pdf/2405.02766v1)|null|\n", "2405.02717": "|**2024-05-04**|**AFter: Attention-based Fusion Router for RGBT Tracking**|AFter\uff1a\u7528\u4e8e RGBT \u8ddf\u8e2a\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u878d\u5408\u8def\u7531\u5668|Andong Lu, Wanyu Wang, Chenglong Li, Jin Tang, Bin Luo|Multi-modal feature fusion as a core investigative component of RGBT tracking emerges numerous fusion studies in recent years. However, existing RGBT tracking methods widely adopt fixed fusion structures to integrate multi-modal feature, which are hard to handle various challenges in dynamic scenarios. To address this problem, this work presents a novel \\emph{A}ttention-based \\emph{F}usion rou\\emph{ter} called AFter, which optimizes the fusion structure to adapt to the dynamic challenging scenarios, for robust RGBT tracking. In particular, we design a fusion structure space based on the hierarchical attention network, each attention-based fusion unit corresponding to a fusion operation and a combination of these attention units corresponding to a fusion structure. Through optimizing the combination of attention-based fusion units, we can dynamically select the fusion structure to adapt to various challenging scenarios. Unlike complex search of different structures in neural architecture search algorithms, we develop a dynamic routing algorithm, which equips each attention-based fusion unit with a router, to predict the combination weights for efficient optimization of the fusion structure. Extensive experiments on five mainstream RGBT tracking datasets demonstrate the superior performance of the proposed AFter against state-of-the-art RGBT trackers. We release the code in https://github.com/Alexadlu/AFter.||[2405.02717v1](http://arxiv.org/pdf/2405.02717v1)|**[link](https://github.com/alexadlu/after)**|\n"}, "Nerf": {"2405.02762": "|**2024-05-04**|**TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for Dynamic UAV-based Scenes**|TK-Planes\uff1a\u7528\u4e8e\u52a8\u6001\u65e0\u4eba\u673a\u573a\u666f\u7684\u5177\u6709\u9ad8\u7ef4\u7279\u5f81\u5411\u91cf\u7684\u5206\u5c42 K \u5e73\u9762|Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon|In this paper, we present a new approach to bridge the domain gap between synthetic and real-world data for un- manned aerial vehicle (UAV)-based perception. Our formu- lation is designed for dynamic scenes, consisting of moving objects or human actions, where the goal is to recognize the pose or actions. We propose an extension of K-Planes Neural Radiance Field (NeRF), wherein our algorithm stores a set of tiered feature vectors. The tiered feature vectors are generated to effectively model conceptual information about a scene as well as an image decoder that transforms output feature maps into RGB images. Our technique leverages the information amongst both static and dynamic objects within a scene and is able to capture salient scene attributes of high altitude videos. We evaluate its performance on challenging datasets, including Okutama Action and UG2, and observe considerable improvement in accuracy over state of the art aerial perception algorithms.||[2405.02762v1](http://arxiv.org/pdf/2405.02762v1)|null|\n", "2405.02568": "|**2024-05-04**|**ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface Uncertainty**|ActiveNeuS\uff1a\u4f7f\u7528\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u4e3b\u52a8 3D \u91cd\u5efa|Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang|Active learning in 3D scene reconstruction has been widely studied, as selecting informative training views is critical for the reconstruction. Recently, Neural Radiance Fields (NeRF) variants have shown performance increases in active 3D reconstruction using image rendering or geometric uncertainty. However, the simultaneous consideration of both uncertainties in selecting informative views remains unexplored, while utilizing different types of uncertainty can reduce the bias that arises in the early training stage with sparse inputs. In this paper, we propose ActiveNeuS, which evaluates candidate views considering both uncertainties. ActiveNeuS provides a way to accumulate image rendering uncertainty while avoiding the bias that the estimated densities can introduce. ActiveNeuS computes the neural implicit surface uncertainty, providing the color uncertainty along with the surface information. It efficiently handles the bias by using the surface information and a grid, enabling the fast selection of diverse viewpoints. Our method outperforms previous works on popular datasets, Blender and DTU, showing that the views selected by ActiveNeuS significantly improve performance.||[2405.02568v1](http://arxiv.org/pdf/2405.02568v1)|null|\n"}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2405.02751": "|**2024-05-04**|**Deep Image Restoration For Image Anti-Forensics**|\u56fe\u50cf\u53cd\u53d6\u8bc1\u7684\u6df1\u5ea6\u56fe\u50cf\u6062\u590d|Eren Tahir, Mert Bal|While image forensics is concerned with whether an image has been tampered with, image anti-forensics attempts to prevent image forensics methods from detecting tampered images. The competition between these two fields started long before the advancement of deep learning. JPEG compression, blurring and noising, which are simple methods by today's standards, have long been used for anti-forensics and have been the subject of much research in both forensics and anti-forensics. Although these traditional methods are old, they make it difficult to detect fake images and are used for data augmentation in training deep image forgery detection models. In addition to making the image difficult to detect, these methods leave traces on the image and consequently degrade the image quality. Separate image forensics methods have also been developed to detect these traces. In this study, we go one step further and improve the image quality after these methods with deep image restoration models and make it harder to detect the forged image. We evaluate the impact of these methods on image quality. We then test both our proposed methods with deep learning and methods without deep learning on the two best existing image manipulation detection models. In the obtained results, we show how existing image forgery detection models fail against the proposed methods. Code implementation will be publicly available at https://github.com/99eren99/DIRFIAF .||[2405.02751v1](http://arxiv.org/pdf/2405.02751v1)|**[link](https://github.com/99eren99/dirfiaf)**|\n", "2405.02698": "|**2024-05-04**|**Stable Diffusion Dataset Generation for Downstream Classification Tasks**|\u7528\u4e8e\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u7684\u7a33\u5b9a\u6269\u6563\u6570\u636e\u96c6\u751f\u6210|Eugenio Lomurno, Matteo D'Oria, Matteo Matteucci|Recent advances in generative artificial intelligence have enabled the creation of high-quality synthetic data that closely mimics real-world data. This paper explores the adaptation of the Stable Diffusion 2.0 model for generating synthetic datasets, using Transfer Learning, Fine-Tuning and generation parameter optimisation techniques to improve the utility of the dataset for downstream classification tasks. We present a class-conditional version of the model that exploits a Class-Encoder and optimisation of key generation parameters. Our methodology led to synthetic datasets that, in a third of cases, produced models that outperformed those trained on real datasets.||[2405.02698v1](http://arxiv.org/pdf/2405.02698v1)|null|\n", "2405.02686": "|**2024-05-04**|**Boosting 3D Neuron Segmentation with 2D Vision Transformer Pre-trained on Natural Images**|\u4f7f\u7528\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684 2D Vision Transformer \u589e\u5f3a 3D \u795e\u7ecf\u5143\u5206\u5272|Yik San Cheng, Runkai Zhao, Heng Wang, Hanchuan Peng, Weidong Cai|Neuron reconstruction, one of the fundamental tasks in neuroscience, rebuilds neuronal morphology from 3D light microscope imaging data. It plays a critical role in analyzing the structure-function relationship of neurons in the nervous system. However, due to the scarcity of neuron datasets and high-quality SWC annotations, it is still challenging to develop robust segmentation methods for single neuron reconstruction. To address this limitation, we aim to distill the consensus knowledge from massive natural image data to aid the segmentation model in learning the complex neuron structures. Specifically, in this work, we propose a novel training paradigm that leverages a 2D Vision Transformer model pre-trained on large-scale natural images to initialize our Transformer-based 3D neuron segmentation model with a tailored 2D-to-3D weight transferring strategy. Our method builds a knowledge sharing connection between the abundant natural and the scarce neuron image domains to improve the 3D neuron segmentation ability in a data-efficiency manner. Evaluated on a popular benchmark, BigNeuron, our method enhances neuron segmentation performance by 8.71% over the model trained from scratch with the same amount of training samples.||[2405.02686v1](http://arxiv.org/pdf/2405.02686v1)|null|\n", "2405.02678": "|**2024-05-04**|**Position Paper: Quo Vadis, Unsupervised Time Series Anomaly Detection?**|\u7acb\u573a\u6587\u4ef6\uff1aQuo Vadis\uff0c\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff1f|M. Saquib Sarfraz, Mei-Yen Chen, Lukas Layer, Kunyu Peng, Marios Koulakis|The current state of machine learning scholarship in Timeseries Anomaly Detection (TAD) is plagued by the persistent use of flawed evaluation metrics, inconsistent benchmarking practices, and a lack of proper justification for the choices made in novel deep learning-based model designs. Our paper presents a critical analysis of the status quo in TAD, revealing the misleading track of current research and highlighting problematic methods, and evaluation practices. Our position advocates for a shift in focus from pursuing only the novelty in model design to improving benchmarking practices, creating non-trivial datasets, and placing renewed emphasis on studying the utility of model architectures for specific tasks. Our findings demonstrate the need for rigorous evaluation protocols, the creation of simple baselines, and the revelation that state-of-the-art deep anomaly detection models effectively learn linear mappings. These findings suggest the need for more exploration and development of simple and interpretable TAD methods. The increment of model complexity in the state-of-the-art deep-learning based models unfortunately offers very little improvement. We offer insights and suggestions for the field to move forward.||[2405.02678v1](http://arxiv.org/pdf/2405.02678v1)|null|\n", "2405.02648": "|**2024-05-04**|**A Conformal Prediction Score that is Robust to Label Noise**|\u5bf9\u566a\u58f0\u6807\u7b7e\u5177\u6709\u9c81\u68d2\u6027\u7684\u4fdd\u5f62\u9884\u6d4b\u5206\u6570|Coby Penso, Jacob Goldberger|Conformal Prediction (CP) quantifies network uncertainty by building a small prediction set with a pre-defined probability that the correct class is within this set. In this study we tackle the problem of CP calibration based on a validation set with noisy labels. We introduce a conformal score that is robust to label noise. The noise-free conformal score is estimated using the noisy labeled data and the noise level. In the test phase the noise-free score is used to form the prediction set. We applied the proposed algorithm to several standard medical imaging classification datasets. We show that our method outperforms current methods by a large margin, in terms of the average size of the prediction set, while maintaining the required coverage.||[2405.02648v1](http://arxiv.org/pdf/2405.02648v1)|null|\n", "2405.02608": "|**2024-05-04**|**UnSAMFlow: Unsupervised Optical Flow Guided by Segment Anything Model**|UnSAMFlow\uff1a\u7531\u5206\u6bb5\u4efb\u610f\u6a21\u578b\u5f15\u5bfc\u7684\u65e0\u76d1\u7763\u5149\u6d41|Shuai Yuan, Lei Luo, Zhuo Hui, Can Pu, Xiaoyu Xiang, Rakesh Ranjan, Denis Demandolx|Traditional unsupervised optical flow methods are vulnerable to occlusions and motion boundaries due to lack of object-level information. Therefore, we propose UnSAMFlow, an unsupervised flow network that also leverages object information from the latest foundation model Segment Anything Model (SAM). We first include a self-supervised semantic augmentation module tailored to SAM masks. We also analyze the poor gradient landscapes of traditional smoothness losses and propose a new smoothness definition based on homography instead. A simple yet effective mask feature module has also been added to further aggregate features on the object level. With all these adaptations, our method produces clear optical flow estimation with sharp boundaries around objects, which outperforms state-of-the-art methods on both KITTI and Sintel datasets. Our method also generalizes well across domains and runs very efficiently.||[2405.02608v1](http://arxiv.org/pdf/2405.02608v1)|**[link](https://github.com/facebookresearch/unsamflow)**|\n", "2405.02591": "|**2024-05-04**|**Better YOLO with Attention-Augmented Network and Enhanced Generalization Performance for Safety Helmet Detection**|\u5177\u6709\u6ce8\u610f\u529b\u589e\u5f3a\u7f51\u7edc\u548c\u589e\u5f3a\u5b89\u5168\u5934\u76d4\u68c0\u6d4b\u6cdb\u5316\u6027\u80fd\u7684\u66f4\u597d YOLO|Shuqi Shen, Junjie Yang|Safety helmets play a crucial role in protecting workers from head injuries in construction sites, where potential hazards are prevalent. However, currently, there is no approach that can simultaneously achieve both model accuracy and performance in complex environments. In this study, we utilized a Yolo-based model for safety helmet detection, achieved a 2% improvement in mAP (mean Average Precision) performance while reducing parameters and Flops count by over 25%. YOLO(You Only Look Once) is a widely used, high-performance, lightweight model architecture that is well suited for complex environments. We presents a novel approach by incorporating a lightweight feature extraction network backbone based on GhostNetv2, integrating attention modules such as Spatial Channel-wise Attention Net(SCNet) and Coordination Attention Net(CANet), and adopting the Gradient Norm Aware optimizer (GAM) for improved generalization ability. In safety-critical environments, the accurate detection and speed of safety helmets plays a pivotal role in preventing occupational hazards and ensuring compliance with safety protocols. This work addresses the pressing need for robust and efficient helmet detection methods, offering a comprehensive framework that not only enhances accuracy but also improves the adaptability of detection models to real-world conditions. Our experimental results underscore the synergistic effects of GhostNetv2, attention modules, and the GAM optimizer, presenting a compelling solution for safety helmet detection that achieves superior performance in terms of accuracy, generalization, and efficiency.||[2405.02591v1](http://arxiv.org/pdf/2405.02591v1)|null|\n", "2405.02564": "|**2024-05-04**|**Leveraging the Human Ventral Visual Stream to Improve Neural Network Robustness**|\u5229\u7528\u4eba\u7c7b\u8179\u4fa7\u89c6\u89c9\u6d41\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u6027|Zhenan Shao, Linjian Ma, Bo Li, Diane M. Beck|Human object recognition exhibits remarkable resilience in cluttered and dynamic visual environments. In contrast, despite their unparalleled performance across numerous visual tasks, Deep Neural Networks (DNNs) remain far less robust than humans, showing, for example, a surprising susceptibility to adversarial attacks involving image perturbations that are (almost) imperceptible to humans. Human object recognition likely owes its robustness, in part, to the increasingly resilient representations that emerge along the hierarchy of the ventral visual cortex. Here we show that DNNs, when guided by neural representations from a hierarchical sequence of regions in the human ventral visual stream, display increasing robustness to adversarial attacks. These neural-guided models also exhibit a gradual shift towards more human-like decision-making patterns and develop hierarchically smoother decision surfaces. Importantly, the resulting representational spaces differ in important ways from those produced by conventional smoothing methods, suggesting that such neural-guidance may provide previously unexplored robustness solutions. Our findings support the gradual emergence of human robustness along the ventral visual hierarchy and suggest that the key to DNN robustness may lie in increasing emulation of the human brain.||[2405.02564v1](http://arxiv.org/pdf/2405.02564v1)|null|\n", "2405.02556": "|**2024-05-04**|**Few-Shot Fruit Segmentation via Transfer Learning**|\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u8fdb\u884c\u5c11\u6837\u672c\u6c34\u679c\u5206\u5272|Jordan A. James, Heather K. Manching, Amanda M. Hulse-Kemp, William J. Beksi|Advancements in machine learning, computer vision, and robotics have paved the way for transformative solutions in various domains, particularly in agriculture. For example, accurate identification and segmentation of fruits from field images plays a crucial role in automating jobs such as harvesting, disease detection, and yield estimation. However, achieving robust and precise infield fruit segmentation remains a challenging task since large amounts of labeled data are required to handle variations in fruit size, shape, color, and occlusion. In this paper, we develop a few-shot semantic segmentation framework for infield fruits using transfer learning. Concretely, our work is aimed at addressing agricultural domains that lack publicly available labeled data. Motivated by similar success in urban scene parsing, we propose specialized pre-training using a public benchmark dataset for fruit transfer learning. By leveraging pre-trained neural networks, accurate semantic segmentation of fruit in the field is achieved with only a few labeled images. Furthermore, we show that models with pre-training learn to distinguish between fruit still on the trees and fruit that have fallen on the ground, and they can effectively transfer the knowledge to the target fruit dataset.||[2405.02556v1](http://arxiv.org/pdf/2405.02556v1)|null|\n", "2405.02538": "|**2024-05-04**|**AdaFPP: Adapt-Focused Bi-Propagating Prototype Learning for Panoramic Activity Recognition**|AdaFPP\uff1a\u7528\u4e8e\u5168\u666f\u6d3b\u52a8\u8bc6\u522b\u7684\u4ee5\u9002\u5e94\u4e3a\u4e2d\u5fc3\u7684\u53cc\u5411\u4f20\u64ad\u539f\u578b\u5b66\u4e60|Meiqi Cao, Rui Yan, Xiangbo Shu, Guangzhao Dai, Yazhou Yao, Guo-Sen Xie|Panoramic Activity Recognition (PAR) aims to identify multi-granularity behaviors performed by multiple persons in panoramic scenes, including individual activities, group activities, and global activities. Previous methods 1) heavily rely on manually annotated detection boxes in training and inference, hindering further practical deployment; or 2) directly employ normal detectors to detect multiple persons with varying size and spatial occlusion in panoramic scenes, blocking the performance gain of PAR. To this end, we consider learning a detector adapting varying-size occluded persons, which is optimized along with the recognition module in the all-in-one framework. Therefore, we propose a novel Adapt-Focused bi-Propagating Prototype learning (AdaFPP) framework to jointly recognize individual, group, and global activities in panoramic activity scenes by learning an adapt-focused detector and multi-granularity prototypes as the pretext tasks in an end-to-end way. Specifically, to accommodate the varying sizes and spatial occlusion of multiple persons in crowed panoramic scenes, we introduce a panoramic adapt-focuser, achieving the size-adapting detection of individuals by comprehensively selecting and performing fine-grained detections on object-dense sub-regions identified through original detections. In addition, to mitigate information loss due to inaccurate individual localizations, we introduce a bi-propagation prototyper that promotes closed-loop interaction and informative consistency across different granularities by facilitating bidirectional information propagation among the individual, group, and global levels. Extensive experiments demonstrate the significant performance of AdaFPP and emphasize its powerful applicability for PAR.||[2405.02538v1](http://arxiv.org/pdf/2405.02538v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "LLM": {}, "Transformer": {"2405.02730": "|**2024-05-04**|**U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers**|U-DiTs\uff1aU \u5f62\u6269\u6563\u53d8\u538b\u5668\u4e2d\u7684\u4e0b\u91c7\u6837\u4ee4\u724c|Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, Yunhe Wang|Diffusion Transformers (DiTs) introduce the transformer architecture to diffusion tasks for latent-space image generation. With an isotropic architecture that chains a series of transformer blocks, DiTs demonstrate competitive performance and good scalability; but meanwhile, the abandonment of U-Net by DiTs and their following improvements is worth rethinking. To this end, we conduct a simple toy experiment by comparing a U-Net architectured DiT with an isotropic one. It turns out that the U-Net architecture only gain a slight advantage amid the U-Net inductive bias, indicating potential redundancies within the U-Net-style DiT. Inspired by the discovery that U-Net backbone features are low-frequency-dominated, we perform token downsampling on the query-key-value tuple for self-attention and bring further improvements despite a considerable amount of reduction in computation. Based on self-attention with downsampled tokens, we propose a series of U-shaped DiTs (U-DiTs) in the paper and conduct extensive experiments to demonstrate the extraordinary performance of U-DiT models. The proposed U-DiT could outperform DiT-XL/2 with only 1/6 of its computation cost. Codes are available at https://github.com/YuchuanTian/U-DiT.||[2405.02730v1](http://arxiv.org/pdf/2405.02730v1)|null|\n", "2405.02692": "|**2024-05-04**|**Diffeomorphic Transformer-based Abdomen MRI-CT Deformable Image Registration**|\u57fa\u4e8e\u5fae\u5206\u540c\u5f62\u53d8\u538b\u5668\u7684\u8179\u90e8 MRI-CT \u53d8\u5f62\u56fe\u50cf\u914d\u51c6|Yang Lei, Luke A. Matkovic, Justin Roper, Tonghe Wang, Jun Zhou, Beth Ghavidel, Mark McDonald, Pretesh Patel, Xiaofeng Yang|This paper aims to create a deep learning framework that can estimate the deformation vector field (DVF) for directly registering abdominal MRI-CT images. The proposed method assumed a diffeomorphic deformation. By using topology-preserved deformation features extracted from the probabilistic diffeomorphic registration model, abdominal motion can be accurately obtained and utilized for DVF estimation. The model integrated Swin transformers, which have demonstrated superior performance in motion tracking, into the convolutional neural network (CNN) for deformation feature extraction. The model was optimized using a cross-modality image similarity loss and a surface matching loss. To compute the image loss, a modality-independent neighborhood descriptor (MIND) was used between the deformed MRI and CT images. The surface matching loss was determined by measuring the distance between the warped coordinates of the surfaces of contoured structures on the MRI and CT images. The deformed MRI image was assessed against the CT image using the target registration error (TRE), Dice similarity coefficient (DSC), and mean surface distance (MSD) between the deformed contours of the MRI image and manual contours of the CT image. When compared to only rigid registration, DIR with the proposed method resulted in an increase of the mean DSC values of the liver and portal vein from 0.850 and 0.628 to 0.903 and 0.763, a decrease of the mean MSD of the liver from 7.216 mm to 3.232 mm, and a decrease of the TRE from 26.238 mm to 8.492 mm. The proposed deformable image registration method based on a diffeomorphic transformer provides an effective and efficient way to generate an accurate DVF from an MRI-CT image pair of the abdomen. It could be utilized in the current treatment planning workflow for liver radiotherapy.||[2405.02692v1](http://arxiv.org/pdf/2405.02692v1)|null|\n", "2405.02571": "|**2024-05-04**|**ViTALS: Vision Transformer for Action Localization in Surgical Nephrectomy**|ViTALS\uff1a\u7528\u4e8e\u80be\u5207\u9664\u624b\u672f\u4e2d\u52a8\u4f5c\u5b9a\u4f4d\u7684\u89c6\u89c9\u8f6c\u6362\u5668|Soumyadeep Chandra, Sayeed Shafayet Chowdhury, Courtney Yong, Chandru P. Sundaram, Kaushik Roy|Surgical action localization is a challenging computer vision problem. While it has promising applications including automated training of surgery procedures, surgical workflow optimization, etc., appropriate model design is pivotal to accomplishing this task. Moreover, the lack of suitable medical datasets adds an additional layer of complexity. To that effect, we introduce a new complex dataset of nephrectomy surgeries called UroSlice. To perform the action localization from these videos, we propose a novel model termed as `ViTALS' (Vision Transformer for Action Localization in Surgical Nephrectomy). Our model incorporates hierarchical dilated temporal convolution layers and inter-layer residual connections to capture the temporal correlations at finer as well as coarser granularities. The proposed approach achieves state-of-the-art performance on Cholec80 and UroSlice datasets (89.8% and 66.1% accuracy, respectively), validating its effectiveness.||[2405.02571v1](http://arxiv.org/pdf/2405.02571v1)|null|\n"}, "3D/CG": {}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2405.02586": "|**2024-05-04**|**Generalizing CLIP to Unseen Domain via Text-Guided Diverse Novel Feature Synthesis**|\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u7684\u591a\u6837\u5316\u65b0\u9896\u7279\u5f81\u5408\u6210\u5c06 CLIP \u63a8\u5e7f\u5230\u770b\u4e0d\u89c1\u7684\u9886\u57df|Siyuan Yan, Cheng Luo, Zhen Yu, Zongyuan Ge|Vision-language foundation models like CLIP have shown impressive zero-shot generalization, but finetuning on downstream datasets can cause overfitting and loss of its generalization ability on unseen domains. Although collecting additional data from new domains of interest is possible, this method is often impractical due to the challenges in obtaining annotated data. To address this, we propose a plug-and-play feature augmentation method called LDFS (Language-Guided Diverse Feature Synthesis) to synthesize new domain features and improve existing CLIP fine-tuning strategies. LDFS has three main contributions: 1) To synthesize novel domain features and promote diversity, we propose an instance-conditional feature augmentation strategy based on a textguided feature augmentation loss. 2) To maintain feature quality after augmenting, we introduce a pairwise regularizer to preserve augmented feature coherence within the CLIP feature space. 3) We propose to use stochastic text feature augmentation to reduce the modality gap and further facilitate the process of text-guided feature synthesis. Extensive experiments show LDFS superiority in improving CLIP generalization ability on unseen domains without collecting data from those domains. The code will be made publicly available.||[2405.02586v1](http://arxiv.org/pdf/2405.02586v1)|null|\n"}, "\u5176\u4ed6": {"2405.02676": "|**2024-05-04**|**Hand-Object Interaction Controller (HOIC): Deep Reinforcement Learning for Reconstructing Interactions with Physics**|\u624b-\u7269\u4f53\u4ea4\u4e92\u63a7\u5236\u5668\uff08HOIC\uff09\uff1a\u7528\u4e8e\u91cd\u5efa\u7269\u7406\u4ea4\u4e92\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60|Haoyu Hu, Xinyu Yi, Zhe Cao, Jun-Hai Yong, Feng Xu|Hand manipulating objects is an important interaction motion in our daily activities. We faithfully reconstruct this motion with a single RGBD camera by a novel deep reinforcement learning method to leverage physics. Firstly, we propose object compensation control which establishes direct object control to make the network training more stable. Meanwhile, by leveraging the compensation force and torque, we seamlessly upgrade the simple point contact model to a more physical-plausible surface contact model, further improving the reconstruction accuracy and physical correctness. Experiments indicate that without involving any heuristic physical rules, this work still successfully involves physics in the reconstruction of hand-object interactions which are complex motions hard to imitate with deep reinforcement learning. Our code and data are available at https://github.com/hu-hy17/HOIC.||[2405.02676v1](http://arxiv.org/pdf/2405.02676v1)|**[link](https://github.com/hu-hy17/hoic)**|\n", "2405.02652": "|**2024-05-04**|**Deep Pulse-Signal Magnification for remote Heart Rate Estimation in Compressed Videos**|\u7528\u4e8e\u538b\u7f29\u89c6\u9891\u4e2d\u8fdc\u7a0b\u5fc3\u7387\u4f30\u8ba1\u7684\u6df1\u5ea6\u8109\u51b2\u4fe1\u53f7\u653e\u5927|Joaquim Comas, Adria Ruiz, Federico Sukno|Recent advancements in remote heart rate measurement (rPPG), motivated by data-driven approaches, have significantly improved accuracy. However, certain challenges, such as video compression, still remain: recovering the rPPG signal from highly compressed videos is particularly complex. Although several studies have highlighted the difficulties and impact of video compression for this, effective solutions remain limited. In this paper, we present a novel approach to address the impact of video compression on rPPG estimation, which leverages a pulse-signal magnification transformation to adapt compressed videos to an uncompressed data domain in which the rPPG signal is magnified. We validate the effectiveness of our model by exhaustive evaluations on two publicly available datasets, UCLA-rPPG and UBFC-rPPG, employing both intra- and cross-database performance at several compression rates. Additionally, we assess the robustness of our approach on two additional highly compressed and widely-used datasets, MAHNOB-HCI and COHFACE, which reveal outstanding heart rate estimation results.||[2405.02652v1](http://arxiv.org/pdf/2405.02652v1)|null|\n", "2405.02581": "|**2024-05-04**|**Stationary Representations: Optimally Approximating Compatibility and Implications for Improved Model Replacements**|\u5e73\u7a33\u8868\u793a\uff1a\u6700\u4f73\u8fd1\u4f3c\u517c\u5bb9\u6027\u548c\u6539\u8fdb\u6a21\u578b\u66ff\u6362\u7684\u542b\u4e49|Niccol\u00f2 Biondi, Federico Pernici, Simone Ricci, Alberto Del Bimbo|Learning compatible representations enables the interchangeable use of semantic features as models are updated over time. This is particularly relevant in search and retrieval systems where it is crucial to avoid reprocessing of the gallery images with the updated model. While recent research has shown promising empirical evidence, there is still a lack of comprehensive theoretical understanding about learning compatible representations. In this paper, we demonstrate that the stationary representations learned by the $d$-Simplex fixed classifier optimally approximate compatibility representation according to the two inequality constraints of its formal definition. This not only establishes a solid foundation for future works in this line of research but also presents implications that can be exploited in practical learning scenarios. An exemplary application is the now-standard practice of downloading and fine-tuning new pre-trained models. Specifically, we show the strengths and critical issues of stationary representations in the case in which a model undergoing sequential fine-tuning is asynchronously replaced by downloading a better-performing model pre-trained elsewhere. Such a representation enables seamless delivery of retrieval service (i.e., no reprocessing of gallery images) and offers improved performance without operational disruptions during model replacement. Code available at: https://github.com/miccunifi/iamcl2r.||[2405.02581v1](http://arxiv.org/pdf/2405.02581v1)|**[link](https://github.com/miccunifi/iamcl2r)**|\n"}}