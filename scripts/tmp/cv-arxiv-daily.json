{"\u751f\u6210\u6a21\u578b": {"2402.14817": "|**2024-02-22**|**Cameras as Rays: Pose Estimation via Ray Diffusion**|\u76f8\u673a\u4f5c\u4e3a\u5149\u7ebf\uff1a\u901a\u8fc7\u5149\u7ebf\u6269\u6563\u8fdb\u884c\u59ff\u52bf\u4f30\u8ba1|Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani|Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (<10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while generalizing to unseen object categories and in-the-wild captures.|\u4f30\u8ba1\u76f8\u673a\u59ff\u6001\u662f 3D \u91cd\u5efa\u7684\u4e00\u9879\u57fa\u672c\u4efb\u52a1\uff0c\u5e76\u4e14\u5728\u7a00\u758f\u89c6\u56fe (<10) \u7684\u60c5\u51b5\u4e0b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e0e\u8ffd\u6c42\u76f8\u673a\u5916\u53c2\u5168\u5c40\u53c2\u6570\u5316\u7684\u81ea\u4e0a\u800c\u4e0b\u9884\u6d4b\u7684\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u76f8\u673a\u59ff\u6001\u7684\u5206\u5e03\u5f0f\u8868\u793a\uff0c\u5c06\u76f8\u673a\u89c6\u4e3a\u4e00\u675f\u5149\u7ebf\u3002\u8fd9\u79cd\u8868\u793a\u5141\u8bb8\u4e0e\u7a7a\u95f4\u56fe\u50cf\u7279\u5f81\u7d27\u5bc6\u8026\u5408\uff0c\u4ece\u800c\u63d0\u9ad8\u59ff\u6001\u7cbe\u5ea6\u3002\u6211\u4eec\u89c2\u5bdf\u5230\u8fd9\u79cd\u8868\u793a\u81ea\u7136\u9002\u5408\u8bbe\u7f6e\u7ea7\u522b\u7684\u8f6c\u6362\u5668\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u5c06\u56fe\u50cf\u5757\u6620\u5c04\u5230\u76f8\u5e94\u7684\u5149\u7ebf\u3002\u4e3a\u4e86\u6355\u83b7\u7a00\u758f\u89c6\u56fe\u59ff\u6001\u63a8\u65ad\u4e2d\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6211\u4eec\u91c7\u7528\u8fd9\u79cd\u65b9\u6cd5\u6765\u5b66\u4e60\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5141\u8bb8\u6211\u4eec\u5728\u63d0\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u91c7\u6837\u5408\u7406\u7684\u6a21\u5f0f\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\uff08\u57fa\u4e8e\u56de\u5f52\u548c\u6269\u6563\uff09\u5c55\u793a\u4e86 CO3D \u4e0a\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u7684\u5bf9\u8c61\u7c7b\u522b\u548c\u91ce\u5916\u6355\u83b7\u3002|[2402.14817v1](http://arxiv.org/pdf/2402.14817v1)|null|\n", "2402.14780": "|**2024-02-22**|**Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models**|\u5b9a\u5236\u89c6\u9891\uff1a\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u4e00\u6b21\u6027\u8fd0\u52a8\u5b9a\u5236|Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, Abhinav Shrivastava|Image customization has been extensively studied in text-to-image (T2I) diffusion models, leading to impressive outcomes and applications. With the emergence of text-to-video (T2V) diffusion models, its temporal counterpart, motion customization, has not yet been well investigated. To address the challenge of one-shot motion customization, we propose Customize-A-Video that models the motion from a single reference video and adapting it to new subjects and scenes with both spatial and temporal varieties. It leverages low-rank adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V diffusion model for specific motion modeling from the reference videos. To disentangle the spatial and temporal information during the training pipeline, we introduce a novel concept of appearance absorbers that detach the original appearance from the single reference video prior to motion learning. Our proposed method can be easily extended to various downstream tasks, including custom video generation and editing, video appearance customization, and multiple motion combination, in a plug-and-play fashion. Our project page can be found at https://anonymous-314.github.io.|\u56fe\u50cf\u5b9a\u5236\u5728\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\uff0c\u4ea7\u751f\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u548c\u5e94\u7528\u3002\u968f\u7740\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6269\u6563\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u5176\u65f6\u95f4\u5bf9\u5e94\u6a21\u578b\u2014\u2014\u8fd0\u52a8\u5b9a\u5236\u2014\u2014\u5c1a\u672a\u5f97\u5230\u5f88\u597d\u7684\u7814\u7a76\u3002\u4e3a\u4e86\u89e3\u51b3\u4e00\u6b21\u6027\u8fd0\u52a8\u5b9a\u5236\u7684\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5b9a\u5236\u89c6\u9891\uff0c\u5b83\u53ef\u4ee5\u6839\u636e\u5355\u4e2a\u53c2\u8003\u89c6\u9891\u5bf9\u8fd0\u52a8\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u4f7f\u5176\u9002\u5e94\u5177\u6709\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u5316\u7684\u65b0\u4e3b\u9898\u548c\u573a\u666f\u3002\u5b83\u5229\u7528\u65f6\u95f4\u6ce8\u610f\u529b\u5c42\u4e0a\u7684\u4f4e\u79e9\u9002\u5e94 (LoRA)\uff0c\u4e3a\u53c2\u8003\u89c6\u9891\u4e2d\u7684\u7279\u5b9a\u8fd0\u52a8\u5efa\u6a21\u5b9a\u5236\u9884\u8bad\u7ec3\u7684 T2V \u6269\u6563\u6a21\u578b\u3002\u4e3a\u4e86\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7406\u6e05\u7a7a\u95f4\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5916\u89c2\u5438\u6536\u5668\u7684\u65b0\u6982\u5ff5\uff0c\u5b83\u5728\u8fd0\u52a8\u5b66\u4e60\u4e4b\u524d\u5c06\u539f\u59cb\u5916\u89c2\u4e0e\u5355\u4e2a\u53c2\u8003\u89c6\u9891\u5206\u79bb\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u4ee5\u5373\u63d2\u5373\u7528\u7684\u65b9\u5f0f\u8f7b\u677e\u6269\u5c55\u5230\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5305\u62ec\u81ea\u5b9a\u4e49\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u3001\u89c6\u9891\u5916\u89c2\u5b9a\u5236\u548c\u591a\u8fd0\u52a8\u7ec4\u5408\u3002\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\u53ef\u4ee5\u5728 https://anonymous-314.github.io \u627e\u5230\u3002|[2402.14780v1](http://arxiv.org/pdf/2402.14780v1)|null|\n", "2402.14741": "|**2024-02-22**|**Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using Self-Supervised Learning**|\u4f7f\u7528\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u5728\u80f8\u90e8 X \u5c04\u7ebf\u4e2d\u8fdb\u884c\u96f6\u6b21\u5c0f\u513f\u7ed3\u6838\u75c5\u68c0\u6d4b|Daniel Capell\u00e1n-Mart\u00edn, Abhijeet Parida, Juan J. G\u00f3mez-Valverde, Ramon Sanchez-Jacob, Pooneh Roshanitabrizi, Marius G. Linguraru, Mar\u00eda J. Ledesma-Carbayo, Syed M. Anwar|Tuberculosis (TB) remains a significant global health challenge, with pediatric cases posing a major concern. The World Health Organization (WHO) advocates for chest X-rays (CXRs) for TB screening. However, visual interpretation by radiologists can be subjective, time-consuming and prone to error, especially in pediatric TB. Artificial intelligence (AI)-driven computer-aided detection (CAD) tools, especially those utilizing deep learning, show promise in enhancing lung disease detection. However, challenges include data scarcity and lack of generalizability. In this context, we propose a novel self-supervised paradigm leveraging Vision Transformers (ViT) for improved TB detection in CXR, enabling zero-shot pediatric TB detection. We demonstrate improvements in TB detection performance ($\\sim$12.7% and $\\sim$13.4% top AUC/AUPR gains in adults and children, respectively) when conducting self-supervised pre-training when compared to fully-supervised (i.e., non pre-trained) ViT models, achieving top performances of 0.959 AUC and 0.962 AUPR in adult TB detection, and 0.697 AUC and 0.607 AUPR in zero-shot pediatric TB detection. As a result, this work demonstrates that self-supervised learning on adult CXRs effectively extends to challenging downstream tasks such as pediatric TB detection, where data are scarce.|\u7ed3\u6838\u75c5\uff08TB\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u7684\u5168\u7403\u5065\u5eb7\u6311\u6218\uff0c\u5176\u4e2d\u513f\u79d1\u75c5\u4f8b\u662f\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\u3002\u4e16\u754c\u536b\u751f\u7ec4\u7ec7 (WHO) \u63d0\u5021\u4f7f\u7528\u80f8\u90e8 X \u5149\u68c0\u67e5 (CXR) \u8fdb\u884c\u7ed3\u6838\u75c5\u7b5b\u67e5\u3002\u7136\u800c\uff0c\u653e\u5c04\u79d1\u533b\u751f\u7684\u76ee\u89c6\u89e3\u91ca\u53ef\u80fd\u662f\u4e3b\u89c2\u7684\u3001\u8017\u65f6\u7684\u5e76\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u7279\u522b\u662f\u5728\u513f\u7ae5\u7ed3\u6838\u75c5\u4e2d\u3002\u4eba\u5de5\u667a\u80fd (AI) \u9a71\u52a8\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u68c0\u6d4b (CAD) \u5de5\u5177\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u5de5\u5177\uff0c\u5728\u589e\u5f3a\u80ba\u90e8\u75be\u75c5\u68c0\u6d4b\u65b9\u9762\u663e\u793a\u51fa\u5e0c\u671b\u3002\u7136\u800c\uff0c\u6311\u6218\u5305\u62ec\u6570\u636e\u7a00\u7f3a\u548c\u7f3a\u4e4f\u666e\u904d\u6027\u3002\u5728\u8fd9\u79cd\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u6211\u76d1\u7763\u8303\u4f8b\uff0c\u5229\u7528 Vision Transformers (ViT) \u6765\u6539\u8fdb CXR \u4e2d\u7684\u7ed3\u6838\u75c5\u68c0\u6d4b\uff0c\u4ece\u800c\u5b9e\u73b0\u96f6\u6837\u672c\u513f\u7ae5\u7ed3\u6838\u75c5\u68c0\u6d4b\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u4e0e\u5b8c\u5168\u76d1\u7763\uff08\u5373\u975e\u9884\u8bad\u7ec3\uff09\u76f8\u6bd4\uff0c\u8fdb\u884c\u81ea\u6211\u76d1\u7763\u9884\u8bad\u7ec3\u65f6\uff0c\u7ed3\u6838\u75c5\u68c0\u6d4b\u6027\u80fd\u6709\u6240\u63d0\u9ad8\uff08\u6210\u4eba\u548c\u513f\u7ae5\u7684\u6700\u9ad8 AUC/AUPR \u5206\u522b\u63d0\u9ad8\u4e86 $\\sim$12.7% \u548c $\\sim$13.4%\uff09\u3002 -\u8bad\u7ec3\u7684\uff09ViT \u6a21\u578b\uff0c\u5728\u6210\u4eba\u7ed3\u6838\u75c5\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86 0.959 AUC \u548c 0.962 AUPR \u7684\u6700\u4f73\u6027\u80fd\uff0c\u5728\u96f6\u6b21\u513f\u7ae5\u7ed3\u6838\u75c5\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86 0.697 AUC \u548c 0.607 AUPR \u7684\u6700\u4f73\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u8fd9\u9879\u5de5\u4f5c\u8868\u660e\uff0c\u6210\u4eba CXR \u7684\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u6709\u6548\u5730\u6269\u5c55\u5230\u5177\u6709\u6311\u6218\u6027\u7684\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f8b\u5982\u6570\u636e\u7a00\u7f3a\u7684\u513f\u79d1\u7ed3\u6838\u75c5\u68c0\u6d4b\u3002|[2402.14741v1](http://arxiv.org/pdf/2402.14741v1)|null|\n", "2402.14683": "|**2024-02-22**|**Visual Hallucinations of Multi-modal Large Language Models**|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5e7b\u89c9|Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong|Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.|\u5e7b\u89c6 (VH) \u662f\u6307\u591a\u6a21\u6001\u6cd5\u5b66\u7855\u58eb (MLLM) \u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u60f3\u8c61\u51fa\u6709\u5173\u56fe\u50cf\u7684\u9519\u8bef\u7ec6\u8282\u3002\u73b0\u6709\u7814\u7a76\u4ec5\u5728\u73b0\u6709\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u53d1\u73b0 VH \u5b9e\u4f8b\uff0c\u7531\u4e8e\u6b64\u7c7b VH \u5b9e\u4f8b\u7684\u591a\u6837\u6027\u6709\u9650\uff0c\u5bfc\u81f4\u5bf9 VH \u4e0b MLLM \u6027\u80fd\u7684\u7406\u89e3\u5b58\u5728\u504f\u5dee\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a VHTest \u7684\u5de5\u5177\u6765\u751f\u6210\u4e00\u7ec4\u4e0d\u540c\u7684 VH \u5b9e\u4f8b\u3002\u5177\u4f53\u6765\u8bf4\uff0cVHTest\u5728\u73b0\u6709\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u627e\u5230\u4e00\u4e9b\u521d\u59cbVH\u5b9e\u4f8b\uff08\u4f8b\u5982COCO\uff09\uff0c\u4e3a\u6bcf\u4e2aVH\u6a21\u5f0f\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08\u4f8b\u5982DALL-E-3\uff09\u751f\u6210VH\u56fe\u50cf\u6839\u636e\u6587\u5b57\u63cf\u8ff0\u3002\u6211\u4eec\u4f7f\u7528 VHTest \u6536\u96c6\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 8 \u79cd VH \u6a21\u5f0f\u4e0b\u7684 1,200 \u4e2a VH \u5b9e\u4f8b\u3002\u6211\u4eec\u53d1\u73b0\u73b0\u6709\u7684 MLLM\uff08\u4f8b\u5982 GPT-4V\u3001LLaVA-1.5 \u548c MiniGPT-v2\uff09\u5bf9\u6211\u4eec\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5927\u90e8\u5206\u5b9e\u4f8b\u4ea7\u751f\u5e7b\u89c9\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u4f7f\u7528\u6211\u4eec\u7684\u57fa\u51c6\u6570\u636e\u96c6\u5fae\u8c03 MLLM \u53ef\u4ee5\u964d\u4f4e\u5176\u4ea7\u751f\u5e7b\u89c9\u7684\u53ef\u80fd\u6027\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u5176\u5728\u5176\u4ed6\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u662f\u516c\u5f00\u7684\uff1ahttps://github.com/wenhuang2000/VHTest\u3002|[2402.14683v1](http://arxiv.org/pdf/2402.14683v1)|null|\n", "2402.14577": "|**2024-02-22**|**Debiasing Text-to-Image Diffusion Models**|\u6d88\u9664\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u504f\u5dee|Ruifei He, Chuhui Xue, Haoru Tan, Wenqing Zhang, Yingchen Yu, Song Bai, Xiaojuan Qi|Learning-based Text-to-Image (TTI) models like Stable Diffusion have revolutionized the way visual content is generated in various domains. However, recent research has shown that nonnegligible social bias exists in current state-of-the-art TTI systems, which raises important concerns. In this work, we target resolving the social bias in TTI diffusion models. We begin by formalizing the problem setting and use the text descriptions of bias groups to establish an unsafe direction for guiding the diffusion process. Next, we simplify the problem into a weight optimization problem and attempt a Reinforcement solver, Policy Gradient, which shows sub-optimal performance with slow convergence. Further, to overcome limitations, we propose an iterative distribution alignment (IDA) method. Despite its simplicity, we show that IDA shows efficiency and fast convergence in resolving the social bias in TTI diffusion models. Our code will be released.|\u57fa\u4e8e\u5b66\u4e60\u7684\u6587\u672c\u5230\u56fe\u50cf (TTI) \u6a21\u578b\uff08\u4f8b\u5982\u7a33\u5b9a\u6269\u6563\uff09\u5f7b\u5e95\u6539\u53d8\u4e86\u5404\u4e2a\u9886\u57df\u4e2d\u89c6\u89c9\u5185\u5bb9\u7684\u751f\u6210\u65b9\u5f0f\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684 TTI \u7cfb\u7edf\u4e2d\u5b58\u5728\u4e0d\u53ef\u5ffd\u89c6\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u8fd9\u5f15\u8d77\u4e86\u4eba\u4eec\u7684\u4e25\u91cd\u62c5\u5fe7\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u89e3\u51b3 TTI \u6269\u6563\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u3002\u6211\u4eec\u9996\u5148\u5c06\u95ee\u9898\u8bbe\u7f6e\u5f62\u5f0f\u5316\uff0c\u5e76\u4f7f\u7528\u504f\u89c1\u7fa4\u4f53\u7684\u6587\u672c\u63cf\u8ff0\u6765\u5efa\u7acb\u4e00\u4e2a\u4e0d\u5b89\u5168\u7684\u65b9\u5411\u6765\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u95ee\u9898\u7b80\u5316\u4e3a\u6743\u91cd\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5c1d\u8bd5\u4f7f\u7528\u5f3a\u5316\u6c42\u89e3\u5668\u201c\u7b56\u7565\u68af\u5ea6\u201d\uff0c\u8be5\u6c42\u89e3\u5668\u8868\u73b0\u51fa\u6536\u655b\u901f\u5ea6\u8f83\u6162\u7684\u6b21\u4f18\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u514b\u670d\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fed\u4ee3\u5206\u5e03\u5bf9\u9f50\uff08IDA\uff09\u65b9\u6cd5\u3002\u5c3d\u7ba1\u5b83\u5f88\u7b80\u5355\uff0c\u4f46\u6211\u4eec\u8868\u660e IDA \u5728\u89e3\u51b3 TTI \u6269\u6563\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u65b9\u9762\u8868\u73b0\u51fa\u6548\u7387\u548c\u5feb\u901f\u6536\u655b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u88ab\u53d1\u5e03\u3002|[2402.14577v1](http://arxiv.org/pdf/2402.14577v1)|null|\n", "2402.14407": "|**2024-02-22**|**Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning**|\u901a\u8fc7\u79bb\u6563\u6269\u6563\u8fdb\u884c\u5927\u89c4\u6a21\u65e0\u52a8\u4f5c\u89c6\u9891\u9884\u8bad\u7ec3\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u7b56\u7565\u5b66\u4e60|Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li|Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning trained on a limited set of robot data. Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches with superior generalization ability. Our project website is available at https://video-diff.github.io/.|\u5b66\u4e60\u80fd\u591f\u5b8c\u6210\u591a\u9879\u4efb\u52a1\u7684\u901a\u624d\u5b9e\u4f53\u4ee3\u7406\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u8fd9\u4e3b\u8981\u6e90\u4e8e\u52a8\u4f5c\u6807\u8bb0\u673a\u5668\u4eba\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5b58\u5728\u5927\u91cf\u7684\u4eba\u7c7b\u89c6\u9891\uff0c\u6355\u6349\u590d\u6742\u7684\u4efb\u52a1\u4ee5\u53ca\u4e0e\u7269\u7406\u4e16\u754c\u7684\u4ea4\u4e92\u3002\u5229\u7528\u65e0\u52a8\u4f5c\u7684\u4eba\u7c7b\u89c6\u9891\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u8f6c\u79fb\u77e5\u8bc6\u4ee5\u901a\u8fc7\u6709\u9650\u7684\u673a\u5668\u4eba\u6f14\u793a\u4fc3\u8fdb\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u7684\u524d\u666f\u5e7f\u9614\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u7edf\u4e00\u7684\u79bb\u6563\u6269\u6563\u5c06\u4eba\u7c7b\u89c6\u9891\u7684\u751f\u6210\u9884\u8bad\u7ec3\u548c\u5c11\u91cf\u5e26\u6709\u52a8\u4f5c\u6807\u8bb0\u7684\u673a\u5668\u4eba\u89c6\u9891\u7684\u7b56\u7565\u5fae\u8c03\u7ed3\u5408\u8d77\u6765\u3002\u6211\u4eec\u9996\u5148\u5c06\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u89c6\u9891\u538b\u7f29\u6210\u7edf\u4e00\u7684\u89c6\u9891\u4ee4\u724c\u3002\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u6211\u4eec\u91c7\u7528\u5177\u6709\u63a9\u6a21\u548c\u66ff\u6362\u6269\u6563\u7b56\u7565\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u6765\u9884\u6d4b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u672a\u6765\u89c6\u9891\u6807\u8bb0\u3002\u5728\u5fae\u8c03\u9636\u6bb5\uff0c\u6211\u4eec\u5229\u7528\u60f3\u8c61\u4e2d\u7684\u672a\u6765\u89c6\u9891\u6765\u6307\u5bfc\u5728\u6709\u9650\u7684\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u4f4e\u7ea7\u52a8\u4f5c\u5b66\u4e60\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u7528\u4e8e\u89c4\u5212\u7684\u9ad8\u4fdd\u771f\u672a\u6765\u89c6\u9891\uff0c\u5e76\u589e\u5f3a\u4e86\u5fae\u8c03\u7b56\u7565\uff0c\u5177\u6709\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u9879\u76ee\u7f51\u7ad9\u4f4d\u4e8e https://video-diff.github.io/\u3002|[2402.14407v1](http://arxiv.org/pdf/2402.14407v1)|null|\n", "2402.14401": "|**2024-02-22**|**Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment**|\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u89c9\u8865\u507f\u5f15\u5bfc\u548c\u89c6\u89c9\u5dee\u5f02\u5206\u6790\u7528\u4e8e\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30|Zhaoyang Wang, Bo Hu, Mingyang Zhang, Jie Li, Leida Li, Maoguo Gong, Xinbo Gao|Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA) methods still suffer from finding a balance between learning feature information at the pixel level of the image and capturing high-level feature information and the efficient utilization of the obtained high-level feature information remains a challenge. As a novel class of state-of-the-art (SOTA) generative model, the diffusion model exhibits the capability to model intricate relationships, enabling a comprehensive understanding of images and possessing a better learning of both high-level and low-level visual features. In view of these, we pioneer the exploration of the diffusion model into the domain of NR-IQA. Firstly, we devise a new diffusion restoration network that leverages the produced enhanced image and noise-containing images, incorporating nonlinear features obtained during the denoising process of the diffusion model, as high-level visual information. Secondly, two visual evaluation branches are designed to comprehensively analyze the obtained high-level feature information. These include the visual compensation guidance branch, grounded in the transformer architecture and noise embedding strategy, and the visual difference analysis branch, built on the ResNet architecture and the residual transposed attention block. Extensive experiments are conducted on seven public NR-IQA datasets, and the results demonstrate that the proposed model outperforms SOTA methods for NR-IQA.|\u73b0\u6709\u7684\u81ea\u7531\u80fd\u5f15\u5bfc\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08NR-IQA\uff09\u65b9\u6cd5\u4ecd\u7136\u9762\u4e34\u7740\u5728\u5b66\u4e60\u56fe\u50cf\u50cf\u7d20\u7ea7\u7279\u5f81\u4fe1\u606f\u548c\u6355\u83b7\u9ad8\u7ea7\u7279\u5f81\u4fe1\u606f\u4ee5\u53ca\u6709\u6548\u5229\u7528\u6240\u83b7\u5f97\u7684\u9ad8\u7ea7\u7279\u5f81\u4fe1\u606f\u4e4b\u95f4\u5bfb\u627e\u5e73\u8861\u7684\u95ee\u9898\u3002\u7ea7\u522b\u7279\u5f81\u4fe1\u606f\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4f5c\u4e3a\u4e00\u7c7b\u65b0\u9896\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u751f\u6210\u6a21\u578b\uff0c\u6269\u6563\u6a21\u578b\u5c55\u73b0\u4e86\u5bf9\u590d\u6742\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u7684\u80fd\u529b\uff0c\u80fd\u591f\u5168\u9762\u7406\u89e3\u56fe\u50cf\u5e76\u66f4\u597d\u5730\u5b66\u4e60\u9ad8\u7ea7\u548c\u4f4e\u7ea7\u89c6\u89c9\u7279\u5f81\u3002\u9274\u4e8e\u8fd9\u4e9b\uff0c\u6211\u4eec\u7387\u5148\u5c06\u6269\u6563\u6a21\u578b\u63a2\u7d22\u5230 NR-IQA \u9886\u57df\u3002\u9996\u5148\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6062\u590d\u7f51\u7edc\uff0c\u5229\u7528\u751f\u6210\u7684\u589e\u5f3a\u56fe\u50cf\u548c\u542b\u566a\u58f0\u56fe\u50cf\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u53bb\u566a\u8fc7\u7a0b\u4e2d\u83b7\u5f97\u7684\u975e\u7ebf\u6027\u7279\u5f81\u4f5c\u4e3a\u9ad8\u7ea7\u89c6\u89c9\u4fe1\u606f\u3002\u5176\u6b21\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u89c6\u89c9\u8bc4\u4f30\u5206\u652f\u6765\u7efc\u5408\u5206\u6790\u83b7\u5f97\u7684\u9ad8\u7ea7\u7279\u5f81\u4fe1\u606f\u3002\u5176\u4e2d\u5305\u62ec\u57fa\u4e8e Transformer \u67b6\u6784\u548c\u566a\u58f0\u5d4c\u5165\u7b56\u7565\u7684\u89c6\u89c9\u8865\u507f\u5f15\u5bfc\u5206\u652f\uff0c\u4ee5\u53ca\u57fa\u4e8e ResNet \u67b6\u6784\u548c\u6b8b\u5dee\u8f6c\u7f6e\u6ce8\u610f\u5757\u6784\u5efa\u7684\u89c6\u89c9\u5dee\u5f02\u5206\u6790\u5206\u652f\u3002\u5728\u4e03\u4e2a\u516c\u5171 NR-IQA \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4f18\u4e8e NR-IQA \u7684 SOTA \u65b9\u6cd5\u3002|[2402.14401v1](http://arxiv.org/pdf/2402.14401v1)|null|\n", "2402.14398": "|**2024-02-22**|**Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing**|\u6e10\u8fdb\u6b8b\u5dee\u5bf9\u9f50\uff1aGAN \u53cd\u6f14\u548c\u56fe\u50cf\u5c5e\u6027\u7f16\u8f91\u7684\u53cc\u6d41\u6846\u67b6|Hao Li, Mengqi Huang, Lei Zhang, Bo Hu, Yi Liu, Zhendong Mao|GAN-based image attribute editing firstly leverages GAN Inversion to project real images into the latent space of GAN and then manipulates corresponding latent codes. Recent inversion methods mainly utilize additional high-bit features to improve image details preservation, as low-bit codes cannot faithfully reconstruct source images, leading to the loss of details. However, during editing, existing works fail to accurately complement the lost details and suffer from poor editability. The main reason is they inject all the lost details indiscriminately at one time, which inherently induces the position and quantity of details to overfit source images, resulting in inconsistent content and artifacts in edited images. This work argues that details should be gradually injected into both the reconstruction and editing process in a multi-stage coarse-to-fine manner for better detail preservation and high editability. Therefore, a novel dual-stream framework is proposed to accurately complement details at each stage. The Reconstruction Stream is employed to embed coarse-to-fine lost details into residual features and then adaptively add them to the GAN generator. In the Editing Stream, residual features are accurately aligned by our Selective Attention mechanism and then injected into the editing process in a multi-stage manner. Extensive experiments have shown the superiority of our framework in both reconstruction accuracy and editing quality compared with existing methods.|\u57fa\u4e8eGAN\u7684\u56fe\u50cf\u5c5e\u6027\u7f16\u8f91\u9996\u5148\u5229\u7528GAN\u53cd\u8f6c\u5c06\u771f\u5b9e\u56fe\u50cf\u6295\u5f71\u5230GAN\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u7136\u540e\u64cd\u7eb5\u76f8\u5e94\u7684\u6f5c\u5728\u4ee3\u7801\u3002\u6700\u8fd1\u7684\u53cd\u6f14\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u989d\u5916\u7684\u9ad8\u4f4d\u7279\u5f81\u6765\u6539\u5584\u56fe\u50cf\u7ec6\u8282\u4fdd\u7559\uff0c\u56e0\u4e3a\u4f4e\u4f4d\u4ee3\u7801\u65e0\u6cd5\u5fe0\u5b9e\u5730\u91cd\u5efa\u6e90\u56fe\u50cf\uff0c\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\u3002\u7136\u800c\uff0c\u73b0\u6709\u4f5c\u54c1\u5728\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u65e0\u6cd5\u51c6\u786e\u8865\u5145\u4e22\u5931\u7684\u7ec6\u8282\uff0c\u53ef\u7f16\u8f91\u6027\u8f83\u5dee\u3002\u4e3b\u8981\u539f\u56e0\u662f\u5b83\u4eec\u4e00\u6b21\u4e0d\u52a0\u533a\u522b\u5730\u6ce8\u5165\u6240\u6709\u4e22\u5931\u7684\u7ec6\u8282\uff0c\u8fd9\u672c\u8d28\u4e0a\u4f1a\u5bfc\u81f4\u7ec6\u8282\u7684\u4f4d\u7f6e\u548c\u6570\u91cf\u4e0e\u6e90\u56fe\u50cf\u8fc7\u5ea6\u62df\u5408\uff0c\u4ece\u800c\u5bfc\u81f4\u7f16\u8f91\u540e\u7684\u56fe\u50cf\u5185\u5bb9\u4e0d\u4e00\u81f4\u548c\u4f2a\u5f71\u3002\u8fd9\u9879\u5de5\u4f5c\u8ba4\u4e3a\uff0c\u7ec6\u8282\u5e94\u8be5\u4ee5\u591a\u9636\u6bb5\u7531\u7c97\u5230\u7ec6\u7684\u65b9\u5f0f\u9010\u6e10\u6ce8\u5165\u91cd\u5efa\u548c\u7f16\u8f91\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u7ec6\u8282\u4fdd\u7559\u548c\u9ad8\u53ef\u7f16\u8f91\u6027\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u6d41\u6846\u67b6\u6765\u51c6\u786e\u8865\u5145\u6bcf\u4e2a\u9636\u6bb5\u7684\u7ec6\u8282\u3002\u91cd\u5efa\u6d41\u7528\u4e8e\u5c06\u7c97\u5230\u7ec6\u7684\u4e22\u5931\u7ec6\u8282\u5d4c\u5165\u5230\u6b8b\u5dee\u7279\u5f81\u4e2d\uff0c\u7136\u540e\u81ea\u9002\u5e94\u5730\u5c06\u5b83\u4eec\u6dfb\u52a0\u5230 GAN \u751f\u6210\u5668\u4e2d\u3002\u5728\u7f16\u8f91\u6d41\u4e2d\uff0c\u6b8b\u4f59\u7279\u5f81\u901a\u8fc7\u6211\u4eec\u7684\u9009\u62e9\u6027\u6ce8\u610f\u673a\u5236\u7cbe\u786e\u5bf9\u9f50\uff0c\u7136\u540e\u4ee5\u591a\u9636\u6bb5\u7684\u65b9\u5f0f\u6ce8\u5165\u5230\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u7f16\u8f91\u8d28\u91cf\u65b9\u9762\u5747\u5177\u6709\u4f18\u8d8a\u6027\u3002|[2402.14398v1](http://arxiv.org/pdf/2402.14398v1)|null|\n", "2402.14349": "|**2024-02-22**|**Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation**|\u5fc3\u5916\u819c\u8102\u80aa\u7ec4\u7ec7\u5206\u5272\u7684\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u548c\u5bf9\u6297\u6027\u6821\u51c6\u5b66\u4e60|Kai Zhao, Zhiming Liu, Jiaqi Liu, Jingbiao Zhou, Bihong Liao, Huifang Tang, Qiuyu Wang, Chunquan Li|Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete large amounts of adipokines to affect the myocardium and coronary arteries. EAT volume and density can be used as independent risk markers measurement of volume by noninvasive magnetic resonance images is the best method of assessing EAT. However, segmenting EAT is challenging due to the low contrast between EAT and pericardial effusion and the presence of motion artifacts. we propose a novel feature latent space multilevel supervision network (SPDNet) with uncertainty-driven and adversarial calibration learning to enhance segmentation for more accurate EAT volume estimation. The network first addresses the blurring of EAT edges due to the medical images in the open medical environments with low quality or out-of-distribution by modeling the uncertainty as a Gaussian distribution in the feature latent space, which using its Bayesian estimation as a regularization constraint to optimize SwinUNETR. Second, an adversarial training strategy is introduced to calibrate the segmentation feature map and consider the multi-scale feature differences between the uncertainty-guided predictive segmentation and the ground truth segmentation, synthesizing the multi-scale adversarial loss directly improves the ability to discriminate the similarity between organizations. Experiments on both the cardiac public MRI dataset (ACDC) and the real-world clinical cohort EAT dataset show that the proposed network outperforms mainstream models, validating that uncertainty-driven and adversarial calibration learning can be used to provide additional information for modeling multi-scale ambiguities.|\u5fc3\u5916\u819c\u8102\u80aa\u7ec4\u7ec7\uff08EAT\uff09\u662f\u4e00\u79cd\u5185\u810f\u8102\u80aa\uff0c\u53ef\u4ee5\u5206\u6ccc\u5927\u91cf\u8102\u80aa\u56e0\u5b50\u6765\u5f71\u54cd\u5fc3\u808c\u548c\u51a0\u72b6\u52a8\u8109\u3002 EAT\u4f53\u79ef\u548c\u5bc6\u5ea6\u53ef\u4f5c\u4e3a\u72ec\u7acb\u7684\u98ce\u9669\u6807\u5fd7\u7269\uff0c\u901a\u8fc7\u65e0\u521b\u78c1\u5171\u632f\u56fe\u50cf\u6d4b\u91cf\u4f53\u79ef\u662f\u8bc4\u4f30EAT\u7684\u6700\u4f73\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u7531\u4e8e EAT \u548c\u5fc3\u5305\u79ef\u6db2\u4e4b\u95f4\u7684\u5bf9\u6bd4\u5ea6\u8f83\u4f4e\u4ee5\u53ca\u8fd0\u52a8\u4f2a\u5f71\u7684\u5b58\u5728\uff0c\u5206\u5272 EAT \u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7279\u5f81\u6f5c\u5728\u7a7a\u95f4\u591a\u7ea7\u76d1\u7763\u7f51\u7edc\uff08SPDNet\uff09\uff0c\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u548c\u5bf9\u6297\u6027\u6821\u51c6\u5b66\u4e60\uff0c\u4ee5\u589e\u5f3a\u5206\u5272\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684 EAT \u4f53\u79ef\u4f30\u8ba1\u3002\u8be5\u7f51\u7edc\u9996\u5148\u901a\u8fc7\u5c06\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e3a\u7279\u5f81\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u4f30\u8ba1\u4f5c\u4e3a\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u5f00\u653e\u533b\u7597\u73af\u5883\u4e2d\u4f4e\u8d28\u91cf\u6216\u5206\u5e03\u4e0d\u5747\u7684\u533b\u5b66\u56fe\u50cf\u5bfc\u81f4\u7684 EAT \u8fb9\u7f18\u6a21\u7cca\u95ee\u9898\u4f18\u5316 SwinUNETR \u7684\u7ea6\u675f\u3002\u5176\u6b21\uff0c\u5f15\u5165\u5bf9\u6297\u6027\u8bad\u7ec3\u7b56\u7565\u6765\u6821\u51c6\u5206\u5272\u7279\u5f81\u56fe\uff0c\u5e76\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u9884\u6d4b\u5206\u5272\u548c\u5730\u9762\u771f\u503c\u5206\u5272\u4e4b\u95f4\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u5dee\u5f02\uff0c\u7efc\u5408\u591a\u5c3a\u5ea6\u5bf9\u6297\u6027\u635f\u5931\u76f4\u63a5\u63d0\u9ad8\u5224\u522b\u76f8\u4f3c\u6027\u7684\u80fd\u529b\u7ec4\u7ec7\u4e4b\u95f4\u3002\u5728\u5fc3\u810f\u516c\u5171 MRI \u6570\u636e\u96c6\uff08ACDC\uff09\u548c\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u961f\u5217 EAT \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7f51\u7edc\u4f18\u4e8e\u4e3b\u6d41\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u548c\u5bf9\u6297\u6027\u6821\u51c6\u5b66\u4e60\u53ef\u7528\u4e8e\u4e3a\u591a\u5c3a\u5ea6\u5efa\u6a21\u63d0\u4f9b\u9644\u52a0\u4fe1\u606f\u542b\u7cca\u4e4b\u5904\u3002|[2402.14349v1](http://arxiv.org/pdf/2402.14349v1)|null|\n", "2402.14314": "|**2024-02-22**|**Typographic Text Generation with Off-the-Shelf Diffusion Model**|\u4f7f\u7528\u73b0\u6210\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u5370\u5237\u6587\u672c|KhayTze Peong, Seiichi Uchida, Daichi Haraguchi|Recent diffusion-based generative models show promise in their ability to generate text images, but limitations in specifying the styles of the generated texts render them insufficient in the realm of typographic design. This paper proposes a typographic text generation system to add and modify text on typographic designs while specifying font styles, colors, and text effects. The proposed system is a novel combination of two off-the-shelf methods for diffusion models, ControlNet and Blended Latent Diffusion. The former functions to generate text images under the guidance of edge conditions specifying stroke contours. The latter blends latent noise in Latent Diffusion Models (LDM) to add typographic text naturally onto an existing background. We first show that given appropriate text edges, ControlNet can generate texts in specified fonts while incorporating effects described by prompts. We further introduce text edge manipulation as an intuitive and customizable way to produce texts with complex effects such as ``shadows'' and ``reflections''. Finally, with the proposed system, we successfully add and modify texts on a predefined background while preserving its overall coherence.|\u6700\u8fd1\u7684\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u6587\u672c\u56fe\u50cf\u7684\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5e0c\u671b\uff0c\u4f46\u662f\u6307\u5b9a\u751f\u6210\u6587\u672c\u7684\u6837\u5f0f\u7684\u9650\u5236\u4f7f\u5f97\u5b83\u4eec\u5728\u7248\u5f0f\u8bbe\u8ba1\u9886\u57df\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5370\u5237\u6587\u672c\u751f\u6210\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u5370\u5237\u8bbe\u8ba1\u4e0a\u6dfb\u52a0\u548c\u4fee\u6539\u6587\u672c\uff0c\u540c\u65f6\u6307\u5b9a\u5b57\u4f53\u6837\u5f0f\u3001\u989c\u8272\u548c\u6587\u672c\u6548\u679c\u3002\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u662f\u4e24\u79cd\u73b0\u6210\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff08ControlNet \u548c\u6df7\u5408\u6f5c\u5728\u6269\u6563\uff09\u7684\u65b0\u9896\u7ec4\u5408\u3002\u524d\u8005\u7684\u529f\u80fd\u662f\u5728\u6307\u5b9a\u7b14\u5212\u8f6e\u5ed3\u7684\u8fb9\u7f18\u6761\u4ef6\u7684\u6307\u5bfc\u4e0b\u751f\u6210\u6587\u672c\u56fe\u50cf\u3002\u540e\u8005\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b (LDM) \u4e2d\u7684\u6f5c\u5728\u566a\u58f0\uff0c\u5c06\u5370\u5237\u6587\u672c\u81ea\u7136\u5730\u6dfb\u52a0\u5230\u73b0\u6709\u80cc\u666f\u4e0a\u3002\u6211\u4eec\u9996\u5148\u8bc1\u660e\uff0c\u7ed9\u5b9a\u9002\u5f53\u7684\u6587\u672c\u8fb9\u7f18\uff0cControlNet \u53ef\u4ee5\u751f\u6210\u6307\u5b9a\u5b57\u4f53\u7684\u6587\u672c\uff0c\u540c\u65f6\u7ed3\u5408\u63d0\u793a\u63cf\u8ff0\u7684\u6548\u679c\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u6587\u672c\u8fb9\u7f18\u64cd\u4f5c\u4f5c\u4e3a\u4e00\u79cd\u76f4\u89c2\u4e14\u53ef\u5b9a\u5236\u7684\u65b9\u5f0f\u6765\u751f\u6210\u5177\u6709\u590d\u6742\u6548\u679c\uff08\u4f8b\u5982\u201c\u9634\u5f71\u201d\u548c\u201c\u53cd\u5c04\u201d\uff09\u7684\u6587\u672c\u3002\u6700\u540e\uff0c\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\uff0c\u6211\u4eec\u6210\u529f\u5730\u5728\u9884\u5b9a\u4e49\u80cc\u666f\u4e0a\u6dfb\u52a0\u548c\u4fee\u6539\u6587\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u6574\u4f53\u8fde\u8d2f\u6027\u3002|[2402.14314v1](http://arxiv.org/pdf/2402.14314v1)|null|\n", "2402.14311": "|**2024-02-22**|**Font Style Interpolation with Diffusion Models**|\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5b57\u4f53\u6837\u5f0f\u63d2\u503c|Tetta Kondo, Shumpei Takezaki, Daichi Haraguchi, Seiichi Uchida|Fonts have huge variations in their styles and give readers different impressions. Therefore, generating new fonts is worthy of giving new impressions to readers. In this paper, we employ diffusion models to generate new font styles by interpolating a pair of reference fonts with different styles. More specifically, we propose three different interpolation approaches, image-blending, condition-blending, and noise-blending, with the diffusion models. We perform qualitative and quantitative experimental analyses to understand the style generation ability of the three approaches. According to experimental results, three proposed approaches can generate not only expected font styles but also somewhat serendipitous font styles. We also compare the approaches with a state-of-the-art style-conditional Latin-font generative network model to confirm the validity of using the diffusion models for the style interpolation task.|\u5b57\u4f53\u7684\u98ce\u683c\u5dee\u5f02\u5f88\u5927\uff0c\u7ed9\u8bfb\u8005\u5e26\u6765\u4e0d\u540c\u7684\u5370\u8c61\u3002\u56e0\u6b64\uff0c\u751f\u6210\u65b0\u7684\u5b57\u4f53\u503c\u5f97\u7ed9\u8bfb\u8005\u5e26\u6765\u65b0\u7684\u5370\u8c61\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u6269\u6563\u6a21\u578b\u901a\u8fc7\u63d2\u5165\u4e00\u5bf9\u5177\u6709\u4e0d\u540c\u6837\u5f0f\u7684\u53c2\u8003\u5b57\u4f53\u6765\u751f\u6210\u65b0\u7684\u5b57\u4f53\u6837\u5f0f\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u63d2\u503c\u65b9\u6cd5\uff0c\u5373\u56fe\u50cf\u6df7\u5408\u3001\u6761\u4ef6\u6df7\u5408\u548c\u566a\u58f0\u6df7\u5408\u4ee5\u53ca\u6269\u6563\u6a21\u578b\u3002\u6211\u4eec\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u5206\u6790\uff0c\u4ee5\u4e86\u89e3\u4e09\u79cd\u65b9\u6cd5\u7684\u98ce\u683c\u751f\u6210\u80fd\u529b\u3002\u6839\u636e\u5b9e\u9a8c\u7ed3\u679c\uff0c\u63d0\u51fa\u7684\u4e09\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u4ee5\u751f\u6210\u9884\u671f\u7684\u5b57\u4f53\u6837\u5f0f\uff0c\u8fd8\u53ef\u4ee5\u751f\u6210\u4e00\u4e9b\u5076\u7136\u7684\u5b57\u4f53\u6837\u5f0f\u3002\u6211\u4eec\u8fd8\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u4e0e\u6700\u5148\u8fdb\u7684\u98ce\u683c\u6761\u4ef6\u62c9\u4e01\u5b57\u4f53\u751f\u6210\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u786e\u8ba4\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u98ce\u683c\u63d2\u503c\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002|[2402.14311v1](http://arxiv.org/pdf/2402.14311v1)|null|\n", "2402.14300": "|**2024-02-22**|**A Simple Framework Uniting Visual In-context Learning with Masked Image Modeling to Improve Ultrasound Segmentation**|\u5c06\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0e\u63a9\u6a21\u56fe\u50cf\u5efa\u6a21\u76f8\u7ed3\u5408\u4ee5\u6539\u8fdb\u8d85\u58f0\u5206\u5272\u7684\u7b80\u5355\u6846\u67b6|Yuyue Zhou, Banafshe Felfeliyan, Shrimanti Ghosh, Jessica Knight, Fatima Alves-Pereira, Christopher Keen, Jessica K\u00fcpper, Abhilash Rakkunedeth Hareendranathan, Jacob L. Jaremko|Conventional deep learning models deal with images one-by-one, requiring costly and time-consuming expert labeling in the field of medical imaging, and domain-specific restriction limits model generalizability. Visual in-context learning (ICL) is a new and exciting area of research in computer vision. Unlike conventional deep learning, ICL emphasizes the model's ability to adapt to new tasks based on given examples quickly. Inspired by MAE-VQGAN, we proposed a new simple visual ICL method called SimICL, combining visual ICL pairing images with masked image modeling (MIM) designed for self-supervised learning. We validated our method on bony structures segmentation in a wrist ultrasound (US) dataset with limited annotations, where the clinical objective was to segment bony structures to help with further fracture detection. We used a test set containing 3822 images from 18 patients for bony region segmentation. SimICL achieved an remarkably high Dice coeffient (DC) of 0.96 and Jaccard Index (IoU) of 0.92, surpassing state-of-the-art segmentation and visual ICL models (a maximum DC 0.86 and IoU 0.76), with SimICL DC and IoU increasing up to 0.10 and 0.16. This remarkably high agreement with limited manual annotations indicates SimICL could be used for training AI models even on small US datasets. This could dramatically decrease the human expert time required for image labeling compared to conventional approaches, and enhance the real-world use of AI assistance in US image analysis.|\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u56fe\u50cf\u8fdb\u884c\u4e00\u5f20\u4e00\u5f20\u7684\u5904\u7406\uff0c\u5728\u533b\u5b66\u6210\u50cf\u9886\u57df\u9700\u8981\u6602\u8d35\u4e14\u8017\u65f6\u7684\u4e13\u5bb6\u6807\u8bb0\uff0c\u5e76\u4e14\u7279\u5b9a\u9886\u57df\u7684\u9650\u5236\u9650\u5236\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u3002\u89c6\u89c9\u60c5\u5883\u5b66\u4e60\uff08ICL\uff09\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e00\u4e2a\u4ee4\u4eba\u5174\u594b\u7684\u65b0\u7814\u7a76\u9886\u57df\u3002\u4e0e\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u4e0d\u540c\uff0cICL \u5f3a\u8c03\u6a21\u578b\u6839\u636e\u7ed9\u5b9a\u793a\u4f8b\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u80fd\u529b\u3002\u53d7 MAE-VQGAN \u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SimICL \u7684\u65b0\u7684\u7b80\u5355\u89c6\u89c9 ICL \u65b9\u6cd5\uff0c\u5c06\u89c6\u89c9 ICL \u914d\u5bf9\u56fe\u50cf\u4e0e\u4e13\u4e3a\u81ea\u76d1\u7763\u5b66\u4e60\u800c\u8bbe\u8ba1\u7684\u63a9\u6a21\u56fe\u50cf\u5efa\u6a21 (MIM) \u76f8\u7ed3\u5408\u3002\u6211\u4eec\u5728\u5e26\u6709\u6709\u9650\u6ce8\u91ca\u7684\u8155\u90e8\u8d85\u58f0\uff08\u7f8e\u56fd\uff09\u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u9aa8\u7ed3\u6784\u5206\u5272\u65b9\u6cd5\uff0c\u5176\u4e2d\u4e34\u5e8a\u76ee\u6807\u662f\u5206\u5272\u9aa8\u7ed3\u6784\u4ee5\u5e2e\u52a9\u8fdb\u4e00\u6b65\u7684\u9aa8\u6298\u68c0\u6d4b\u3002\u6211\u4eec\u4f7f\u7528\u5305\u542b\u6765\u81ea 18 \u540d\u60a3\u8005\u7684 3822 \u4e2a\u56fe\u50cf\u7684\u6d4b\u8bd5\u96c6\u8fdb\u884c\u9aa8\u533a\u57df\u5206\u5272\u3002 SimICL \u5b9e\u73b0\u4e86\u975e\u5e38\u9ad8\u7684 Dice \u7cfb\u6570 (DC) 0.96 \u548c Jaccard \u6307\u6570 (IoU) 0.92\uff0c\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5206\u5272\u548c\u89c6\u89c9 ICL \u6a21\u578b\uff08\u6700\u5927 DC 0.86 \u548c IoU 0.76\uff09\uff0c\u5e76\u4e14 SimICL DC \u548c IoU \u4e0d\u65ad\u589e\u52a0\u9ad8\u8fbe 0.10 \u548c 0.16\u3002\u8fd9\u79cd\u4e0e\u6709\u9650\u7684\u624b\u52a8\u6ce8\u91ca\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\u8868\u660e SimICL \u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3 AI \u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4ee5\u5728\u7f8e\u56fd\u7684\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8fd9\u53ef\u4ee5\u5927\u5927\u51cf\u5c11\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u56fe\u50cf\u6807\u8bb0\u6240\u9700\u7684\u65f6\u95f4\uff0c\u5e76\u589e\u5f3a\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u5728\u7f8e\u56fd\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5b9e\u9645\u4f7f\u7528\u3002|[2402.14300v1](http://arxiv.org/pdf/2402.14300v1)|null|\n", "2402.14253": "|**2024-02-22**|**MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion**|MVD$^2$\uff1a\u7528\u4e8e\u591a\u89c6\u56fe\u6269\u6563\u7684\u9ad8\u6548\u591a\u89c6\u56fe 3D \u91cd\u5efa|Xin-Yang Zheng, Hao Pan, Yu-Xiao Guo, Xin Tong, Yang Liu|As a promising 3D generation technique, multiview diffusion (MVD) has received a lot of attention due to its advantages in terms of generalizability, quality, and efficiency. By finetuning pretrained large image diffusion models with 3D data, the MVD methods first generate multiple views of a 3D object based on an image or text prompt and then reconstruct 3D shapes with multiview 3D reconstruction. However, the sparse views and inconsistent details in the generated images make 3D reconstruction challenging. We present MVD$^2$, an efficient 3D reconstruction method for multiview diffusion (MVD) images. MVD$^2$ aggregates image features into a 3D feature volume by projection and convolution and then decodes volumetric features into a 3D mesh. We train MVD$^2$ with 3D shape collections and MVD images prompted by rendered views of 3D shapes. To address the discrepancy between the generated multiview images and ground-truth views of the 3D shapes, we design a simple-yet-efficient view-dependent training scheme. MVD$^2$ improves the 3D generation quality of MVD and is fast and robust to various MVD methods. After training, it can efficiently decode 3D meshes from multiview images within one second. We train MVD$^2$ with Zero-123++ and ObjectVerse-LVIS 3D dataset and demonstrate its superior performance in generating 3D models from multiview images generated by different MVD methods, using both synthetic and real images as prompts.|\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684 3D \u751f\u6210\u6280\u672f\uff0c\u591a\u89c6\u56fe\u6269\u6563\uff08MVD\uff09\u7531\u4e8e\u5176\u5728\u901a\u7528\u6027\u3001\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\u800c\u53d7\u5230\u4e86\u5e7f\u6cdb\u7684\u5173\u6ce8\u3002\u901a\u8fc7\u4f7f\u7528 3D \u6570\u636e\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u5927\u578b\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0cMVD \u65b9\u6cd5\u9996\u5148\u6839\u636e\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u751f\u6210 3D \u5bf9\u8c61\u7684\u591a\u4e2a\u89c6\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u591a\u89c6\u56fe 3D \u91cd\u5efa\u6765\u91cd\u5efa 3D \u5f62\u72b6\u3002\u7136\u800c\uff0c\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u7a00\u758f\u89c6\u56fe\u548c\u4e0d\u4e00\u81f4\u7684\u7ec6\u8282\u4f7f\u5f97 3D \u91cd\u5efa\u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86 MVD$^2$\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u89c6\u56fe\u6269\u6563 (MVD) \u56fe\u50cf\u7684\u9ad8\u6548 3D \u91cd\u5efa\u65b9\u6cd5\u3002 MVD$^2$\u901a\u8fc7\u6295\u5f71\u548c\u5377\u79ef\u5c06\u56fe\u50cf\u7279\u5f81\u805a\u5408\u62103D\u7279\u5f81\u4f53\u79ef\uff0c\u7136\u540e\u5c06\u4f53\u79ef\u7279\u5f81\u89e3\u7801\u62103D\u7f51\u683c\u3002\u6211\u4eec\u4f7f\u7528 3D \u5f62\u72b6\u96c6\u5408\u548c 3D \u5f62\u72b6\u6e32\u67d3\u89c6\u56fe\u63d0\u793a\u7684 MVD \u56fe\u50cf\u6765\u8bad\u7ec3 MVD$^2$\u3002\u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u4e0e 3D \u5f62\u72b6\u7684\u771f\u5b9e\u89c6\u56fe\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u89c6\u56fe\u76f8\u5173\u8bad\u7ec3\u65b9\u6848\u3002 MVD$^2$ \u63d0\u9ad8\u4e86 MVD \u7684 3D \u751f\u6210\u8d28\u91cf\uff0c\u5e76\u4e14\u5bf9\u5404\u79cd MVD \u65b9\u6cd5\u5feb\u901f\u4e14\u9c81\u68d2\u3002\u8bad\u7ec3\u540e\uff0c\u5b83\u53ef\u4ee5\u5728\u4e00\u79d2\u5185\u6709\u6548\u5730\u4ece\u591a\u89c6\u56fe\u56fe\u50cf\u4e2d\u89e3\u7801 3D \u7f51\u683c\u3002\u6211\u4eec\u4f7f\u7528 Zero-123++ \u548c ObjectVerse-LVIS 3D \u6570\u636e\u96c6\u8bad\u7ec3 MVD$^2$\uff0c\u5e76\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u4f5c\u4e3a\u63d0\u793a\uff0c\u5c55\u793a\u4e86\u5176\u4ece\u4e0d\u540c MVD \u65b9\u6cd5\u751f\u6210\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u751f\u6210 3D \u6a21\u578b\u7684\u5353\u8d8a\u6027\u80fd\u3002|[2402.14253v1](http://arxiv.org/pdf/2402.14253v1)|null|\n"}, "\u591a\u6a21\u6001": {"2402.14818": "|**2024-02-22**|**PALO: A Polyglot Large Multimodal Model for 5B People**|PALO\uff1a\u9762\u5411 5B \u4eba\u7fa4\u7684\u591a\u8bed\u8a00\u5927\u578b\u591a\u6a21\u5f0f\u6a21\u578b|Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao M. Anwer, Tim Baldwin, Michael Felsberg, Fahad S. Khan|In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \\textsc{Palo}. \\textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\\sim$5B people (65\\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the generalization and scalability where we observe substantial improvements compared to strong baselines. We also propose the first multilingual multimodal benchmark for the forthcoming approaches to evaluate their vision-language reasoning capabilities across languages. Code: https://github.com/mbzuai-oryx/PALO.|\u4e3a\u4e86\u8ffd\u6c42\u66f4\u5177\u5305\u5bb9\u6027\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a \\textsc{Palo} \u7684\u5927\u578b\u591a\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\u3002 \\textsc{Palo} \u63d0\u4f9b 10 \u79cd\u4e3b\u200b\u200b\u8981\u8bed\u8a00\u7684\u89c6\u89c9\u63a8\u7406\u529f\u80fd\uff0c\u5305\u62ec\u82f1\u8bed\u3001\u4e2d\u6587\u3001\u5370\u5730\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u6cd5\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u3001\u5b5f\u52a0\u62c9\u8bed\u3001\u4fc4\u8bed\u3001\u4e4c\u5c14\u90fd\u8bed\u548c\u65e5\u8bed\uff0c\u8986\u76d6\u603b\u5171 $\\sim$5B \u4eba\uff0865\\\u5360\u4e16\u754c\u4eba\u53e3\u7684\u767e\u5206\u6bd4\uff09\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d89\u53ca\u4e00\u79cd\u534a\u81ea\u52a8\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u4f7f\u7528\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u591a\u6a21\u5f0f\u6307\u4ee4\u6570\u636e\u96c6\u4ece\u82f1\u8bed\u8c03\u6574\u4e3a\u76ee\u6807\u8bed\u8a00\uff0c\u4ece\u800c\u786e\u4fdd\u9ad8\u8bed\u8a00\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u7531\u4e8e\u6700\u5c11\u7684\u624b\u52a8\u5de5\u4f5c\u800c\u5141\u8bb8\u53ef\u6269\u5c55\u6027\u3002\u4e0d\u540c\u6307\u4ee4\u96c6\u7684\u7ed3\u5408\u6709\u52a9\u4e8e\u6211\u4eec\u63d0\u9ad8\u591a\u79cd\u8bed\u8a00\u7684\u6574\u4f53\u6027\u80fd\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\uff0c\u5982\u5370\u5730\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u3001\u5b5f\u52a0\u62c9\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\u3002\u751f\u6210\u7684\u6a21\u578b\u5728\u4e09\u4e2a\u5c3a\u5ea6\uff081.7B\u30017B \u548c 13B \u53c2\u6570\uff09\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u663e\u793a\u6cdb\u5316\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e0e\u5f3a\u5927\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u4e86\u663e\u7740\u7684\u6539\u8fdb\u3002\u6211\u4eec\u8fd8\u4e3a\u5373\u5c06\u63a8\u51fa\u7684\u65b9\u6cd5\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u591a\u8bed\u8a00\u591a\u6a21\u5f0f\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30\u5176\u8de8\u8bed\u8a00\u7684\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002\u4ee3\u7801\uff1ahttps://github.com/mbzuai-oryx/PALO\u3002|[2402.14818v1](http://arxiv.org/pdf/2402.14818v1)|null|\n", "2402.14804": "|**2024-02-22**|**Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset**|\u4f7f\u7528 MATH-Vision \u6570\u636e\u96c6\u6d4b\u91cf\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406|Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, Hongsheng Li|Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements in LMMs. Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development. The project is available at https://mathvision-cuhk.github.io|\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u7684\u6700\u65b0\u8fdb\u5c55\u5728\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e2d\u7684\u6570\u5b66\u63a8\u7406\u65b9\u9762\u663e\u793a\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u6a21\u578b\u5728 MathVista \u7b49\u73b0\u6709\u57fa\u51c6\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u8868\u73b0\u3002\u7136\u800c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u8fd9\u4e9b\u57fa\u51c6\u6240\u6db5\u76d6\u7684\u95ee\u9898\u591a\u6837\u6027\u548c\u4e3b\u9898\u5e7f\u5ea6\u5b58\u5728\u663e\u7740\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 MATH-Vision (MATH-V) \u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u96c6\u5408\uff0c\u5305\u542b 3,040 \u4e2a\u9ad8\u8d28\u91cf\u6570\u5b66\u95ee\u9898\uff0c\u5176\u89c6\u89c9\u80cc\u666f\u6765\u81ea\u771f\u5b9e\u7684\u6570\u5b66\u7ade\u8d5b\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u8de8\u8d8a 16 \u4e2a\u4e0d\u540c\u7684\u6570\u5b66\u5b66\u79d1\uff0c\u5206\u4e3a 5 \u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u4e3a\u8bc4\u4f30 LMM \u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u5957\u5168\u9762\u4e14\u591a\u6837\u5316\u7684\u6311\u6218\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u5f53\u524d LMM \u4e0e\u4eba\u7c7b\u5728 MATH-V \u4e0a\u7684\u8868\u73b0\u4e4b\u95f4\u5b58\u5728\u663e\u7740\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86 LMM \u8fdb\u4e00\u6b65\u8fdb\u6b65\u7684\u5fc5\u8981\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8be6\u7ec6\u7684\u5206\u7c7b\u53ef\u4ee5\u5bf9 LMM \u8fdb\u884c\u5f7b\u5e95\u7684\u9519\u8bef\u5206\u6790\uff0c\u4e3a\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\u548c\u5f00\u53d1\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002\u8be5\u9879\u76ee\u4f4d\u4e8e https://mathvision-cuhk.github.io|[2402.14804v1](http://arxiv.org/pdf/2402.14804v1)|null|\n", "2402.14767": "|**2024-02-22**|**DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models**|DualFocus\uff1a\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6574\u5408\u5b8f\u89c2\u548c\u5fae\u89c2\u89c6\u89d2|Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, Jiaqi Wang|We present DualFocus, a novel framework for integrating macro and micro perspectives within multi-modal large language models (MLLMs) to enhance vision-language task performance. Current MLLMs typically singularly focus on inputs at a predefined resolution, resulting in deficiencies in detailed questions involving local regions. We introduced a DualFocus mechanism where the model concentrates on the image from a macro perspective, responses to the question, and identifies suitable sub-regions to zoom in for subsequent micro perspective analysis. Via the integration of answers from both macro and micro perspectives, the model is adept at addressing tasks that encompass global, detailed, and combined considerations. To endows the DualFocus mechanism in MLLMs, we curated a tailored dataset derived from the Visual Genome (VG) and adapted it to align with the training regimen of DualFocus. Through comparative studies across different model sizes and benchmarks, we demonstrate DualFocus's superiority in balancing detailed examination with holistic insight, significantly reducing hallucination instances in MLLMs and improving their performance in various vision-language tasks.|\u6211\u4eec\u63d0\u51fa\u4e86 DualFocus\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u5b8f\u89c2\u548c\u5fae\u89c2\u89c6\u89d2\u96c6\u6210\u5230\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLM) \u4e2d\uff0c\u4ee5\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\u3002\u5f53\u524d\u7684 MLLM \u901a\u5e38\u53ea\u5173\u6ce8\u9884\u5b9a\u4e49\u5206\u8fa8\u7387\u7684\u8f93\u5165\uff0c\u5bfc\u81f4\u6d89\u53ca\u5f53\u5730\u533a\u57df\u7684\u8be6\u7ec6\u95ee\u9898\u5b58\u5728\u7f3a\u9677\u3002\u6211\u4eec\u5f15\u5165\u4e86 DualFocus \u673a\u5236\uff0c\u6a21\u578b\u4ece\u5b8f\u89c2\u89d2\u5ea6\u805a\u7126\u56fe\u50cf\uff0c\u56de\u7b54\u95ee\u9898\uff0c\u5e76\u8bc6\u522b\u5408\u9002\u7684\u5b50\u533a\u57df\u8fdb\u884c\u653e\u5927\u4ee5\u8fdb\u884c\u540e\u7eed\u7684\u5fae\u89c2\u900f\u89c6\u5206\u6790\u3002\u901a\u8fc7\u6574\u5408\u5b8f\u89c2\u548c\u5fae\u89c2\u89d2\u5ea6\u7684\u7b54\u6848\uff0c\u8be5\u6a21\u578b\u64c5\u957f\u89e3\u51b3\u5305\u542b\u5168\u5c40\u3001\u8be6\u7ec6\u548c\u7efc\u5408\u8003\u8651\u7684\u4efb\u52a1\u3002\u4e3a\u4e86\u5728 MLLM \u4e2d\u8d4b\u4e88 DualFocus \u673a\u5236\uff0c\u6211\u4eec\u7b56\u5212\u4e86\u4e00\u4e2a\u6e90\u81ea\u89c6\u89c9\u57fa\u56e0\u7ec4 (VG) \u7684\u5b9a\u5236\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4e86\u8c03\u6574\u4ee5\u4e0e DualFocus \u7684\u8bad\u7ec3\u65b9\u6848\u4fdd\u6301\u4e00\u81f4\u3002\u901a\u8fc7\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u548c\u57fa\u51c6\u7684\u6bd4\u8f83\u7814\u7a76\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 DualFocus \u5728\u5e73\u8861\u8be6\u7ec6\u68c0\u67e5\u4e0e\u6574\u4f53\u6d1e\u5bdf\u529b\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u663e\u7740\u51cf\u5c11 MLLM \u4e2d\u7684\u5e7b\u89c9\u5b9e\u4f8b\u5e76\u63d0\u9ad8\u5176\u5728\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002|[2402.14767v1](http://arxiv.org/pdf/2402.14767v1)|null|\n", "2402.14623": "|**2024-02-22**|**RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation**|RoboScript\uff1a\u8de8\u771f\u5b9e\u548c\u6a21\u62df\u7684\u81ea\u7531\u5f62\u5f0f\u64cd\u4f5c\u4efb\u52a1\u7684\u4ee3\u7801\u751f\u6210|Junting Chen, Yao Mu, Qiaojun Yu, Tianming Wei, Silang Wu, Zhecheng Yuan, Zhixuan Liang, Chao Yang, Kaipeng Zhang, Wenqi Shao, et.al.|Rapid progress in high-level task planning and code generation for open-world robot manipulation has been witnessed in Embodied AI. However, previous studies put much effort into general common sense reasoning and task planning capabilities of large-scale language or multi-modal models, relatively little effort on ensuring the deployability of generated code on real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control. To bridge this ``ideal-to-real'' gap, this paper presents \\textbf{RobotScript}, a platform for 1) a deployable robot manipulation pipeline powered by code generation; and 2) a code generation benchmark for robot manipulation tasks in free-form natural language. The RobotScript platform addresses this gap by emphasizing the unified interface with both simulation and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syntax compliance and simulation validation with Gazebo. We demonstrate the adaptability of our code generation framework across multiple robot embodiments, including the Franka and UR5 robot arms, and multiple grippers. Additionally, our benchmark assesses reasoning abilities for physical space and constraints, highlighting the differences between GPT-3.5, GPT-4, and Gemini in handling complex physical interactions. Finally, we present a thorough evaluation on the whole system, exploring how each module in the pipeline: code generation, perception, motion planning, and even object geometric properties, impact the overall performance of the system.|Embodied AI \u89c1\u8bc1\u4e86\u5f00\u653e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u548c\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u5feb\u901f\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u4ee5\u524d\u7684\u7814\u7a76\u5c06\u5927\u91cf\u7cbe\u529b\u653e\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6216\u591a\u6a21\u6001\u6a21\u578b\u7684\u4e00\u822c\u5e38\u8bc6\u63a8\u7406\u548c\u4efb\u52a1\u89c4\u5212\u80fd\u529b\u4e0a\uff0c\u800c\u5728\u786e\u4fdd\u751f\u6210\u7684\u4ee3\u7801\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u53ef\u90e8\u7f72\u6027\u4ee5\u53ca\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5176\u4ed6\u57fa\u672c\u7ec4\u4ef6\u4e0a\u6295\u5165\u7684\u7cbe\u529b\u76f8\u5bf9\u8f83\u5c11\uff0c\u5305\u62ec\u673a\u5668\u4eba\u611f\u77e5\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u79cd\u201c\u7406\u60f3\u4e0e\u73b0\u5b9e\u201d\u7684\u5dee\u8ddd\uff0c\u672c\u6587\u63d0\u51fa\u4e86 \\textbf{RobotScript}\uff0c\u8fd9\u662f\u4e00\u4e2a\u5e73\u53f0\uff0c\u7528\u4e8e 1) \u7531\u4ee3\u7801\u751f\u6210\u9a71\u52a8\u7684\u53ef\u90e8\u7f72\u673a\u5668\u4eba\u64cd\u4f5c\u7ba1\u9053\uff1b 2\uff09\u81ea\u7531\u5f62\u5f0f\u81ea\u7136\u8bed\u8a00\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u3002 RobotScript \u5e73\u53f0\u57fa\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf (ROS) \u7684\u62bd\u8c61\uff0c\u5f3a\u8c03\u4e0e\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u7684\u7edf\u4e00\u63a5\u53e3\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u8fd9\u4e00\u5dee\u8ddd\uff0c\u786e\u4fdd\u8bed\u6cd5\u5408\u89c4\u6027\u548c Gazebo \u7684\u6a21\u62df\u9a8c\u8bc1\u3002\u6211\u4eec\u5c55\u793a\u4e86\u4ee3\u7801\u751f\u6210\u6846\u67b6\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5b9e\u65bd\u4f8b\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u5305\u62ec Franka \u548c UR5 \u673a\u5668\u4eba\u624b\u81c2\u4ee5\u53ca\u591a\u4e2a\u5939\u5177\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u8bc4\u4f30\u4e86\u7269\u7406\u7a7a\u95f4\u548c\u7ea6\u675f\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7a81\u51fa\u4e86 GPT-3.5\u3001GPT-4 \u548c Gemini \u5728\u5904\u7406\u590d\u6742\u7269\u7406\u4ea4\u4e92\u65b9\u9762\u7684\u5dee\u5f02\u3002\u6700\u540e\uff0c\u6211\u4eec\u5bf9\u6574\u4e2a\u7cfb\u7edf\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u63a2\u7d22\u7ba1\u9053\u4e2d\u7684\u6bcf\u4e2a\u6a21\u5757\uff1a\u4ee3\u7801\u751f\u6210\u3001\u611f\u77e5\u3001\u8fd0\u52a8\u89c4\u5212\uff0c\u751a\u81f3\u5bf9\u8c61\u51e0\u4f55\u5c5e\u6027\uff0c\u5982\u4f55\u5f71\u54cd\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3002|[2402.14623v1](http://arxiv.org/pdf/2402.14623v1)|null|\n", "2402.14545": "|**2024-02-22**|**Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective**|\u5c11\u5373\u662f\u591a\uff1a\u4ece EOS \u51b3\u7b56\u89d2\u5ea6\u51cf\u8f7b\u591a\u6a21\u6001\u5e7b\u89c9|Zihao Yue, Liang Zhang, Qin Jin|Large Multimodal Models (LMMs) often suffer from multimodal hallucinations, wherein they may create content that is not present in the visual inputs. In this paper, we explore a new angle of this issue: overly detailed training data hinders the model's ability to timely terminate generation, leading to continued outputs beyond visual perception limits. By investigating how the model decides to terminate generation with EOS, the special end-of-sentence token, we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image. This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs. To take advantage of such potential, we explore two methods to mitigate multimodal hallucinations: a training objective that enables the model to reduce hallucinations by learning from regular instruction data, and a data filtering strategy to prevent harmful training data from exacerbating model hallucinations. Both methods significantly improve the hallucination performance of LMMs, without requiring any additional data or knowledge.|\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u7ecf\u5e38\u906d\u53d7\u591a\u6a21\u6001\u5e7b\u89c9\u7684\u56f0\u6270\uff0c\u5176\u4e2d\u5b83\u4eec\u53ef\u80fd\u4f1a\u521b\u5efa\u89c6\u89c9\u8f93\u5165\u4e2d\u4e0d\u5b58\u5728\u7684\u5185\u5bb9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u8fd9\u4e2a\u95ee\u9898\u7684\u65b0\u89d2\u5ea6\uff1a\u8fc7\u4e8e\u8be6\u7ec6\u7684\u8bad\u7ec3\u6570\u636e\u963b\u788d\u4e86\u6a21\u578b\u53ca\u65f6\u7ec8\u6b62\u751f\u6210\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u6301\u7eed\u8f93\u51fa\u8d85\u51fa\u89c6\u89c9\u611f\u77e5\u9650\u5236\u3002\u901a\u8fc7\u7814\u7a76\u6a21\u578b\u5982\u4f55\u51b3\u5b9a\u7528 EOS\uff08\u7279\u6b8a\u7684\u53e5\u5c3e\u6807\u8bb0\uff09\u7ec8\u6b62\u751f\u6210\uff0c\u6211\u4eec\u53d1\u73b0\u6a21\u578b\u901a\u8fc7\u5c06\u751f\u6210\u7684\u6587\u672c\u4e0e\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83\u6765\u8bc4\u4f30\u6574\u4e2a\u5e8f\u5217\u7684\u5b8c\u6574\u6027\u3002\u8fd9\u4e00\u89c2\u5bdf\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5177\u6709\u6839\u636e\u5176\u89c6\u89c9\u611f\u77e5\u505a\u51fa\u6b63\u786e EOS \u51b3\u7b56\u7684\u5185\u5728\u6f5c\u529b\uff0c\u4ee5\u907f\u514d\u8f93\u51fa\u8fc7\u957f\u3002\u4e3a\u4e86\u5229\u7528\u8fd9\u79cd\u6f5c\u529b\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e24\u79cd\u51cf\u8f7b\u591a\u6a21\u6001\u5e7b\u89c9\u7684\u65b9\u6cd5\uff1a\u8bad\u7ec3\u76ee\u6807\u4f7f\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u4ece\u5e38\u89c4\u6307\u4ee4\u6570\u636e\u4e2d\u5b66\u4e60\u6765\u51cf\u5c11\u5e7b\u89c9\uff0c\u4ee5\u53ca\u6570\u636e\u8fc7\u6ee4\u7b56\u7565\u4ee5\u9632\u6b62\u6709\u5bb3\u7684\u8bad\u7ec3\u6570\u636e\u52a0\u5267\u6a21\u578b\u5e7b\u89c9\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u663e\u7740\u63d0\u9ad8\u4e86 LMM \u7684\u5e7b\u89c9\u6027\u80fd\uff0c\u800c\u4e0d\u9700\u8981\u4efb\u4f55\u989d\u5916\u7684\u6570\u636e\u6216\u77e5\u8bc6\u3002|[2402.14545v1](http://arxiv.org/pdf/2402.14545v1)|null|\n", "2402.14418": "|**2024-02-22**|**Uncertainty-Aware Evaluation for Vision-Language Models**|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30|Vasily Kostumov, Bulat Nutfullin, Oleg Pilipenko, Eugene Ilyushin|Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs.   Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities.   Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.|GPT-4\u3001LLaVA \u548c CogVLM \u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6700\u8fd1\u56e0\u5176\u5728\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u51fa\u8272\u8868\u73b0\u800c\u5e7f\u53d7\u6b22\u8fce\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e00\u4e2a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff1a\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u5bf9\u4e8e VLM \u7684\u7efc\u5408\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u758f\u5ffd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7eb3\u5165\u8bc4\u4f30 VLM \u7684\u57fa\u51c6\u3002\u6211\u4eec\u7684\u5206\u6790\u6db5\u76d6 20 \u591a\u4e2a VLM\uff0c\u91cd\u70b9\u5173\u6ce8\u591a\u9879\u9009\u62e9\u89c6\u89c9\u95ee\u7b54 (VQA) \u4efb\u52a1\u3002\u6211\u4eec\u68c0\u67e5\u4e86 5 \u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\uff0c\u4ee5\u8bc4\u4f30\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u529f\u80fd\u3002\u4f7f\u7528\u5171\u5f62\u9884\u6d4b\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6211\u4eec\u8bc1\u660e\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4e0e\u5176\u51c6\u786e\u6027\u4e0d\u4e00\u81f4\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8868\u660e\u5177\u6709\u6700\u9ad8\u51c6\u786e\u5ea6\u7684\u6a21\u578b\u4e5f\u53ef\u80fd\u5177\u6709\u6700\u9ad8\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u8bc1\u5b9e\u4e86\u6d4b\u91cf VLM \u7684\u4e0d\u786e\u5b9a\u6027\u7684\u91cd\u8981\u6027\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u7ed3\u679c\u8fd8\u63ed\u793a\u4e86\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0e\u5176\u8bed\u8a00\u6a21\u578b\u90e8\u5206\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002|[2402.14418v1](http://arxiv.org/pdf/2402.14418v1)|null|\n"}, "Nerf": {"2402.14792": "|**2024-02-22**|**Consolidating Attention Features for Multi-view Image Editing**|\u6574\u5408\u591a\u89c6\u56fe\u56fe\u50cf\u7f16\u8f91\u7684\u6ce8\u610f\u529b\u7279\u5f81|Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, Fernando De la Torre|Large-scale text-to-image models enable a wide range of image editing techniques, using text prompts or even spatial controls. However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results. In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views. We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in self-attention layers significantly influence the image structure. Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries. To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images. Once trained, QNeRF can render 3D-consistent queries, which are then softly injected back into the self-attention layers during generation, greatly improving multi-view consistency. We refine the process through a progressive, iterative method that better consolidates queries across the diffusion timesteps. We compare our method to a range of existing techniques and demonstrate that it can achieve better multi-view consistency and higher fidelity to the input scene. These advantages allow us to train NeRFs with fewer visual artifacts, that are better aligned with the target geometry.|\u5927\u578b\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u652f\u6301\u591a\u79cd\u56fe\u50cf\u7f16\u8f91\u6280\u672f\uff0c\u4f7f\u7528\u6587\u672c\u63d0\u793a\u751a\u81f3\u7a7a\u95f4\u63a7\u5236\u3002\u7136\u800c\uff0c\u5c06\u8fd9\u4e9b\u7f16\u8f91\u65b9\u6cd5\u5e94\u7528\u4e8e\u63cf\u7ed8\u5355\u4e2a\u573a\u666f\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u4f1a\u5bfc\u81f4 3D \u4e0d\u4e00\u81f4\u7684\u7ed3\u679c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4e13\u6ce8\u4e8e\u57fa\u4e8e\u7a7a\u95f4\u63a7\u5236\u7684\u51e0\u4f55\u64cd\u4f5c\uff0c\u5e76\u4ecb\u7ecd\u4e00\u79cd\u8de8\u5404\u79cd\u89c6\u56fe\u6574\u5408\u7f16\u8f91\u8fc7\u7a0b\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u57fa\u4e8e\u4e24\u4e2a\u89c1\u89e3\uff1a\uff081\uff09\u5728\u6574\u4e2a\u751f\u6210\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e00\u81f4\u7684\u7279\u5f81\u6709\u52a9\u4e8e\u5b9e\u73b0\u591a\u89c6\u56fe\u7f16\u8f91\u7684\u4e00\u81f4\u6027\uff0c\uff082\uff09\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u7684\u67e5\u8be2\u663e\u7740\u5f71\u54cd\u56fe\u50cf\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u5f3a\u5236\u67e5\u8be2\u7684\u4e00\u81f4\u6027\u6765\u63d0\u9ad8\u7f16\u8f91\u56fe\u50cf\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 QNeRF\uff0c\u8fd9\u662f\u4e00\u79cd\u6839\u636e\u7f16\u8f91\u56fe\u50cf\u7684\u5185\u90e8\u67e5\u8be2\u7279\u5f81\u8fdb\u884c\u8bad\u7ec3\u7684\u795e\u7ecf\u8f90\u5c04\u573a\u3002\u7ecf\u8fc7\u8bad\u7ec3\u540e\uff0cQNeRF \u53ef\u4ee5\u6e32\u67d3 3D \u4e00\u81f4\u7684\u67e5\u8be2\uff0c\u7136\u540e\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5c06\u5176\u8f6f\u6ce8\u5165\u56de\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\uff0c\u4ece\u800c\u6781\u5927\u5730\u63d0\u9ad8\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002\u6211\u4eec\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8fed\u4ee3\u65b9\u6cd5\u6539\u8fdb\u6d41\u7a0b\uff0c\u66f4\u597d\u5730\u6574\u5408\u6269\u6563\u65f6\u95f4\u6b65\u957f\u4e2d\u7684\u67e5\u8be2\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u4e00\u7cfb\u5217\u73b0\u6709\u6280\u672f\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u8bc1\u660e\u5b83\u53ef\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u5bf9\u8f93\u5165\u573a\u666f\u7684\u66f4\u9ad8\u4fdd\u771f\u5ea6\u3002\u8fd9\u4e9b\u4f18\u70b9\u4f7f\u6211\u4eec\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u89c6\u89c9\u4f2a\u5f71\u6765\u8bad\u7ec3 NeRF\uff0c\u5e76\u4e14\u4e0e\u76ee\u6807\u51e0\u4f55\u5f62\u72b6\u66f4\u597d\u5730\u5bf9\u9f50\u3002|[2402.14792v1](http://arxiv.org/pdf/2402.14792v1)|null|\n", "2402.14586": "|**2024-02-22**|**FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View Synthesis**|FrameNeRF\uff1a\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u5c0f\u6837\u672c\u65b0\u9896\u89c6\u56fe\u5408\u6210\u6846\u67b6|Yan Xing, Pan Wang, Ligang Liu, Daolun Li, Li Zhang|We present a novel framework, called FrameNeRF, designed to apply off-the-shelf fast high-fidelity NeRF models with fast training speed and high rendering quality for few-shot novel view synthesis tasks. The training stability of fast high-fidelity models is typically constrained to dense views, making them unsuitable for few-shot novel view synthesis tasks. To address this limitation, we utilize a regularization model as a data generator to produce dense views from sparse inputs, facilitating subsequent training of fast high-fidelity models. Since these dense views are pseudo ground truth generated by the regularization model, original sparse images are then used to fine-tune the fast high-fidelity model. This process helps the model learn realistic details and correct artifacts introduced in earlier stages. By leveraging an off-the-shelf regularization model and a fast high-fidelity model, our approach achieves state-of-the-art performance across various benchmark datasets.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a FrameNeRF \u7684\u65b0\u9896\u6846\u67b6\uff0c\u65e8\u5728\u5e94\u7528\u73b0\u6210\u7684\u5feb\u901f\u9ad8\u4fdd\u771f NeRF \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u5feb\u901f\u8bad\u7ec3\u901f\u5ea6\u548c\u9ad8\u6e32\u67d3\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u5c11\u6837\u672c\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u4efb\u52a1\u3002\u5feb\u901f\u9ad8\u4fdd\u771f\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u901a\u5e38\u53d7\u9650\u4e8e\u5bc6\u96c6\u89c6\u56fe\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u4e0d\u9002\u5408\u5c11\u91cf\u7684\u65b0\u9896\u89c6\u56fe\u5408\u6210\u4efb\u52a1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u5229\u7528\u6b63\u5219\u5316\u6a21\u578b\u4f5c\u4e3a\u6570\u636e\u751f\u6210\u5668\uff0c\u4ece\u7a00\u758f\u8f93\u5165\u751f\u6210\u5bc6\u96c6\u89c6\u56fe\uff0c\u4ece\u800c\u4fc3\u8fdb\u5feb\u901f\u9ad8\u4fdd\u771f\u6a21\u578b\u7684\u540e\u7eed\u8bad\u7ec3\u3002\u7531\u4e8e\u8fd9\u4e9b\u5bc6\u96c6\u89c6\u56fe\u662f\u7531\u6b63\u5219\u5316\u6a21\u578b\u751f\u6210\u7684\u4f2a\u5730\u9762\u5b9e\u51b5\uff0c\u56e0\u6b64\u539f\u59cb\u7a00\u758f\u56fe\u50cf\u968f\u540e\u7528\u4e8e\u5fae\u8c03\u5feb\u901f\u9ad8\u4fdd\u771f\u6a21\u578b\u3002\u6b64\u8fc7\u7a0b\u6709\u52a9\u4e8e\u6a21\u578b\u5b66\u4e60\u771f\u5b9e\u7684\u7ec6\u8282\u5e76\u7ea0\u6b63\u65e9\u671f\u9636\u6bb5\u5f15\u5165\u7684\u5de5\u4ef6\u3002\u901a\u8fc7\u5229\u7528\u73b0\u6210\u7684\u6b63\u5219\u5316\u6a21\u578b\u548c\u5feb\u901f\u9ad8\u4fdd\u771f\u6a21\u578b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2402.14586v1](http://arxiv.org/pdf/2402.14586v1)|null|\n", "2402.14464": "|**2024-02-22**|**NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection**|NeRF-Det++\uff1a\u5c06\u8bed\u4e49\u63d0\u793a\u548c\u900f\u89c6\u611f\u77e5\u6df1\u5ea6\u76d1\u7763\u7ed3\u5408\u8d77\u6765\u8fdb\u884c\u5ba4\u5185\u591a\u89c6\u56fe 3D \u68c0\u6d4b|Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang|NeRF-Det has achieved impressive performance in indoor multi-view 3D detection by innovatively utilizing NeRF to enhance representation learning. Despite its notable performance, we uncover three decisive shortcomings in its current design, including semantic ambiguity, inappropriate sampling, and insufficient utilization of depth supervision. To combat the aforementioned problems, we present three corresponding solutions: 1) Semantic Enhancement. We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors. 2) Perspective-aware Sampling. Instead of employing the uniform sampling strategy, we put forward the perspective-aware sampling policy that samples densely near the camera while sparsely in the distance, more effectively collecting the valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to directly regressing the depth values that are difficult to optimize, we divide the depth range of each scene into a fixed number of ordinal bins and reformulate the depth prediction as the combination of the classification of depth bins as well as the regression of the residual depth values, thereby benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets. Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9% in mAP@0.25 and +3.5% in mAP@0.50$. The code will be publicly at https://github.com/mrsempress/NeRF-Detplusplus.|NeRF-Det \u901a\u8fc7\u521b\u65b0\u5730\u5229\u7528 NeRF \u6765\u589e\u5f3a\u8868\u793a\u5b66\u4e60\uff0c\u5728\u5ba4\u5185\u591a\u89c6\u56fe 3D \u68c0\u6d4b\u4e2d\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u5c3d\u7ba1\u5176\u6027\u80fd\u663e\u7740\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u5176\u5f53\u524d\u8bbe\u8ba1\u4e2d\u7684\u4e09\u4e2a\u51b3\u5b9a\u6027\u7f3a\u9677\uff0c\u5305\u62ec\u8bed\u4e49\u6a21\u7cca\u3001\u91c7\u6837\u4e0d\u5f53\u548c\u6df1\u5ea6\u76d1\u7763\u5229\u7528\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u79cd\u76f8\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff1a1\uff09\u8bed\u4e49\u589e\u5f3a\u3002\u6211\u4eec\u5c06\u514d\u8d39\u63d0\u4f9b\u7684 3D \u5206\u5272\u6ce8\u91ca\u6295\u5f71\u5230 2D \u5e73\u9762\u4e0a\uff0c\u5e76\u5229\u7528\u76f8\u5e94\u7684 2D \u8bed\u4e49\u56fe\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u663e\u7740\u589e\u5f3a\u4e86\u591a\u89c6\u56fe\u68c0\u6d4b\u5668\u7684\u8bed\u4e49\u611f\u77e5\u3002 2) \u89c6\u89d2\u611f\u77e5\u91c7\u6837\u3002\u6211\u4eec\u6ca1\u6709\u91c7\u7528\u7edf\u4e00\u91c7\u6837\u7b56\u7565\uff0c\u800c\u662f\u63d0\u51fa\u4e86\u900f\u89c6\u611f\u77e5\u91c7\u6837\u7b56\u7565\uff0c\u5728\u76f8\u673a\u9644\u8fd1\u5bc6\u96c6\u91c7\u6837\uff0c\u5728\u8fdc\u5904\u7a00\u758f\u91c7\u6837\uff0c\u66f4\u6709\u6548\u5730\u6536\u96c6\u6709\u4ef7\u503c\u7684\u51e0\u4f55\u7ebf\u7d22\u3002 3\uff09\u5e8f\u6570\u5269\u4f59\u6df1\u5ea6\u76d1\u7763\u3002\u4e0e\u76f4\u63a5\u56de\u5f52\u96be\u4ee5\u4f18\u5316\u7684\u6df1\u5ea6\u503c\u76f8\u53cd\uff0c\u6211\u4eec\u5c06\u6bcf\u4e2a\u573a\u666f\u7684\u6df1\u5ea6\u8303\u56f4\u5212\u5206\u4e3a\u56fa\u5b9a\u6570\u91cf\u7684\u5e8f\u6570\u7bb1\uff0c\u5e76\u5c06\u6df1\u5ea6\u9884\u6d4b\u91cd\u65b0\u8868\u8ff0\u4e3a\u6df1\u5ea6\u7bb1\u7684\u5206\u7c7b\u4ee5\u53ca\u6df1\u5ea6\u7bb1\u7684\u56de\u5f52\u7684\u7ec4\u5408\u3002\u5269\u4f59\u6df1\u5ea6\u503c\uff0c\u4ece\u800c\u6709\u5229\u4e8e\u6df1\u5ea6\u5b66\u4e60\u8fc7\u7a0b\u3002\u7531\u6b64\u4ea7\u751f\u7684\u7b97\u6cd5 NeRF-Det++ \u5728 ScanNetV2 \u548c ARKITScenes \u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u4e86\u5438\u5f15\u4eba\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728 ScanNetV2 \u4e2d\uff0cNeRF-Det++ \u5728 mAP@0.25 \u4e2d\u6bd4\u7ade\u4e89\u5bf9\u624b NeRF-Det \u7684\u6027\u80fd\u9ad8\u51fa +1.9%\uff0c\u5728 mAP@0.50$ \u4e2d\u6bd4\u7ade\u4e89\u5bf9\u624b NeRF-Det \u9ad8\u51fa +3.5%\u3002\u8be5\u4ee3\u7801\u5c06\u5728 https://github.com/mrsempress/NeRF-Detplusplus \u4e0a\u516c\u5f00\u3002|[2402.14464v1](http://arxiv.org/pdf/2402.14464v1)|null|\n", "2402.14415": "|**2024-02-22**|**TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via Direct Taylor-based Grid Optimization**|TaylorGrid\uff1a\u901a\u8fc7\u76f4\u63a5\u57fa\u4e8e\u6cf0\u52d2\u7684\u7f51\u683c\u4f18\u5316\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684\u9690\u5f0f\u573a\u5b66\u4e60|Renyi Mao, Qingshan Xu, Peng Zheng, Ye Wang, Tieru Wu, Rui Ma|Coordinate-based neural implicit representation or implicit fields have been widely studied for 3D geometry representation or novel view synthesis. Recently, a series of efforts have been devoted to accelerating the speed and improving the quality of the coordinate-based implicit field learning. Instead of learning heavy MLPs to predict the neural implicit values for the query coordinates, neural voxels or grids combined with shallow MLPs have been proposed to achieve high-quality implicit field learning with reduced optimization time. On the other hand, lightweight field representations such as linear grid have been proposed to further improve the learning speed. In this paper, we aim for both fast and high-quality implicit field learning, and propose TaylorGrid, a novel implicit field representation which can be efficiently computed via direct Taylor expansion optimization on 2D or 3D grids. As a general representation, TaylorGrid can be adapted to different implicit fields learning tasks such as SDF learning or NeRF. From extensive quantitative and qualitative comparisons, TaylorGrid achieves a balance between the linear grid and neural voxels, showing its superiority in fast and high-quality implicit field learning.|\u57fa\u4e8e\u5750\u6807\u7684\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u6216\u9690\u5f0f\u573a\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\u7528\u4e8e 3D \u51e0\u4f55\u8868\u793a\u6216\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u3002\u6700\u8fd1\uff0c\u4e00\u7cfb\u5217\u7684\u52aa\u529b\u81f4\u529b\u4e8e\u52a0\u5feb\u57fa\u4e8e\u5750\u6807\u7684\u9690\u5f0f\u573a\u5b66\u4e60\u7684\u901f\u5ea6\u548c\u63d0\u9ad8\u8d28\u91cf\u3002\u795e\u7ecf\u4f53\u7d20\u6216\u7f51\u683c\u4e0e\u6d45 MLP \u76f8\u7ed3\u5408\uff0c\u800c\u4e0d\u662f\u5b66\u4e60\u91cd MLP \u6765\u9884\u6d4b\u67e5\u8be2\u5750\u6807\u7684\u795e\u7ecf\u9690\u5f0f\u503c\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u9690\u5f0f\u573a\u5b66\u4e60\uff0c\u5e76\u51cf\u5c11\u4f18\u5316\u65f6\u95f4\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u7ebf\u6027\u7f51\u683c\u7b49\u8f7b\u91cf\u7ea7\u573a\u8868\u793a\u88ab\u63d0\u51fa\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5b66\u4e60\u901f\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u5feb\u901f\u548c\u9ad8\u8d28\u91cf\u7684\u9690\u5f0f\u573a\u5b66\u4e60\uff0c\u5e76\u63d0\u51fa\u4e86 TaylorGrid\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u9690\u5f0f\u573a\u8868\u793a\uff0c\u53ef\u4ee5\u901a\u8fc7 2D \u6216 3D \u7f51\u683c\u4e0a\u7684\u76f4\u63a5\u6cf0\u52d2\u5c55\u5f00\u4f18\u5316\u6765\u6709\u6548\u8ba1\u7b97\u3002\u4f5c\u4e3a\u901a\u7528\u8868\u793a\uff0cTaylorGrid \u53ef\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u9690\u5f0f\u9886\u57df\u5b66\u4e60\u4efb\u52a1\uff0c\u4f8b\u5982 SDF \u5b66\u4e60\u6216 NeRF\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\uff0cTaylorGrid\u5b9e\u73b0\u4e86\u7ebf\u6027\u7f51\u683c\u548c\u795e\u7ecf\u4f53\u7d20\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u663e\u793a\u4e86\u5176\u5728\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u9690\u5f0f\u573a\u5b66\u4e60\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002|[2402.14415v1](http://arxiv.org/pdf/2402.14415v1)|null|\n", "2402.14196": "|**2024-02-22**|**Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields**|Mip-Grid\uff1a\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u6297\u952f\u9f7f\u7f51\u683c\u8868\u793a|Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park|Despite the remarkable achievements of neural radiance fields (NeRF) in representing 3D scenes and generating novel view images, the aliasing issue, rendering \"jaggies\" or \"blurry\" images at varying camera distances, remains unresolved in most existing approaches. The recently proposed mip-NeRF has addressed this challenge by rendering conical frustums instead of rays. However, it relies on MLP architecture to represent the radiance fields, missing out on the fast training speed offered by the latest grid-based methods. In this work, we present mip-Grid, a novel approach that integrates anti-aliasing techniques into grid-based representations for radiance fields, mitigating the aliasing artifacts while enjoying fast training time. The proposed method generates multi-scale grids by applying simple convolution operations over a shared grid representation and uses the scale-aware coordinate to retrieve features at different scales from the generated multi-scale grids. To test the effectiveness, we integrated the proposed method into the two recent representative grid-based methods, TensoRF and K-Planes. Experimental results demonstrate that mip-Grid greatly improves the rendering performance of both methods and even outperforms mip-NeRF on multi-scale datasets while achieving significantly faster training time. For code and demo videos, please see https://stnamjef.github.io/mipgrid.github.io/.|\u5c3d\u7ba1\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u5728\u8868\u793a 3D \u573a\u666f\u548c\u751f\u6210\u65b0\u9896\u7684\u89c6\u56fe\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u5c31\uff0c\u4f46\u5728\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u5728\u4e0d\u540c\u76f8\u673a\u8ddd\u79bb\u6e32\u67d3\u201c\u952f\u9f7f\u201d\u6216\u201c\u6a21\u7cca\u201d\u56fe\u50cf\u7684\u6df7\u53e0\u95ee\u9898\u4ecd\u7136\u6ca1\u6709\u5f97\u5230\u89e3\u51b3\u3002\u6700\u8fd1\u63d0\u51fa\u7684 mip-NeRF \u901a\u8fc7\u6e32\u67d3\u622a\u5934\u5706\u9525\u4f53\u800c\u4e0d\u662f\u5c04\u7ebf\u89e3\u51b3\u4e86\u8fd9\u4e00\u6311\u6218\u3002\u7136\u800c\uff0c\u5b83\u4f9d\u8d56 MLP \u67b6\u6784\u6765\u8868\u793a\u8f90\u5c04\u573a\uff0c\u9519\u8fc7\u4e86\u6700\u65b0\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\u6240\u63d0\u4f9b\u7684\u5feb\u901f\u8bad\u7ec3\u901f\u5ea6\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 mip-Grid\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5b83\u5c06\u6297\u952f\u9f7f\u6280\u672f\u96c6\u6210\u5230\u57fa\u4e8e\u7f51\u683c\u7684\u8f90\u5c04\u573a\u8868\u793a\u4e2d\uff0c\u5728\u4eab\u53d7\u5feb\u901f\u8bad\u7ec3\u65f6\u95f4\u7684\u540c\u65f6\u51cf\u8f7b\u952f\u9f7f\u4f2a\u5f71\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5728\u5171\u4eab\u7f51\u683c\u8868\u793a\u4e0a\u5e94\u7528\u7b80\u5355\u7684\u5377\u79ef\u8fd0\u7b97\u6765\u751f\u6210\u591a\u5c3a\u5ea6\u7f51\u683c\uff0c\u5e76\u4f7f\u7528\u5c3a\u5ea6\u611f\u77e5\u5750\u6807\u4ece\u751f\u6210\u7684\u591a\u5c3a\u5ea6\u7f51\u683c\u4e2d\u68c0\u7d22\u4e0d\u540c\u5c3a\u5ea6\u7684\u7279\u5f81\u3002\u4e3a\u4e86\u6d4b\u8bd5\u6709\u6548\u6027\uff0c\u6211\u4eec\u5c06\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u96c6\u6210\u5230\u6700\u8fd1\u4e24\u79cd\u4ee3\u8868\u6027\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5 TensoRF \u548c K-Planes \u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cmip-Grid \u6781\u5927\u5730\u63d0\u9ad8\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u6e32\u67d3\u6027\u80fd\uff0c\u751a\u81f3\u5728\u591a\u5c3a\u5ea6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e mip-NeRF\uff0c\u540c\u65f6\u663e\u7740\u7f29\u77ed\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002\u4ee3\u7801\u548c\u6f14\u793a\u89c6\u9891\u8bf7\u53c2\u89c1https://stnamjef.github.io/mipgrid.github.io/\u3002|[2402.14196v1](http://arxiv.org/pdf/2402.14196v1)|null|\n"}, "3DGS": {"2402.14650": "|**2024-02-22**|**GaussianPro: 3D Gaussian Splatting with Progressive Propagation**|GaussianPro\uff1a\u5177\u6709\u6e10\u8fdb\u4f20\u64ad\u7684 3D \u9ad8\u65af\u6cfc\u6e85|Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, Xuejin Chen|The advent of 3D Gaussian Splatting (3DGS) has recently brought about a revolution in the field of neural rendering, facilitating high-quality renderings at real-time speed. However, 3DGS heavily depends on the initialized point cloud produced by Structure-from-Motion (SfM) techniques. When tackling with large-scale scenes that unavoidably contain texture-less surfaces, the SfM techniques always fail to produce enough points in these surfaces and cannot provide good initialization for 3DGS. As a result, 3DGS suffers from difficult optimization and low-quality renderings. In this paper, inspired by classical multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that applies a progressive propagation strategy to guide the densification of the 3D Gaussians. Compared to the simple split and clone strategies used in 3DGS, our method leverages the priors of the existing reconstructed geometries of the scene and patch matching techniques to produce new Gaussians with accurate positions and orientations. Experiments on both large-scale and small-scale scenes validate the effectiveness of our method, where our method significantly surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in terms of PSNR.|3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u51fa\u73b0\u6700\u8fd1\u7ed9\u795e\u7ecf\u6e32\u67d3\u9886\u57df\u5e26\u6765\u4e86\u4e00\u573a\u9769\u547d\uff0c\u4fc3\u8fdb\u4e86\u5b9e\u65f6\u901f\u5ea6\u7684\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002\u7136\u800c\uff0c3DGS \u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u8fd0\u52a8\u7ed3\u6784 (SfM) \u6280\u672f\u751f\u6210\u7684\u521d\u59cb\u5316\u70b9\u4e91\u3002\u5f53\u5904\u7406\u4e0d\u53ef\u907f\u514d\u5730\u5305\u542b\u65e0\u7eb9\u7406\u8868\u9762\u7684\u5927\u578b\u573a\u666f\u65f6\uff0cSfM \u6280\u672f\u603b\u662f\u65e0\u6cd5\u5728\u8fd9\u4e9b\u8868\u9762\u4e2d\u4ea7\u751f\u8db3\u591f\u7684\u70b9\uff0c\u5e76\u4e14\u65e0\u6cd5\u4e3a 3DGS \u63d0\u4f9b\u826f\u597d\u7684\u521d\u59cb\u5316\u3002\u56e0\u6b64\uff0c3DGS \u9762\u4e34\u4f18\u5316\u56f0\u96be\u548c\u6e32\u67d3\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u53d7\u7ecf\u5178\u591a\u89c6\u56fe\u7acb\u4f53 (MVS) \u6280\u672f\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GaussianPro\uff0c\u8fd9\u662f\u4e00\u79cd\u5e94\u7528\u6e10\u8fdb\u4f20\u64ad\u7b56\u7565\u6765\u6307\u5bfc 3D \u9ad8\u65af\u7684\u81f4\u5bc6\u5316\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u4e0e 3DGS \u4e2d\u4f7f\u7528\u7684\u7b80\u5355\u5206\u5272\u548c\u514b\u9686\u7b56\u7565\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u573a\u666f\u7684\u73b0\u6709\u91cd\u5efa\u51e0\u4f55\u5f62\u72b6\u7684\u5148\u9a8c\u548c\u8865\u4e01\u5339\u914d\u6280\u672f\u6765\u751f\u6210\u5177\u6709\u51c6\u786e\u4f4d\u7f6e\u548c\u65b9\u5411\u7684\u65b0\u9ad8\u65af\u3002\u5927\u89c4\u6a21\u548c\u5c0f\u89c4\u6a21\u573a\u666f\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728Waymo\u6570\u636e\u96c6\u4e0a\u663e\u7740\u8d85\u8d8a\u4e863DGS\uff0c\u5728PSNR\u65b9\u9762\u8868\u73b0\u51fa1.15dB\u7684\u6539\u8fdb\u3002|[2402.14650v1](http://arxiv.org/pdf/2402.14650v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.14340": "|**2024-02-22**|**TIE-KD: Teacher-Independent and Explainable Knowledge Distillation for Monocular Depth Estimation**|TIE-KD\uff1a\u7528\u4e8e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u72ec\u7acb\u4e8e\u6559\u5e08\u4e14\u53ef\u89e3\u91ca\u7684\u77e5\u8bc6\u84b8\u998f|Sangwon Choi, Daejune Choi, Duksu Kim|Monocular depth estimation (MDE) is essential for numerous applications yet is impeded by the substantial computational demands of accurate deep learning models. To mitigate this, we introduce a novel Teacher-Independent Explainable Knowledge Distillation (TIE-KD) framework that streamlines the knowledge transfer from complex teacher models to compact student networks, eliminating the need for architectural similarity. The cornerstone of TIE-KD is the Depth Probability Map (DPM), an explainable feature map that interprets the teacher's output, enabling feature-based knowledge distillation solely from the teacher's response. This approach allows for efficient student learning, leveraging the strengths of feature-based distillation. Extensive evaluation of the KITTI dataset indicates that TIE-KD not only outperforms conventional response-based KD methods but also demonstrates consistent efficacy across diverse teacher and student architectures. The robustness and adaptability of TIE-KD underscore its potential for applications requiring efficient and interpretable models, affirming its practicality for real-world deployment.|\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1 (MDE) \u5bf9\u4e8e\u8bb8\u591a\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u5230\u7cbe\u786e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5927\u91cf\u8ba1\u7b97\u9700\u6c42\u7684\u963b\u788d\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u72ec\u7acb\u4e8e\u6559\u5e08\u7684\u53ef\u89e3\u91ca\u77e5\u8bc6\u84b8\u998f\uff08TIE-KD\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7b80\u5316\u4e86\u4ece\u590d\u6742\u7684\u6559\u5e08\u6a21\u578b\u5230\u7d27\u51d1\u7684\u5b66\u751f\u7f51\u7edc\u7684\u77e5\u8bc6\u8f6c\u79fb\uff0c\u6d88\u9664\u4e86\u5bf9\u67b6\u6784\u76f8\u4f3c\u6027\u7684\u9700\u8981\u3002 TIE-KD \u7684\u57fa\u77f3\u662f\u6df1\u5ea6\u6982\u7387\u56fe (DPM)\uff0c\u8fd9\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u56fe\uff0c\u53ef\u4ee5\u89e3\u91ca\u6559\u5e08\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u80fd\u591f\u4ec5\u4ece\u6559\u5e08\u7684\u54cd\u5e94\u4e2d\u63d0\u53d6\u57fa\u4e8e\u7279\u5f81\u7684\u77e5\u8bc6\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5229\u7528\u57fa\u4e8e\u7279\u5f81\u7684\u84b8\u998f\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u5b66\u751f\u7684\u9ad8\u6548\u5b66\u4e60\u3002\u5bf9 KITTI \u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cTIE-KD \u4e0d\u4ec5\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u54cd\u5e94\u7684 KD \u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u4e0d\u540c\u7684\u6559\u5e08\u548c\u5b66\u751f\u67b6\u6784\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u529f\u6548\u3002 TIE-KD \u7684\u7a33\u5065\u6027\u548c\u9002\u5e94\u6027\u51f8\u663e\u4e86\u5176\u5728\u9700\u8981\u9ad8\u6548\u548c\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u5b9e\u7528\u6027\u3002|[2402.14340v1](http://arxiv.org/pdf/2402.14340v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.14812": "|**2024-02-22**|**WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition**|WeakSAM\uff1a\u5206\u5272\u4efb\u4f55\u4e1c\u897f\u6ee1\u8db3\u5f31\u76d1\u7763\u5b9e\u4f8b\u7ea7\u8bc6\u522b|Lianghui Zhu, Junwei Zhou, Yan Liu, Xin Hao, Wenyu Liu, Xinggang Wang|Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with large margins, i.e. average improvements of 7.4% and 8.5%, respectively. The code is available at \\url{https://github.com/hustvl/WeakSAM}.|\u4f7f\u7528\u4e0d\u7cbe\u786e\u76d1\u7763\u7684\u5f31\u76d1\u7763\u89c6\u89c9\u8bc6\u522b\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u5b66\u4e60\u95ee\u9898\u3002\u5b83\u663e\u7740\u964d\u4f4e\u4e86\u4eba\u5de5\u6807\u8bb0\u6210\u672c\uff0c\u5e76\u4e14\u4f20\u7edf\u4e0a\u4f9d\u8d56\u4e8e\u591a\u5b9e\u4f8b\u5b66\u4e60\u548c\u4f2a\u6807\u8bb0\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 WeakSAM\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5373\u5206\u6bb5\u4efb\u610f\u6a21\u578b\uff08SAM\uff09\uff09\u4e2d\u5305\u542b\u7684\u9884\u5148\u5b66\u4e60\u7684\u4e16\u754c\u77e5\u8bc6\u6765\u89e3\u51b3\u5f31\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\uff08WSOD\uff09\u548c\u5206\u5272\u95ee\u9898\u3002 WeakSAM \u901a\u8fc7\u81ea\u9002\u5e94 PGT \u751f\u6210\u548c\u611f\u5174\u8da3\u533a\u57df (RoI) \u4e22\u5f03\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf WSOD \u518d\u8bad\u7ec3\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff0c\u5373\u4f2a\u5730\u9762\u5b9e\u51b5 (PGT) \u4e0d\u5b8c\u6574\u6027\u548c\u5608\u6742\u7684 PGT \u5b9e\u4f8b\u3002\u5b83\u8fd8\u89e3\u51b3\u4e86 SAM \u5728\u81ea\u52a8\u5bf9\u8c61\u68c0\u6d4b\u548c\u5206\u5272\u65f6\u9700\u8981\u63d0\u793a\u548c\u7c7b\u522b\u65e0\u610f\u8bc6\u7684\u95ee\u9898\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cWeakSAM \u5728 WSOD \u548c WSIS \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u7740\u8d85\u8d8a\u4e86\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5927\u5e45\u63d0\u5347\uff0c\u5373\u5e73\u5747\u5206\u522b\u63d0\u9ad8\u4e86 7.4% \u548c 8.5%\u3002\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/hustvl/WeakSAM} \u83b7\u53d6\u3002|[2402.14812v1](http://arxiv.org/pdf/2402.14812v1)|null|\n", "2402.14720": "|**2024-02-22**|**A Transformer Model for Boundary Detection in Continuous Sign Language**|\u8fde\u7eed\u624b\u8bed\u8fb9\u754c\u68c0\u6d4b\u7684 Transformer \u6a21\u578b|Razieh Rastgoo, Kourosh Kiani, Sergio Escalera|Sign Language Recognition (SLR) has garnered significant attention from researchers in recent years, particularly the intricate domain of Continuous Sign Language Recognition (CSLR), which presents heightened complexity compared to Isolated Sign Language Recognition (ISLR). One of the prominent challenges in CSLR pertains to accurately detecting the boundaries of isolated signs within a continuous video stream. Additionally, the reliance on handcrafted features in existing models poses a challenge to achieving optimal accuracy. To surmount these challenges, we propose a novel approach utilizing a Transformer-based model. Unlike traditional models, our approach focuses on enhancing accuracy while eliminating the need for handcrafted features. The Transformer model is employed for both ISLR and CSLR. The training process involves using isolated sign videos, where hand keypoint features extracted from the input video are enriched using the Transformer model. Subsequently, these enriched features are forwarded to the final classification layer. The trained model, coupled with a post-processing method, is then applied to detect isolated sign boundaries within continuous sign videos. The evaluation of our model is conducted on two distinct datasets, including both continuous signs and their corresponding isolated signs, demonstrates promising results.|\u8fd1\u5e74\u6765\uff0c\u624b\u8bed\u8bc6\u522b (SLR) \u5f15\u8d77\u4e86\u7814\u7a76\u4eba\u5458\u7684\u6781\u5927\u5173\u6ce8\uff0c\u7279\u522b\u662f\u8fde\u7eed\u624b\u8bed\u8bc6\u522b (CSLR) \u7684\u590d\u6742\u9886\u57df\uff0c\u4e0e\u5b64\u7acb\u624b\u8bed\u8bc6\u522b (ISLR) \u76f8\u6bd4\uff0c\u5176\u590d\u6742\u6027\u66f4\u9ad8\u3002 CSLR \u7684\u7a81\u51fa\u6311\u6218\u4e4b\u4e00\u6d89\u53ca\u51c6\u786e\u68c0\u6d4b\u8fde\u7eed\u89c6\u9891\u6d41\u4e2d\u5b64\u7acb\u6807\u5fd7\u7684\u8fb9\u754c\u3002\u6b64\u5916\uff0c\u73b0\u6709\u6a21\u578b\u5bf9\u624b\u5de5\u7279\u5f81\u7684\u4f9d\u8d56\u5bf9\u5b9e\u73b0\u6700\u4f73\u7cbe\u5ea6\u63d0\u51fa\u4e86\u6311\u6218\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u3002\u4e0e\u4f20\u7edf\u6a21\u578b\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u540c\u65f6\u6d88\u9664\u5bf9\u624b\u5de5\u5236\u4f5c\u7279\u5f81\u7684\u9700\u6c42\u3002 ISLR \u548c CSLR \u5747\u91c7\u7528 Transformer \u6a21\u578b\u3002\u8bad\u7ec3\u8fc7\u7a0b\u6d89\u53ca\u4f7f\u7528\u5b64\u7acb\u7684\u624b\u52bf\u89c6\u9891\uff0c\u5176\u4e2d\u4f7f\u7528 Transformer \u6a21\u578b\u4e30\u5bcc\u4e86\u4ece\u8f93\u5165\u89c6\u9891\u4e2d\u63d0\u53d6\u7684\u624b\u90e8\u5173\u952e\u70b9\u7279\u5f81\u3002\u968f\u540e\uff0c\u8fd9\u4e9b\u4e30\u5bcc\u7684\u7279\u5f81\u88ab\u8f6c\u53d1\u5230\u6700\u7ec8\u7684\u5206\u7c7b\u5c42\u3002\u7136\u540e\uff0c\u5c06\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u4e0e\u540e\u5904\u7406\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u68c0\u6d4b\u8fde\u7eed\u6807\u5fd7\u89c6\u9891\u4e2d\u7684\u5b64\u7acb\u6807\u5fd7\u8fb9\u754c\u3002\u6211\u4eec\u7684\u6a21\u578b\u7684\u8bc4\u4f30\u662f\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\uff0c\u5305\u62ec\u8fde\u7eed\u7b26\u53f7\u548c\u76f8\u5e94\u7684\u5b64\u7acb\u7b26\u53f7\uff0c\u663e\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002|[2402.14720v1](http://arxiv.org/pdf/2402.14720v1)|null|\n", "2402.14707": "|**2024-02-22**|**Two-stage Cytopathological Image Synthesis for Augmenting Cervical Abnormality Screening**|\u7528\u4e8e\u589e\u5f3a\u5bab\u9888\u5f02\u5e38\u7b5b\u67e5\u7684\u4e24\u9636\u6bb5\u7ec6\u80de\u75c5\u7406\u5b66\u56fe\u50cf\u5408\u6210|Zhenrong Shen, Manman Fei, Xin Wang, Jiangdong Cai, Sheng Wang, Lichi Zhang, Qian Wang|Automatic thin-prep cytologic test (TCT) screening can assist pathologists in finding cervical abnormality towards accurate and efficient cervical cancer diagnosis. Current automatic TCT screening systems mostly involve abnormal cervical cell detection, which generally requires large-scale and diverse training data with high-quality annotations to achieve promising performance. Pathological image synthesis is naturally raised to minimize the efforts in data collection and annotation. However, it is challenging to generate realistic large-size cytopathological images while simultaneously synthesizing visually plausible appearances for small-size abnormal cervical cells. In this paper, we propose a two-stage image synthesis framework to create synthetic data for augmenting cervical abnormality screening. In the first Global Image Generation stage, a Normal Image Generator is designed to generate cytopathological images full of normal cervical cells. In the second Local Cell Editing stage, normal cells are randomly selected from the generated images and then are converted to different types of abnormal cells using the proposed Abnormal Cell Synthesizer. Both Normal Image Generator and Abnormal Cell Synthesizer are built upon the pre-trained Stable Diffusion via parameter-efficient fine-tuning methods for customizing cytopathological image contents and extending spatial layout controllability, respectively. Our experiments demonstrate the synthetic image quality, diversity, and controllability of the proposed synthesis framework, and validate its data augmentation effectiveness in enhancing the performance of abnormal cervical cell detection.|\u81ea\u52a8\u8584\u5c42\u7ec6\u80de\u5b66\u68c0\u6d4b (TCT) \u7b5b\u67e5\u53ef\u4ee5\u5e2e\u52a9\u75c5\u7406\u5b66\u5bb6\u53d1\u73b0\u5bab\u9888\u5f02\u5e38\uff0c\u4ece\u800c\u5b9e\u73b0\u51c6\u786e\u3001\u9ad8\u6548\u7684\u5bab\u9888\u764c\u8bca\u65ad\u3002\u76ee\u524d\u7684\u81ea\u52a8TCT\u7b5b\u67e5\u7cfb\u7edf\u5927\u591a\u6d89\u53ca\u5f02\u5e38\u5bab\u9888\u7ec6\u80de\u68c0\u6d4b\uff0c\u8fd9\u901a\u5e38\u9700\u8981\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u548c\u9ad8\u8d28\u91cf\u7684\u6ce8\u91ca\u624d\u80fd\u83b7\u5f97\u6709\u5e0c\u671b\u7684\u6027\u80fd\u3002\u75c5\u7406\u56fe\u50cf\u5408\u6210\u81ea\u7136\u800c\u7136\u5730\u88ab\u63d0\u51fa\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u6570\u636e\u6536\u96c6\u548c\u6ce8\u91ca\u7684\u5de5\u4f5c\u91cf\u3002\u7136\u800c\uff0c\u751f\u6210\u903c\u771f\u7684\u5927\u5c3a\u5bf8\u7ec6\u80de\u75c5\u7406\u5b66\u56fe\u50cf\uff0c\u540c\u65f6\u5408\u6210\u5c0f\u5c3a\u5bf8\u5f02\u5e38\u5bab\u9888\u7ec6\u80de\u7684\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u5916\u89c2\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u56fe\u50cf\u5408\u6210\u6846\u67b6\u6765\u521b\u5efa\u5408\u6210\u6570\u636e\u4ee5\u589e\u5f3a\u5bab\u9888\u5f02\u5e38\u7b5b\u67e5\u3002\u5728\u7b2c\u4e00\u4e2a\u5168\u5c40\u56fe\u50cf\u751f\u6210\u9636\u6bb5\uff0c\u6b63\u5e38\u56fe\u50cf\u751f\u6210\u5668\u65e8\u5728\u751f\u6210\u5145\u6ee1\u6b63\u5e38\u5bab\u9888\u7ec6\u80de\u7684\u7ec6\u80de\u75c5\u7406\u5b66\u56fe\u50cf\u3002\u5728\u7b2c\u4e8c\u4e2a\u5c40\u90e8\u7ec6\u80de\u7f16\u8f91\u9636\u6bb5\uff0c\u4ece\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u968f\u673a\u9009\u62e9\u6b63\u5e38\u7ec6\u80de\uff0c\u7136\u540e\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u5f02\u5e38\u7ec6\u80de\u5408\u6210\u5668\u5c06\u5176\u8f6c\u6362\u4e3a\u4e0d\u540c\u7c7b\u578b\u7684\u5f02\u5e38\u7ec6\u80de\u3002\u6b63\u5e38\u56fe\u50cf\u751f\u6210\u5668\u548c\u5f02\u5e38\u7ec6\u80de\u5408\u6210\u5668\u5747\u57fa\u4e8e\u9884\u5148\u8bad\u7ec3\u7684\u7a33\u5b9a\u6269\u6563\uff0c\u901a\u8fc7\u53c2\u6570\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u5206\u522b\u5b9a\u5236\u7ec6\u80de\u75c5\u7406\u5b66\u56fe\u50cf\u5185\u5bb9\u548c\u6269\u5c55\u7a7a\u95f4\u5e03\u5c40\u53ef\u63a7\u6027\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u5408\u6210\u6846\u67b6\u7684\u5408\u6210\u56fe\u50cf\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u53ef\u63a7\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6570\u636e\u589e\u5f3a\u5728\u589e\u5f3a\u5f02\u5e38\u5bab\u9888\u7ec6\u80de\u68c0\u6d4b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002|[2402.14707v1](http://arxiv.org/pdf/2402.14707v1)|null|\n", "2402.14695": "|**2024-02-22**|**QIS : Interactive Segmentation via Quasi-Conformal Mappings**|QIS\uff1a\u901a\u8fc7\u51c6\u5171\u5f62\u6620\u5c04\u8fdb\u884c\u4ea4\u4e92\u5f0f\u5206\u5272|Han Zhang, Daoping Zhang, Lok Ming Lui|Image segmentation plays a crucial role in extracting important objects of interest from images, enabling various applications. While existing methods have shown success in segmenting clean images, they often struggle to produce accurate segmentation results when dealing with degraded images, such as those containing noise or occlusions. To address this challenge, interactive segmentation has emerged as a promising approach, allowing users to provide meaningful input to guide the segmentation process. However, an important problem in interactive segmentation lies in determining how to incorporate minimal yet meaningful user guidance into the segmentation model. In this paper, we propose the quasi-conformal interactive segmentation (QIS) model, which incorporates user input in the form of positive and negative clicks. Users mark a few pixels belonging to the object region as positive clicks, indicating that the segmentation model should include a region around these clicks. Conversely, negative clicks are provided on pixels belonging to the background, instructing the model to exclude the region near these clicks from the segmentation mask. Additionally, the segmentation mask is obtained by deforming a template mask with the same topology as the object of interest using an orientation-preserving quasiconformal mapping. This approach helps to avoid topological errors in the segmentation results. We provide a thorough analysis of the proposed model, including theoretical support for the ability of QIS to include or exclude regions of interest or disinterest based on the user's indication. To evaluate the performance of QIS, we conduct experiments on synthesized images, medical images, natural images and noisy natural images. The results demonstrate the efficacy of our proposed method.|\u56fe\u50cf\u5206\u5272\u5728\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u91cd\u8981\u7684\u611f\u5174\u8da3\u5bf9\u8c61\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4ece\u800c\u5b9e\u73b0\u5404\u79cd\u5e94\u7528\u3002\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u5272\u5e72\u51c0\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u5904\u7406\u9000\u5316\u56fe\u50cf\uff08\u4f8b\u5982\u5305\u542b\u566a\u58f0\u6216\u906e\u6321\u7684\u56fe\u50cf\uff09\u65f6\uff0c\u5b83\u4eec\u901a\u5e38\u96be\u4ee5\u4ea7\u751f\u51c6\u786e\u7684\u5206\u5272\u7ed3\u679c\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u4ea4\u4e92\u5f0f\u5206\u5272\u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u5b83\u5141\u8bb8\u7528\u6237\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u8f93\u5165\u6765\u6307\u5bfc\u5206\u5272\u8fc7\u7a0b\u3002\u7136\u800c\uff0c\u4ea4\u4e92\u5f0f\u5206\u5272\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u5728\u4e8e\u786e\u5b9a\u5982\u4f55\u5c06\u6700\u5c11\u4f46\u6709\u610f\u4e49\u7684\u7528\u6237\u6307\u5bfc\u5408\u5e76\u5230\u5206\u5272\u6a21\u578b\u4e2d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u51c6\u5171\u5f62\u4ea4\u4e92\u5f0f\u5206\u5272\uff08QIS\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ee5\u6b63\u9762\u548c\u8d1f\u9762\u70b9\u51fb\u7684\u5f62\u5f0f\u7eb3\u5165\u7528\u6237\u8f93\u5165\u3002\u7528\u6237\u5c06\u5c5e\u4e8e\u5bf9\u8c61\u533a\u57df\u7684\u4e00\u4e9b\u50cf\u7d20\u6807\u8bb0\u4e3a\u6b63\u70b9\u51fb\uff0c\u8868\u660e\u5206\u5272\u6a21\u578b\u5e94\u5305\u62ec\u8fd9\u4e9b\u70b9\u51fb\u5468\u56f4\u7684\u533a\u57df\u3002\u76f8\u53cd\uff0c\u5bf9\u5c5e\u4e8e\u80cc\u666f\u7684\u50cf\u7d20\u63d0\u4f9b\u8d1f\u70b9\u51fb\uff0c\u6307\u793a\u6a21\u578b\u4ece\u5206\u5272\u63a9\u6a21\u4e2d\u6392\u9664\u8fd9\u4e9b\u70b9\u51fb\u9644\u8fd1\u7684\u533a\u57df\u3002\u6b64\u5916\uff0c\u5206\u5272\u63a9\u6a21\u662f\u901a\u8fc7\u4f7f\u7528\u4fdd\u6301\u65b9\u5411\u7684\u51c6\u5171\u5f62\u6620\u5c04\u5bf9\u5177\u6709\u4e0e\u611f\u5174\u8da3\u5bf9\u8c61\u76f8\u540c\u7684\u62d3\u6251\u7684\u6a21\u677f\u63a9\u6a21\u8fdb\u884c\u53d8\u5f62\u6765\u83b7\u5f97\u7684\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u52a9\u4e8e\u907f\u514d\u5206\u5272\u7ed3\u679c\u4e2d\u7684\u62d3\u6251\u9519\u8bef\u3002\u6211\u4eec\u5bf9\u6240\u63d0\u51fa\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u5305\u62ec QIS \u6839\u636e\u7528\u6237\u6307\u793a\u5305\u542b\u6216\u6392\u9664\u611f\u5174\u8da3\u6216\u4e0d\u611f\u5174\u8da3\u533a\u57df\u7684\u80fd\u529b\u7684\u7406\u8bba\u652f\u6301\u3002\u4e3a\u4e86\u8bc4\u4f30 QIS \u7684\u6027\u80fd\uff0c\u6211\u4eec\u5bf9\u5408\u6210\u56fe\u50cf\u3001\u533b\u5b66\u56fe\u50cf\u3001\u81ea\u7136\u56fe\u50cf\u548c\u566a\u58f0\u81ea\u7136\u56fe\u50cf\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2402.14695v1](http://arxiv.org/pdf/2402.14695v1)|null|\n", "2402.14665": "|**2024-02-22**|**Quadruplet Loss For Improving the Robustness to Face Morphing Attacks**|\u56db\u8054\u4f53\u635f\u5931\u63d0\u9ad8\u9762\u5bf9\u53d8\u5f62\u653b\u51fb\u7684\u9c81\u68d2\u6027|Iurii Medvedev, Nuno Gon\u00e7alves|Recent advancements in deep learning have revolutionized technology and security measures, necessitating robust identification methods. Biometric approaches, leveraging personalized characteristics, offer a promising solution. However, Face Recognition Systems are vulnerable to sophisticated attacks, notably face morphing techniques, enabling the creation of fraudulent documents. In this study, we introduce a novel quadruplet loss function for increasing the robustness of face recognition systems against morphing attacks. Our approach involves specific sampling of face image quadruplets, combined with face morphs, for network training. Experimental results demonstrate the efficiency of our strategy in improving the robustness of face recognition networks against morphing attacks.|\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\u5f7b\u5e95\u6539\u53d8\u4e86\u6280\u672f\u548c\u5b89\u5168\u63aa\u65bd\uff0c\u9700\u8981\u5f3a\u5927\u7684\u8bc6\u522b\u65b9\u6cd5\u3002\u5229\u7528\u4e2a\u6027\u5316\u7279\u5f81\u7684\u751f\u7269\u8bc6\u522b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5f88\u5bb9\u6613\u53d7\u5230\u590d\u6742\u7684\u653b\u51fb\uff0c\u7279\u522b\u662f\u4eba\u8138\u53d8\u5f62\u6280\u672f\uff0c\u4ece\u800c\u53ef\u4ee5\u521b\u5efa\u6b3a\u8bc8\u6027\u6587\u6863\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56db\u5143\u7ec4\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u9488\u5bf9\u53d8\u5f62\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d89\u53ca\u5bf9\u9762\u90e8\u56fe\u50cf\u56db\u8054\u4f53\u8fdb\u884c\u7279\u5b9a\u91c7\u6837\uff0c\u5e76\u7ed3\u5408\u9762\u90e8\u53d8\u5f62\u6765\u8fdb\u884c\u7f51\u7edc\u8bad\u7ec3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u7b56\u7565\u5728\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u7f51\u7edc\u62b5\u5fa1\u53d8\u5f62\u653b\u51fb\u7684\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6548\u7387\u3002|[2402.14665v1](http://arxiv.org/pdf/2402.14665v1)|null|\n", "2402.14611": "|**2024-02-22**|**Overcoming Dimensional Collapse in Self-supervised Contrastive Learning for Medical Image Segmentation**|\u514b\u670d\u533b\u5b66\u56fe\u50cf\u5206\u5272\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u7ef4\u5ea6\u5d29\u6e83|Jamshid Hassanpour, Vinkle Srivastav, Didier Mutter, Nicolas Padoy|Self-supervised learning (SSL) approaches have achieved great success when the amount of labeled data is limited. Within SSL, models learn robust feature representations by solving pretext tasks. One such pretext task is contrastive learning, which involves forming pairs of similar and dissimilar input samples, guiding the model to distinguish between them. In this work, we investigate the application of contrastive learning to the domain of medical image analysis. Our findings reveal that MoCo v2, a state-of-the-art contrastive learning method, encounters dimensional collapse when applied to medical images. This is attributed to the high degree of inter-image similarity shared between the medical images. To address this, we propose two key contributions: local feature learning and feature decorrelation. Local feature learning improves the ability of the model to focus on the local regions of the image, while feature decorrelation removes the linear dependence among the features. Our experimental findings demonstrate that our contributions significantly enhance the model's performance in the downstream task of medical segmentation, both in the linear evaluation and full fine-tuning settings. This work illustrates the importance of effectively adapting SSL techniques to the characteristics of medical imaging tasks.|\u5f53\u6807\u8bb0\u6570\u636e\u91cf\u6709\u9650\u65f6\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u65b9\u6cd5\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u5728 SSL \u4e2d\uff0c\u6a21\u578b\u901a\u8fc7\u89e3\u51b3\u501f\u53e3\u4efb\u52a1\u6765\u5b66\u4e60\u5f3a\u5927\u7684\u7279\u5f81\u8868\u793a\u3002\u5176\u4e2d\u4e00\u4e2a\u501f\u53e3\u4efb\u52a1\u662f\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5b83\u6d89\u53ca\u5f62\u6210\u76f8\u4f3c\u548c\u4e0d\u76f8\u4f3c\u7684\u8f93\u5165\u6837\u672c\u5bf9\uff0c\u6307\u5bfc\u6a21\u578b\u533a\u5206\u5b83\u4eec\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5bf9\u6bd4\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u7684\u5e94\u7528\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cMoCo v2\uff08\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff09\u5728\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u65f6\u4f1a\u9047\u5230\u7ef4\u5ea6\u5d29\u6e83\u3002\u8fd9\u5f52\u56e0\u4e8e\u533b\u5b66\u56fe\u50cf\u4e4b\u95f4\u5171\u4eab\u7684\u9ad8\u5ea6\u56fe\u50cf\u95f4\u76f8\u4f3c\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u4e2a\u5173\u952e\u8d21\u732e\uff1a\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u548c\u7279\u5f81\u53bb\u76f8\u5173\u3002\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u63d0\u9ad8\u4e86\u6a21\u578b\u5173\u6ce8\u56fe\u50cf\u5c40\u90e8\u533a\u57df\u7684\u80fd\u529b\uff0c\u800c\u7279\u5f81\u53bb\u76f8\u5173\u5219\u6d88\u9664\u4e86\u7279\u5f81\u4e4b\u95f4\u7684\u7ebf\u6027\u76f8\u5173\u6027\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u8d21\u732e\u663e\u7740\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u533b\u5b66\u5206\u5272\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u8bba\u662f\u5728\u7ebf\u6027\u8bc4\u4f30\u8fd8\u662f\u5b8c\u5168\u5fae\u8c03\u8bbe\u7f6e\u4e2d\u3002\u8fd9\u9879\u5de5\u4f5c\u8bf4\u660e\u4e86\u6709\u6548\u5730\u4f7f SSL \u6280\u672f\u9002\u5e94\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002|[2402.14611v1](http://arxiv.org/pdf/2402.14611v1)|null|\n", "2402.14591": "|**2024-02-22**|**High-Speed Detector For Low-Powered Devices In Aerial Grasping**|\u9002\u7528\u4e8e\u7a7a\u4e2d\u6293\u53d6\u4f4e\u529f\u8017\u8bbe\u5907\u7684\u9ad8\u901f\u63a2\u6d4b\u5668|Ashish Kumar, Laxmidhar Behera|Autonomous aerial harvesting is a highly complex problem because it requires numerous interdisciplinary algorithms to be executed on mini low-powered computing devices. Object detection is one such algorithm that is compute-hungry. In this context, we make the following contributions: (i) Fast Fruit Detector (FFD), a resource-efficient, single-stage, and postprocessing-free object detector based on our novel latent object representation (LOR) module, query assignment, and prediction strategy. FFD achieves 100FPS@FP32 precision on the latest 10W NVIDIA Jetson-NX embedded device while co-existing with other time-critical sub-systems such as control, grasping, SLAM, a major achievement of this work. (ii) a method to generate vast amounts of training data without exhaustive manual labelling of fruit images since they consist of a large number of instances, which increases the labelling cost and time. (iii) an open-source fruit detection dataset having plenty of very small-sized instances that are difficult to detect. Our exhaustive evaluations on our and MinneApple dataset show that FFD, being only a single-scale detector, is more accurate than many representative detectors, e.g. FFD is better than single-scale Faster-RCNN by 10.7AP, multi-scale Faster-RCNN by 2.3AP, and better than latest single-scale YOLO-v8 by 8AP and multi-scale YOLO-v8 by 0.3 while being considerably faster.|\u81ea\u4e3b\u7a7a\u4e2d\u6536\u5272\u662f\u4e00\u4e2a\u975e\u5e38\u590d\u6742\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5728\u5fae\u578b\u4f4e\u529f\u7387\u8ba1\u7b97\u8bbe\u5907\u4e0a\u6267\u884c\u5927\u91cf\u8de8\u5b66\u79d1\u7b97\u6cd5\u3002\u5bf9\u8c61\u68c0\u6d4b\u5c31\u662f\u8fd9\u6837\u4e00\u79cd\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u7684\u7b97\u6cd5\u3002\u5728\u8fd9\u65b9\u9762\uff0c\u6211\u4eec\u505a\u51fa\u4e86\u4ee5\u4e0b\u8d21\u732e\uff1a\uff08i\uff09\u5feb\u901f\u6c34\u679c\u68c0\u6d4b\u5668\uff08FFD\uff09\uff0c\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u3001\u5355\u9636\u6bb5\u4e14\u65e0\u9700\u540e\u5904\u7406\u7684\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u57fa\u4e8e\u6211\u4eec\u65b0\u9896\u7684\u6f5c\u5728\u5bf9\u8c61\u8868\u793a\uff08LOR\uff09\u6a21\u5757\u3001\u67e5\u8be2\u5206\u914d\u3001\u548c\u9884\u6d4b\u7b56\u7565\u3002 FFD\u5728\u6700\u65b0\u768410W NVIDIA Jetson-NX\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86100FPS@FP32\u7cbe\u5ea6\uff0c\u540c\u65f6\u4e0e\u63a7\u5236\u3001\u6293\u53d6\u3001SLAM\u7b49\u5176\u4ed6\u65f6\u95f4\u5173\u952e\u7684\u5b50\u7cfb\u7edf\u5171\u5b58\uff0c\u662f\u8fd9\u9879\u5de5\u4f5c\u7684\u91cd\u5927\u6210\u5c31\u3002 \uff08ii\uff09\u4e00\u79cd\u751f\u6210\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u5bf9\u6c34\u679c\u56fe\u50cf\u8fdb\u884c\u8be6\u5c3d\u7684\u624b\u52a8\u6807\u8bb0\uff0c\u56e0\u4e3a\u5b83\u4eec\u7531\u5927\u91cf\u5b9e\u4f8b\u7ec4\u6210\uff0c\u8fd9\u589e\u52a0\u4e86\u6807\u8bb0\u6210\u672c\u548c\u65f6\u95f4\u3002 (iii) \u4e00\u4e2a\u5f00\u6e90\u6c34\u679c\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5927\u91cf\u96be\u4ee5\u68c0\u6d4b\u7684\u975e\u5e38\u5c0f\u7684\u5b9e\u4f8b\u3002\u6211\u4eec\u5bf9\u6211\u4eec\u548c MinneApple \u6570\u636e\u96c6\u7684\u8be6\u5c3d\u8bc4\u4f30\u8868\u660e\uff0cFFD \u53ea\u662f\u4e00\u4e2a\u5355\u5c3a\u5ea6\u68c0\u6d4b\u5668\uff0c\u6bd4\u8bb8\u591a\u4ee3\u8868\u6027\u68c0\u6d4b\u5668\uff08\u4f8b\u5982FFD \u6bd4\u5355\u5c3a\u5ea6 Faster-RCNN \u597d 10.7AP\uff0c\u6bd4\u591a\u5c3a\u5ea6 Faster-RCNN \u597d 2.3AP\uff0c\u6bd4\u6700\u65b0\u7684\u5355\u5c3a\u5ea6 YOLO-v8 \u597d 8AP\uff0c\u6bd4\u591a\u5c3a\u5ea6 YOLO-v8 \u597d 0.3\uff0c\u540c\u65f6\u901f\u5ea6\u66f4\u5feb\u3002|[2402.14591v1](http://arxiv.org/pdf/2402.14591v1)|null|\n", "2402.14509": "|**2024-02-22**|**Deep vessel segmentation based on a new combination of vesselness filters**|\u57fa\u4e8e\u8840\u7ba1\u8fc7\u6ee4\u5668\u65b0\u7ec4\u5408\u7684\u6df1\u5c42\u8840\u7ba1\u5206\u5272|Guillaume Garret, Antoine Vacavant, Carole Frindel|Vascular segmentation represents a crucial clinical task, yet its automation remains challenging. Because of the recent strides in deep learning, vesselness filters, which can significantly aid the learning process, have been overlooked. This study introduces an innovative filter fusion method crafted to amplify the effectiveness of vessel segmentation models. Our investigation seeks to establish the merits of a filter-based learning approach through a comparative analysis. Specifically, we contrast the performance of a U-Net model trained on CT images with an identical U-Net configuration trained on vesselness hyper-volumes using matching parameters. Our findings, based on two vascular datasets, highlight improved segmentations, especially for small vessels, when the model's learning is exposed to vessel-enhanced inputs.|\u8840\u7ba1\u5206\u5272\u662f\u4e00\u9879\u81f3\u5173\u91cd\u8981\u7684\u4e34\u5e8a\u4efb\u52a1\uff0c\u4f46\u5176\u81ea\u52a8\u5316\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u7531\u4e8e\u6700\u8fd1\u6df1\u5ea6\u5b66\u4e60\u7684\u8fdb\u6b65\uff0c\u53ef\u4ee5\u663e\u7740\u5e2e\u52a9\u5b66\u4e60\u8fc7\u7a0b\u7684\u8840\u7ba1\u8fc7\u6ee4\u5668\u88ab\u5ffd\u89c6\u4e86\u3002\u8fd9\u9879\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6ee4\u6ce2\u5668\u878d\u5408\u65b9\u6cd5\uff0c\u65e8\u5728\u589e\u5f3a\u8840\u7ba1\u5206\u5272\u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u8c03\u67e5\u65e8\u5728\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u6765\u786e\u5b9a\u57fa\u4e8e\u8fc7\u6ee4\u5668\u7684\u5b66\u4e60\u65b9\u6cd5\u7684\u4f18\u70b9\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c06\u5728 CT \u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684 U-Net \u6a21\u578b\u7684\u6027\u80fd\u4e0e\u4f7f\u7528\u5339\u914d\u53c2\u6570\u5728\u8840\u7ba1\u8d85\u4f53\u79ef\u4e0a\u8bad\u7ec3\u7684\u76f8\u540c U-Net \u914d\u7f6e\u8fdb\u884c\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u57fa\u4e8e\u4e24\u4e2a\u8840\u7ba1\u6570\u636e\u96c6\uff0c\u7a81\u51fa\u663e\u793a\u4e86\u5f53\u6a21\u578b\u7684\u5b66\u4e60\u66b4\u9732\u4e8e\u8840\u7ba1\u589e\u5f3a\u8f93\u5165\u65f6\uff0c\u5206\u5272\u5f97\u5230\u4e86\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5c0f\u8840\u7ba1\u3002|[2402.14509v1](http://arxiv.org/pdf/2402.14509v1)|null|\n", "2402.14505": "|**2024-02-22**|**Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition**|\u5b9e\u73b0\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65e0\u7f1d\u9002\u5e94|Feng Lu, Lijun Zhang, Xiangyuan Lan, Shuting Dong, Yaowei Wang, Chun Yuan|Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to guide effective adaptation, we propose a mutual nearest neighbor local feature loss, which ensures proper dense local features are produced for local matching and avoids time-consuming spatial verification in re-ranking. Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time, and uses about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based spatial verification. It ranks 1st on the MSLS challenge leaderboard (at the time of submission). The code is released at https://github.com/Lu-Feng/SelaVPR.|\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5728\u901a\u7528\u89c6\u89c9\u5b66\u4e60\u4efb\u52a1\u4e2d\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u6a21\u578b\u53ef\u4ee5\u4e3a\u5404\u79cd\u89c6\u89c9\u611f\u77e5\u95ee\u9898\u63d0\u4f9b\u6709\u7528\u7684\u7279\u5f81\u8868\u793a\u3002\u7136\u800c\uff0c\u5f88\u5c11\u6709\u4eba\u5c1d\u8bd5\u5728\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\uff08VPR\uff09\u4e2d\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u3002\u7531\u4e8e\u6a21\u578b\u9884\u8bad\u7ec3\u548cVPR\u4efb\u52a1\u5728\u8bad\u7ec3\u76ee\u6807\u548c\u6570\u636e\u4e0a\u5b58\u5728\u56fa\u6709\u7684\u5dee\u5f02\uff0c\u5982\u4f55\u5f25\u5408\u5dee\u8ddd\u5e76\u5145\u5206\u91ca\u653eVPR\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u9700\u8981\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u5b9e\u73b0 VPR \u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65e0\u7f1d\u9002\u5e94\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e3a\u4e86\u83b7\u5f97\u4e13\u6ce8\u4e8e\u533a\u5206\u5730\u70b9\u7684\u663e\u7740\u5730\u6807\u7684\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df7\u5408\u9002\u5e94\u65b9\u6cd5\u6765\u6709\u6548\u5730\u5b9e\u73b0\u5168\u5c40\u548c\u5c40\u90e8\u9002\u5e94\uff0c\u5176\u4e2d\u4ec5\u8c03\u6574\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u800c\u4e0d\u8c03\u6574\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u6307\u5bfc\u6709\u6548\u7684\u9002\u5e94\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u76f8\u4e92\u6700\u8fd1\u90bb\u5c40\u90e8\u7279\u5f81\u635f\u5931\uff0c\u8fd9\u786e\u4fdd\u4e3a\u5c40\u90e8\u5339\u914d\u4ea7\u751f\u9002\u5f53\u7684\u5bc6\u96c6\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u907f\u514d\u91cd\u65b0\u6392\u5e8f\u65f6\u8017\u65f6\u7684\u7a7a\u95f4\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u6570\u636e\u548c\u8bad\u7ec3\u65f6\u95f4\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u4ec5\u4f7f\u7528\u57fa\u4e8e RANSAC \u7a7a\u95f4\u9a8c\u8bc1\u7684\u4e24\u9636\u6bb5 VPR \u65b9\u6cd5\u7684 3% \u68c0\u7d22\u8fd0\u884c\u65f6\u95f4\u3002\u5b83\u5728 MSLS \u6311\u6218\u6392\u884c\u699c\u4e0a\u6392\u540d\u7b2c\u4e00\uff08\u63d0\u4ea4\u65f6\uff09\u3002\u4ee3\u7801\u53d1\u5e03\u4e8ehttps://github.com/Lu-Feng/SelaVPR\u3002|[2402.14505v1](http://arxiv.org/pdf/2402.14505v1)|**[link](https://github.com/Lu-Feng/SelaVPR)**|\n", "2402.14469": "|**2024-02-22**|**Reimagining Anomalies: What If Anomalies Were Normal?**|\u91cd\u65b0\u60f3\u8c61\u5f02\u5e38\uff1a\u5982\u679c\u5f02\u5e38\u662f\u6b63\u5e38\u7684\u600e\u4e48\u529e\uff1f|Philipp Liznerski, Saurabh Varshneya, Ece Calikus, Sophie Fellenz, Marius Kloft|Deep learning-based methods have achieved a breakthrough in image anomaly detection, but their complexity introduces a considerable challenge to understanding why an instance is predicted to be anomalous. We introduce a novel explanation method that generates multiple counterfactual examples for each anomaly, capturing diverse concepts of anomalousness. A counterfactual example is a modification of the anomaly that is perceived as normal by the anomaly detector. The method provides a high-level semantic explanation of the mechanism that triggered the anomaly detector, allowing users to explore \"what-if scenarios.\" Qualitative and quantitative analyses across various image datasets show that the method applied to state-of-the-art anomaly detectors can achieve high-quality semantic explanations of detectors.|\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u5176\u590d\u6742\u6027\u7ed9\u7406\u89e3\u4e3a\u4ec0\u4e48\u5b9e\u4f8b\u88ab\u9884\u6d4b\u4e3a\u5f02\u5e38\u5e26\u6765\u4e86\u76f8\u5f53\u5927\u7684\u6311\u6218\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4e3a\u6bcf\u4e2a\u5f02\u5e38\u751f\u6210\u591a\u4e2a\u53cd\u4e8b\u5b9e\u793a\u4f8b\uff0c\u6355\u83b7\u4e0d\u540c\u7684\u5f02\u5e38\u6982\u5ff5\u3002\u53cd\u4e8b\u5b9e\u793a\u4f8b\u662f\u5bf9\u88ab\u5f02\u5e38\u68c0\u6d4b\u5668\u89c6\u4e3a\u6b63\u5e38\u7684\u5f02\u5e38\u8fdb\u884c\u7684\u4fee\u6539\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u89e6\u53d1\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u673a\u5236\u7684\u9ad8\u7ea7\u8bed\u4e49\u89e3\u91ca\uff0c\u5141\u8bb8\u7528\u6237\u63a2\u7d22\u201c\u5047\u8bbe\u573a\u666f\u201d\u3002\u5bf9\u5404\u79cd\u56fe\u50cf\u6570\u636e\u96c6\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u8868\u660e\uff0c\u5e94\u7528\u4e8e\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u68c0\u6d4b\u5668\u7684\u9ad8\u8d28\u91cf\u8bed\u4e49\u89e3\u91ca\u3002|[2402.14469v1](http://arxiv.org/pdf/2402.14469v1)|null|\n", "2402.14461": "|**2024-02-22**|**S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR**|S^2Former-OR\uff1a\u7528\u4e8e OR \u4e2d\u573a\u666f\u56fe\u751f\u6210\u7684\u5355\u7ea7\u53cc\u5cf0\u53d8\u538b\u5668|Jialun Pei, Diandian Guo, Jingyang Zhang, Manxi Lin, Yueming Jin, Pheng-Ann Heng|Scene graph generation (SGG) of surgical procedures is crucial in enhancing holistically cognitive intelligence in the operating room (OR). However, previous works have primarily relied on the multi-stage learning that generates semantic scene graphs dependent on intermediate processes with pose estimation and object detection, which may compromise model efficiency and efficacy, also impose extra annotation burden. In this study, we introduce a novel single-stage bimodal transformer framework for SGG in the OR, termed S^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3D point clouds for SGG in an end-to-end manner. Concretely, our model embraces a View-Sync Transfusion scheme to encourage multi-view visual information interaction. Concurrently, a Geometry-Visual Cohesion operation is designed to integrate the synergic 2D semantic features into 3D point cloud features. Moreover, based on the augmented feature, we propose a novel relation-sensitive transformer decoder that embeds dynamic entity-pair queries and relational trait priors, which enables the direct prediction of entity-pair relations for graph generation without intermediate steps. Extensive experiments have validated the superior SGG performance and lower computational cost of S^2Former-OR on 4D-OR benchmark, compared with current OR-SGG methods, e.g., 3% Precision increase and 24.2M reduction in model parameters. We further compared our method with generic single-stage SGG methods with broader metrics for a comprehensive evaluation, with consistently better performance achieved. The code will be made available.|\u5916\u79d1\u624b\u672f\u7684\u573a\u666f\u56fe\u751f\u6210 (SGG) \u5bf9\u4e8e\u589e\u5f3a\u624b\u672f\u5ba4 (OR) \u7684\u6574\u4f53\u8ba4\u77e5\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4ee5\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u4f9d\u8d56\u4e8e\u591a\u9636\u6bb5\u5b66\u4e60\uff0c\u751f\u6210\u4f9d\u8d56\u4e8e\u59ff\u6001\u4f30\u8ba1\u548c\u5bf9\u8c61\u68c0\u6d4b\u7684\u4e2d\u95f4\u8fc7\u7a0b\u7684\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u8fd9\u53ef\u80fd\u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u6548\u7387\u548c\u529f\u6548\uff0c\u8fd8\u4f1a\u5e26\u6765\u989d\u5916\u7684\u6ce8\u91ca\u8d1f\u62c5\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4e3a OR \u4e2d\u7684 SGG \u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5355\u7ea7\u53cc\u6a21\u6001 Transformer \u6846\u67b6\uff0c\u79f0\u4e3a S^2Former-OR\uff0c\u65e8\u5728\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u4e92\u8865\u5730\u5229\u7528 SGG \u7684\u591a\u89c6\u56fe 2D \u573a\u666f\u548c 3D \u70b9\u4e91\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u91c7\u7528\u89c6\u56fe\u540c\u6b65\u4f20\u8f93\u65b9\u6848\u6765\u9f13\u52b1\u591a\u89c6\u56fe\u89c6\u89c9\u4fe1\u606f\u4ea4\u4e92\u3002\u540c\u65f6\uff0c\u51e0\u4f55-\u89c6\u89c9\u51dd\u805a\u64cd\u4f5c\u65e8\u5728\u5c06\u534f\u540c 2D \u8bed\u4e49\u7279\u5f81\u96c6\u6210\u5230 3D \u70b9\u4e91\u7279\u5f81\u4e2d\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u589e\u5f3a\u7684\u7279\u5f81\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5173\u7cfb\u654f\u611f\u8f6c\u6362\u5668\u89e3\u7801\u5668\uff0c\u5b83\u5d4c\u5165\u4e86\u52a8\u6001\u5b9e\u4f53\u5bf9\u67e5\u8be2\u548c\u5173\u7cfb\u7279\u5f81\u5148\u9a8c\uff0c\u8fd9\u4f7f\u5f97\u80fd\u591f\u76f4\u63a5\u9884\u6d4b\u5b9e\u4f53\u5bf9\u5173\u7cfb\u4ee5\u8fdb\u884c\u56fe\u751f\u6210\uff0c\u800c\u65e0\u9700\u4e2d\u95f4\u6b65\u9aa4\u3002\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 S^2Former-OR \u5728 4D-OR \u57fa\u51c6\u4e0a\u4e0e\u5f53\u524d OR-SGG \u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u4f18\u8d8a\u7684 SGG \u6027\u80fd\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4f8b\u5982\uff0c\u7cbe\u5ea6\u63d0\u9ad8\u4e86 3%\uff0c\u6a21\u578b\u53c2\u6570\u51cf\u5c11\u4e86 24.2M\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u5177\u6709\u66f4\u5e7f\u6cdb\u6307\u6807\u7684\u901a\u7528\u5355\u9636\u6bb5 SGG \u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u5e76\u59cb\u7ec8\u53d6\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002\u8be5\u4ee3\u7801\u5c06\u53ef\u4f9b\u4f7f\u7528\u3002|[2402.14461v1](http://arxiv.org/pdf/2402.14461v1)|null|\n", "2402.14400": "|**2024-02-22**|**Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks**|\u4f7f\u7528\u81ea\u9002\u5e94\u56fe\u5377\u79ef\u7f51\u7edc\u5efa\u6a21 3D \u5a74\u513f\u52a8\u529b\u5b66|Daniel Holmberg, Manu Airaksinen, Viviana Marchi, Andrea Guzzetta, Anna Kivi, Leena Haataja, Sampsa Vanhatalo, Teemu Roos|Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions. Spontaneous motor activity, or `kinetics', is shown to provide a powerful surrogate measure of upcoming neurodevelopment. However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures. Here, we follow an alternative approach, predicting infants' neurodevelopmental maturation based on data-driven evaluation of individual motor patterns. We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive graph convolutional networks to predict the actual age. We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features.|\u5a74\u513f\u795e\u7ecf\u53d1\u80b2\u8bc4\u4f30\u7684\u53ef\u9760\u65b9\u6cd5\u5bf9\u4e8e\u53ca\u65e9\u53d1\u73b0\u53ef\u80fd\u9700\u8981\u53ca\u65f6\u5e72\u9884\u7684\u533b\u7597\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002\u81ea\u53d1\u8fd0\u52a8\u6d3b\u52a8\u6216\u201c\u52a8\u529b\u5b66\u201d\u88ab\u8bc1\u660e\u53ef\u4ee5\u4e3a\u5373\u5c06\u5230\u6765\u7684\u795e\u7ecf\u53d1\u80b2\u63d0\u4f9b\u5f3a\u6709\u529b\u7684\u66ff\u4ee3\u6d4b\u91cf\u3002\u7136\u800c\uff0c\u5176\u8bc4\u4f30\u603b\u4f53\u4e0a\u662f\u5b9a\u6027\u7684\u548c\u4e3b\u89c2\u7684\uff0c\u4fa7\u91cd\u4e8e\u89c6\u89c9\u8bc6\u522b\u7684\u3001\u7279\u5b9a\u5e74\u9f84\u7684\u624b\u52bf\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u91c7\u7528\u53e6\u4e00\u79cd\u65b9\u6cd5\uff0c\u6839\u636e\u5bf9\u4e2a\u4f53\u8fd0\u52a8\u6a21\u5f0f\u7684\u6570\u636e\u9a71\u52a8\u8bc4\u4f30\u6765\u9884\u6d4b\u5a74\u513f\u7684\u795e\u7ecf\u53d1\u80b2\u6210\u719f\u5ea6\u3002\u6211\u4eec\u5229\u7528\u7ecf\u8fc7\u59ff\u52bf\u4f30\u8ba1\u5904\u7406\u7684\u5a74\u513f 3D \u89c6\u9891\u8bb0\u5f55\u6765\u63d0\u53d6\u89e3\u5256\u6807\u5fd7\u7684\u65f6\u7a7a\u5e8f\u5217\uff0c\u5e76\u5e94\u7528\u81ea\u9002\u5e94\u56fe\u5377\u79ef\u7f51\u7edc\u6765\u9884\u6d4b\u5b9e\u9645\u5e74\u9f84\u3002\u6211\u4eec\u8868\u660e\uff0c\u6211\u4eec\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u624b\u52a8\u8bbe\u8ba1\u7279\u5f81\u7684\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u53d6\u5f97\u4e86\u6539\u8fdb\u3002|[2402.14400v1](http://arxiv.org/pdf/2402.14400v1)|null|\n", "2402.14395": "|**2024-02-22**|**Semantic Image Synthesis with Unconditional Generator**|\u4f7f\u7528\u65e0\u6761\u4ef6\u751f\u6210\u5668\u8fdb\u884c\u8bed\u4e49\u56fe\u50cf\u5408\u6210|Jungwoo Chae, Hyunin Cho, Sooyeon Go, Kyungmook Choi, Youngjung Uh|Semantic image synthesis (SIS) aims to generate realistic images that match given semantic masks. Despite recent advances allowing high-quality results and precise spatial control, they require a massive semantic segmentation dataset for training the models. Instead, we propose to employ a pre-trained unconditional generator and rearrange its feature maps according to proxy masks. The proxy masks are prepared from the feature maps of random samples in the generator by simple clustering. The feature rearranger learns to rearrange original feature maps to match the shape of the proxy masks that are either from the original sample itself or from random samples. Then we introduce a semantic mapper that produces the proxy masks from various input conditions including semantic masks. Our method is versatile across various applications such as free-form spatial editing of real images, sketch-to-photo, and even scribble-to-photo. Experiments validate advantages of our method on a range of datasets: human faces, animal faces, and buildings.|\u8bed\u4e49\u56fe\u50cf\u5408\u6210\uff08SIS\uff09\u65e8\u5728\u751f\u6210\u4e0e\u7ed9\u5b9a\u8bed\u4e49\u63a9\u6a21\u5339\u914d\u7684\u903c\u771f\u56fe\u50cf\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u8fdb\u5c55\u5141\u8bb8\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\u548c\u7cbe\u786e\u7684\u7a7a\u95f4\u63a7\u5236\uff0c\u4f46\u5b83\u4eec\u9700\u8981\u5927\u91cf\u7684\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u6a21\u578b\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u9884\u5148\u8bad\u7ec3\u7684\u65e0\u6761\u4ef6\u751f\u6210\u5668\uff0c\u5e76\u6839\u636e\u4ee3\u7406\u63a9\u7801\u91cd\u65b0\u6392\u5217\u5176\u7279\u5f81\u56fe\u3002\u4ee3\u7406\u63a9\u7801\u662f\u901a\u8fc7\u7b80\u5355\u7684\u805a\u7c7b\u4ece\u751f\u6210\u5668\u4e2d\u968f\u673a\u6837\u672c\u7684\u7279\u5f81\u56fe\u51c6\u5907\u7684\u3002\u7279\u5f81\u91cd\u65b0\u6392\u5217\u5668\u5b66\u4e60\u91cd\u65b0\u6392\u5217\u539f\u59cb\u7279\u5f81\u56fe\u4ee5\u5339\u914d\u6765\u81ea\u539f\u59cb\u6837\u672c\u672c\u8eab\u6216\u6765\u81ea\u968f\u673a\u6837\u672c\u7684\u4ee3\u7406\u63a9\u7801\u7684\u5f62\u72b6\u3002\u7136\u540e\u6211\u4eec\u5f15\u5165\u4e00\u4e2a\u8bed\u4e49\u6620\u5c04\u5668\uff0c\u5b83\u6839\u636e\u5404\u79cd\u8f93\u5165\u6761\u4ef6\uff08\u5305\u62ec\u8bed\u4e49\u63a9\u7801\uff09\u751f\u6210\u4ee3\u7406\u63a9\u7801\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9002\u7528\u4e8e\u5404\u79cd\u5e94\u7528\uff0c\u4f8b\u5982\u771f\u5b9e\u56fe\u50cf\u7684\u81ea\u7531\u5f62\u5f0f\u7a7a\u95f4\u7f16\u8f91\u3001\u8349\u56fe\u5230\u7167\u7247\uff0c\u751a\u81f3\u6d82\u9e26\u5230\u7167\u7247\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e00\u7cfb\u5217\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u52bf\uff1a\u4eba\u8138\u3001\u52a8\u7269\u9762\u5b54\u548c\u5efa\u7b51\u7269\u3002|[2402.14395v1](http://arxiv.org/pdf/2402.14395v1)|null|\n", "2402.14392": "|**2024-02-22**|**Reading Relevant Feature from Global Representation Memory for Visual Object Tracking**|\u4ece\u5168\u5c40\u8868\u793a\u5b58\u50a8\u5668\u4e2d\u8bfb\u53d6\u76f8\u5173\u7279\u5f81\u4ee5\u8fdb\u884c\u89c6\u89c9\u5bf9\u8c61\u8ddf\u8e2a|Xinyu Zhou, Pinxue Guo, Lingyi Hong, Jinglun Li, Wei Zhang, Weifeng Ge, Wenqiang Zhang|Reference features from a template or historical frames are crucial for visual object tracking. Prior works utilize all features from a fixed template or memory for visual object tracking. However, due to the dynamic nature of videos, the required reference historical information for different search regions at different time steps is also inconsistent. Therefore, using all features in the template and memory can lead to redundancy and impair tracking performance. To alleviate this issue, we propose a novel tracking paradigm, consisting of a relevance attention mechanism and a global representation memory, which can adaptively assist the search region in selecting the most relevant historical information from reference features. Specifically, the proposed relevance attention mechanism in this work differs from previous approaches in that it can dynamically choose and build the optimal global representation memory for the current frame by accessing cross-frame information globally. Moreover, it can flexibly read the relevant historical information from the constructed memory to reduce redundancy and counteract the negative effects of harmful information. Extensive experiments validate the effectiveness of the proposed method, achieving competitive performance on five challenging datasets with 71 FPS.|\u6a21\u677f\u6216\u5386\u53f2\u5e27\u7684\u53c2\u8003\u7279\u5f81\u5bf9\u4e8e\u89c6\u89c9\u5bf9\u8c61\u8ddf\u8e2a\u81f3\u5173\u91cd\u8981\u3002\u5148\u524d\u7684\u4f5c\u54c1\u5229\u7528\u56fa\u5b9a\u6a21\u677f\u6216\u5185\u5b58\u4e2d\u7684\u6240\u6709\u529f\u80fd\u6765\u8fdb\u884c\u89c6\u89c9\u5bf9\u8c61\u8ddf\u8e2a\u3002\u7136\u800c\uff0c\u7531\u4e8e\u89c6\u9891\u7684\u52a8\u6001\u7279\u6027\uff0c\u4e0d\u540c\u641c\u7d22\u533a\u57df\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u6240\u9700\u7684\u53c2\u8003\u5386\u53f2\u4fe1\u606f\u4e5f\u4e0d\u4e00\u81f4\u3002\u56e0\u6b64\uff0c\u4f7f\u7528\u6a21\u677f\u548c\u5185\u5b58\u4e2d\u7684\u6240\u6709\u7279\u5f81\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5197\u4f59\u5e76\u635f\u5bb3\u8ddf\u8e2a\u6027\u80fd\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8ddf\u8e2a\u8303\u5f0f\uff0c\u7531\u76f8\u5173\u6027\u6ce8\u610f\u673a\u5236\u548c\u5168\u5c40\u8868\u793a\u8bb0\u5fc6\u7ec4\u6210\uff0c\u5b83\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u5e2e\u52a9\u641c\u7d22\u533a\u57df\u4ece\u53c2\u8003\u7279\u5f81\u4e2d\u9009\u62e9\u6700\u76f8\u5173\u7684\u5386\u53f2\u4fe1\u606f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e2d\u63d0\u51fa\u7684\u76f8\u5173\u6027\u6ce8\u610f\u673a\u5236\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u5168\u5c40\u8bbf\u95ee\u8de8\u5e27\u4fe1\u606f\u6765\u52a8\u6001\u9009\u62e9\u548c\u6784\u5efa\u5f53\u524d\u5e27\u7684\u6700\u4f73\u5168\u5c40\u8868\u793a\u8bb0\u5fc6\u3002\u800c\u4e14\uff0c\u5b83\u53ef\u4ee5\u7075\u6d3b\u5730\u4ece\u6784\u5efa\u7684\u8bb0\u5fc6\u4e2d\u8bfb\u53d6\u76f8\u5173\u5386\u53f2\u4fe1\u606f\uff0c\u51cf\u5c11\u5197\u4f59\uff0c\u62b5\u6d88\u6709\u5bb3\u4fe1\u606f\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u4e94\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u4ee5 71 FPS \u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002|[2402.14392v1](http://arxiv.org/pdf/2402.14392v1)|null|\n", "2402.14354": "|**2024-02-22**|**GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints**|GAM-Depth\uff1a\u5229\u7528\u68af\u5ea6\u611f\u77e5\u63a9\u6a21\u548c\u8bed\u4e49\u7ea6\u675f\u7684\u81ea\u76d1\u7763\u5ba4\u5185\u6df1\u5ea6\u4f30\u8ba1|Anqi Cheng, Zhiyuan Yang, Haiyue Zhu, Kezhi Mao|Self-supervised depth estimation has evolved into an image reconstruction task that minimizes a photometric loss. While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries. To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints. The gradient-aware mask enables adaptive and robust supervision for both key areas and textureless regions by allocating weights based on gradient magnitudes.The incorporation of semantic constraints for indoor self-supervised depth estimation improves depth discrepancies at object boundaries, leveraging a co-optimization network and proxy semantic labels derived from a pretrained segmentation model. Experimental studies on three indoor datasets, including NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing methods and achieves state-of-the-art performance, signifying a meaningful step forward in indoor depth estimation. Our code will be available at https://github.com/AnqiCheng1234/GAM-Depth.|\u81ea\u76d1\u7763\u6df1\u5ea6\u4f30\u8ba1\u5df2\u7ecf\u53d1\u5c55\u6210\u4e3a\u4e00\u79cd\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\uff0c\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5149\u5ea6\u635f\u5931\u3002\u867d\u7136\u6700\u8fd1\u7684\u65b9\u6cd5\u5728\u5ba4\u5185\u6df1\u5ea6\u4f30\u8ba1\u65b9\u9762\u53d6\u5f97\u4e86\u957f\u8db3\u7684\u8fdb\u6b65\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u5728\u65e0\u7eb9\u7406\u533a\u57df\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5e76\u4e14\u5728\u5bf9\u8c61\u8fb9\u754c\u5904\u4ea7\u751f\u4ee4\u4eba\u4e0d\u6ee1\u610f\u7684\u6df1\u5ea6\u5dee\u5f02\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GAM-Depth\uff0c\u5b83\u662f\u57fa\u4e8e\u4e24\u4e2a\u65b0\u9896\u7684\u7ec4\u4ef6\u5f00\u53d1\u7684\uff1a\u68af\u5ea6\u611f\u77e5\u63a9\u6a21\u548c\u8bed\u4e49\u7ea6\u675f\u3002\u68af\u5ea6\u611f\u77e5\u63a9\u6a21\u901a\u8fc7\u6839\u636e\u68af\u5ea6\u5927\u5c0f\u5206\u914d\u6743\u91cd\uff0c\u80fd\u591f\u5bf9\u5173\u952e\u533a\u57df\u548c\u65e0\u7eb9\u7406\u533a\u57df\u8fdb\u884c\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u7684\u76d1\u7763\u3002\u5ba4\u5185\u81ea\u76d1\u7763\u6df1\u5ea6\u4f30\u8ba1\u7684\u8bed\u4e49\u7ea6\u675f\u7684\u7ed3\u5408\u6539\u5584\u4e86\u5bf9\u8c61\u8fb9\u754c\u5904\u7684\u6df1\u5ea6\u5dee\u5f02\uff0c\u5229\u7528\u534f\u540c\u4f18\u5316\u7f51\u7edc\u4ee5\u53ca\u4ece\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u6d3e\u751f\u7684\u4ee3\u7406\u8bed\u4e49\u6807\u7b7e\u3002\u5bf9 NYUv2\u3001ScanNet \u548c InteriorNet \u7b49\u4e09\u4e2a\u5ba4\u5185\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7814\u7a76\u8868\u660e\uff0cGAM-Depth \u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8fd9\u6807\u5fd7\u7740\u5ba4\u5185\u6df1\u5ea6\u4f30\u8ba1\u5411\u524d\u8fc8\u51fa\u4e86\u6709\u610f\u4e49\u7684\u4e00\u6b65\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728 https://github.com/AnqiCheng1234/GAM-Depth \u4e0a\u63d0\u4f9b\u3002|[2402.14354v1](http://arxiv.org/pdf/2402.14354v1)|null|\n", "2402.14327": "|**2024-02-22**|**Subobject-level Image Tokenization**|\u5b50\u5bf9\u8c61\u7ea7\u56fe\u50cf\u6807\u8bb0\u5316|Delong Chen, Samuel Cahyawijaya, Jianfeng Liu, Baoyuan Wang, Pascale Fung|Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning. Empirical results demonstrated that our subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descriptions compared to the traditional patch-level tokenization. Codes and models will be open-sourced at https://github.com/ChenDelong1999/subobjects.|\u57fa\u4e8e Transformer \u7684\u89c6\u89c9\u6a21\u578b\u901a\u5e38\u5c06\u56fe\u50cf\u6807\u8bb0\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u65b9\u5f62\u5757\u4f5c\u4e3a\u8f93\u5165\u5355\u5143\uff0c\u7f3a\u4e4f\u5bf9\u56fe\u50cf\u5185\u5bb9\u7684\u9002\u5e94\u6027\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86\u56fa\u6709\u7684\u50cf\u7d20\u5206\u7ec4\u7ed3\u6784\u3002\u53d7\u8bed\u8a00\u6a21\u578b\u4e2d\u5e7f\u6cdb\u91c7\u7528\u7684\u5b50\u8bcd\u6807\u8bb0\u5316\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5b50\u5bf9\u8c61\u7ea7\u522b\u7684\u56fe\u50cf\u6807\u8bb0\u5668\uff0c\u5176\u4e2d\u5b50\u5bf9\u8c61\u7531\u5206\u5272\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u5206\u5272\u4efb\u4f55\u6a21\u578b\uff09\u83b7\u5f97\u7684\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u56fe\u50cf\u7247\u6bb5\u8868\u793a\u3002\u4e3a\u4e86\u5b9e\u73b0\u57fa\u4e8e\u5b50\u5bf9\u8c61\u6807\u8bb0\u5316\u7684\u5b66\u4e60\u7cfb\u7edf\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165\u4e86\u5e8f\u5217\u5230\u5e8f\u5217\u81ea\u52a8\u7f16\u7801\u5668\uff08SeqAE\uff09\uff0c\u5c06\u4e0d\u540c\u5927\u5c0f\u548c\u5f62\u72b6\u7684\u5b50\u5bf9\u8c61\u7247\u6bb5\u538b\u7f29\u4e3a\u7d27\u51d1\u7684\u5d4c\u5165\u5411\u91cf\uff0c\u7136\u540e\u5c06\u5b50\u5bf9\u8c61\u5d4c\u5165\u8f93\u5165\u5230\u89c6\u89c9\u8bed\u8a00\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b66\u4e60\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u8865\u4e01\u7ea7\u6807\u8bb0\u5316\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u5b50\u5bf9\u8c61\u7ea7\u6807\u8bb0\u5316\u663e\u7740\u4fc3\u8fdb\u4e86\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u5bf9\u8c61\u548c\u5c5e\u6027\u63cf\u8ff0\u7684\u6709\u6548\u5b66\u4e60\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728 https://github.com/ChenDelong1999/subobjects \u5f00\u6e90\u3002|[2402.14327v1](http://arxiv.org/pdf/2402.14327v1)|null|\n", "2402.14309": "|**2024-02-22**|**YOLO-TLA: An Efficient and Lightweight Small Object Detection Model based on YOLOv5**|YOLO-TLA\uff1a\u57fa\u4e8eYOLOv5\u7684\u9ad8\u6548\u8f7b\u91cf\u5c0f\u7269\u4f53\u68c0\u6d4b\u6a21\u578b|Peng Gao, Chun-Lin Ji, Tao Yu, Ru-Yue Yuan|Object detection, a crucial aspect of computer vision, has seen significant advancements in accuracy and robustness. Despite these advancements, practical applications still face notable challenges, primarily the inaccurate detection or missed detection of small objects. In this paper, we propose YOLO-TLA, an advanced object detection model building on YOLOv5. We first introduce an additional detection layer for small objects in the neck network pyramid architecture, thereby producing a feature map of a larger scale to discern finer features of small objects. Further, we integrate the C3CrossCovn module into the backbone network. This module uses sliding window feature extraction, which effectively minimizes both computational demand and the number of parameters, rendering the model more compact. Additionally, we have incorporated a global attention mechanism into the backbone network. This mechanism combines the channel information with global information to create a weighted feature map. This feature map is tailored to highlight the attributes of the object of interest, while effectively ignoring irrelevant details. In comparison to the baseline YOLOv5s model, our newly developed YOLO-TLA model has shown considerable improvements on the MS COCO validation dataset, with increases of 4.6% in mAP@0.5 and 4% in mAP@0.5:0.95, all while keeping the model size compact at 9.49M parameters. Further extending these improvements to the YOLOv5m model, the enhanced version exhibited a 1.7% and 1.9% increase in mAP@0.5 and mAP@0.5:0.95, respectively, with a total of 27.53M parameters. These results validate the YOLO-TLA model's efficient and effective performance in small object detection, achieving high accuracy with fewer parameters and computational demands.|\u76ee\u6807\u68c0\u6d4b\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u9762\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u8fdb\u6b65\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fd9\u4e9b\u8fdb\u6b65\uff0c\u5b9e\u9645\u5e94\u7528\u4ecd\u7136\u9762\u4e34\u7740\u663e\u7740\u7684\u6311\u6218\uff0c\u4e3b\u8981\u662f\u5c0f\u7269\u4f53\u68c0\u6d4b\u4e0d\u51c6\u786e\u6216\u6f0f\u68c0\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 YOLO-TLA\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e YOLOv5 \u7684\u9ad8\u7ea7\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002\u6211\u4eec\u9996\u5148\u5728\u9888\u90e8\u7f51\u7edc\u91d1\u5b57\u5854\u67b6\u6784\u4e2d\u5f15\u5165\u4e00\u4e2a\u9488\u5bf9\u5c0f\u7269\u4f53\u7684\u9644\u52a0\u68c0\u6d4b\u5c42\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u5927\u5c3a\u5ea6\u7684\u7279\u5f81\u56fe\u6765\u8fa8\u522b\u5c0f\u7269\u4f53\u7684\u66f4\u7cbe\u7ec6\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06 C3CrossCovn \u6a21\u5757\u96c6\u6210\u5230\u4e3b\u5e72\u7f51\u7edc\u4e2d\u3002\u8be5\u6a21\u5757\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u7279\u5f81\u63d0\u53d6\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8ba1\u7b97\u9700\u6c42\u548c\u53c2\u6570\u6570\u91cf\uff0c\u4f7f\u6a21\u578b\u66f4\u52a0\u7d27\u51d1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5728\u4e3b\u5e72\u7f51\u7edc\u4e2d\u7eb3\u5165\u4e86\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u3002\u8be5\u673a\u5236\u5c06\u901a\u9053\u4fe1\u606f\u4e0e\u5168\u5c40\u4fe1\u606f\u76f8\u7ed3\u5408\u4ee5\u521b\u5efa\u52a0\u6743\u7279\u5f81\u56fe\u3002\u8be5\u7279\u5f81\u56fe\u7ecf\u8fc7\u5b9a\u5236\uff0c\u53ef\u4ee5\u7a81\u51fa\u663e\u793a\u611f\u5174\u8da3\u5bf9\u8c61\u7684\u5c5e\u6027\uff0c\u540c\u65f6\u6709\u6548\u5730\u5ffd\u7565\u4e0d\u76f8\u5173\u7684\u7ec6\u8282\u3002\u4e0e\u57fa\u7ebf YOLOv5s \u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u4eec\u65b0\u5f00\u53d1\u7684 YOLO-TLA \u6a21\u578b\u5728 MS COCO \u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u76f8\u5f53\u5927\u7684\u6539\u8fdb\uff0cmAP@0.5 \u589e\u52a0\u4e86 4.6%\uff0cmAP@0.5:0.95 \u589e\u52a0\u4e86 4%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u4e0d\u53d8\u5c3a\u5bf8\u7d27\u51d1\uff0c\u53c2\u6570\u4e3a 9.49M\u3002\u5c06\u8fd9\u4e9b\u6539\u8fdb\u8fdb\u4e00\u6b65\u6269\u5c55\u5230YOLOv5m\u6a21\u578b\uff0c\u589e\u5f3a\u7248\u672c\u7684mAP@0.5\u548cmAP@0.5:0.95\u5206\u522b\u589e\u52a0\u4e861.7%\u548c1.9%\uff0c\u603b\u5171\u670927.53M\u53c2\u6570\u3002\u8fd9\u4e9b\u7ed3\u679c\u9a8c\u8bc1\u4e86YOLO-TLA\u6a21\u578b\u5728\u5c0f\u7269\u4f53\u68c0\u6d4b\u65b9\u9762\u9ad8\u6548\u4e14\u6709\u6548\u7684\u6027\u80fd\uff0c\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u548c\u8ba1\u7b97\u9700\u6c42\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3002|[2402.14309v1](http://arxiv.org/pdf/2402.14309v1)|null|\n", "2402.14280": "|**2024-02-22**|**Secure Navigation using Landmark-based Localization in a GPS-denied Environment**|\u5728 GPS \u62d2\u7edd\u7684\u73af\u5883\u4e2d\u4f7f\u7528\u57fa\u4e8e\u5730\u6807\u7684\u5b9a\u4f4d\u8fdb\u884c\u5b89\u5168\u5bfc\u822a|Ganesh Sapkota, Sanjay Madria|In modern battlefield scenarios, the reliance on GPS for navigation can be a critical vulnerability. Adversaries often employ tactics to deny or deceive GPS signals, necessitating alternative methods for the localization and navigation of mobile troops. Range-free localization methods such as DV-HOP rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in a dynamic and sparse network topology. Vision-based approaches like SLAM and Visual Odometry use sensor fusion techniques for map generation and pose estimation that are more sophisticated and computationally expensive. This paper proposes a novel framework that integrates landmark-based localization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the future state of moving entities along the battlefield. Our framework utilizes safe trajectory information generated by the troop control center by considering identifiable landmarks and pre-defined hazard maps. It performs point inclusion tests on the convex hull of the trajectory segments to ensure the safety and survivability of a moving entity and determines the next point forward decisions. We present a simulated battlefield scenario for two different approaches (with EKF and without EKF) that guide a moving entity through an obstacle and hazard-free path. Using the proposed method, we observed a percent error of 6.51% lengthwise in safe trajectory estimation with an Average Displacement Error (ADE) of 2.97m and a Final Displacement Error (FDE) of 3.27m. The results demonstrate that our approach not only ensures the safety of the mobile units by keeping them within the secure trajectory but also enhances operational effectiveness by adapting to the evolving threat landscape.|\u5728\u73b0\u4ee3\u6218\u573a\u573a\u666f\u4e2d\uff0c\u4f9d\u8d56 GPS \u8fdb\u884c\u5bfc\u822a\u53ef\u80fd\u662f\u4e00\u4e2a\u4e25\u91cd\u7684\u6f0f\u6d1e\u3002\u5bf9\u624b\u7ecf\u5e38\u91c7\u7528\u7b56\u7565\u6765\u62d2\u7edd\u6216\u6b3a\u9a97 GPS \u4fe1\u53f7\uff0c\u56e0\u6b64\u9700\u8981\u91c7\u7528\u66ff\u4ee3\u65b9\u6cd5\u6765\u5bf9\u673a\u52a8\u90e8\u961f\u8fdb\u884c\u5b9a\u4f4d\u548c\u5bfc\u822a\u3002 DV-HOP \u7b49\u65e0\u8303\u56f4\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u65e0\u7ebf\u7535\u7684\u951a\u70b9\u53ca\u5176\u5e73\u5747\u8df3\u8dc3\u8ddd\u79bb\uff0c\u8fd9\u5728\u52a8\u6001\u548c\u7a00\u758f\u7f51\u7edc\u62d3\u6251\u4e2d\u4f1a\u53d7\u5230\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002 SLAM \u548c\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7b49\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\u4f7f\u7528\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\u6765\u751f\u6210\u5730\u56fe\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u8fd9\u4e9b\u6280\u672f\u66f4\u52a0\u590d\u6742\u4e14\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5b83\u5c06\u57fa\u4e8e\u5730\u6807\u7684\u5b9a\u4f4d\uff08LanBLoc\uff09\u4e0e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u76f8\u7ed3\u5408\uff0c\u4ee5\u9884\u6d4b\u6218\u573a\u4e0a\u79fb\u52a8\u5b9e\u4f53\u7684\u672a\u6765\u72b6\u6001\u3002\u6211\u4eec\u7684\u6846\u67b6\u901a\u8fc7\u8003\u8651\u53ef\u8bc6\u522b\u7684\u5730\u6807\u548c\u9884\u5b9a\u4e49\u7684\u5371\u9669\u5730\u56fe\uff0c\u5229\u7528\u90e8\u961f\u63a7\u5236\u4e2d\u5fc3\u751f\u6210\u7684\u5b89\u5168\u8f68\u8ff9\u4fe1\u606f\u3002\u5b83\u5bf9\u8f68\u8ff9\u6bb5\u7684\u51f8\u5305\u8fdb\u884c\u70b9\u5305\u542b\u6d4b\u8bd5\uff0c\u4ee5\u786e\u4fdd\u79fb\u52a8\u5b9e\u4f53\u7684\u5b89\u5168\u6027\u548c\u751f\u5b58\u6027\uff0c\u5e76\u786e\u5b9a\u4e0b\u4e00\u4e2a\u70b9\u7684\u524d\u8fdb\u51b3\u7b56\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0d\u540c\u65b9\u6cd5\uff08\u4f7f\u7528 EKF \u548c\u4e0d\u4f7f\u7528 EKF\uff09\u7684\u6a21\u62df\u6218\u573a\u573a\u666f\uff0c\u5f15\u5bfc\u79fb\u52a8\u5b9e\u4f53\u7a7f\u8fc7\u969c\u788d\u7269\u548c\u65e0\u5371\u9669\u7684\u8def\u5f84\u3002\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5b89\u5168\u8f68\u8ff9\u4f30\u8ba1\u7684\u7eb5\u5411\u767e\u5206\u6bd4\u8bef\u5dee\u4e3a 6.51%\uff0c\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee (ADE) \u4e3a 2.97m\uff0c\u6700\u7ec8\u4f4d\u79fb\u8bef\u5dee (FDE) \u4e3a 3.27m\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u901a\u8fc7\u5c06\u79fb\u52a8\u5355\u5143\u4fdd\u6301\u5728\u5b89\u5168\u8f68\u8ff9\u5185\u6765\u786e\u4fdd\u79fb\u52a8\u5355\u5143\u7684\u5b89\u5168\uff0c\u800c\u4e14\u8fd8\u901a\u8fc7\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u5a01\u80c1\u5f62\u52bf\u6765\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\u3002|[2402.14280v1](http://arxiv.org/pdf/2402.14280v1)|null|\n", "2402.14241": "|**2024-02-22**|**A Self-supervised Pressure Map human keypoint Detection Approch: Optimizing Generalization and Computational Efficiency Across Datasets**|\u81ea\u76d1\u7763\u538b\u529b\u56fe\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\u65b9\u6cd5\uff1a\u4f18\u5316\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u548c\u8ba1\u7b97\u6548\u7387|Chengzhang Yu, Xianjun Yang, Wenxia Bao, Shaonan Wang, Zhiming Yao|In environments where RGB images are inadequate, pressure maps is a viable alternative, garnering scholarly attention. This study introduces a novel self-supervised pressure map keypoint detection (SPMKD) method, addressing the current gap in specialized designs for human keypoint extraction from pressure maps. Central to our contribution is the Encoder-Fuser-Decoder (EFD) model, which is a robust framework that integrates a lightweight encoder for precise human keypoint detection, a fuser for efficient gradient propagation, and a decoder that transforms human keypoints into reconstructed pressure maps. This structure is further enhanced by the Classification-to-Regression Weight Transfer (CRWT) method, which fine-tunes accuracy through initial classification task training. This innovation not only enhances human keypoint generalization without manual annotations but also showcases remarkable efficiency and generalization, evidenced by a reduction to only $5.96\\%$ in FLOPs and $1.11\\%$ in parameter count compared to the baseline methods.|\u5728 RGB \u56fe\u50cf\u4e0d\u8db3\u7684\u73af\u5883\u4e2d\uff0c\u538b\u529b\u56fe\u662f\u4e00\u79cd\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5f15\u8d77\u4e86\u5b66\u672f\u754c\u7684\u5173\u6ce8\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u538b\u529b\u56fe\u5173\u952e\u70b9\u68c0\u6d4b\uff08SPMKD\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u4ece\u538b\u529b\u56fe\u4e2d\u63d0\u53d6\u4eba\u4f53\u5173\u952e\u70b9\u7684\u4e13\u95e8\u8bbe\u8ba1\u4e2d\u7684\u5dee\u8ddd\u3002\u6211\u4eec\u8d21\u732e\u7684\u6838\u5fc3\u662f\u7f16\u7801\u5668-\u878d\u5408\u5668-\u89e3\u7801\u5668\uff08EFD\uff09\u6a21\u578b\uff0c\u5b83\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u7528\u4e8e\u7cbe\u786e\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u3001\u7528\u4e8e\u9ad8\u6548\u68af\u5ea6\u4f20\u64ad\u7684\u878d\u5408\u5668\u4ee5\u53ca\u5c06\u4eba\u4f53\u5173\u952e\u70b9\u8f6c\u6362\u4e3a\u91cd\u5efa\u538b\u529b\u56fe\u7684\u89e3\u7801\u5668\u3002\u8fd9\u79cd\u7ed3\u6784\u901a\u8fc7\u5206\u7c7b\u5230\u56de\u5f52\u6743\u91cd\u8f6c\u79fb\uff08CRWT\uff09\u65b9\u6cd5\u8fdb\u4e00\u6b65\u589e\u5f3a\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u521d\u59cb\u5206\u7c7b\u4efb\u52a1\u8bad\u7ec3\u6765\u5fae\u8c03\u51c6\u786e\u6027\u3002\u8fd9\u9879\u521b\u65b0\u4e0d\u4ec5\u5728\u65e0\u9700\u624b\u52a8\u6ce8\u91ca\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u4e86\u4eba\u7c7b\u5173\u952e\u70b9\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u4e14\u8fd8\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cFLOP \u6b21\u6570\u51cf\u5c11\u81f3\u4ec5 $5.96\\%$\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u81f3 $1.11\\%$\u3002|[2402.14241v1](http://arxiv.org/pdf/2402.14241v1)|null|\n", "2402.14205": "|**2024-02-22**|**Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer**|\u4f7f\u7528\u4fee\u8865\u9891\u8c31\u56fe\u53d8\u538b\u5668\u7684\u538b\u7f29\u9c81\u68d2\u5408\u6210\u8bed\u97f3\u68c0\u6d4b|Amit Kumar Singh Yadav, Ziyue Xiang, Kratika Bhagtani, Paolo Bestagini, Stefano Tubaro, Edward J. Delp|Many deep learning synthetic speech generation tools are readily available. The use of synthetic speech has caused financial fraud, impersonation of people, and misinformation to spread. For this reason forensic methods that can detect synthetic speech have been proposed. Existing methods often overfit on one dataset and their performance reduces substantially in practical scenarios such as detecting synthetic speech shared on social platforms. In this paper we propose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a synthetic speech detector that converts a time domain speech signal to a mel-spectrogram and processes it in patches using a transformer neural network. We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our experiments show that PS3DT performs well on ASVspoof2019 dataset compared to other approaches using spectrogram for synthetic speech detection. We also investigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT generalizes well than several existing methods on detecting synthetic speech from an out-of-distribution dataset. We also evaluate robustness of PS3DT to detect telephone quality synthetic speech and synthetic speech shared on social platforms (compressed speech). PS3DT is robust to compression and can detect telephone quality synthetic speech better than several existing methods.|\u8bb8\u591a\u6df1\u5ea6\u5b66\u4e60\u5408\u6210\u8bed\u97f3\u751f\u6210\u5de5\u5177\u90fd\u5f88\u5bb9\u6613\u83b7\u5f97\u3002\u5408\u6210\u8bed\u97f3\u7684\u4f7f\u7528\u5bfc\u81f4\u4e86\u91d1\u878d\u6b3a\u8bc8\u3001\u5192\u5145\u4ed6\u4eba\u548c\u9519\u8bef\u4fe1\u606f\u7684\u4f20\u64ad\u3002\u56e0\u6b64\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u53ef\u4ee5\u68c0\u6d4b\u5408\u6210\u8bed\u97f3\u7684\u53d6\u8bc1\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u8fc7\u5ea6\u62df\u5408\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u573a\u666f\u4e2d\uff08\u4f8b\u5982\u68c0\u6d4b\u793e\u4ea4\u5e73\u53f0\u4e0a\u5171\u4eab\u7684\u5408\u6210\u8bed\u97f3\uff09\uff0c\u5176\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8865\u4e01\u9891\u8c31\u56fe\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u53d8\u538b\u5668\uff08PS3DT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u5668\uff0c\u53ef\u5c06\u65f6\u57df\u8bed\u97f3\u4fe1\u53f7\u8f6c\u6362\u4e3a\u6885\u5c14\u9891\u8c31\u56fe\uff0c\u5e76\u4f7f\u7528\u53d8\u538b\u5668\u795e\u7ecf\u7f51\u7edc\u5bf9\u5176\u8fdb\u884c\u8865\u4e01\u5904\u7406\u3002\u6211\u4eec\u8bc4\u4f30\u4e86 PS3DT \u5728 ASVspoof2019 \u6570\u636e\u96c6\u4e0a\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f7f\u7528\u9891\u8c31\u56fe\u8fdb\u884c\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u7684\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\uff0cPS3DT \u5728 ASVspoof2019 \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86 PS3DT \u5728 In-the-Wild \u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u3002 PS3DT \u6bd4\u51e0\u79cd\u73b0\u6709\u7684\u4ece\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e2d\u68c0\u6d4b\u5408\u6210\u8bed\u97f3\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u8fd8\u8bc4\u4f30\u4e86 PS3DT \u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u68c0\u6d4b\u7535\u8bdd\u8d28\u91cf\u5408\u6210\u8bed\u97f3\u548c\u793e\u4ea4\u5e73\u53f0\u4e0a\u5171\u4eab\u7684\u5408\u6210\u8bed\u97f3\uff08\u538b\u7f29\u8bed\u97f3\uff09\u3002 PS3DT \u5bf9\u538b\u7f29\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u6bd4\u51e0\u79cd\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u5730\u68c0\u6d4b\u7535\u8bdd\u8d28\u91cf\u5408\u6210\u8bed\u97f3\u3002|[2402.14205v1](http://arxiv.org/pdf/2402.14205v1)|null|\n", "2402.14185": "|**2024-02-22**|**HINT: High-quality INPainting Transformer with Mask-Aware Encoding and Enhanced Attention**|\u63d0\u793a\uff1a\u5177\u6709\u63a9\u6a21\u611f\u77e5\u7f16\u7801\u548c\u589e\u5f3a\u6ce8\u610f\u529b\u7684\u9ad8\u8d28\u91cf INPainting Transformer|Shuang Chen, Amir Atapour-Abarghouei, Hubert P. H. Shum|Existing image inpainting methods leverage convolution-based downsampling approaches to reduce spatial dimensions. This may result in information loss from corrupted images where the available information is inherently sparse, especially for the scenario of large missing regions. Recent advances in self-attention mechanisms within transformers have led to significant improvements in many computer vision tasks including inpainting. However, limited by the computational costs, existing methods cannot fully exploit the efficacy of long-range modelling capabilities of such models. In this paper, we propose an end-to-end High-quality INpainting Transformer, abbreviated as HINT, which consists of a novel mask-aware pixel-shuffle downsampling module (MPD) to preserve the visible information extracted from the corrupted image while maintaining the integrity of the information available for high-level inferences made within the model. Moreover, we propose a Spatially-activated Channel Attention Layer (SCAL), an efficient self-attention mechanism interpreting spatial awareness to model the corrupted image at multiple scales. To further enhance the effectiveness of SCAL, motivated by recent advanced in speech recognition, we introduce a sandwich structure that places feed-forward networks before and after the SCAL module. We demonstrate the superior performance of HINT compared to contemporary state-of-the-art models on four datasets, CelebA, CelebA-HQ, Places2, and Dunhuang.|\u73b0\u6709\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5229\u7528\u57fa\u4e8e\u5377\u79ef\u7684\u4e0b\u91c7\u6837\u65b9\u6cd5\u6765\u51cf\u5c11\u7a7a\u95f4\u7ef4\u5ea6\u3002\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u635f\u574f\u56fe\u50cf\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\uff0c\u5176\u4e2d\u53ef\u7528\u4fe1\u606f\u672c\u8d28\u4e0a\u662f\u7a00\u758f\u7684\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u9762\u79ef\u7f3a\u5931\u533a\u57df\u7684\u60c5\u51b5\u3002 Transformer \u5185\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6700\u65b0\u8fdb\u5c55\u5bfc\u81f4\u4e86\u5305\u62ec\u4fee\u590d\u5728\u5185\u7684\u8bb8\u591a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u663e\u7740\u6539\u8fdb\u3002\u7136\u800c\uff0c\u53d7\u8ba1\u7b97\u6210\u672c\u7684\u9650\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6b64\u7c7b\u6a21\u578b\u7684\u8fdc\u7a0b\u5efa\u6a21\u80fd\u529b\u7684\u529f\u6548\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u9ad8\u8d28\u91cf INpainting Transformer\uff0c\u7f29\u5199\u4e3a HINT\uff0c\u5b83\u7531\u4e00\u79cd\u65b0\u9896\u7684\u63a9\u6a21\u611f\u77e5\u50cf\u7d20\u6d17\u724c\u4e0b\u91c7\u6837\u6a21\u5757\uff08MPD\uff09\u7ec4\u6210\uff0c\u7528\u4e8e\u4fdd\u7559\u4ece\u635f\u574f\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u53ef\u89c1\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u7528\u4e8e\u6a21\u578b\u5185\u8fdb\u884c\u9ad8\u7ea7\u63a8\u8bba\u7684\u4fe1\u606f\u7684\u5b8c\u6574\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7a7a\u95f4\u6fc0\u6d3b\u901a\u9053\u6ce8\u610f\u5c42\uff08SCAL\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u6ce8\u610f\u673a\u5236\uff0c\u89e3\u91ca\u7a7a\u95f4\u610f\u8bc6\u4ee5\u5728\u591a\u4e2a\u5c3a\u5ea6\u4e0a\u5bf9\u635f\u574f\u7684\u56fe\u50cf\u8fdb\u884c\u5efa\u6a21\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8 SCAL \u7684\u6709\u6548\u6027\uff0c\u53d7\u8bed\u97f3\u8bc6\u522b\u9886\u57df\u6700\u65b0\u8fdb\u5c55\u7684\u63a8\u52a8\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4e09\u660e\u6cbb\u7ed3\u6784\uff0c\u5c06\u524d\u9988\u7f51\u7edc\u653e\u7f6e\u5728 SCAL \u6a21\u5757\u4e4b\u524d\u548c\u4e4b\u540e\u3002\u6211\u4eec\u5728 CelebA\u3001CelebA-HQ\u3001Places2 \u548c Dunhuang \u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86 HINT \u4e0e\u5f53\u4ee3\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u6bd4\u7684\u5353\u8d8a\u6027\u80fd\u3002|[2402.14185v1](http://arxiv.org/pdf/2402.14185v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "LLM": {}, "Transformer": {"2402.14797": "|**2024-02-22**|**Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis**|Snap Video\uff1a\u7528\u4e8e\u6587\u672c\u5230\u89c6\u9891\u5408\u6210\u7684\u7f29\u653e\u65f6\u7a7a\u8f6c\u6362\u5668|Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et.al.|Contemporary models for generating images show remarkable quality and versatility. Swayed by these advantages, the research community repurposes them to generate videos. Since video content is highly redundant, we argue that naively bringing advances of image models to the video generation domain reduces motion fidelity, visual quality and impairs scalability. In this work, we build Snap Video, a video-first model that systematically addresses these challenges. To do that, we first extend the EDM framework to take into account spatially and temporally redundant pixels and naturally support video generation. Second, we show that a U-Net - a workhorse behind image generation - scales poorly when generating videos, requiring significant computational overhead. Hence, we propose a new transformer-based architecture that trains 3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us to efficiently train a text-to-video model with billions of parameters for the first time, reach state-of-the-art results on a number of benchmarks, and generate videos with substantially higher quality, temporal consistency, and motion complexity. The user studies showed that our model was favored by a large margin over the most recent methods. See our website at https://snap-research.github.io/snapvideo/.|\u7528\u4e8e\u751f\u6210\u56fe\u50cf\u7684\u5f53\u4ee3\u6a21\u578b\u663e\u793a\u51fa\u5353\u8d8a\u7684\u8d28\u91cf\u548c\u591a\u529f\u80fd\u6027\u3002\u53d7\u8fd9\u4e9b\u4f18\u52bf\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u754c\u5c06\u5b83\u4eec\u91cd\u65b0\u7528\u4e8e\u751f\u6210\u89c6\u9891\u3002\u7531\u4e8e\u89c6\u9891\u5185\u5bb9\u9ad8\u5ea6\u5197\u4f59\uff0c\u6211\u4eec\u8ba4\u4e3a\u5929\u771f\u5730\u5c06\u56fe\u50cf\u6a21\u578b\u7684\u8fdb\u6b65\u5f15\u5165\u89c6\u9891\u751f\u6210\u9886\u57df\u4f1a\u964d\u4f4e\u8fd0\u52a8\u4fdd\u771f\u5ea6\u3001\u89c6\u89c9\u8d28\u91cf\u5e76\u635f\u5bb3\u53ef\u6269\u5c55\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u6784\u5efa\u4e86 Snap Video\uff0c\u8fd9\u662f\u4e00\u79cd\u89c6\u9891\u4f18\u5148\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u7cfb\u7edf\u5730\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u6269\u5c55 EDM \u6846\u67b6\u4ee5\u8003\u8651\u7a7a\u95f4\u548c\u65f6\u95f4\u5197\u4f59\u50cf\u7d20\u5e76\u81ea\u7136\u652f\u6301\u89c6\u9891\u751f\u6210\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5c55\u793a\u4e86 U-Net\uff08\u56fe\u50cf\u751f\u6210\u80cc\u540e\u7684\u4e3b\u529b\uff09\u5728\u751f\u6210\u89c6\u9891\u65f6\u6269\u5c55\u6027\u5f88\u5dee\uff0c\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e Transformer \u7684\u67b6\u6784\uff0c\u5176\u8bad\u7ec3\u901f\u5ea6\u6bd4 U-Net \u5feb 3.31 \u500d\uff08\u63a8\u7406\u901f\u5ea6\u5feb\u7ea6 4.5 \u500d\uff09\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u9996\u6b21\u6709\u6548\u5730\u8bad\u7ec3\u5177\u6709\u6570\u5341\u4ebf\u4e2a\u53c2\u6570\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u5728\u8bb8\u591a\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u751f\u6210\u5177\u6709\u66f4\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u7684\u89c6\u9891\u590d\u6742\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u6bd4\u6700\u65b0\u7684\u65b9\u6cd5\u66f4\u53d7\u9752\u7750\u3002\u8bf7\u53c2\u9605\u6211\u4eec\u7684\u7f51\u7ad9 https://snap-research.github.io/snapvideo/\u3002|[2402.14797v1](http://arxiv.org/pdf/2402.14797v1)|null|\n", "2402.14654": "|**2024-02-22**|**Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot**|Multi-HMR\uff1a\u5355\u6b21\u591a\u4eba\u5168\u8eab\u4eba\u4f53\u7f51\u683c\u6062\u590d|Fabien Baradel, Matthieu Armando, Salma Galaaoui, Romain Br\u00e9gier, Philippe Weinzaepfel, Gr\u00e9gory Rogez, Thomas Lucas|We present Multi-HMR, a strong single-shot model for multi-person 3D human mesh recovery from a single RGB image. Predictions encompass the whole body, i.e, including hands and facial expressions, using the SMPL-X parametric model and spatial location in the camera coordinate system. Our model detects people by predicting coarse 2D heatmaps of person centers, using features produced by a standard Vision Transformer (ViT) backbone. It then predicts their whole-body pose, shape and spatial location using a new cross-attention module called the Human Prediction Head (HPH), with one query per detected center token, attending to the entire set of features. As direct prediction of SMPL-X parameters yields suboptimal results, we introduce CUFFS; the Close-Up Frames of Full-Body Subjects dataset, containing humans close to the camera with diverse hand poses. We show that incorporating this dataset into training further enhances predictions, particularly for hands, enabling us to achieve state-of-the-art performance. Multi-HMR also optionally accounts for camera intrinsics, if available, by encoding camera ray directions for each image token. This simple design achieves strong performance on whole-body and body-only benchmarks simultaneously. We train models with various backbone sizes and input resolutions. In particular, using a ViT-S backbone and $448\\times448$ input images already yields a fast and competitive model with respect to state-of-the-art methods, while considering larger models and higher resolutions further improve performance.|\u6211\u4eec\u63a8\u51fa\u4e86 Multi-HMR\uff0c\u8fd9\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u5355\u6b21\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u5355\u4e2a RGB \u56fe\u50cf\u6062\u590d\u591a\u4eba 3D \u4eba\u4f53\u7f51\u683c\u3002\u4f7f\u7528 SMPL-X \u53c2\u6570\u6a21\u578b\u548c\u76f8\u673a\u5750\u6807\u7cfb\u4e2d\u7684\u7a7a\u95f4\u4f4d\u7f6e\uff0c\u9884\u6d4b\u6db5\u76d6\u6574\u4e2a\u8eab\u4f53\uff0c\u5373\u5305\u62ec\u624b\u548c\u9762\u90e8\u8868\u60c5\u3002\u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7\u4f7f\u7528\u6807\u51c6 Vision Transformer (ViT) \u4e3b\u5e72\u751f\u6210\u7684\u7279\u5f81\u6765\u9884\u6d4b\u4eba\u5458\u4e2d\u5fc3\u7684\u7c97\u7565 2D \u70ed\u56fe\u6765\u68c0\u6d4b\u4eba\u5458\u3002\u7136\u540e\uff0c\u5b83\u4f7f\u7528\u79f0\u4e3a\u4eba\u7c7b\u9884\u6d4b\u5934 (HPH) \u7684\u65b0\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u6765\u9884\u6d4b\u4ed6\u4eec\u7684\u5168\u8eab\u59ff\u52bf\u3001\u5f62\u72b6\u548c\u7a7a\u95f4\u4f4d\u7f6e\uff0c\u6bcf\u4e2a\u68c0\u6d4b\u5230\u7684\u4e2d\u5fc3\u6807\u8bb0\u6709\u4e00\u4e2a\u67e5\u8be2\uff0c\u5173\u6ce8\u6574\u7ec4\u7279\u5f81\u3002\u7531\u4e8e\u76f4\u63a5\u9884\u6d4b SMPL-X \u53c2\u6570\u4f1a\u4ea7\u751f\u6b21\u4f18\u7ed3\u679c\uff0c\u56e0\u6b64\u6211\u4eec\u5f15\u5165\u4e86 CUFFS\uff1b\u5168\u8eab\u4e3b\u4f53\u7279\u5199\u955c\u5934\u6570\u636e\u96c6\uff0c\u5305\u542b\u9760\u8fd1\u76f8\u673a\u4e14\u5177\u6709\u4e0d\u540c\u624b\u52bf\u7684\u4eba\u7c7b\u3002\u6211\u4eec\u8868\u660e\uff0c\u5c06\u8be5\u6570\u636e\u96c6\u7eb3\u5165\u8bad\u7ec3\u4e2d\u53ef\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u624b\u90e8\u7684\u9884\u6d4b\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5982\u679c\u53ef\u7528\uff0cMulti-HMR \u8fd8\u53ef\u4ee5\u901a\u8fc7\u5bf9\u6bcf\u4e2a\u56fe\u50cf\u6807\u8bb0\u7684\u76f8\u673a\u5149\u7ebf\u65b9\u5411\u8fdb\u884c\u7f16\u7801\u6765\u8003\u8651\u76f8\u673a\u5185\u5728\u7279\u6027\u3002\u8fd9\u79cd\u7b80\u5355\u7684\u8bbe\u8ba1\u540c\u65f6\u5728\u5168\u8eab\u548c\u4ec5\u8eab\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002\u6211\u4eec\u8bad\u7ec3\u5177\u6709\u5404\u79cd\u4e3b\u5e72\u5c3a\u5bf8\u548c\u8f93\u5165\u5206\u8fa8\u7387\u7684\u6a21\u578b\u3002\u7279\u522b\u662f\uff0c\u4f7f\u7528 ViT-S \u4e3b\u5e72\u548c 448\\times448$ \u8f93\u5165\u56fe\u50cf\u5df2\u7ecf\u4ea7\u751f\u4e86\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u800c\u8a00\u5feb\u901f\u4e14\u6709\u7ade\u4e89\u529b\u7684\u6a21\u578b\uff0c\u540c\u65f6\u8003\u8651\u66f4\u5927\u7684\u6a21\u578b\u548c\u66f4\u9ad8\u5206\u8fa8\u7387\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002|[2402.14654v1](http://arxiv.org/pdf/2402.14654v1)|null|\n", "2402.14454": "|**2024-02-22**|**CCPA: Long-term Person Re-Identification via Contrastive Clothing and Pose Augmentation**|CCPA\uff1a\u901a\u8fc7\u5bf9\u6bd4\u670d\u88c5\u548c\u59ff\u52bf\u589e\u5f3a\u8fdb\u884c\u957f\u671f\u4eba\u5458\u91cd\u65b0\u8bc6\u522b|Vuong D. Nguyen, Shishir K. Shah|Long-term Person Re-Identification (LRe-ID) aims at matching an individual across cameras after a long period of time, presenting variations in clothing, pose, and viewpoint. In this work, we propose CCPA: Contrastive Clothing and Pose Augmentation framework for LRe-ID. Beyond appearance, CCPA captures body shape information which is cloth-invariant using a Relation Graph Attention Network. Training a robust LRe-ID model requires a wide range of clothing variations and expensive cloth labeling, which is lacked in current LRe-ID datasets. To address this, we perform clothing and pose transfer across identities to generate images of more clothing variations and of different persons wearing similar clothing. The augmented batch of images serve as inputs to our proposed Fine-grained Contrastive Losses, which not only supervise the Re-ID model to learn discriminative person embeddings under long-term scenarios but also ensure in-distribution data generation. Results on LRe-ID datasets demonstrate the effectiveness of our CCPA framework.|\u957f\u671f\u4eba\u5458\u91cd\u65b0\u8bc6\u522b\uff08LRe-ID\uff09\u65e8\u5728\u5728\u5f88\u957f\u4e00\u6bb5\u65f6\u95f4\u540e\u901a\u8fc7\u6444\u50cf\u673a\u5339\u914d\u4e2a\u4eba\uff0c\u5448\u73b0\u670d\u88c5\u3001\u59ff\u52bf\u548c\u89c2\u70b9\u7684\u53d8\u5316\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 CCPA\uff1aLRe-ID \u7684\u5bf9\u6bd4\u670d\u88c5\u548c\u59ff\u52bf\u589e\u5f3a\u6846\u67b6\u3002\u9664\u4e86\u5916\u89c2\u4e4b\u5916\uff0cCCPA \u4f7f\u7528\u5173\u7cfb\u56fe\u6ce8\u610f\u7f51\u7edc\u6355\u83b7\u8863\u670d\u4e0d\u53d8\u7684\u8eab\u4f53\u5f62\u72b6\u4fe1\u606f\u3002\u8bad\u7ec3\u5f3a\u5927\u7684 LRe-ID \u6a21\u578b\u9700\u8981\u5e7f\u6cdb\u7684\u670d\u88c5\u53d8\u5316\u548c\u6602\u8d35\u7684\u5e03\u6599\u6807\u7b7e\uff0c\u800c\u5f53\u524d\u7684 LRe-ID \u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u8fd9\u4e9b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u8de8\u8eab\u4efd\u8fdb\u884c\u670d\u88c5\u548c\u59ff\u52bf\u8f6c\u79fb\uff0c\u4ee5\u751f\u6210\u66f4\u591a\u670d\u88c5\u53d8\u5316\u4ee5\u53ca\u7a7f\u7740\u76f8\u4f3c\u670d\u88c5\u7684\u4e0d\u540c\u4eba\u7684\u56fe\u50cf\u3002\u589e\u5f3a\u540e\u7684\u56fe\u50cf\u6279\u6b21\u4f5c\u4e3a\u6211\u4eec\u63d0\u51fa\u7684\u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u635f\u5931\u7684\u8f93\u5165\uff0c\u5b83\u4e0d\u4ec5\u76d1\u7763 Re-ID \u6a21\u578b\u4ee5\u5b66\u4e60\u957f\u671f\u573a\u666f\u4e0b\u7684\u6709\u533a\u522b\u7684\u4eba\u7269\u5d4c\u5165\uff0c\u800c\u4e14\u8fd8\u786e\u4fdd\u5206\u5e03\u6570\u636e\u751f\u6210\u3002 LRe-ID \u6570\u636e\u96c6\u7684\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684 CCPA \u6846\u67b6\u7684\u6709\u6548\u6027\u3002|[2402.14454v1](http://arxiv.org/pdf/2402.14454v1)|null|\n", "2402.14313": "|**2024-02-22**|**Learning to Kern -- Set-wise Estimation of Optimal Letter Space**|\u5b66\u4e60\u7d27\u7f29\u2014\u2014\u6700\u4f73\u5b57\u6bcd\u7a7a\u95f4\u7684\u96c6\u5408\u4f30\u8ba1|Kei Nakatsuru, Seiichi Uchida|Kerning is the task of setting appropriate horizontal spaces for all possible letter pairs of a certain font. One of the difficulties of kerning is that the appropriate space differs for each letter pair. Therefore, for a total of 52 capital and small letters, we need to adjust $52 \\times 52 = 2704$ different spaces. Another difficulty is that there is neither a general procedure nor criterion for automatic kerning; therefore, kerning is still done manually or with heuristics. In this paper, we tackle kerning by proposing two machine-learning models, called pairwise and set-wise models. The former is a simple deep neural network that estimates the letter space for two given letter images. In contrast, the latter is a Transformer-based model and estimates the letter spaces for three or more given letter images. For example, the set-wise model simultaneously estimates 2704 spaces for 52 letter images for a certain font. Among the two models, the set-wise model is not only more efficient but also more accurate because its internal self-attention mechanism allows for more consistent kerning for all letters. Experimental results on about 2500 Google fonts and their quantitative and qualitative analyses show that the set-wise model has an average estimation error of only about 5.3 pixels when the average letter space of all fonts and letter pairs is about 115 pixels.|\u5b57\u8ddd\u8c03\u6574\u662f\u4e3a\u67d0\u79cd\u5b57\u4f53\u7684\u6240\u6709\u53ef\u80fd\u7684\u5b57\u6bcd\u5bf9\u8bbe\u7f6e\u9002\u5f53\u7684\u6c34\u5e73\u95f4\u8ddd\u7684\u4efb\u52a1\u3002\u5b57\u8ddd\u8c03\u6574\u7684\u56f0\u96be\u4e4b\u4e00\u662f\u6bcf\u4e2a\u5b57\u6bcd\u5bf9\u7684\u9002\u5f53\u95f4\u8ddd\u4e0d\u540c\u3002\u56e0\u6b64\uff0c\u603b\u517152\u4e2a\u5927\u5c0f\u5199\u5b57\u6bcd\uff0c\u6211\u4eec\u9700\u8981\u8c03\u6574$52 \\times 52 = 2704$\u4e0d\u540c\u7684\u7a7a\u683c\u3002\u53e6\u4e00\u4e2a\u56f0\u96be\u662f\u81ea\u52a8\u5b57\u8ddd\u8c03\u6574\u65e2\u6ca1\u6709\u901a\u7528\u7684\u7a0b\u5e8f\u4e5f\u6ca1\u6709\u6807\u51c6\u3002\u56e0\u6b64\uff0c\u5b57\u8ddd\u8c03\u6574\u4ecd\u7136\u662f\u624b\u52a8\u6216\u542f\u53d1\u5f0f\u5b8c\u6210\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u4e24\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u89e3\u51b3\u5b57\u8ddd\u8c03\u6574\u95ee\u9898\uff0c\u79f0\u4e3a\u6210\u5bf9\u6a21\u578b\u548c\u96c6\u5408\u6a21\u578b\u3002\u524d\u8005\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u4f30\u8ba1\u4e24\u4e2a\u7ed9\u5b9a\u5b57\u6bcd\u56fe\u50cf\u7684\u5b57\u6bcd\u7a7a\u95f4\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u540e\u8005\u662f\u57fa\u4e8e Transformer \u7684\u6a21\u578b\uff0c\u53ef\u4f30\u8ba1\u4e09\u4e2a\u6216\u66f4\u591a\u7ed9\u5b9a\u5b57\u6bcd\u56fe\u50cf\u7684\u5b57\u6bcd\u7a7a\u95f4\u3002\u4f8b\u5982\uff0cset-wise \u6a21\u578b\u540c\u65f6\u4f30\u8ba1\u67d0\u79cd\u5b57\u4f53\u7684 52 \u4e2a\u5b57\u6bcd\u56fe\u50cf\u7684 2704 \u4e2a\u7a7a\u683c\u3002\u5728\u8fd9\u4e24\u4e2a\u6a21\u578b\u4e2d\uff0cset-wise \u6a21\u578b\u4e0d\u4ec5\u66f4\u9ad8\u6548\uff0c\u800c\u4e14\u66f4\u51c6\u786e\uff0c\u56e0\u4e3a\u5176\u5185\u90e8\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5141\u8bb8\u6240\u6709\u5b57\u6bcd\u7684\u5b57\u8ddd\u8c03\u6574\u66f4\u52a0\u4e00\u81f4\u3002\u5bf9\u7ea62500\u79cd\u8c37\u6b4c\u5b57\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u53ca\u5176\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u5f53\u6240\u6709\u5b57\u4f53\u548c\u5b57\u6bcd\u5bf9\u7684\u5e73\u5747\u5b57\u6bcd\u95f4\u8ddd\u7ea6\u4e3a115\u50cf\u7d20\u65f6\uff0cset-wise\u6a21\u578b\u7684\u5e73\u5747\u4f30\u8ba1\u8bef\u5dee\u4ec5\u4e3a\u7ea65.3\u50cf\u7d20\u3002|[2402.14313v1](http://arxiv.org/pdf/2402.14313v1)|null|\n"}, "3D/CG": {"2402.14642": "|**2024-02-22**|**Distributed Radiance Fields for Edge Video Compression and Metaverse Integration in Autonomous Driving**|\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8fb9\u7f18\u89c6\u9891\u538b\u7f29\u548c\u5143\u5b87\u5b99\u96c6\u6210\u7684\u5206\u5e03\u5f0f\u8f90\u5c04\u573a|Eugen \u0160lapak, Mat\u00fa\u0161 Dopiriak, Mohammad Abdullah Al Faruque, Juraj Gazda, Marco Levorato|The metaverse is a virtual space that combines physical and digital elements, creating immersive and connected digital worlds. For autonomous mobility, it enables new possibilities with edge computing and digital twins (DTs) that offer virtual prototyping, prediction, and more. DTs can be created with 3D scene reconstruction methods that capture the real world's geometry, appearance, and dynamics. However, sending data for real-time DT updates in the metaverse, such as camera images and videos from connected autonomous vehicles (CAVs) to edge servers, can increase network congestion, costs, and latency, affecting metaverse services. Herein, a new method is proposed based on distributed radiance fields (RFs), multi-access edge computing (MEC) network for video compression and metaverse DT updates. RF-based encoder and decoder are used to create and restore representations of camera images. The method is evaluated on a dataset of camera images from the CARLA simulator. Data savings of up to 80% were achieved for H.264 I-frame - P-frame pairs by using RFs instead of I-frames, while maintaining high peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) qualitative metrics for the reconstructed images. Possible uses and challenges for the metaverse and autonomous mobility are also discussed.|\u865a\u62df\u5b87\u5b99\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u7269\u7406\u548c\u6570\u5b57\u5143\u7d20\u7684\u865a\u62df\u7a7a\u95f4\uff0c\u521b\u9020\u4e86\u6c89\u6d78\u5f0f\u3001\u4e92\u8054\u7684\u6570\u5b57\u4e16\u754c\u3002\u5bf9\u4e8e\u81ea\u4e3b\u79fb\u52a8\u6027\uff0c\u5b83\u901a\u8fc7\u8fb9\u7f18\u8ba1\u7b97\u548c\u63d0\u4f9b\u865a\u62df\u539f\u578b\u3001\u9884\u6d4b\u7b49\u529f\u80fd\u7684\u6570\u5b57\u5b6a\u751f (DT) \u5b9e\u73b0\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002 DT \u53ef\u4ee5\u4f7f\u7528 3D \u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u6765\u521b\u5efa\uff0c\u4ee5\u6355\u83b7\u73b0\u5b9e\u4e16\u754c\u7684\u51e0\u4f55\u5f62\u72b6\u3001\u5916\u89c2\u548c\u52a8\u6001\u3002\u7136\u800c\uff0c\u5728\u5143\u5b87\u5b99\u4e2d\u53d1\u9001\u5b9e\u65f6 DT \u66f4\u65b0\u6570\u636e\uff08\u4f8b\u5982\u4ece\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86 (CAV) \u5230\u8fb9\u7f18\u670d\u52a1\u5668\u7684\u6444\u50cf\u5934\u56fe\u50cf\u548c\u89c6\u9891\uff09\u53ef\u80fd\u4f1a\u589e\u52a0\u7f51\u7edc\u62e5\u585e\u3001\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u4ece\u800c\u5f71\u54cd\u5143\u5b87\u5b99\u670d\u52a1\u3002\u5728\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5f0f\u8f90\u5c04\u573a\uff08RF\uff09\u3001\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u7f51\u7edc\u7684\u89c6\u9891\u538b\u7f29\u548c\u5143\u8282DT\u66f4\u65b0\u7684\u65b0\u65b9\u6cd5\u3002\u57fa\u4e8e RF \u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7528\u4e8e\u521b\u5efa\u548c\u6062\u590d\u76f8\u673a\u56fe\u50cf\u7684\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5728\u6765\u81ea CARLA \u6a21\u62df\u5668\u7684\u76f8\u673a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u901a\u8fc7\u4f7f\u7528 RF \u4ee3\u66ff I \u5e27\uff0cH.264 I \u5e27 - P \u5e27\u5bf9\u53ef\u8282\u7701\u9ad8\u8fbe 80% \u7684\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5cf0\u503c\u4fe1\u566a\u6bd4 (PSNR) \u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u6d4b\u91cf (SSIM) \uff09\u91cd\u5efa\u56fe\u50cf\u7684\u5b9a\u6027\u6307\u6807\u3002\u8fd8\u8ba8\u8bba\u4e86\u865a\u62df\u5b87\u5b99\u548c\u81ea\u4e3b\u79fb\u52a8\u6027\u7684\u53ef\u80fd\u7528\u9014\u548c\u6311\u6218\u3002|[2402.14642v1](http://arxiv.org/pdf/2402.14642v1)|null|\n", "2402.14316": "|**2024-02-22**|**Place Anything into Any Video**|\u5c06\u4efb\u4f55\u5185\u5bb9\u653e\u5165\u4efb\u4f55\u89c6\u9891\u4e2d|Ziling Liu, Jinyu Yang, Mingqi Gao, Feng Zheng|Controllable video editing has demonstrated remarkable potential across diverse applications, particularly in scenarios where capturing or re-capturing real-world videos is either impractical or costly. This paper introduces a novel and efficient system named Place-Anything, which facilitates the insertion of any object into any video solely based on a picture or text description of the target object or element. The system comprises three modules: 3D generation, video reconstruction, and 3D target insertion. This integrated approach offers an efficient and effective solution for producing and editing high-quality videos by seamlessly inserting realistic objects. Through a user study, we demonstrate that our system can effortlessly place any object into any video using just a photograph of the object. Our demo video can be found at https://youtu.be/afXqgLLRnTE. Please also visit our project page https://place-anything.github.io to get access.|\u53ef\u63a7\u89c6\u9891\u7f16\u8f91\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6355\u83b7\u6216\u91cd\u65b0\u6355\u83b7\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u4e0d\u5207\u5b9e\u9645\u6216\u6210\u672c\u9ad8\u6602\u7684\u60c5\u51b5\u4e0b\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a Place-Anything \u7684\u65b0\u9896\u800c\u9ad8\u6548\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u53ef\u4ee5\u4ec5\u6839\u636e\u76ee\u6807\u5bf9\u8c61\u6216\u5143\u7d20\u7684\u56fe\u7247\u6216\u6587\u672c\u63cf\u8ff0\u5c06\u4efb\u4f55\u5bf9\u8c61\u63d2\u5165\u5230\u4efb\u4f55\u89c6\u9891\u4e2d\u3002\u8be5\u7cfb\u7edf\u5305\u62ec\u4e09\u4e2a\u6a21\u5757\uff1a3D\u751f\u6210\u3001\u89c6\u9891\u91cd\u5efa\u548c3D\u76ee\u6807\u63d2\u5165\u3002\u8fd9\u79cd\u96c6\u6210\u65b9\u6cd5\u901a\u8fc7\u65e0\u7f1d\u63d2\u5165\u903c\u771f\u7684\u5bf9\u8c61\uff0c\u4e3a\u5236\u4f5c\u548c\u7f16\u8f91\u9ad8\u8d28\u91cf\u89c6\u9891\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff0c\u6211\u4eec\u8bc1\u660e\u6211\u4eec\u7684\u7cfb\u7edf\u53ea\u9700\u4f7f\u7528\u5bf9\u8c61\u7684\u7167\u7247\u5373\u53ef\u8f7b\u677e\u5730\u5c06\u4efb\u4f55\u5bf9\u8c61\u653e\u5165\u4efb\u4f55\u89c6\u9891\u4e2d\u3002\u6211\u4eec\u7684\u6f14\u793a\u89c6\u9891\u53ef\u4ee5\u5728 https://youtu.be/afXqgLLRnTE \u627e\u5230\u3002\u53e6\u8bf7\u8bbf\u95ee\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762 https://place-anything.github.io \u8fdb\u884c\u8bbf\u95ee\u3002|[2402.14316v1](http://arxiv.org/pdf/2402.14316v1)|null|\n", "2402.14215": "|**2024-02-22**|**Swin3D++: Effective Multi-Source Pretraining for 3D Indoor Scene Understanding**|Swin3D++\uff1a\u7528\u4e8e 3D \u5ba4\u5185\u573a\u666f\u7406\u89e3\u7684\u6709\u6548\u591a\u6e90\u9884\u8bad\u7ec3|Yu-Qi Yang, Yu-Xiao Guo, Yang Liu|Data diversity and abundance are essential for improving the performance and generalization of models in natural language processing and 2D vision. However, 3D vision domain suffers from the lack of 3D data, and simply combining multiple 3D datasets for pretraining a 3D backbone does not yield significant improvement, due to the domain discrepancies among different 3D datasets that impede effective feature learning. In this work, we identify the main sources of the domain discrepancies between 3D indoor scene datasets, and propose Swin3D++, an enhanced architecture based on Swin3D for efficient pretraining on multi-source 3D point clouds. Swin3D++ introduces domain-specific mechanisms to Swin3D's modules to address domain discrepancies and enhance the network capability on multi-source pretraining. Moreover, we devise a simple source-augmentation strategy to increase the pretraining data scale and facilitate supervised pretraining. We validate the effectiveness of our design, and demonstrate that Swin3D++ surpasses the state-of-the-art 3D pretraining methods on typical indoor scene understanding tasks. Our code and models will be released at https://github.com/microsoft/Swin3D|\u6570\u636e\u591a\u6837\u6027\u548c\u4e30\u5bcc\u6027\u5bf9\u4e8e\u63d0\u9ad8\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c 2D \u89c6\u89c9\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c3D \u89c6\u89c9\u9886\u57df\u7f3a\u4e4f 3D \u6570\u636e\uff0c\u5e76\u4e14\u7b80\u5355\u5730\u7ec4\u5408\u591a\u4e2a 3D \u6570\u636e\u96c6\u6765\u9884\u8bad\u7ec3 3D \u4e3b\u5e72\u7f51\u5e76\u4e0d\u80fd\u4ea7\u751f\u663e\u7740\u7684\u6539\u8fdb\uff0c\u56e0\u4e3a\u4e0d\u540c 3D \u6570\u636e\u96c6\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u5f02\u963b\u788d\u4e86\u6709\u6548\u7684\u7279\u5f81\u5b66\u4e60\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86 3D \u5ba4\u5185\u573a\u666f\u6570\u636e\u96c6\u4e4b\u95f4\u57df\u5dee\u5f02\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86 Swin3D++\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e Swin3D \u7684\u589e\u5f3a\u67b6\u6784\uff0c\u7528\u4e8e\u5bf9\u591a\u6e90 3D \u70b9\u4e91\u8fdb\u884c\u6709\u6548\u7684\u9884\u8bad\u7ec3\u3002 Swin3D++ \u5728 Swin3D \u7684\u6a21\u5757\u4e2d\u5f15\u5165\u4e86\u7279\u5b9a\u9886\u57df\u7684\u673a\u5236\uff0c\u4ee5\u89e3\u51b3\u9886\u57df\u5dee\u5f02\u5e76\u589e\u5f3a\u7f51\u7edc\u5728\u591a\u6e90\u9884\u8bad\u7ec3\u4e0a\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u6e90\u589e\u5f3a\u7b56\u7565\u6765\u589e\u52a0\u9884\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u5e76\u4fc3\u8fdb\u76d1\u7763\u9884\u8bad\u7ec3\u3002\u6211\u4eec\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u660e Swin3D++ \u5728\u5178\u578b\u7684\u5ba4\u5185\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684 3D \u9884\u8bad\u7ec3\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728 https://github.com/microsoft/Swin3D \u53d1\u5e03|[2402.14215v1](http://arxiv.org/pdf/2402.14215v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.14566": "|**2024-02-22**|**Self-supervised Visualisation of Medical Image Datasets**|\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u7684\u81ea\u76d1\u7763\u53ef\u89c6\u5316|Ifeoma Veronica Nwabufo, Jan Niklas B\u00f6hm, Philipp Berens, Dmitry Kobak|Self-supervised learning methods based on data augmentations, such as SimCLR, BYOL, or DINO, allow obtaining semantically meaningful representations of image datasets and are widely used prior to supervised fine-tuning. A recent self-supervised learning method, $t$-SimCNE, uses contrastive learning to directly train a 2D representation suitable for visualisation. When applied to natural image datasets, $t$-SimCNE yields 2D visualisations with semantically meaningful clusters. In this work, we used $t$-SimCNE to visualise medical image datasets, including examples from dermatology, histology, and blood microscopy. We found that increasing the set of data augmentations to include arbitrary rotations improved the results in terms of class separability, compared to data augmentations used for natural images. Our 2D representations show medically relevant structures and can be used to aid data exploration and annotation, improving on common approaches for data visualisation.|\u57fa\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08\u4f8b\u5982 SimCLR\u3001BYOL \u6216 DINO\uff09\u5141\u8bb8\u83b7\u5f97\u56fe\u50cf\u6570\u636e\u96c6\u7684\u8bed\u4e49\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u5e76\u4e14\u5728\u76d1\u7763\u5fae\u8c03\u4e4b\u524d\u5f97\u5230\u5e7f\u6cdb\u4f7f\u7528\u3002\u6700\u8fd1\u7684\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5 $t$-SimCNE \u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u76f4\u63a5\u8bad\u7ec3\u9002\u5408\u53ef\u89c6\u5316\u7684 2D \u8868\u793a\u3002\u5f53\u5e94\u7528\u4e8e\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u65f6\uff0c$t$-SimCNE \u4ea7\u751f\u5177\u6709\u8bed\u4e49\u6709\u610f\u4e49\u7684\u7c07\u7684 2D \u53ef\u89c6\u5316\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 $t$-SimCNE \u6765\u53ef\u89c6\u5316\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u62ec\u6765\u81ea\u76ae\u80a4\u75c5\u5b66\u3001\u7ec4\u7ec7\u5b66\u548c\u8840\u6db2\u663e\u5fae\u955c\u7684\u793a\u4f8b\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4e0e\u7528\u4e8e\u81ea\u7136\u56fe\u50cf\u7684\u6570\u636e\u589e\u5f3a\u76f8\u6bd4\uff0c\u589e\u52a0\u6570\u636e\u589e\u5f3a\u96c6\u4ee5\u5305\u62ec\u4efb\u610f\u65cb\u8f6c\u53ef\u4ee5\u6539\u5584\u7c7b\u53ef\u5206\u79bb\u6027\u65b9\u9762\u7684\u7ed3\u679c\u3002\u6211\u4eec\u7684 2D \u8868\u793a\u663e\u793a\u533b\u5b66\u76f8\u5173\u7ed3\u6784\uff0c\u53ef\u7528\u4e8e\u5e2e\u52a9\u6570\u636e\u63a2\u7d22\u548c\u6ce8\u91ca\uff0c\u6539\u8fdb\u6570\u636e\u53ef\u89c6\u5316\u7684\u5e38\u89c1\u65b9\u6cd5\u3002|[2402.14566v1](http://arxiv.org/pdf/2402.14566v1)|null|\n", "2402.14551": "|**2024-02-22**|**CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion**|CLCE\uff1a\u4e00\u79cd\u6539\u8fdb\u4ea4\u53c9\u71b5\u548c\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u5b9e\u73b0\u4f18\u5316\u5b66\u4e60\u878d\u5408\u7684\u65b9\u6cd5|Zijun Long, George Killick, Lipeng Zhuang, Gerardo Aragon-Camarasa, Zaiqiao Meng, Richard Mccreadie|State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches. To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware Contrastive Learning with CE. Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance. Experimental results demonstrate that CLCE significantly outperforms CE in Top-1 accuracy across twelve benchmarks, achieving gains of up to 3.52% in few-shot learning scenarios and 3.41% in transfer learning settings with the BEiT-3 model. Importantly, our proposed CLCE approach effectively mitigates the dependency of contrastive learning on large batch sizes such as 4096 samples per batch, a limitation that has previously constrained the application of contrastive learning in budget-limited hardware environments.|\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u56fe\u50cf\u6a21\u578b\u4e3b\u8981\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u521d\u59cb\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\uff08CE\uff09\u8fdb\u884c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5fae\u8c03\u3002\u7136\u800c\uff0c\u4e8b\u5b9e\u8bc1\u660e\uff0cCE \u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u7a33\u5b9a\u6027\u3002\u867d\u7136\u6700\u8fd1\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u7684\u5de5\u4f5c\u901a\u8fc7\u63d0\u9ad8\u5d4c\u5165\u7684\u8d28\u91cf\u548c\u4ea7\u751f\u66f4\u597d\u7684\u51b3\u7b56\u8fb9\u754c\u6765\u89e3\u51b3\u5176\u4e2d\u4e00\u4e9b\u9650\u5236\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u5ffd\u89c6\u786c\u8d1f\u6316\u6398\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4f9d\u8d56\u4e8e\u4f7f\u7528\u5927\u6837\u672c\u6279\u6b21\u7684\u8d44\u6e90\u5bc6\u96c6\u578b\u548c\u7f13\u6162\u7684\u8bad\u7ec3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a CLCE \u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u5b83\u5c06\u6807\u7b7e\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u4e0e CE \u96c6\u6210\u5728\u4e00\u8d77\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u4fdd\u6301\u4e86\u4e24\u79cd\u635f\u5931\u51fd\u6570\u7684\u4f18\u52bf\uff0c\u800c\u4e14\u8fd8\u4ee5\u534f\u540c\u7684\u65b9\u5f0f\u5229\u7528\u786c\u8d1f\u6316\u6398\u6765\u63d0\u9ad8\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCLCE \u5728 12 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684 Top-1 \u51c6\u786e\u7387\u65b9\u9762\u663e\u7740\u4f18\u4e8e CE\uff0c\u5728\u4f7f\u7528 BEiT-3 \u6a21\u578b\u7684\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe 3.52% \u7684\u589e\u76ca\uff0c\u5728\u8fc1\u79fb\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86 3.41% \u7684\u589e\u76ca\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u63d0\u51fa\u7684 CLCE \u65b9\u6cd5\u6709\u6548\u5730\u51cf\u8f7b\u4e86\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u5927\u6279\u91cf\u5927\u5c0f\uff08\u4f8b\u5982\u6bcf\u6279 4096 \u4e2a\u6837\u672c\uff09\u7684\u4f9d\u8d56\uff0c\u8fd9\u4e00\u9650\u5236\u6b64\u524d\u9650\u5236\u4e86\u5bf9\u6bd4\u5b66\u4e60\u5728\u9884\u7b97\u6709\u9650\u7684\u786c\u4ef6\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002|[2402.14551v1](http://arxiv.org/pdf/2402.14551v1)|null|\n"}, "\u5176\u4ed6": {"2402.14815": "|**2024-02-22**|**Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging**|\u533b\u5b66\u5f71\u50cf\u4e2d\u4e13\u5bb6\u7ea7\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u5dee|Yuzhe Yang, Yujia Liu, Xin Liu, Avanti Gulhane, Domenico Mastrodicasa, Wei Wu, Edward J Wang, Dushyant W Sahani, Shwetak Patel|Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients. The manifestation of such biases could systematically delay essential medical care for certain patient subgroups. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black female patients. Such demographic biases present over a wide range of pathologies and demographic attributes. Further analysis of the model embedding uncovers its significant encoding of demographic information. Deploying AI systems with these biases in medical imaging can intensify pre-existing care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical application.|\u4eba\u5de5\u667a\u80fd (AI) \u7684\u8fdb\u6b65\u5df2\u5728\u533b\u5b66\u6210\u50cf\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u4e13\u5bb6\u7ea7\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u81ea\u6211\u76d1\u7763\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u68c0\u6d4b\u5e7f\u6cdb\u7684\u75c5\u7406\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u663e\u5f0f\u7684\u8bad\u7ec3\u6ce8\u91ca\u3002\u7136\u800c\uff0c\u81f3\u5173\u91cd\u8981\u7684\u662f\u8981\u786e\u4fdd\u8fd9\u4e9b\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u4e0d\u4f1a\u53cd\u6620\u6216\u653e\u5927\u4eba\u7c7b\u504f\u89c1\uff0c\u4ece\u800c\u4f7f\u5973\u6027\u6216\u9ed1\u4eba\u60a3\u8005\u7b49\u5386\u53f2\u4e0a\u88ab\u8fb9\u7f18\u5316\u7684\u7fa4\u4f53\u5904\u4e8e\u4e0d\u5229\u5730\u4f4d\u3002\u8fd9\u79cd\u504f\u89c1\u7684\u8868\u73b0\u53ef\u80fd\u4f1a\u7cfb\u7edf\u6027\u5730\u5ef6\u8fdf\u67d0\u4e9b\u60a3\u8005\u4e9a\u7fa4\u7684\u57fa\u672c\u533b\u7597\u62a4\u7406\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u8de8\u4e94\u4e2a\u5168\u7403\u6765\u6e90\u7684\u6570\u636e\u96c6\u8c03\u67e5\u4e86\u80f8\u90e8 X \u5c04\u7ebf\u8bca\u65ad\u4e2d\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u7684\u7b97\u6cd5\u516c\u5e73\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7ecf\u8fc7\u59d4\u5458\u4f1a\u8ba4\u8bc1\u7684\u653e\u5c04\u79d1\u533b\u751f\u76f8\u6bd4\uff0c\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u59cb\u7ec8\u5bf9\u8fb9\u7f18\u7fa4\u4f53\u8bca\u65ad\u4e0d\u8db3\uff0c\u5728\u4ea4\u53c9\u4e9a\u7ec4\uff08\u4f8b\u5982\u9ed1\u4eba\u5973\u6027\u60a3\u8005\uff09\u4e2d\u7684\u8bca\u65ad\u7387\u751a\u81f3\u66f4\u9ad8\u3002\u8fd9\u79cd\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u5b58\u5728\u4e8e\u5e7f\u6cdb\u7684\u75c5\u7406\u5b66\u548c\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u4e2d\u3002\u5bf9\u6a21\u578b\u5d4c\u5165\u7684\u8fdb\u4e00\u6b65\u5206\u6790\u63ed\u793a\u4e86\u5176\u5bf9\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u7684\u91cd\u8981\u7f16\u7801\u3002\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u90e8\u7f72\u5177\u6709\u8fd9\u4e9b\u504f\u89c1\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u53ef\u80fd\u4f1a\u52a0\u5267\u73b0\u6709\u7684\u62a4\u7406\u5dee\u5f02\uff0c\u5bf9\u516c\u5e73\u7684\u533b\u7597\u670d\u52a1\u6784\u6210\u6f5c\u5728\u6311\u6218\uff0c\u5e76\u5f15\u53d1\u6709\u5173\u5176\u4e34\u5e8a\u5e94\u7528\u7684\u9053\u5fb7\u95ee\u9898\u3002|[2402.14815v1](http://arxiv.org/pdf/2402.14815v1)|null|\n", "2402.14810": "|**2024-02-22**|**GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion**|GeneOH Diffusion\uff1a\u901a\u8fc7\u53bb\u566a\u6269\u6563\u5b9e\u73b0\u53ef\u63a8\u5e7f\u7684\u624b-\u7269\u4f53\u4ea4\u4e92\u53bb\u566a|Xueyi Liu, Li Yi|In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data samples from a whitened noise space to a clean data manifold and a \"denoising via diffusion\" strategy which can handle input trajectories with various noise patterns by first diffusing them to align with the whitened noise space and cleaning via the canonical denoiser. Extensive experiments on four benchmarks with significant domain variations demonstrate the superior effectiveness of our method. GeneOH Diffusion also shows promise for various downstream applications. Project website: https://meowuu7.github.io/GeneOH-Diffusion/.|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u624b\u4e0e\u7269\u4f53\u4ea4\u4e92\uff08HOI\uff09\u53bb\u566a\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u7ed9\u5b9a\u9519\u8bef\u7684\u4ea4\u4e92\u5e8f\u5217\uff0c\u76ee\u6807\u662f\u7ec6\u5316\u4e0d\u6b63\u786e\u7684\u624b\u90e8\u8f68\u8ff9\uff0c\u4ee5\u6d88\u9664\u4ea4\u4e92\u4f2a\u5f71\uff0c\u4ee5\u83b7\u5f97\u611f\u77e5\u4e0a\u771f\u5b9e\u7684\u5e8f\u5217\u3002\u8fd9\u4e00\u6311\u6218\u6d89\u53ca\u590d\u6742\u7684\u4ea4\u4e92\u566a\u58f0\uff0c\u5305\u62ec\u4e0d\u81ea\u7136\u7684\u624b\u90e8\u59ff\u52bf\u548c\u4e0d\u6b63\u786e\u7684\u624b\u90e8-\u7269\u4f53\u5173\u7cfb\uff0c\u4ee5\u53ca\u5bf9\u65b0\u4ea4\u4e92\u548c\u4e0d\u540c\u566a\u58f0\u6a21\u5f0f\u8fdb\u884c\u7a33\u5065\u6cdb\u5316\u7684\u5fc5\u8981\u6027\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5 GeneOH Diffusion \u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u540d\u4e3a GeneOH \u7684\u521b\u65b0\u7684\u4ee5\u63a5\u89e6\u4e3a\u4e2d\u5fc3\u7684 HOI \u8868\u793a\u548c\u65b0\u7684\u57df\u901a\u7528\u53bb\u566a\u65b9\u6848\u3002\u4ee5\u8054\u7cfb\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8868\u793a GeneOH \u4ee5\u4fe1\u606f\u65b9\u5f0f\u53c2\u6570\u5316 HOI \u8fc7\u7a0b\uff0c\u4fc3\u8fdb\u8de8\u5404\u79cd HOI \u573a\u666f\u7684\u589e\u5f3a\u6cdb\u5316\u3002\u65b0\u7684\u53bb\u566a\u65b9\u6848\u5305\u62ec\u4e00\u4e2a\u89c4\u8303\u7684\u53bb\u566a\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ecf\u8fc7\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5c06\u566a\u58f0\u6570\u636e\u6837\u672c\u4ece\u767d\u5316\u566a\u58f0\u7a7a\u95f4\u6295\u5f71\u5230\u5e72\u51c0\u7684\u6570\u636e\u6d41\u5f62\uff0c\u4ee5\u53ca\u201c\u901a\u8fc7\u6269\u6563\u53bb\u566a\u201d\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u53ef\u4ee5\u901a\u8fc7\u9996\u5148\u5c06\u5b83\u4eec\u6269\u6563\u5230\u4e0e\u901a\u8fc7\u89c4\u8303\u964d\u566a\u5668\u8fdb\u884c\u767d\u5316\u566a\u58f0\u7a7a\u95f4\u548c\u200b\u200b\u6e05\u6d01\u3002\u5bf9\u56db\u4e2a\u5177\u6709\u663e\u7740\u57df\u53d8\u5316\u7684\u57fa\u51c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u5353\u8d8a\u6709\u6548\u6027\u3002 GeneOH Diffusion \u8fd8\u663e\u793a\u51fa\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\u7684\u524d\u666f\u3002\u9879\u76ee\u7f51\u7ad9\uff1ahttps://meowuu7.github.io/GeneOH-Diffusion/\u3002|[2402.14810v1](http://arxiv.org/pdf/2402.14810v1)|null|\n", "2402.14795": "|**2024-02-22**|**CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation**|Cyber\u200b\u200bDemo\uff1a\u589e\u5f3a\u6a21\u62df\u4eba\u4f53\u6f14\u793a\u4ee5\u5b9e\u73b0\u73b0\u5b9e\u4e16\u754c\u7684\u7075\u5de7\u64cd\u4f5c|Jun Wang, Yuzhe Qin, Kaiming Kuang, Yigit Korkmaz, Akhilan Gurumoorthy, Hao Su, Xiaolong Wang|We introduce CyberDemo, a novel approach to robotic imitation learning that leverages simulated human demonstrations for real-world tasks. By incorporating extensive data augmentation in a simulated environment, CyberDemo outperforms traditional in-domain real-world demonstrations when transferred to the real world, handling diverse physical and visual conditions. Regardless of its affordability and convenience in data collection, CyberDemo outperforms baseline methods in terms of success rates across various tasks and exhibits generalizability with previously unseen objects. For example, it can rotate novel tetra-valve and penta-valve, despite human demonstrations only involving tri-valves. Our research demonstrates the significant potential of simulated human demonstrations for real-world dexterous manipulation tasks. More details can be found at https://cyber-demo.github.io|\u6211\u4eec\u63a8\u51fa Cyber\u200b\u200bDemo\uff0c\u8fd9\u662f\u4e00\u79cd\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u6a21\u62df\u4eba\u7c7b\u6f14\u793a\u6765\u5b8c\u6210\u73b0\u5b9e\u4e16\u754c\u7684\u4efb\u52a1\u3002\u901a\u8fc7\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7ed3\u5408\u5e7f\u6cdb\u7684\u6570\u636e\u589e\u5f3a\uff0cCyber\u200b\u200bDemo \u5728\u8f6c\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u65f6\u4f18\u4e8e\u4f20\u7edf\u7684\u57df\u5185\u73b0\u5b9e\u4e16\u754c\u6f14\u793a\uff0c\u53ef\u4ee5\u5904\u7406\u4e0d\u540c\u7684\u7269\u7406\u548c\u89c6\u89c9\u6761\u4ef6\u3002\u65e0\u8bba\u6570\u636e\u6536\u96c6\u7684\u7ecf\u6d4e\u6027\u548c\u4fbf\u5229\u6027\u5982\u4f55\uff0cCyber\u200b\u200bDemo \u5728\u5404\u79cd\u4efb\u52a1\u7684\u6210\u529f\u7387\u65b9\u9762\u90fd\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e76\u4e14\u5bf9\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u5bf9\u8c61\u8868\u73b0\u51fa\u666e\u904d\u6027\u3002\u4f8b\u5982\uff0c\u5b83\u53ef\u4ee5\u65cb\u8f6c\u65b0\u9896\u7684\u56db\u9600\u548c\u4e94\u9600\uff0c\u5c3d\u7ba1\u4eba\u7c7b\u6f14\u793a\u53ea\u6d89\u53ca\u4e09\u9600\u3002\u6211\u4eec\u7684\u7814\u7a76\u8bc1\u660e\u4e86\u6a21\u62df\u4eba\u4f53\u6f14\u793a\u5bf9\u4e8e\u73b0\u5b9e\u4e16\u754c\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u7684\u5de8\u5927\u6f5c\u529b\u3002\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728 https://cyber-demo.github.io \u627e\u5230|[2402.14795v1](http://arxiv.org/pdf/2402.14795v1)|null|\n", "2402.14489": "|**2024-02-22**|**A Class of Topological Pseudodistances for Fast Comparison of Persistence Diagrams**|\u4e00\u7c7b\u7528\u4e8e\u5feb\u901f\u6bd4\u8f83\u6301\u4e45\u56fe\u7684\u62d3\u6251\u4f2a\u8ddd\u79bb|Rolando Kindelan Nu\u00f1ez, Mircea Petrache, Mauricio Cerda, Nancy Hitschfeld|Persistence diagrams (PD)s play a central role in topological data analysis, and are used in an ever increasing variety of applications. The comparison of PD data requires computing comparison metrics among large sets of PDs, with metrics which are accurate, theoretically sound, and fast to compute. Especially for denser multi-dimensional PDs, such comparison metrics are lacking. While on the one hand, Wasserstein-type distances have high accuracy and theoretical guarantees, they incur high computational cost. On the other hand, distances between vectorizations such as Persistence Statistics (PS)s have lower computational cost, but lack the accuracy guarantees and in general they are not guaranteed to distinguish PDs (i.e. the two PS vectors of different PDs may be equal). In this work we introduce a class of pseudodistances called Extended Topological Pseudodistances (ETD)s, which have tunable complexity, and can approximate Sliced and classical Wasserstein distances at the high-complexity extreme, while being computationally lighter and close to Persistence Statistics at the lower complexity extreme, and thus allow users to interpolate between the two metrics. We build theoretical comparisons to show how to fit our new distances at an intermediate level between persistence vectorizations and Wasserstein distances. We also experimentally verify that ETDs outperform PSs in terms of accuracy and outperform Wasserstein and Sliced Wasserstein distances in terms of computational complexity.|\u6301\u4e45\u6027\u56fe (PD) \u5728\u62d3\u6251\u6570\u636e\u5206\u6790\u4e2d\u53d1\u6325\u7740\u6838\u5fc3\u4f5c\u7528\uff0c\u5e76\u4e14\u7528\u4e8e\u8d8a\u6765\u8d8a\u591a\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u3002 PD \u6570\u636e\u7684\u6bd4\u8f83\u9700\u8981\u8ba1\u7b97\u5927\u91cf PD \u4e4b\u95f4\u7684\u6bd4\u8f83\u6307\u6807\uff0c\u8fd9\u4e9b\u6307\u6807\u51c6\u786e\u3001\u7406\u8bba\u4e0a\u5408\u7406\u4e14\u8ba1\u7b97\u901f\u5ea6\u5feb\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u66f4\u5bc6\u96c6\u7684\u591a\u7ef4PD\uff0c\u7f3a\u4e4f\u8fd9\u6837\u7684\u6bd4\u8f83\u6307\u6807\u3002\u4e00\u65b9\u9762\uff0cWasserstein \u578b\u8ddd\u79bb\u5177\u6709\u8f83\u9ad8\u7684\u7cbe\u5ea6\u548c\u7406\u8bba\u4fdd\u8bc1\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u8bf8\u5982\u6301\u4e45\u6027\u7edf\u8ba1\uff08PS\uff09\u4e4b\u7c7b\u7684\u5411\u91cf\u5316\u4e4b\u95f4\u7684\u8ddd\u79bb\u5177\u6709\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u7f3a\u4e4f\u51c6\u786e\u6027\u4fdd\u8bc1\uff0c\u5e76\u4e14\u901a\u5e38\u5b83\u4eec\u4e0d\u80fd\u4fdd\u8bc1\u533a\u5206PD\uff08\u5373\u4e0d\u540cPD\u7684\u4e24\u4e2aPS\u5411\u91cf\u53ef\u80fd\u76f8\u7b49\uff09\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u7c7b\u79f0\u4e3a\u6269\u5c55\u62d3\u6251\u4f2a\u8ddd\u79bb\uff08ETD\uff09\u7684\u4f2a\u8ddd\u79bb\uff0c\u5b83\u5177\u6709\u53ef\u8c03\u7684\u590d\u6742\u5ea6\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u9ad8\u590d\u6742\u5ea6\u6781\u9650\u4e0b\u8fd1\u4f3c\u5207\u7247\u548c\u7ecf\u5178 Wasserstein \u8ddd\u79bb\uff0c\u540c\u65f6\u5728\u8f83\u4f4e\u590d\u6742\u5ea6\u4e0b\u8ba1\u7b97\u91cf\u66f4\u8f7b\u4e14\u63a5\u8fd1\u6301\u4e45\u6027\u7edf\u8ba1\u590d\u6742\u6027\u6781\u7aef\uff0c\u56e0\u6b64\u5141\u8bb8\u7528\u6237\u5728\u4e24\u200b\u200b\u4e2a\u6307\u6807\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\u3002\u6211\u4eec\u5efa\u7acb\u4e86\u7406\u8bba\u6bd4\u8f83\uff0c\u4ee5\u5c55\u793a\u5982\u4f55\u5728\u6301\u4e45\u6027\u77e2\u91cf\u5316\u548c Wasserstein \u8ddd\u79bb\u4e4b\u95f4\u7684\u4e2d\u95f4\u6c34\u5e73\u4e0a\u62df\u5408\u6211\u4eec\u7684\u65b0\u8ddd\u79bb\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1 ETD \u5728\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e PS\uff0c\u5728\u8ba1\u7b97\u590d\u6742\u6027\u65b9\u9762\u4f18\u4e8e Wasserstein \u548c\u5207\u7247 Wasserstein \u8ddd\u79bb\u3002|[2402.14489v1](http://arxiv.org/pdf/2402.14489v1)|null|\n", "2402.14456": "|**2024-02-22**|**VLPose: Bridging the Domain Gap in Pose Estimation with Language-Vision Tuning**|VLPose\uff1a\u901a\u8fc7\u8bed\u8a00\u89c6\u89c9\u8c03\u6574\u5f25\u5408\u59ff\u52bf\u4f30\u8ba1\u4e2d\u7684\u9886\u57df\u5dee\u8ddd|Jingyao Li, Pengguang Chen, Xuan Ju, Hong Xu, Jiaya Jia|Thanks to advances in deep learning techniques, Human Pose Estimation (HPE) has achieved significant progress in natural scenarios. However, these models perform poorly in artificial scenarios such as painting and sculpture due to the domain gap, constraining the development of virtual reality and augmented reality. With the growth of model size, retraining the whole model on both natural and artificial data is computationally expensive and inefficient. Our research aims to bridge the domain gap between natural and artificial scenarios with efficient tuning strategies. Leveraging the potential of language models, we enhance the adaptability of traditional pose estimation models across diverse scenarios with a novel framework called VLPose. VLPose leverages the synergy between language and vision to extend the generalization and robustness of pose estimation models beyond the traditional domains. Our approach has demonstrated improvements of 2.26% and 3.74% on HumanArt and MSCOCO, respectively, compared to state-of-the-art tuning strategies.|\u5f97\u76ca\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\uff08HPE\uff09\u5728\u81ea\u7136\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u7531\u4e8e\u9886\u57df\u5dee\u8ddd\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u7ed8\u753b\u3001\u96d5\u5851\u7b49\u4eba\u5de5\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5236\u7ea6\u4e86\u865a\u62df\u73b0\u5b9e\u548c\u589e\u5f3a\u73b0\u5b9e\u7684\u53d1\u5c55\u3002\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u589e\u957f\uff0c\u5728\u81ea\u7136\u6570\u636e\u548c\u4eba\u5de5\u6570\u636e\u4e0a\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u6602\u8d35\u4e14\u6548\u7387\u4f4e\u4e0b\u3002\u6211\u4eec\u7684\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6709\u6548\u7684\u8c03\u6574\u7b56\u7565\u6765\u5f25\u5408\u81ea\u7136\u573a\u666f\u548c\u4eba\u5de5\u573a\u666f\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u6211\u4eec\u901a\u8fc7\u79f0\u4e3a VLPose \u7684\u65b0\u9896\u6846\u67b6\u589e\u5f3a\u4e86\u4f20\u7edf\u59ff\u52bf\u4f30\u8ba1\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u3002 VLPose \u5229\u7528\u8bed\u8a00\u548c\u89c6\u89c9\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5c06\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u6269\u5c55\u5230\u4f20\u7edf\u9886\u57df\u4e4b\u5916\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u8c03\u6574\u7b56\u7565\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 HumanArt \u548c MSCOCO \u4e0a\u5206\u522b\u8868\u73b0\u51fa 2.26% \u548c 3.74% \u7684\u6539\u8fdb\u3002|[2402.14456v1](http://arxiv.org/pdf/2402.14456v1)|null|\n", "2402.14371": "|**2024-02-22**|**HR-APR: APR-agnostic Framework with Uncertainty Estimation and Hierarchical Refinement for Camera Relocalisation**|HR-APR\uff1a\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u76f8\u673a\u91cd\u5b9a\u4f4d\u5206\u5c42\u7ec6\u5316\u7684 APR \u4e0d\u53ef\u77e5\u6846\u67b6|Changkun Liu, Shuai Chen, Yukun Zhao, Huajian Huang, Victor Prisacariu, Tristan Braud|Absolute Pose Regressors (APRs) directly estimate camera poses from monocular images, but their accuracy is unstable for different queries. Uncertainty-aware APRs provide uncertainty information on the estimated pose, alleviating the impact of these unreliable predictions. However, existing uncertainty modelling techniques are often coupled with a specific APR architecture, resulting in suboptimal performance compared to state-of-the-art (SOTA) APR methods. This work introduces a novel APR-agnostic framework, HR-APR, that formulates uncertainty estimation as cosine similarity estimation between the query and database features. It does not rely on or affect APR network architecture, which is flexible and computationally efficient. In addition, we take advantage of the uncertainty for pose refinement to enhance the performance of APR. The extensive experiments demonstrate the effectiveness of our framework, reducing 27.4\\% and 15.2\\% of computational overhead on the 7Scenes and Cambridge Landmarks datasets while maintaining the SOTA accuracy in single-image APRs.|\u7edd\u5bf9\u59ff\u52bf\u56de\u5f52\u5668\uff08APR\uff09\u76f4\u63a5\u4ece\u5355\u76ee\u56fe\u50cf\u4f30\u8ba1\u76f8\u673a\u59ff\u52bf\uff0c\u4f46\u5bf9\u4e8e\u4e0d\u540c\u7684\u67e5\u8be2\uff0c\u5176\u51c6\u786e\u6027\u4e0d\u7a33\u5b9a\u3002\u4e0d\u786e\u5b9a\u6027\u611f\u77e5 APR \u63d0\u4f9b\u4e86\u4f30\u8ba1\u59ff\u6001\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\uff0c\u51cf\u8f7b\u4e86\u8fd9\u4e9b\u4e0d\u53ef\u9760\u9884\u6d4b\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u6280\u672f\u901a\u5e38\u4e0e\u7279\u5b9a\u7684 APR \u67b6\u6784\u76f8\u7ed3\u5408\uff0c\u5bfc\u81f4\u4e0e\u6700\u5148\u8fdb\u7684 (SOTA) APR \u65b9\u6cd5\u76f8\u6bd4\u6027\u80fd\u6b20\u4f73\u3002\u8fd9\u9879\u5de5\u4f5c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684 APR \u4e0d\u53ef\u77e5\u6846\u67b6 HR-APR\uff0c\u5b83\u5c06\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u516c\u5f0f\u5316\u4e3a\u67e5\u8be2\u548c\u6570\u636e\u5e93\u7279\u5f81\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4f30\u8ba1\u3002\u5b83\u4e0d\u4f9d\u8d56\u4e5f\u4e0d\u5f71\u54cdAPR\u7f51\u7edc\u67b6\u6784\uff0c\u7075\u6d3b\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u59ff\u6001\u7ec6\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u589e\u5f3a APR \u7684\u6027\u80fd\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5728 7Scenes \u548c Cambridge Landmarks \u6570\u636e\u96c6\u4e0a\u51cf\u5c11\u4e86 27.4% \u548c 15.2% \u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u5355\u56fe\u50cf APR \u7684 SOTA \u7cbe\u5ea6\u3002|[2402.14371v1](http://arxiv.org/pdf/2402.14371v1)|null|\n", "2402.14345": "|**2024-02-22**|**An Error-Matching Exclusion Method for Accelerating Visual SLAM**|\u4e00\u79cd\u52a0\u901f\u89c6\u89c9SLAM\u7684\u9519\u8bef\u5339\u914d\u6392\u9664\u65b9\u6cd5|Shaojie Zhang, Yinghui Wang, Jiaxing Ma, Jinlong Yang, Tao Yan, Liangyi Huang, Mingfeng Wang|In Visual SLAM, achieving accurate feature matching consumes a significant amount of time, severely impacting the real-time performance of the system. This paper proposes an accelerated method for Visual SLAM by integrating GMS (Grid-based Motion Statistics) with RANSAC (Random Sample Consensus) for the removal of mismatched features. The approach first utilizes the GMS algorithm to estimate the quantity of matched pairs within the neighborhood and ranks the matches based on their confidence. Subsequently, the Random Sample Consensus (RANSAC) algorithm is employed to further eliminate mismatched features. To address the time-consuming issue of randomly selecting all matched pairs, this method transforms it into the problem of prioritizing sample selection from high-confidence matches. This enables the iterative solution of the optimal model. Experimental results demonstrate that the proposed method achieves a comparable accuracy to the original GMS-RANSAC while reducing the average runtime by 24.13% on the KITTI, TUM desk, and TUM doll datasets.|\u5728\u89c6\u89c9SLAM\u4e2d\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u7279\u5f81\u5339\u914d\u9700\u8981\u6d88\u8017\u5927\u91cf\u65f6\u95f4\uff0c\u4e25\u91cd\u5f71\u54cd\u7cfb\u7edf\u7684\u5b9e\u65f6\u6027\u80fd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9 SLAM \u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06 GMS\uff08\u57fa\u4e8e\u7f51\u683c\u7684\u8fd0\u52a8\u7edf\u8ba1\uff09\u4e0e RANSAC\uff08\u968f\u673a\u6837\u672c\u5171\u8bc6\uff09\u76f8\u7ed3\u5408\u6765\u53bb\u9664\u4e0d\u5339\u914d\u7684\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5229\u7528 GMS \u7b97\u6cd5\u6765\u4f30\u8ba1\u90bb\u57df\u5185\u5339\u914d\u5bf9\u7684\u6570\u91cf\uff0c\u5e76\u6839\u636e\u5176\u7f6e\u4fe1\u5ea6\u5bf9\u5339\u914d\u8fdb\u884c\u6392\u540d\u3002\u968f\u540e\uff0c\u91c7\u7528\u968f\u673a\u6837\u672c\u4e00\u81f4\u6027\uff08RANSAC\uff09\u7b97\u6cd5\u8fdb\u4e00\u6b65\u6d88\u9664\u4e0d\u5339\u914d\u7684\u7279\u5f81\u3002\u4e3a\u4e86\u89e3\u51b3\u968f\u673a\u9009\u62e9\u6240\u6709\u5339\u914d\u5bf9\u7684\u8017\u65f6\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5c06\u5176\u8f6c\u5316\u4e3a\u4ece\u9ad8\u7f6e\u4fe1\u5ea6\u5339\u914d\u4e2d\u4f18\u5148\u9009\u62e9\u6837\u672c\u7684\u95ee\u9898\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u8fed\u4ee3\u6c42\u89e3\u6700\u4f18\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728 KITTI\u3001TUM desk \u548c TUM doll \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u539f\u59cb GMS-RANSAC \u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u5c06\u5e73\u5747\u8fd0\u884c\u65f6\u95f4\u964d\u4f4e\u4e86 24.13%\u3002|[2402.14345v1](http://arxiv.org/pdf/2402.14345v1)|null|\n", "2402.14304": "|**2024-02-22**|**Vision-Language Navigation with Embodied Intelligence: A Survey**|\u5177\u8eab\u667a\u80fd\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff1a\u4e00\u9879\u8c03\u67e5|Peng Gao, Peng Wang, Feng Gao, Fei Wang, Ruyue Yuan|As a long-term vision in the field of artificial intelligence, the core goal of embodied intelligence is to improve the perception, understanding, and interaction capabilities of agents and the environment. Vision-language navigation (VLN), as a critical research path to achieve embodied intelligence, focuses on exploring how agents use natural language to communicate effectively with humans, receive and understand instructions, and ultimately rely on visual information to achieve accurate navigation. VLN integrates artificial intelligence, natural language processing, computer vision, and robotics. This field faces technical challenges but shows potential for application such as human-computer interaction. However, due to the complex process involved from language understanding to action execution, VLN faces the problem of aligning visual information and language instructions, improving generalization ability, and many other challenges. This survey systematically reviews the research progress of VLN and details the research direction of VLN with embodied intelligence. After a detailed summary of its system architecture and research based on methods and commonly used benchmark datasets, we comprehensively analyze the problems and challenges faced by current research and explore the future development direction of this field, aiming to provide a practical reference for researchers.|\u4f5c\u4e3a\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u957f\u671f\u613f\u666f\uff0c\u4f53\u73b0\u667a\u80fd\u7684\u6838\u5fc3\u76ee\u6807\u662f\u63d0\u9ad8\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u7684\u611f\u77e5\u3001\u7406\u89e3\u548c\u4ea4\u4e92\u80fd\u529b\u3002\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4f5c\u4e3a\u5b9e\u73b0\u5177\u8eab\u667a\u80fd\u7684\u5173\u952e\u7814\u7a76\u8def\u5f84\uff0c\u91cd\u70b9\u63a2\u7d22\u667a\u80fd\u4f53\u5982\u4f55\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4e0e\u4eba\u7c7b\u6709\u6548\u6c9f\u901a\uff0c\u63a5\u6536\u548c\u7406\u89e3\u6307\u4ee4\uff0c\u5e76\u6700\u7ec8\u4f9d\u9760\u89c6\u89c9\u4fe1\u606f\u5b9e\u73b0\u7cbe\u786e\u5bfc\u822a\u3002 VLN \u96c6\u6210\u4e86\u4eba\u5de5\u667a\u80fd\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u6280\u672f\u3002\u8be5\u9886\u57df\u9762\u4e34\u6280\u672f\u6311\u6218\uff0c\u4f46\u663e\u793a\u51fa\u4eba\u673a\u4ea4\u4e92\u7b49\u5e94\u7528\u6f5c\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4ece\u8bed\u8a00\u7406\u89e3\u5230\u52a8\u4f5c\u6267\u884c\u6d89\u53ca\u590d\u6742\u7684\u8fc7\u7a0b\uff0cVLN\u9762\u4e34\u7740\u89c6\u89c9\u4fe1\u606f\u548c\u8bed\u8a00\u6307\u4ee4\u7684\u5bf9\u9f50\u3001\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u7b49\u8bf8\u591a\u6311\u6218\u3002\u672c\u7efc\u8ff0\u7cfb\u7edf\u56de\u987e\u4e86VLN\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5e76\u8be6\u7ec6\u9610\u8ff0\u4e86\u5177\u8eab\u667a\u80fd\u7684VLN\u7684\u7814\u7a76\u65b9\u5411\u3002\u6211\u4eec\u8be6\u7ec6\u603b\u7ed3\u4e86\u5176\u7cfb\u7edf\u67b6\u6784\u4ee5\u53ca\u57fa\u4e8e\u65b9\u6cd5\u548c\u5e38\u7528\u57fa\u51c6\u6570\u636e\u96c6\u7684\u7814\u7a76\uff0c\u5168\u9762\u5206\u6790\u4e86\u5f53\u524d\u7814\u7a76\u9762\u4e34\u7684\u95ee\u9898\u548c\u6311\u6218\uff0c\u5e76\u63a2\u8ba8\u4e86\u8be5\u9886\u57df\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5b9e\u9645\u53c2\u8003\u3002|[2402.14304v1](http://arxiv.org/pdf/2402.14304v1)|null|\n", "2402.14281": "|**2024-02-22**|**A Landmark-Aware Visual Navigation Dataset**|\u5730\u6807\u611f\u77e5\u89c6\u89c9\u5bfc\u822a\u6570\u636e\u96c6|Faith Johnson, Bryan Bo Cao, Kristin Dana, Shubham Jain, Ashwin Ashok|Map representation learned by expert demonstrations has shown promising research value. However, recent advancements in the visual navigation field face challenges due to the lack of human datasets in the real world for efficient supervised representation learning of the environments. We present a Landmark-Aware Visual Navigation (LAVN) dataset to allow for supervised learning of human-centric exploration policies and map building. We collect RGB observation and human point-click pairs as a human annotator explores virtual and real-world environments with the goal of full coverage exploration of the space. The human annotators also provide distinct landmark examples along each trajectory, which we intuit will simplify the task of map or graph building and localization. These human point-clicks serve as direct supervision for waypoint prediction when learning to explore in environments. Our dataset covers a wide spectrum of scenes, including rooms in indoor environments, as well as walkways outdoors. Dataset is available at DOI: 10.5281/zenodo.10608067.|\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u5b66\u4e60\u7684\u5730\u56fe\u8868\u793a\u663e\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7814\u7a76\u4ef7\u503c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u7f3a\u4e4f\u6709\u6548\u7684\u73af\u5883\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7684\u4eba\u7c7b\u6570\u636e\u96c6\uff0c\u89c6\u89c9\u5bfc\u822a\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u9762\u4e34\u7740\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5730\u6807\u611f\u77e5\u89c6\u89c9\u5bfc\u822a\uff08LAVN\uff09\u6570\u636e\u96c6\uff0c\u4ee5\u5141\u8bb8\u5bf9\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u63a2\u7d22\u7b56\u7565\u548c\u5730\u56fe\u6784\u5efa\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u3002\u5f53\u4eba\u7c7b\u6ce8\u91ca\u8005\u63a2\u7d22\u865a\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u65f6\uff0c\u6211\u4eec\u6536\u96c6 RGB \u89c2\u5bdf\u548c\u4eba\u7c7b\u70b9\u51fb\u5bf9\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u7a7a\u95f4\u7684\u5168\u8986\u76d6\u63a2\u7d22\u3002\u4eba\u7c7b\u6ce8\u91ca\u8005\u8fd8\u6cbf\u7740\u6bcf\u4e2a\u8f68\u8ff9\u63d0\u4f9b\u4e0d\u540c\u7684\u5730\u6807\u793a\u4f8b\uff0c\u6211\u4eec\u76f4\u89c9\u5730\u8ba4\u4e3a\u8fd9\u5c06\u7b80\u5316\u5730\u56fe\u6216\u56fe\u5f62\u6784\u5efa\u548c\u5b9a\u4f4d\u7684\u4efb\u52a1\u3002\u5f53\u5b66\u4e60\u5728\u73af\u5883\u4e2d\u63a2\u7d22\u65f6\uff0c\u8fd9\u4e9b\u4eba\u7c7b\u70b9\u51fb\u53ef\u4ee5\u4f5c\u4e3a\u8def\u5f84\u70b9\u9884\u6d4b\u7684\u76f4\u63a5\u76d1\u7763\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u5e7f\u6cdb\u7684\u573a\u666f\uff0c\u5305\u62ec\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u623f\u95f4\u4ee5\u53ca\u5ba4\u5916\u7684\u8d70\u9053\u3002\u6570\u636e\u96c6\u53ef\u4ece DOI \u83b7\u53d6\uff1a10.5281/zenodo.10608067\u3002|[2402.14281v1](http://arxiv.org/pdf/2402.14281v1)|null|\n", "2402.14246": "|**2024-02-22**|**Reconstruction-Based Anomaly Localization via Knowledge-Informed Self-Training**|\u901a\u8fc7\u57fa\u4e8e\u77e5\u8bc6\u7684\u81ea\u6211\u8bad\u7ec3\u8fdb\u884c\u57fa\u4e8e\u91cd\u5efa\u7684\u5f02\u5e38\u5b9a\u4f4d|Cheng Qian, Xiaoxian Lao, Chunguang Li|Anomaly localization, which involves localizing anomalous regions within images, is a significant industrial task. Reconstruction-based methods are widely adopted for anomaly localization because of their low complexity and high interpretability. Most existing reconstruction-based methods only use normal samples to construct model. If anomalous samples are appropriately utilized in the process of anomaly localization, the localization performance can be improved. However, usually only weakly labeled anomalous samples are available, which limits the improvement. In many cases, we can obtain some knowledge of anomalies summarized by domain experts. Taking advantage of such knowledge can help us better utilize the anomalous samples and thus further improve the localization performance. In this paper, we propose a novel reconstruction-based method named knowledge-informed self-training (KIST) which integrates knowledge into reconstruction model through self-training. Specifically, KIST utilizes weakly labeled anomalous samples in addition to the normal ones and exploits knowledge to yield pixel-level pseudo-labels of the anomalous samples. Based on the pseudo labels, a novel loss which promotes the reconstruction of normal pixels while suppressing the reconstruction of anomalous pixels is used. We conduct experiments on different datasets and demonstrate the advantages of KIST over the existing reconstruction-based methods.|\u5f02\u5e38\u5b9a\u4f4d\u6d89\u53ca\u5b9a\u4f4d\u56fe\u50cf\u5185\u7684\u5f02\u5e38\u533a\u57df\uff0c\u662f\u4e00\u9879\u91cd\u8981\u7684\u5de5\u4e1a\u4efb\u52a1\u3002\u57fa\u4e8e\u91cd\u6784\u7684\u65b9\u6cd5\u56e0\u5176\u4f4e\u590d\u6742\u6027\u548c\u9ad8\u53ef\u89e3\u91ca\u6027\u800c\u88ab\u5e7f\u6cdb\u7528\u4e8e\u5f02\u5e38\u5b9a\u4f4d\u3002\u5927\u591a\u6570\u73b0\u6709\u7684\u57fa\u4e8e\u91cd\u5efa\u7684\u65b9\u6cd5\u4ec5\u4f7f\u7528\u6b63\u5e38\u6837\u672c\u6765\u6784\u5efa\u6a21\u578b\u3002\u5982\u679c\u5728\u5f02\u5e38\u5b9a\u4f4d\u8fc7\u7a0b\u4e2d\u9002\u5f53\u5229\u7528\u5f02\u5e38\u6837\u672c\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5b9a\u4f4d\u6027\u80fd\u3002\u7136\u800c\uff0c\u901a\u5e38\u53ea\u6709\u5f31\u6807\u8bb0\u7684\u5f02\u5e38\u6837\u672c\u53ef\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u6539\u8fdb\u3002\u5f88\u591a\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97\u9886\u57df\u4e13\u5bb6\u603b\u7ed3\u7684\u4e00\u4e9b\u5f02\u5e38\u77e5\u8bc6\u3002\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u66f4\u597d\u5730\u5229\u7528\u5f02\u5e38\u6837\u672c\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5b9a\u4f4d\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u91cd\u5efa\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u77e5\u8bc6\u77e5\u60c5\u81ea\u8bad\u7ec3\uff08KIST\uff09\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u8bad\u7ec3\u5c06\u77e5\u8bc6\u96c6\u6210\u5230\u91cd\u5efa\u6a21\u578b\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0cKIST \u9664\u4e86\u6b63\u5e38\u6837\u672c\u4e4b\u5916\u8fd8\u5229\u7528\u5f31\u6807\u8bb0\u7684\u5f02\u5e38\u6837\u672c\uff0c\u5e76\u5229\u7528\u77e5\u8bc6\u6765\u751f\u6210\u5f02\u5e38\u6837\u672c\u7684\u50cf\u7d20\u7ea7\u4f2a\u6807\u7b7e\u3002\u57fa\u4e8e\u4f2a\u6807\u7b7e\uff0c\u4f7f\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u635f\u5931\uff0c\u5b83\u4fc3\u8fdb\u6b63\u5e38\u50cf\u7d20\u7684\u91cd\u5efa\uff0c\u540c\u65f6\u6291\u5236\u5f02\u5e38\u50cf\u7d20\u7684\u91cd\u5efa\u3002\u6211\u4eec\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u8bc1\u660e\u4e86 KIST \u76f8\u5bf9\u4e8e\u73b0\u6709\u57fa\u4e8e\u91cd\u5efa\u7684\u65b9\u6cd5\u7684\u4f18\u52bf\u3002|[2402.14246v1](http://arxiv.org/pdf/2402.14246v1)|null|\n"}}