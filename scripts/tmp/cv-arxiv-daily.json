{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.18083": "|**2024-01-31**|**Improved Scene Landmark Detection for Camera Localization**|\u6539\u8fdb\u4e86\u76f8\u673a\u5b9a\u4f4d\u7684\u573a\u666f\u5730\u6807\u68c0\u6d4b|Tien Do, Sudipta N. Sinha|Camera localization methods based on retrieval, local feature matching, and 3D structure-based pose estimation are accurate but require high storage, are slow, and are not privacy-preserving. A method based on scene landmark detection (SLD) was recently proposed to address these limitations. It involves training a convolutional neural network (CNN) to detect a few predetermined, salient, scene-specific 3D points or landmarks and computing camera pose from the associated 2D-3D correspondences. Although SLD outperformed existing learning-based approaches, it was notably less accurate than 3D structure-based methods. In this paper, we show that the accuracy gap was due to insufficient model capacity and noisy labels during training. To mitigate the capacity issue, we propose to split the landmarks into subgroups and train a separate network for each subgroup. To generate better training labels, we propose using dense reconstructions to estimate visibility of scene landmarks. Finally, we present a compact architecture to improve memory efficiency. Accuracy wise, our approach is on par with state of the art structure based methods on the INDOOR-6 dataset but runs significantly faster and uses less storage. Code and models can be found at https://github.com/microsoft/SceneLandmarkLocalization.|\u57fa\u4e8e\u68c0\u7d22\u3001\u5c40\u90e8\u7279\u5f81\u5339\u914d\u548c\u57fa\u4e8e 3D \u7ed3\u6784\u7684\u59ff\u6001\u4f30\u8ba1\u7684\u76f8\u673a\u5b9a\u4f4d\u65b9\u6cd5\u867d\u7136\u51c6\u786e\uff0c\u4f46\u9700\u8981\u9ad8\u5b58\u50a8\u7a7a\u95f4\u3001\u901f\u5ea6\u6162\u4e14\u4e0d\u4fdd\u62a4\u9690\u79c1\u3002\u6700\u8fd1\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u5730\u6807\u68c0\u6d4b\uff08SLD\uff09\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002\u5b83\u6d89\u53ca\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u6765\u68c0\u6d4b\u4e00\u4e9b\u9884\u5148\u786e\u5b9a\u7684\u3001\u663e\u7740\u7684\u3001\u7279\u5b9a\u4e8e\u573a\u666f\u7684 3D \u70b9\u6216\u5730\u6807\uff0c\u5e76\u6839\u636e\u76f8\u5173\u7684 2D-3D \u5bf9\u5e94\u5173\u7cfb\u8ba1\u7b97\u76f8\u673a\u59ff\u52bf\u3002\u5c3d\u7ba1 SLD \u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u7684\u51c6\u786e\u5ea6\u660e\u663e\u4f4e\u4e8e\u57fa\u4e8e 3D \u7ed3\u6784\u7684\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8868\u660e\u51c6\u786e\u6027\u5dee\u8ddd\u662f\u7531\u4e8e\u8bad\u7ec3\u671f\u95f4\u6a21\u578b\u5bb9\u91cf\u4e0d\u8db3\u548c\u566a\u58f0\u6807\u7b7e\u9020\u6210\u7684\u3002\u4e3a\u4e86\u7f13\u89e3\u5bb9\u91cf\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u5c06\u5730\u6807\u5206\u6210\u5b50\u7ec4\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5b50\u7ec4\u8bad\u7ec3\u4e00\u4e2a\u5355\u72ec\u7684\u7f51\u7edc\u3002\u4e3a\u4e86\u751f\u6210\u66f4\u597d\u7684\u8bad\u7ec3\u6807\u7b7e\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u5bc6\u96c6\u91cd\u5efa\u6765\u4f30\u8ba1\u573a\u666f\u5730\u6807\u7684\u53ef\u89c1\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u7684\u67b6\u6784\u6765\u63d0\u9ad8\u5185\u5b58\u6548\u7387\u3002\u5728\u51c6\u786e\u6027\u65b9\u9762\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e INDOOR-6 \u6570\u636e\u96c6\u4e0a\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u7ed3\u6784\u7684\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u8fd0\u884c\u901f\u5ea6\u660e\u663e\u66f4\u5feb\uff0c\u5e76\u4e14\u4f7f\u7528\u7684\u5b58\u50a8\u7a7a\u95f4\u66f4\u5c11\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u4ee5\u5728 https://github.com/microsoft/SceneLandmarkLocalization \u627e\u5230\u3002|[2401.18083v1](http://arxiv.org/pdf/2401.18083v1)|null|\n", "2401.18054": "|**2024-01-31**|**Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition**|\u57fa\u4e8e\u9aa8\u67b6\u7684\u52a8\u4f5c\u8bc6\u522b\u7684\u8fde\u7eed\u56fe\u5b66\u4e60\u7684\u57fa\u51c6\u654f\u611f\u6027|Wei Wei, Tom De Schepper, Kevin Mets|Continual learning (CL) is the research field that aims to build machine learning models that can accumulate knowledge continuously over different tasks without retraining from scratch. Previous studies have shown that pre-training graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020) after fine-tuning, a setting which is closely related to CL. Thus, we focus on studying GNN in the continual graph learning (CGL) setting. We propose the first continual graph learning benchmark for spatio-temporal graphs and use it to benchmark well-known CGL methods in this novel setting. The benchmark is based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action recognition. Beyond benchmarking for standard performance metrics, we study the class and task-order sensitivity of CGL methods, i.e., the impact of learning order on each class/task's performance, and the architectural sensitivity of CGL methods with backbone GNN at various widths and depths. We reveal that task-order robust methods can still be class-order sensitive and observe results that contradict previous empirical observations on architectural sensitivity in CL.|\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u662f\u4e00\u4e2a\u65e8\u5728\u6784\u5efa\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u7814\u7a76\u9886\u57df\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u4e0d\u65ad\u79ef\u7d2f\u77e5\u8bc6\uff0c\u800c\u65e0\u9700\u4ece\u5934\u5f00\u59cb\u91cd\u65b0\u8bad\u7ec3\u3002\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u5fae\u8c03\u540e\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8d1f\u8fc1\u79fb\uff08Hu et al., 2020\uff09\uff0c\u8fd9\u4e00\u8bbe\u7f6e\u4e0e CL \u5bc6\u5207\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4e13\u6ce8\u4e8e\u5728\u8fde\u7eed\u56fe\u5b66\u4e60\uff08CGL\uff09\u73af\u5883\u4e2d\u7814\u7a76 GNN\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u65f6\u7a7a\u56fe\u7684\u8fde\u7eed\u56fe\u5b66\u4e60\u57fa\u51c6\uff0c\u5e76\u7528\u5b83\u5728\u8fd9\u79cd\u65b0\u9896\u7684\u73af\u5883\u4e2d\u5bf9\u8457\u540d\u7684 CGL \u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u57fa\u4e8e N-UCLA \u548c NTU-RGB+D \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u57fa\u4e8e\u9aa8\u67b6\u7684\u52a8\u4f5c\u8bc6\u522b\u3002\u9664\u4e86\u6807\u51c6\u6027\u80fd\u6307\u6807\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u7814\u7a76\u4e86 CGL \u65b9\u6cd5\u7684\u7c7b\u548c\u4efb\u52a1\u987a\u5e8f\u654f\u611f\u6027\uff0c\u5373\u5b66\u4e60\u987a\u5e8f\u5bf9\u6bcf\u4e2a\u7c7b/\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5177\u6709\u4e0d\u540c\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684\u9aa8\u5e72 GNN \u7684 CGL \u65b9\u6cd5\u7684\u67b6\u6784\u654f\u611f\u6027\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4efb\u52a1\u987a\u5e8f\u9c81\u68d2\u65b9\u6cd5\u4ecd\u7136\u53ef\u4ee5\u5bf9\u7c7b\u987a\u5e8f\u654f\u611f\uff0c\u5e76\u4e14\u89c2\u5bdf\u5230\u7684\u7ed3\u679c\u4e0e\u4e4b\u524d\u5bf9 CL \u67b6\u6784\u654f\u611f\u6027\u7684\u7ecf\u9a8c\u89c2\u5bdf\u76f8\u77db\u76fe\u3002|[2401.18054v1](http://arxiv.org/pdf/2401.18054v1)|null|\n", "2401.17992": "|**2024-01-31**|**Multilinear Operator Networks**|\u591a\u7ebf\u6027\u7b97\u5b50\u7f51\u7edc|Yixin Cheng, Grigorios G. Chrysos, Markos Georgopoulos, Volkan Cevher|Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated. On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have yet to perform on par with modern architectures. In this work, we aim close this gap and propose MONet, which relies solely on multilinear operators. The core layer of MONet, called Mu-Layer, captures multiplicative interactions of the elements of the input token. MONet captures high-degree interactions of the input elements and we demonstrate the efficacy of our approach on a series of image recognition and scientific computing benchmarks. The proposed model outperforms prior polynomial networks and performs on par with modern architectures. We believe that MONet can inspire further research on models that use entirely multilinear operations.|\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u56fe\u50cf\u8bc6\u522b\u65b9\u9762\u5177\u6709\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f46\u5bf9\u6fc0\u6d3b\u51fd\u6570\u7684\u4f9d\u8d56\u4ecd\u7136\u662f\u4e00\u4e2a\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u63a2\u7d22\u7684\u9886\u57df\uff0c\u5e76\u4e14\u5c1a\u672a\u6d88\u9664\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u591a\u9879\u5f0f\u7f51\u7edc\u662f\u4e00\u7c7b\u4e0d\u9700\u8981\u6fc0\u6d3b\u51fd\u6570\u7684\u6a21\u578b\uff0c\u4f46\u5c1a\u672a\u8fbe\u5230\u4e0e\u73b0\u4ee3\u67b6\u6784\u76f8\u5f53\u7684\u6027\u80fd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u5e76\u63d0\u51fa MONet\uff0c\u5b83\u4ec5\u4f9d\u8d56\u4e8e\u591a\u7ebf\u6027\u7b97\u5b50\u3002 MONet \u7684\u6838\u5fc3\u5c42\u79f0\u4e3a Mu-Layer\uff0c\u6355\u83b7\u8f93\u5165\u4ee4\u724c\u5143\u7d20\u7684\u4e58\u6cd5\u4ea4\u4e92\u3002 MONet \u6355\u83b7\u8f93\u5165\u5143\u7d20\u7684\u9ad8\u5ea6\u4ea4\u4e92\uff0c\u6211\u4eec\u5728\u4e00\u7cfb\u5217\u56fe\u50cf\u8bc6\u522b\u548c\u79d1\u5b66\u8ba1\u7b97\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4f18\u4e8e\u5148\u524d\u7684\u591a\u9879\u5f0f\u7f51\u7edc\uff0c\u5e76\u4e14\u4e0e\u73b0\u4ee3\u67b6\u6784\u7684\u6027\u80fd\u76f8\u5f53\u3002\u6211\u4eec\u76f8\u4fe1 MONet \u53ef\u4ee5\u6fc0\u53d1\u5bf9\u5b8c\u5168\u4f7f\u7528\u591a\u7ebf\u6027\u8fd0\u7b97\u7684\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002|[2401.17992v1](http://arxiv.org/pdf/2401.17992v1)|null|\n", "2401.17985": "|**2024-01-31**|**Shrub of a thousand faces: an individual segmentation from satellite images using deep learning**|\u5343\u9762\u704c\u6728\uff1a\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5bf9\u536b\u661f\u56fe\u50cf\u8fdb\u884c\u5355\u72ec\u5206\u5272|Rohaifa Khaldi, Siham Tabik, Sergio Puertas-Ruiz, Julio Pe\u00f1as de Giles, Jos\u00e9 Antonio H\u00f3dar Correa, Regino Zamora, Domingo Alcaraz Segura|Monitoring the distribution and size structure of long-living shrubs, such as Juniperus communis, can be used to estimate the long-term effects of climate change on high-mountain and high latitude ecosystems. Historical aerial very-high resolution imagery offers a retrospective tool to monitor shrub growth and distribution at high precision. Currently, deep learning models provide impressive results for detecting and delineating the contour of objects with defined shapes. However, adapting these models to detect natural objects that express complex growth patterns, such as junipers, is still a challenging task.   This research presents a novel approach that leverages remotely sensed RGB imagery in conjunction with Mask R-CNN-based instance segmentation models to individually delineate Juniperus shrubs above the treeline in Sierra Nevada (Spain). In this study, we propose a new data construction design that consists in using photo interpreted (PI) and field work (FW) data to respectively develop and externally validate the model. We also propose a new shrub-tailored evaluation algorithm based on a new metric called Multiple Intersections over Ground Truth Area (MIoGTA) to assess and optimize the model shrub delineation performance. Finally, we deploy the developed model for the first time to generate a wall-to-wall map of Juniperus individuals.   The experimental results demonstrate the efficiency of our dual data construction approach in overcoming the limitations associated with traditional field survey methods. They also highlight the robustness of MIoGTA metric in evaluating instance segmentation models on species with complex growth patterns showing more resilience against data annotation uncertainty. Furthermore, they show the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone in delineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%, respectively.|\u76d1\u6d4b\u957f\u5bff\u704c\u6728\uff08\u5982\u675c\u677e\uff09\u7684\u5206\u5e03\u548c\u5927\u5c0f\u7ed3\u6784\uff0c\u53ef\u7528\u4e8e\u4f30\u8ba1\u6c14\u5019\u53d8\u5316\u5bf9\u9ad8\u5c71\u548c\u9ad8\u7eac\u5ea6\u751f\u6001\u7cfb\u7edf\u7684\u957f\u671f\u5f71\u54cd\u3002\u5386\u53f2\u822a\u7a7a\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u56de\u987e\u6027\u5de5\u5177\uff0c\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u76d1\u6d4b\u704c\u6728\u751f\u957f\u548c\u5206\u5e03\u3002\u76ee\u524d\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u68c0\u6d4b\u548c\u63cf\u7ed8\u5177\u6709\u5b9a\u4e49\u5f62\u72b6\u7684\u7269\u4f53\u8f6e\u5ed3\u65b9\u9762\u63d0\u4f9b\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u8c03\u6574\u8fd9\u4e9b\u6a21\u578b\u6765\u68c0\u6d4b\u8868\u8fbe\u590d\u6742\u751f\u957f\u6a21\u5f0f\u7684\u81ea\u7136\u7269\u4f53\uff08\u4f8b\u5982\u675c\u677e\uff09\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u9065\u611f RGB \u56fe\u50cf\u4e0e\u57fa\u4e8e Mask R-CNN \u7684\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u5355\u72ec\u63cf\u7ed8\u5185\u534e\u8fbe\u5c71\u8109\uff08\u897f\u73ed\u7259\uff09\u6811\u7ebf\u4e0a\u65b9\u7684\u675c\u677e\u704c\u6728\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u6784\u5efa\u8bbe\u8ba1\uff0c\u5176\u4e2d\u5305\u62ec\u4f7f\u7528\u7167\u7247\u89e3\u91ca\uff08PI\uff09\u548c\u73b0\u573a\u5de5\u4f5c\uff08FW\uff09\u6570\u636e\u5206\u522b\u5f00\u53d1\u548c\u5916\u90e8\u9a8c\u8bc1\u6a21\u578b\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u704c\u6728\u5b9a\u5236\u8bc4\u4f30\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u79f0\u4e3a\u5730\u9762\u771f\u5b9e\u533a\u57df\u591a\u91cd\u4ea4\u53c9\u70b9\uff08MIoGTA\uff09\u7684\u65b0\u6307\u6807\uff0c\u4ee5\u8bc4\u4f30\u548c\u4f18\u5316\u6a21\u578b\u704c\u6728\u63cf\u7ed8\u6027\u80fd\u3002\u6700\u540e\uff0c\u6211\u4eec\u9996\u6b21\u90e8\u7f72\u5f00\u53d1\u7684\u6a21\u578b\u6765\u751f\u6210\u675c\u677e\u4e2a\u4f53\u7684\u5168\u9762\u5730\u56fe\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u53cc\u6570\u636e\u6784\u5efa\u65b9\u6cd5\u5728\u514b\u670d\u4e0e\u4f20\u7edf\u73b0\u573a\u8c03\u67e5\u65b9\u6cd5\u76f8\u5173\u7684\u5c40\u9650\u6027\u65b9\u9762\u7684\u6548\u7387\u3002\u4ed6\u4eec\u8fd8\u5f3a\u8c03\u4e86 MIoGTA \u6307\u6807\u5728\u8bc4\u4f30\u5177\u6709\u590d\u6742\u751f\u957f\u6a21\u5f0f\u7684\u7269\u79cd\u7684\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u65f6\u7684\u7a33\u5065\u6027\uff0c\u663e\u793a\u51fa\u9488\u5bf9\u6570\u636e\u6ce8\u91ca\u4e0d\u786e\u5b9a\u6027\u7684\u66f4\u5f3a\u5f39\u6027\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u5c55\u793a\u4e86\u4f7f\u7528\u5e26\u6709 ResNet101-C4 \u4e3b\u5e72\u7684 Mask R-CNN \u6765\u63cf\u7ed8 PI \u548c FW \u704c\u6728\u7684\u6709\u6548\u6027\uff0c\u5206\u522b\u5b9e\u73b0\u4e86 87.87% \u548c 76.86% \u7684 F1 \u5206\u6570\u3002|[2401.17985v1](http://arxiv.org/pdf/2401.17985v1)|null|\n", "2401.17981": "|**2024-01-31**|**Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study**|\u4f7f\u7528\u89c6\u89c9\u68c0\u6d4b\u6a21\u578b\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff1a\u5b9e\u8bc1\u7814\u7a76|Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen|Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a notable advancement in multimodal understanding. We release our codes to facilitate further exploration into the fine-grained multimodal dialogue capabilities of MLLMs.|\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u96c6\u6210\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\u65b9\u9762\u5177\u6709\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46\u5728\u51c6\u786e\u89e3\u91ca\u8be6\u7ec6\u7684\u89c6\u89c9\u5143\u7d20\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u9879\u5173\u4e8e\u901a\u8fc7\u6700\u5148\u8fdb\u7684 (SOTA) \u5bf9\u8c61\u68c0\u6d4b\u548c\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u6a21\u578b\u589e\u5f3a MLLM \u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u4ee5\u63d0\u9ad8\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7406\u89e3\u5e76\u51cf\u5c11\u54cd\u5e94\u4e2d\u7684\u5e7b\u89c9\u3002\u6211\u4eec\u7684\u7814\u7a76\u8c03\u67e5\u4e86\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u6d4b\u4fe1\u606f\u6ce8\u5165\u3001\u8fd9\u79cd\u6ce8\u5165\u5bf9 MLLM \u539f\u59cb\u80fd\u529b\u7684\u5f71\u54cd\u4ee5\u53ca\u68c0\u6d4b\u6a21\u578b\u7684\u4e92\u6362\u6027\u3002\u6211\u4eec\u5bf9 LLaVA-1.5\u3001DINO \u548c PaddleOCRv2 \u200b\u200b\u7b49\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86 MLLM \u5728\u7279\u5b9a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u800c\u4e14\u4fdd\u6301\u4e86\u5176\u539f\u6709\u7684\u4f18\u52bf\u3002\u7531\u6b64\u4ea7\u751f\u7684\u589e\u5f3a\u578b MLLM \u5728 10 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684 9 \u4e2a\u4e0a\u4f18\u4e8e SOTA \u6a21\u578b\uff0c\u5728\u6807\u51c6\u5316\u5e73\u5747\u5206\u6570\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 12.99% \u7684\u6539\u8fdb\uff0c\u6807\u5fd7\u7740\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u7684\u663e\u7740\u8fdb\u6b65\u3002\u6211\u4eec\u53d1\u5e03\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u63a2\u7d22 MLLM \u7684\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u5bf9\u8bdd\u529f\u80fd\u3002|[2401.17981v1](http://arxiv.org/pdf/2401.17981v1)|null|\n", "2401.17972": "|**2024-01-31**|**MelNet: A Real-Time Deep Learning Algorithm for Object Detection**|MelNet\uff1a\u4e00\u79cd\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684\u5b9e\u65f6\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5|Yashar Azadvatan, Murat Kurt|In this study, a novel deep learning algorithm for object detection, named MelNet, was introduced. MelNet underwent training utilizing the KITTI dataset for object detection. Following 300 training epochs, MelNet attained an mAP (mean average precision) score of 0.732. Additionally, three alternative models -YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI dataset and juxtaposed with MelNet for object detection.   The outcomes underscore the efficacy of employing transfer learning in certain instances. Notably, preexisting models trained on prominent datasets (e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding underscores the viability of creating a new model tailored to a specific scenario and training it on a specific dataset. This investigation demonstrates that training MelNet exclusively on the KITTI dataset also surpasses EfficientDet after 150 epochs. Consequently, post-training, MelNet's performance closely aligns with that of other pre-trained models.|\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76ee\u6807\u68c0\u6d4b\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u79f0\u4e3a MelNet\u3002 MelNet \u4f7f\u7528 KITTI \u6570\u636e\u96c6\u8fdb\u884c\u4e86\u76ee\u6807\u68c0\u6d4b\u8bad\u7ec3\u3002\u7ecf\u8fc7 300 \u4e2a\u8bad\u7ec3\u5468\u671f\u540e\uff0cMelNet \u7684 mAP\uff08\u5e73\u5747\u7cbe\u5ea6\uff09\u5f97\u5206\u4e3a 0.732\u3002\u6b64\u5916\uff0c\u4e09\u4e2a\u66ff\u4ee3\u6a21\u578b - YOLOv5\u3001EfficientDet \u548c Faster-RCNN-MobileNetv3 - \u5728 KITTI \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4e0e MelNet \u5e76\u5217\u8fdb\u884c\u5bf9\u8c61\u68c0\u6d4b\u3002\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u8457\u540d\u6570\u636e\u96c6\uff08\u4f8b\u5982 ImageNet\u3001COCO \u548c Pascal VOC\uff09\u4e0a\u8bad\u7ec3\u7684\u73b0\u6709\u6a21\u578b\u4ea7\u751f\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002\u53e6\u4e00\u9879\u53d1\u73b0\u5f3a\u8c03\u4e86\u521b\u5efa\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u7684\u65b0\u6a21\u578b\u5e76\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u5bf9\u5176\u8fdb\u884c\u8bad\u7ec3\u7684\u53ef\u884c\u6027\u3002\u8fd9\u9879\u8c03\u67e5\u8868\u660e\uff0c\u4ec5\u5728 KITTI \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3 MelNet \u5728 150 \u4e2a epoch \u540e\u4e5f\u8d85\u8fc7\u4e86 EfficientDet\u3002\u56e0\u6b64\uff0c\u8bad\u7ec3\u540e\uff0cMelNet \u7684\u6027\u80fd\u4e0e\u5176\u4ed6\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u975e\u5e38\u63a5\u8fd1\u3002|[2401.17972v1](http://arxiv.org/pdf/2401.17972v1)|null|\n", "2401.17948": "|**2024-01-31**|**HyperZ$\\cdot$Z$\\cdot$W Operator Connects Slow-Fast Networks for Full Context Interaction**|HyperZ$\\cdot$Z$\\cdot$W \u8fd0\u7b97\u7b26\u8fde\u63a5\u6162\u901f\u7f51\u7edc\u4ee5\u5b9e\u73b0\u5168\u4e0a\u4e0b\u6587\u4ea4\u4e92|Harvie Zhang|The self-attention mechanism utilizes large implicit weight matrices, programmed through dot product-based activations with very few trainable parameters, to enable long sequence modeling. In this paper, we investigate the possibility of discarding residual learning by employing large implicit kernels to achieve full context interaction at each layer of the network. To accomplish it, we introduce coordinate-based implicit MLPs as a slow network to generate hyper-kernels for another fast convolutional network. To get context-varying weights for fast dynamic encoding, we propose a $\\mathrm{Hyper}\\mathcal{Z{\\cdot}Z{\\cdot}W}$ operator that connects hyper-kernels ($\\mathcal{W}$) and hidden activations ($\\mathcal{Z}$) through simple elementwise multiplication, followed by convolution of $\\mathcal{Z}$ using the context-dependent $\\mathcal{W}$. Based on this design, we present a novel Terminator architecture that integrates hyper-kernels of different sizes to produce multi-branch hidden representations for enhancing the feature extraction capability of each layer. Additionally, a bottleneck layer is employed to compress the concatenated channels, allowing only valuable information to propagate to the subsequent layers. Notably, our model incorporates several innovative components and exhibits excellent properties, such as introducing local feedback error for updating the slow network, stable zero-mean features, faster training convergence, and fewer model parameters. Extensive experimental results on pixel-level 1D and 2D image classification benchmarks demonstrate the superior performance of our architecture.|\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5229\u7528\u5927\u578b\u9690\u5f0f\u6743\u91cd\u77e9\u9635\uff0c\u901a\u8fc7\u57fa\u4e8e\u70b9\u79ef\u7684\u6fc0\u6d3b\u548c\u5f88\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u8fdb\u884c\u7f16\u7a0b\uff0c\u4ee5\u5b9e\u73b0\u957f\u5e8f\u5217\u5efa\u6a21\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u901a\u8fc7\u91c7\u7528\u5927\u578b\u9690\u5f0f\u5185\u6838\u6765\u653e\u5f03\u6b8b\u5dee\u5b66\u4e60\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u5728\u7f51\u7edc\u7684\u6bcf\u4e00\u5c42\u5b9e\u73b0\u5b8c\u6574\u7684\u4e0a\u4e0b\u6587\u4ea4\u4e92\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u5f15\u5165\u57fa\u4e8e\u5750\u6807\u7684\u9690\u5f0f MLP \u4f5c\u4e3a\u6162\u901f\u7f51\u7edc\uff0c\u4e3a\u53e6\u4e00\u4e2a\u5feb\u901f\u5377\u79ef\u7f51\u7edc\u751f\u6210\u8d85\u5185\u6838\u3002\u4e3a\u4e86\u83b7\u5f97\u5feb\u901f\u52a8\u6001\u7f16\u7801\u7684\u4e0a\u4e0b\u6587\u53d8\u5316\u6743\u91cd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fde\u63a5\u8d85\u5185\u6838\u7684 $\\mathrm{Hyper}\\mathcal{Z{\\cdot}Z{\\cdot}W}$ \u8fd0\u7b97\u7b26 ($\\mathcal{W}$ \uff09\u548c\u9690\u85cf\u6fc0\u6d3b\uff08$\\mathcal{Z}$\uff09\u901a\u8fc7\u7b80\u5355\u7684\u5143\u7d20\u4e58\u6cd5\uff0c\u7136\u540e\u4f7f\u7528\u4e0a\u4e0b\u6587\u76f8\u5173\u7684$\\mathcal{W}$\u8fdb\u884c$\\mathcal{Z}$\u7684\u5377\u79ef\u3002\u57fa\u4e8e\u6b64\u8bbe\u8ba1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ec8\u7ed3\u8005\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u96c6\u6210\u4e86\u4e0d\u540c\u5927\u5c0f\u7684\u8d85\u5185\u6838\u4ee5\u4ea7\u751f\u591a\u5206\u652f\u9690\u85cf\u8868\u793a\uff0c\u4ee5\u589e\u5f3a\u6bcf\u5c42\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u74f6\u9888\u5c42\u6765\u538b\u7f29\u7ea7\u8054\u901a\u9053\uff0c\u53ea\u5141\u8bb8\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u4f20\u64ad\u5230\u540e\u7eed\u5c42\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u6a21\u578b\u7ed3\u5408\u4e86\u591a\u4e2a\u521b\u65b0\u7ec4\u4ef6\uff0c\u5e76\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u7279\u6027\uff0c\u4f8b\u5982\u5f15\u5165\u5c40\u90e8\u53cd\u9988\u8bef\u5dee\u6765\u66f4\u65b0\u6162\u901f\u7f51\u7edc\u3001\u7a33\u5b9a\u7684\u96f6\u5747\u503c\u7279\u5f81\u3001\u66f4\u5feb\u7684\u8bad\u7ec3\u6536\u655b\u548c\u66f4\u5c11\u7684\u6a21\u578b\u53c2\u6570\u3002\u50cf\u7d20\u7ea7\u4e00\u7ef4\u548c\u4e8c\u7ef4\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u67b6\u6784\u7684\u5353\u8d8a\u6027\u80fd\u3002|[2401.17948v1](http://arxiv.org/pdf/2401.17948v1)|null|\n", "2401.17916": "|**2024-01-31**|**Source-free Domain Adaptive Object Detection in Remote Sensing Images**|\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b|Weixing Liu, Jun Liu, Xin Su, Han Nie, Bin Luo|Recent studies have used unsupervised domain adaptive object detection (UDAOD) methods to bridge the domain gap in remote sensing (RS) images. However, UDAOD methods typically assume that the source domain data can be accessed during the domain adaptation process. This setting is often impractical in the real world due to RS data privacy and transmission difficulty. To address this challenge, we propose a practical source-free object detection (SFOD) setting for RS images, which aims to perform target domain adaptation using only the source pre-trained model. We propose a new SFOD method for RS images consisting of two parts: perturbed domain generation and alignment. The proposed multilevel perturbation constructs the perturbed domain in a simple yet efficient form by perturbing the domain-variant features at the image level and feature level according to the color and style bias. The proposed multilevel alignment calculates feature and label consistency between the perturbed domain and the target domain across the teacher-student network, and introduces the distillation of feature prototype to mitigate the noise of pseudo-labels. By requiring the detector to be consistent in the perturbed domain and the target domain, the detector is forced to focus on domaininvariant features. Extensive results of three synthetic-to-real experiments and three cross-sensor experiments have validated the effectiveness of our method which does not require access to source domain RS images. Furthermore, experiments on computer vision datasets show that our method can be extended to other fields as well. Our code will be available at: https://weixliu.github.io/ .|\u6700\u8fd1\u7684\u7814\u7a76\u4f7f\u7528\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\uff08UDAOD\uff09\u65b9\u6cd5\u6765\u5f25\u5408\u9065\u611f\uff08RS\uff09\u56fe\u50cf\u4e2d\u7684\u57df\u5dee\u8ddd\u3002\u7136\u800c\uff0cUDAOD\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6e90\u57df\u6570\u636e\u53ef\u4ee5\u5728\u57df\u9002\u5e94\u8fc7\u7a0b\u4e2d\u88ab\u8bbf\u95ee\u3002\u7531\u4e8eRS\u6570\u636e\u9690\u79c1\u548c\u4f20\u8f93\u56f0\u96be\uff0c\u8fd9\u79cd\u8bbe\u7f6e\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u901a\u5e38\u4e0d\u5207\u5b9e\u9645\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684 RS \u56fe\u50cf\u65e0\u6e90\u5bf9\u8c61\u68c0\u6d4b\uff08SFOD\uff09\u8bbe\u7f6e\uff0c\u5176\u76ee\u7684\u662f\u4ec5\u4f7f\u7528\u6e90\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u6267\u884c\u76ee\u6807\u57df\u81ea\u9002\u5e94\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 RS \u56fe\u50cf SFOD \u65b9\u6cd5\uff0c\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff1a\u6270\u52a8\u57df\u751f\u6210\u548c\u5bf9\u9f50\u3002\u6240\u63d0\u51fa\u7684\u591a\u7ea7\u6270\u52a8\u901a\u8fc7\u6839\u636e\u989c\u8272\u548c\u98ce\u683c\u504f\u5dee\u5728\u56fe\u50cf\u7ea7\u522b\u548c\u7279\u5f81\u7ea7\u522b\u6270\u52a8\u57df\u53d8\u4f53\u7279\u5f81\uff0c\u4ee5\u7b80\u5355\u800c\u6709\u6548\u7684\u5f62\u5f0f\u6784\u9020\u6270\u52a8\u57df\u3002\u6240\u63d0\u51fa\u7684\u591a\u7ea7\u5bf9\u9f50\u8ba1\u7b97\u5e08\u751f\u7f51\u7edc\u4e2d\u6270\u52a8\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u7279\u5f81\u548c\u6807\u7b7e\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165\u7279\u5f81\u539f\u578b\u7684\u84b8\u998f\u6765\u51cf\u8f7b\u4f2a\u6807\u7b7e\u7684\u566a\u58f0\u3002\u901a\u8fc7\u8981\u6c42\u68c0\u6d4b\u5668\u5728\u6270\u52a8\u57df\u548c\u76ee\u6807\u57df\u4e2d\u4fdd\u6301\u4e00\u81f4\uff0c\u68c0\u6d4b\u5668\u88ab\u8feb\u5173\u6ce8\u57df\u4e0d\u53d8\u7279\u5f81\u3002\u4e09\u4e2a\u5408\u6210\u771f\u5b9e\u5b9e\u9a8c\u548c\u4e09\u4e2a\u8de8\u4f20\u611f\u5668\u5b9e\u9a8c\u7684\u5e7f\u6cdb\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u8bbf\u95ee\u6e90\u57df\u9065\u611f\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4e5f\u53ef\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u9886\u57df\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u63d0\u4f9b\uff1ahttps://weixliu.github.io/\u3002|[2401.17916v1](http://arxiv.org/pdf/2401.17916v1)|null|\n", "2401.17904": "|**2024-01-31**|**Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation**|Hi-SAM\uff1a\u7ed3\u5408 Segment Anything \u6a21\u578b\u8fdb\u884c\u5206\u5c42\u6587\u672c\u5206\u5272|Maoyuan Ye, Jing Zhang, Juhua Liu, Chenyu Liu, Baocai Yin, Cong Liu, Bo Du, Dacheng Tao|The Segment Anything Model (SAM), a profound vision foundation model pre-trained on a large-scale dataset, breaks the boundaries of general segmentation and sparks various downstream applications. This paper introduces Hi-SAM, a unified model leveraging SAM for hierarchical text segmentation. Hi-SAM excels in text segmentation across four hierarchies, including stroke, word, text-line, and paragraph, while realizing layout analysis as well. Specifically, we first turn SAM into a high-quality text stroke segmentation (TSS) model through a parameter-efficient fine-tuning approach. We use this TSS model to iteratively generate the text stroke labels in a semi-automatical manner, unifying labels across the four text hierarchies in the HierText dataset. Subsequently, with these complete labels, we launch the end-to-end trainable Hi-SAM based on the TSS architecture with a customized hierarchical mask decoder. During inference, Hi-SAM offers both automatic mask generation (AMG) mode and promptable segmentation mode. In terms of the AMG mode, Hi-SAM segments text stroke foreground masks initially, then samples foreground points for hierarchical text mask generation and achieves layout analysis in passing. As for the promptable mode, Hi-SAM provides word, text-line, and paragraph masks with a single point click. Experimental results show the state-of-the-art performance of our TSS model: 84.86% fgIOU on Total-Text and 88.96% fgIOU on TextSeg for text stroke segmentation. Moreover, compared to the previous specialist for joint hierarchical detection and layout analysis on HierText, Hi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on the text-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout analysis, requiring 20x fewer training epochs. The code is available at https://github.com/ymy-k/Hi-SAM.|Segment Anything Model (SAM) \u662f\u4e00\u79cd\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u6253\u7834\u4e86\u4e00\u822c\u5206\u5272\u7684\u754c\u9650\uff0c\u5e76\u6fc0\u53d1\u4e86\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 Hi-SAM\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528 SAM \u8fdb\u884c\u5206\u5c42\u6587\u672c\u5206\u5272\u7684\u7edf\u4e00\u6a21\u578b\u3002 Hi-SAM \u64c5\u957f\u8de8\u56db\u4e2a\u5c42\u6b21\u7684\u6587\u672c\u5206\u5272\uff0c\u5305\u62ec\u7b14\u753b\u3001\u5355\u8bcd\u3001\u6587\u672c\u884c\u548c\u6bb5\u843d\uff0c\u540c\u65f6\u8fd8\u5b9e\u73b0\u5e03\u5c40\u5206\u6790\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u5c06 SAM \u8f6c\u53d8\u4e3a\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u7b14\u5212\u5206\u5272\uff08TSS\uff09\u6a21\u578b\u3002\u6211\u4eec\u4f7f\u7528\u6b64 TSS \u6a21\u578b\u4ee5\u534a\u81ea\u52a8\u65b9\u5f0f\u8fed\u4ee3\u751f\u6210\u6587\u672c\u7b14\u5212\u6807\u7b7e\uff0c\u7edf\u4e00 HierText \u6570\u636e\u96c6\u4e2d\u56db\u4e2a\u6587\u672c\u5c42\u6b21\u7ed3\u6784\u7684\u6807\u7b7e\u3002\u968f\u540e\uff0c\u6709\u4e86\u8fd9\u4e9b\u5b8c\u6574\u7684\u6807\u7b7e\uff0c\u6211\u4eec\u63a8\u51fa\u4e86\u57fa\u4e8e TSS \u67b6\u6784\u548c\u5b9a\u5236\u5206\u5c42\u63a9\u6a21\u89e3\u7801\u5668\u7684\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3 Hi-SAM\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0cHi-SAM \u63d0\u4f9b\u81ea\u52a8\u63a9\u6a21\u751f\u6210\uff08AMG\uff09\u6a21\u5f0f\u548c\u63d0\u793a\u5206\u5272\u6a21\u5f0f\u3002\u5c31AMG\u6a21\u5f0f\u800c\u8a00\uff0cHi-SAM\u9996\u5148\u5bf9\u6587\u672c\u7b14\u753b\u524d\u666f\u63a9\u6a21\u8fdb\u884c\u5206\u5272\uff0c\u7136\u540e\u5bf9\u524d\u666f\u70b9\u8fdb\u884c\u91c7\u6837\u4ee5\u8fdb\u884c\u5206\u5c42\u6587\u672c\u63a9\u6a21\u751f\u6210\uff0c\u5e76\u987a\u4fbf\u5b9e\u73b0\u5e03\u5c40\u5206\u6790\u3002\u81f3\u4e8e\u63d0\u793a\u6a21\u5f0f\uff0cHi-SAM \u901a\u8fc7\u5355\u51fb\u5373\u53ef\u63d0\u4f9b\u5355\u8bcd\u3001\u6587\u672c\u884c\u548c\u6bb5\u843d\u63a9\u7801\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u6211\u4eec\u7684 TSS \u6a21\u578b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u5bf9\u4e8e\u6587\u672c\u7b14\u5212\u5206\u5272\uff0cTotal-Text \u4e0a\u7684 fgIOU \u4e3a 84.86%\uff0cTextSeg \u4e0a\u7684 fgIOU \u4e3a 88.96%\u3002\u6b64\u5916\uff0c\u4e0e\u4e4b\u524d\u5728 HierText \u4e0a\u8fdb\u884c\u8054\u5408\u5206\u5c42\u68c0\u6d4b\u548c\u5e03\u5c40\u5206\u6790\u7684\u4e13\u5bb6\u76f8\u6bd4\uff0cHi-SAM \u53d6\u5f97\u4e86\u663e\u7740\u7684\u6539\u8fdb\uff1a\u5728\u6587\u672c\u884c\u7ea7\u522b\u4e0a\u63d0\u9ad8\u4e86 4.73% PQ \u548c 5.39% F1\uff0c\u5728\u6bb5\u843d\u7ea7\u522b\u5e03\u5c40\u4e0a\u63d0\u9ad8\u4e86 5.49% PQ \u548c 7.39% F1\u5206\u6790\uff0c\u9700\u8981\u7684\u8bad\u7ec3\u6b21\u6570\u51cf\u5c11\u4e86 20 \u500d\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/ymy-k/Hi-SAM \u83b7\u53d6\u3002|[2401.17904v1](http://arxiv.org/pdf/2401.17904v1)|null|\n", "2401.17881": "|**2024-01-31**|**PVLR: Prompt-driven Visual-Linguistic Representation Learning for Multi-Label Image Recognition**|PVLR\uff1a\u7528\u4e8e\u591a\u6807\u7b7e\u56fe\u50cf\u8bc6\u522b\u7684\u63d0\u793a\u9a71\u52a8\u7684\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u5b66\u4e60|Hao Tan, Zichang Tan, Jun Li, Jun Wan, Zhen Lei|Multi-label image recognition is a fundamental task in computer vision. Recently, vision-language models have made notable advancements in this area. However, previous methods often failed to effectively leverage the rich knowledge within language models and instead incorporated label semantics into visual features in a unidirectional manner. In this paper, we propose a Prompt-driven Visual-Linguistic Representation Learning (PVLR) framework to better leverage the capabilities of the linguistic modality. In PVLR, we first introduce a dual-prompting strategy comprising Knowledge-Aware Prompting (KAP) and Context-Aware Prompting (CAP). KAP utilizes fixed prompts to capture the intrinsic semantic knowledge and relationships across all labels, while CAP employs learnable prompts to capture context-aware label semantics and relationships. Later, we propose an Interaction and Fusion Module (IFM) to interact and fuse the representations obtained from KAP and CAP. In contrast to the unidirectional fusion in previous works, we introduce a Dual-Modal Attention (DMA) that enables bidirectional interaction between textual and visual features, yielding context-aware label representations and semantic-related visual representations, which are subsequently used to calculate similarities and generate final predictions for all labels. Extensive experiments on three popular datasets including MS-COCO, Pascal VOC 2007, and NUS-WIDE demonstrate the superiority of PVLR.|\u591a\u6807\u7b7e\u56fe\u50cf\u8bc6\u522b\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u9879\u57fa\u672c\u4efb\u52a1\u3002\u6700\u8fd1\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e00\u9886\u57df\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u4ee5\u524d\u7684\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u5229\u7528\u8bed\u8a00\u6a21\u578b\u4e2d\u4e30\u5bcc\u7684\u77e5\u8bc6\uff0c\u800c\u662f\u4ee5\u5355\u5411\u7684\u65b9\u5f0f\u5c06\u6807\u7b7e\u8bed\u4e49\u5408\u5e76\u5230\u89c6\u89c9\u7279\u5f81\u4e2d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u793a\u9a71\u52a8\u7684\u89c6\u89c9\u8bed\u8a00\u8868\u5f81\u5b66\u4e60\uff08PVLR\uff09\u6846\u67b6\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u8bed\u8a00\u6a21\u6001\u7684\u529f\u80fd\u3002\u5728PVLR\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165\u4e86\u4e00\u79cd\u53cc\u91cd\u63d0\u793a\u7b56\u7565\uff0c\u5305\u62ec\u77e5\u8bc6\u611f\u77e5\u63d0\u793a\uff08KAP\uff09\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\uff08CAP\uff09\u3002 KAP \u5229\u7528\u56fa\u5b9a\u63d0\u793a\u6765\u6355\u83b7\u6240\u6709\u6807\u7b7e\u7684\u5185\u5728\u8bed\u4e49\u77e5\u8bc6\u548c\u5173\u7cfb\uff0c\u800c CAP \u5229\u7528\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u6765\u6355\u83b7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6807\u7b7e\u8bed\u4e49\u548c\u5173\u7cfb\u3002\u540e\u6765\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u548c\u878d\u5408\u6a21\u5757\uff08IFM\uff09\u6765\u4ea4\u4e92\u548c\u878d\u5408\u4ece KAP \u548c CAP \u83b7\u5f97\u7684\u8868\u793a\u3002\u4e0e\u4e4b\u524d\u4f5c\u54c1\u4e2d\u7684\u5355\u5411\u878d\u5408\u76f8\u6bd4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u53cc\u6a21\u6001\u6ce8\u610f\u529b\uff08DMA\uff09\uff0c\u5b83\u80fd\u591f\u5b9e\u73b0\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u4e4b\u95f4\u7684\u53cc\u5411\u4ea4\u4e92\uff0c\u4ea7\u751f\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6807\u7b7e\u8868\u793a\u548c\u8bed\u4e49\u76f8\u5173\u7684\u89c6\u89c9\u8868\u793a\uff0c\u968f\u540e\u7528\u4e8e\u8ba1\u7b97\u76f8\u4f3c\u6027\u5e76\u751f\u6210\u6240\u6709\u6807\u7b7e\u7684\u6700\u7ec8\u9884\u6d4b\u3002\u5728 MS-COCO\u3001Pascal VOC 2007 \u548c NUS-WIDE \u7b49\u4e09\u4e2a\u6d41\u884c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86 PVLR \u7684\u4f18\u8d8a\u6027\u3002|[2401.17881v1](http://arxiv.org/pdf/2401.17881v1)|null|\n", "2401.17879": "|**2024-01-31**|**AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error**|AEROBLADE\uff1a\u4f7f\u7528\u81ea\u52a8\u7f16\u7801\u5668\u91cd\u5efa\u8bef\u5dee\u5bf9\u6f5c\u5728\u6269\u6563\u56fe\u50cf\u8fdb\u884c\u514d\u8bad\u7ec3\u68c0\u6d4b|Jonas Ricker, Denis Lukovnikov, Asja Fischer|With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation. A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs). In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space. Despite their relevance, the forensic analysis of LDMs is still in its infancy. In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space. We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error. Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training. We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs including Stable Diffusion and Midjourney. Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions.|\u5229\u7528\u6700\u65b0\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u4efb\u4f55\u4eba\u90fd\u53ef\u4ee5\u751f\u6210\u5177\u6709\u4efb\u610f\u5185\u5bb9\u7684\u5177\u6709\u6b3a\u9a97\u6027\u7684\u771f\u5b9e\u56fe\u50cf\uff0c\u4ece\u800c\u52a0\u5267\u4e86\u89c6\u89c9\u865a\u5047\u4fe1\u606f\u65e5\u76ca\u589e\u957f\u7684\u5a01\u80c1\u3002\u4ee5\u4f4e\u8ba1\u7b97\u6210\u672c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u5173\u952e\u63a8\u52a8\u56e0\u7d20\u662f\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u7684\u5f00\u53d1\u3002\u4e0e\u4f20\u7edf\u7684\u6269\u6563\u6a21\u578b\u76f8\u6bd4\uff0cLDM \u5728\u9884\u8bad\u7ec3\u7684\u81ea\u52a8\u7f16\u7801\u5668 (AE) \u7684\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u800c\u4e0d\u662f\u9ad8\u7ef4\u56fe\u50cf\u7a7a\u95f4\u4e2d\u6267\u884c\u53bb\u566a\u8fc7\u7a0b\u3002\u5c3d\u7ba1\u5177\u6709\u76f8\u5173\u6027\uff0cLDM \u7684\u6cd5\u8bc1\u5206\u6790\u4ecd\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AEROBLADE\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528 LDM \u56fa\u6709\u7ec4\u4ef6\u7684\u65b0\u9896\u68c0\u6d4b\u65b9\u6cd5\uff1a\u7528\u4e8e\u5728\u56fe\u50cf\u548c\u6f5c\u5728\u7a7a\u95f4\u4e4b\u95f4\u8f6c\u6362\u56fe\u50cf\u7684 AE\u3002\u6211\u4eec\u53d1\u73b0\uff0cAE \u53ef\u4ee5\u6bd4\u771f\u5b9e\u56fe\u50cf\u66f4\u51c6\u786e\u5730\u91cd\u5efa\u751f\u6210\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u5141\u8bb8\u57fa\u4e8e\u91cd\u5efa\u8bef\u5dee\u7684\u7b80\u5355\u68c0\u6d4b\u65b9\u6cd5\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6613\u4e8e\u5b9e\u73b0\uff0c\u4e0d\u9700\u8981\u4efb\u4f55\u8bad\u7ec3\uff0c\u4f46\u51e0\u4e4e\u4e0e\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u7684\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u76f8\u5339\u914d\u3002\u6211\u4eec\u51ed\u7ecf\u9a8c\u8bc1\u660e AEROBLADE \u53ef\u6709\u6548\u5bf9\u6297\u6700\u5148\u8fdb\u7684 LDM\uff0c\u5305\u62ec\u7a33\u5b9a\u6269\u6563\u548c\u4e2d\u7a0b\u3002\u9664\u4e86\u68c0\u6d4b\u4e4b\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fd8\u53ef\u4ee5\u5bf9\u56fe\u50cf\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u8fd9\u53ef\u7528\u4e8e\u8bc6\u522b\u4fee\u590d\u533a\u57df\u3002|[2401.17879v1](http://arxiv.org/pdf/2401.17879v1)|null|\n", "2401.17874": "|**2024-01-31**|**VR-based generation of photorealistic synthetic data for training hand-object tracking models**|\u57fa\u4e8e VR \u751f\u6210\u903c\u771f\u7684\u5408\u6210\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec3\u624b\u90e8\u7269\u4f53\u8ddf\u8e2a\u6a21\u578b|Chengyan Zhang, Rahul Chaudhari|Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training. Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object pose) on 2D images. To address these issues, we present \"blender-hoisynth\", an interactive synthetic data generator based on the Blender software. Blender-hoisynth can scalably generate and automatically annotate visual HOI training data. Other competing approaches usually generate synthetic HOI data compeletely without human input. While this may be beneficial in some scenarios, HOI applications inherently necessitate direct control over the HOIs as an expression of human intent. With blender-hoisynth, it is possible for users to interact with objects via virtual hands using standard Virtual Reality hardware. The synthetically generated data are characterized by a high degree of photorealism and contain visually plausible and physically realistic videos of hands grasping objects and moving them around in 3D. To demonstrate the efficacy of our data generation, we replace large parts of the training data in the well-known DexYCB dataset with hoisynth data and train a state-of-the-art HOI reconstruction model with it. We show that there is no significant degradation in the model performance despite the data replacement.|\u7528\u4e8e\u7cbe\u786e\u8ddf\u8e2a 3D \u624b\u90e8\u7269\u4f53\u4ea4\u4e92 (HOI) \u7684\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u975e\u4e13\u5bb6\u6765\u8bf4\uff0c\u5728 2D \u56fe\u50cf\u4e0a\u6807\u8bb0 3D \u5730\u9762\u5b9e\u51b5\uff08\u4f8b\u5982 6DoF \u5bf9\u8c61\u59ff\u52bf\uff09\u5e76\u4e0d\u76f4\u89c2\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63a8\u51fa\u4e86\u201cblender-hoisynth\u201d\uff0c\u4e00\u4e2a\u57fa\u4e8e Blender \u8f6f\u4ef6\u7684\u4ea4\u4e92\u5f0f\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u3002 Blender-hoisynth \u53ef\u4ee5\u53ef\u6269\u5c55\u5730\u751f\u6210\u5e76\u81ea\u52a8\u6ce8\u91ca\u53ef\u89c6\u5316 HOI \u8bad\u7ec3\u6570\u636e\u3002\u5176\u4ed6\u7ade\u4e89\u65b9\u6cd5\u901a\u5e38\u5b8c\u5168\u65e0\u9700\u4eba\u5de5\u200b\u200b\u8f93\u5165\u5373\u53ef\u751f\u6210\u5408\u6210 HOI \u6570\u636e\u3002\u867d\u7136\u8fd9\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u662f\u6709\u76ca\u7684\uff0c\u4f46 HOI \u5e94\u7528\u7a0b\u5e8f\u672c\u8d28\u4e0a\u9700\u8981\u76f4\u63a5\u63a7\u5236 HOI \u4f5c\u4e3a\u4eba\u7c7b\u610f\u56fe\u7684\u8868\u8fbe\u3002\u501f\u52a9 Blender-hoisynth\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u865a\u62df\u73b0\u5b9e\u786c\u4ef6\u901a\u8fc7\u865a\u62df\u624b\u4e0e\u5bf9\u8c61\u8fdb\u884c\u4ea4\u4e92\u3002\u5408\u6210\u751f\u6210\u7684\u6570\u636e\u5177\u6709\u9ad8\u5ea6\u771f\u5b9e\u611f\u7684\u7279\u70b9\uff0c\u5305\u542b\u89c6\u89c9\u4e0a\u5408\u7406\u4e14\u7269\u7406\u4e0a\u771f\u5b9e\u7684\u624b\u90e8\u6293\u63e1\u7269\u4f53\u5e76\u4ee5 3D \u65b9\u5f0f\u79fb\u52a8\u7269\u4f53\u7684\u89c6\u9891\u3002\u4e3a\u4e86\u8bc1\u660e\u6211\u4eec\u6570\u636e\u751f\u6210\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u7528 hoisynth \u6570\u636e\u66ff\u6362\u4e86\u8457\u540d\u7684 DexYCB \u6570\u636e\u96c6\u4e2d\u7684\u5927\u90e8\u5206\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u7528\u5b83\u8bad\u7ec3\u4e86\u6700\u5148\u8fdb\u7684 HOI \u91cd\u5efa\u6a21\u578b\u3002\u6211\u4eec\u8868\u660e\uff0c\u5c3d\u7ba1\u8fdb\u884c\u4e86\u6570\u636e\u66ff\u6362\uff0c\u6a21\u578b\u6027\u80fd\u5e76\u6ca1\u6709\u663e\u7740\u4e0b\u964d\u3002|[2401.17874v1](http://arxiv.org/pdf/2401.17874v1)|null|\n", "2401.17868": "|**2024-01-31**|**Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model**|\u5377\u79ef\u9047\u89c1 LoRA\uff1a\u5206\u6bb5\u4efb\u610f\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03|Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, Chun Yuan|The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic segmentation tasks.|Segment Anything Model (SAM) \u662f\u56fe\u50cf\u5206\u5272\u7684\u57fa\u7840\u6846\u67b6\u3002\u867d\u7136\u5b83\u5728\u5178\u578b\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u663e\u7740\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5f53\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u548c\u9065\u611f\u7b49\u4e13\u4e1a\u9886\u57df\u65f6\uff0c\u5176\u4f18\u52bf\u5c31\u4f1a\u51cf\u5f31\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u672c\u6587\u5f15\u5165\u4e86 Conv-LoRA\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u8d85\u8f7b\u91cf\u7ea7\u5377\u79ef\u53c2\u6570\u96c6\u6210\u5230\u4f4e\u79e9\u9002\u5e94 (LoRA) \u4e2d\uff0cConv-LoRA \u53ef\u4ee5\u5c06\u4e0e\u56fe\u50cf\u76f8\u5173\u7684\u5f52\u7eb3\u504f\u5dee\u6ce8\u5165\u5230\u666e\u901a ViT \u7f16\u7801\u5668\u4e2d\uff0c\u8fdb\u4e00\u6b65\u5f3a\u5316 SAM \u7684\u5c40\u90e8\u5148\u9a8c\u5047\u8bbe\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cConv-LoRA \u4e0d\u4ec5\u4fdd\u7559\u4e86 SAM \u5e7f\u6cdb\u7684\u5206\u5272\u77e5\u8bc6\uff0c\u800c\u4e14\u8fd8\u6062\u590d\u4e86\u5176\u5b66\u4e60\u9ad8\u7ea7\u56fe\u50cf\u8bed\u4e49\u7684\u80fd\u529b\uff0c\u800c\u8fd9\u79cd\u80fd\u529b\u53d7\u5230 SAM \u524d\u666f-\u80cc\u666f\u5206\u5272\u9884\u8bad\u7ec3\u7684\u9650\u5236\u3002\u8de8\u591a\u4e2a\u9886\u57df\u7684\u4e0d\u540c\u57fa\u51c6\u7684\u7efc\u5408\u5b9e\u9a8c\u5f3a\u8c03\u4e86 Conv-LoRA \u5728\u4f7f SAM \u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002|[2401.17868v1](http://arxiv.org/pdf/2401.17868v1)|null|\n", "2401.17857": "|**2024-01-31**|**Semantic Anything in 3D Gaussians**|3D \u9ad8\u65af\u4e2d\u7684\u4efb\u4f55\u8bed\u4e49|Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li, Zhaoxiang Zhang|3D Gaussian Splatting has emerged as an alternative 3D representation of Neural Radiance Fields (NeRFs), benefiting from its high-quality rendering results and real-time rendering speed. Considering the 3D Gaussian representation remains unparsed, it is necessary first to execute object segmentation within this domain. Subsequently, scene editing and collision detection can be performed, proving vital to a multitude of applications, such as virtual reality (VR), augmented reality (AR), game/movie production, etc. In this paper, we propose a novel approach to achieve object segmentation in 3D Gaussian via an interactive procedure without any training process and learned parameters. We refer to the proposed method as SA-GS, for Segment Anything in 3D Gaussians. Given a set of clicked points in a single input view, SA-GS can generalize SAM to achieve 3D consistent segmentation via the proposed multi-view mask generation and view-wise label assignment methods. We also propose a cross-view label-voting approach to assign labels from different views. In addition, in order to address the boundary roughness issue of segmented objects resulting from the non-negligible spatial sizes of 3D Gaussian located at the boundary, SA-GS incorporates the simple but effective Gaussian Decomposition scheme. Extensive experiments demonstrate that SA-GS achieves high-quality 3D segmentation results, which can also be easily applied for scene editing and collision detection tasks. Codes will be released soon.|\u53d7\u76ca\u4e8e\u5176\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u7ed3\u679c\u548c\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\uff0c3D \u9ad8\u65af\u5206\u5e03\u5df2\u6210\u4e3a\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u7684\u66ff\u4ee3 3D \u8868\u793a\u5f62\u5f0f\u3002\u8003\u8651\u5230 3D \u9ad8\u65af\u8868\u793a\u4ecd\u672a\u89e3\u6790\uff0c\u6709\u5fc5\u8981\u9996\u5148\u5728\u8be5\u57df\u5185\u6267\u884c\u5bf9\u8c61\u5206\u5272\u3002\u968f\u540e\uff0c\u53ef\u4ee5\u6267\u884c\u573a\u666f\u7f16\u8f91\u548c\u78b0\u649e\u68c0\u6d4b\uff0c\u8fd9\u5bf9\u4e8e\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u3001\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u3001\u6e38\u620f/\u7535\u5f71\u5236\u4f5c\u7b49\u591a\u79cd\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u901a\u8fc7\u4ea4\u4e92\u5f0f\u7a0b\u5e8f\u5b9e\u73b0 3D \u9ad8\u65af\u5bf9\u8c61\u5206\u5272\uff0c\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u8fc7\u7a0b\u548c\u5b66\u4e60\u53c2\u6570\u3002\u6211\u4eec\u5c06\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u79f0\u4e3a SA-GS\uff0c\u5373 3D \u9ad8\u65af\u4e2d\u7684\u4efb\u610f\u5206\u6bb5\u3002\u7ed9\u5b9a\u5355\u4e2a\u8f93\u5165\u89c6\u56fe\u4e2d\u7684\u4e00\u7ec4\u70b9\u51fb\u70b9\uff0cSA-GS \u53ef\u4ee5\u63a8\u5e7f SAM\uff0c\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u591a\u89c6\u56fe\u63a9\u6a21\u751f\u6210\u548c\u89c6\u56fe\u65b9\u5f0f\u6807\u7b7e\u5206\u914d\u65b9\u6cd5\u6765\u5b9e\u73b0 3D \u4e00\u81f4\u5206\u5272\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u89c6\u56fe\u6807\u7b7e\u6295\u7968\u65b9\u6cd5\u6765\u5206\u914d\u6765\u81ea\u4e0d\u540c\u89c6\u56fe\u7684\u6807\u7b7e\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u89e3\u51b3\u7531\u4e8e\u8fb9\u754c\u5904\u76843D\u9ad8\u65af\u7a7a\u95f4\u5c3a\u5bf8\u4e0d\u53ef\u5ffd\u7565\u800c\u5bfc\u81f4\u7684\u5206\u5272\u5bf9\u8c61\u7684\u8fb9\u754c\u7c97\u7cd9\u5ea6\u95ee\u9898\uff0cSA-GS\u91c7\u7528\u4e86\u7b80\u5355\u4f46\u6709\u6548\u7684\u9ad8\u65af\u5206\u89e3\u65b9\u6848\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSA-GS \u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684 3D \u5206\u5272\u7ed3\u679c\uff0c\u4e5f\u53ef\u4ee5\u8f7b\u677e\u5e94\u7528\u4e8e\u573a\u666f\u7f16\u8f91\u548c\u78b0\u649e\u68c0\u6d4b\u4efb\u52a1\u3002\u4ee3\u7801\u5373\u5c06\u53d1\u5e03\u3002|[2401.17857v1](http://arxiv.org/pdf/2401.17857v1)|null|\n", "2401.17851": "|**2024-01-31**|**Instruction-Guided Scene Text Recognition**|\u6307\u4ee4\u5f15\u5bfc\u7684\u573a\u666f\u6587\u672c\u8bc6\u522b|Yongkun Du, Zhineng Chen, Yuchen Su, Caiyan Jia, Yu-Gang Jiang|Multi-modal models have shown appealing performance in visual tasks recently, as instruction-guided training has evoked the ability to understand fine-grained visual content. However, current methods cannot be trivially applied to scene text recognition (STR) due to the gap between natural and text images. In this paper, we introduce a novel paradigm that formulates STR as an instruction learning problem, and propose instruction-guided scene text recognition (IGTR) to achieve effective cross-modal learning. IGTR first generates rich and diverse instruction triplets of <condition,question,answer>, serving as guidance for nuanced text image understanding. Then, we devise an architecture with dedicated cross-modal feature fusion module, and multi-task answer head to effectively fuse the required instruction and image features for answering questions. Built upon these designs, IGTR facilitates accurate text recognition by comprehending character attributes. Experiments on English and Chinese benchmarks show that IGTR outperforms existing models by significant margins. Furthermore, by adjusting the instructions, IGTR enables various recognition schemes. These include zero-shot prediction, where the model is trained based on instructions not explicitly targeting character recognition, and the recognition of rarely appearing and morphologically similar characters, which were previous challenges for existing models.|\u591a\u6a21\u6001\u6a21\u578b\u6700\u8fd1\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u5f15\u4eba\u6ce8\u76ee\u7684\u6027\u80fd\uff0c\u56e0\u4e3a\u6307\u4ee4\u5f15\u5bfc\u7684\u8bad\u7ec3\u6fc0\u53d1\u4e86\u7406\u89e3\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5185\u5bb9\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u81ea\u7136\u56fe\u50cf\u548c\u6587\u672c\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u4e0d\u80fd\u8f7b\u6613\u5e94\u7528\u4e8e\u573a\u666f\u6587\u672c\u8bc6\u522b\uff08STR\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8303\u5f0f\uff0c\u5c06 STR \u8868\u8ff0\u4e3a\u6307\u4ee4\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6307\u4ee4\u5f15\u5bfc\u573a\u666f\u6587\u672c\u8bc6\u522b\uff08IGTR\uff09\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u6a21\u6001\u5b66\u4e60\u3002 IGTR \u9996\u5148\u751f\u6210\u4e30\u5bcc\u591a\u6837\u7684\u6307\u4ee4\u4e09\u5143\u7ec4<\u6761\u4ef6\u3001\u95ee\u9898\u3001\u7b54\u6848>\uff0c\u4f5c\u4e3a\u7ec6\u81f4\u5165\u5fae\u7684\u6587\u672c\u56fe\u50cf\u7406\u89e3\u7684\u6307\u5bfc\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5177\u6709\u4e13\u7528\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u6a21\u5757\u548c\u591a\u4efb\u52a1\u7b54\u6848\u5934\u7684\u67b6\u6784\uff0c\u4ee5\u6709\u6548\u878d\u5408\u56de\u7b54\u95ee\u9898\u6240\u9700\u7684\u6307\u4ee4\u548c\u56fe\u50cf\u7279\u5f81\u3002\u57fa\u4e8e\u8fd9\u4e9b\u8bbe\u8ba1\uff0cIGTR \u901a\u8fc7\u7406\u89e3\u5b57\u7b26\u5c5e\u6027\u6765\u4fc3\u8fdb\u51c6\u786e\u7684\u6587\u672c\u8bc6\u522b\u3002\u5728\u82f1\u8bed\u548c\u4e2d\u6587\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIGTR \u660e\u663e\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8c03\u6574\u6307\u4ee4\uff0cIGTR\u53ef\u4ee5\u5b9e\u73b0\u5404\u79cd\u8bc6\u522b\u65b9\u6848\u3002\u5176\u4e2d\u5305\u62ec\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u5176\u4e2d\u6a21\u578b\u662f\u6839\u636e\u672a\u660e\u786e\u9488\u5bf9\u5b57\u7b26\u8bc6\u522b\u7684\u6307\u4ee4\u8fdb\u884c\u8bad\u7ec3\u7684\uff0c\u4ee5\u53ca\u5f88\u5c11\u51fa\u73b0\u548c\u5f62\u6001\u76f8\u4f3c\u7684\u5b57\u7b26\u7684\u8bc6\u522b\uff0c\u8fd9\u662f\u73b0\u6709\u6a21\u578b\u4e4b\u524d\u9762\u4e34\u7684\u6311\u6218\u3002|[2401.17851v1](http://arxiv.org/pdf/2401.17851v1)|null|\n", "2401.17828": "|**2024-01-31**|**Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation**|\u5229\u7528 Swin Transformer \u8fdb\u884c\u672c\u5730\u5230\u5168\u5c40\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272|Rozhan Ahmadi, Shohreh Kasaei|In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision. Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs). Due to the localized pattern detection of Convolutional Neural Networks (CNNs), CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background. Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs. However, the use of hierarchical ViTs has not been extensively explored in this field. This work explores the use of Swin Transformer by proposing \"SWTformer\" to enhance the accuracy of the initial seed CAMs by bringing local and global views together. SWTformer-V1 generates class probabilities and CAMs using only the patch tokens as features. SWTformer-V2 incorporates a multi-scale feature fusion mechanism to extract additional information and utilizes a background-aware mechanism to generate more accurate localization maps with improved cross-object discrimination. Based on experiments on the PascalVOC 2012 dataset, SWTformer-V1 achieves a 0.98% mAP higher localization accuracy, outperforming state-of-the-art models. It also yields comparable performance by 0.82% mIoU on average higher than other methods in generating initial localization maps, depending only on the classification network. SWTformer-V2 further improves the accuracy of the generated seed CAMs by 5.32% mIoU, further proving the effectiveness of the local-to-global view provided by the Swin transformer.|\u8fd1\u5e74\u6765\uff0c\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u4f5c\u4e3a\u76d1\u7763\u7684\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u53d7\u5230\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5173\u6ce8\u901a\u8fc7\u7c7b\u6fc0\u6d3b\u56fe\uff08CAM\uff09\u751f\u6210\u4f2a\u6807\u7b7e\u6765\u4fc3\u8fdb\u76d1\u7763\u5b66\u4e60\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u6807\u7b7e\u4e2d\u7f3a\u4e4f\u7a7a\u95f4\u4fe1\u606f\u6240\u5e26\u6765\u7684\u6311\u6218\u3002\u7531\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7684\u5c40\u90e8\u6a21\u5f0f\u68c0\u6d4b\uff0cCAM \u901a\u5e38\u53ea\u5f3a\u8c03\u5bf9\u8c61\u4e2d\u6700\u5177\u8fa8\u522b\u529b\u7684\u90e8\u5206\uff0c\u8fd9\u4f7f\u5f97\u51c6\u786e\u5730\u533a\u5206\u524d\u666f\u5bf9\u8c61\u4e0e\u80cc\u666f\u5bf9\u8c61\u53d8\u5f97\u56f0\u96be\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cVision Transformer (ViT) \u7279\u5f81\u7531\u4e8e\u5176\u5168\u5c40\u89c6\u56fe\uff0c\u5728\u6355\u83b7\u573a\u666f\u5e03\u5c40\u65b9\u9762\u6bd4 CNN \u66f4\u6709\u6548\u3002\u7136\u800c\uff0c\u5206\u5c42 ViT \u7684\u4f7f\u7528\u5c1a\u672a\u5728\u8be5\u9886\u57df\u5f97\u5230\u5e7f\u6cdb\u63a2\u7d22\u3002\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u63d0\u51fa\u201cSWTformer\u201d\u6765\u63a2\u7d22 Swin Transformer \u7684\u4f7f\u7528\uff0c\u901a\u8fc7\u5c06\u672c\u5730\u548c\u5168\u5c40\u89c6\u56fe\u7ed3\u5408\u5728\u4e00\u8d77\u6765\u63d0\u9ad8\u521d\u59cb\u79cd\u5b50 CAM \u7684\u51c6\u786e\u6027\u3002 SWTformer-V1 \u4ec5\u4f7f\u7528\u8865\u4e01\u6807\u8bb0\u4f5c\u4e3a\u7279\u5f81\u6765\u751f\u6210\u7c7b\u6982\u7387\u548c CAM\u3002 SWTformer-V2 \u91c7\u7528\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u673a\u5236\u6765\u63d0\u53d6\u9644\u52a0\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u80cc\u666f\u611f\u77e5\u673a\u5236\u6765\u751f\u6210\u66f4\u51c6\u786e\u7684\u5b9a\u4f4d\u56fe\uff0c\u5e76\u6539\u8fdb\u8de8\u5bf9\u8c61\u8fa8\u522b\u80fd\u529b\u3002\u57fa\u4e8e PascalVOC 2012 \u6570\u636e\u96c6\u7684\u5b9e\u9a8c\uff0cSWTformer-V1 \u5b9e\u73b0\u4e86 0.98% mAP \u7684\u66f4\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002\u5728\u751f\u6210\u521d\u59cb\u5b9a\u4f4d\u56fe\u65b9\u9762\uff0c\u5b83\u7684\u6027\u80fd\u5e73\u5747\u6bd4\u5176\u4ed6\u65b9\u6cd5\u9ad8 0.82% mIoU\uff0c\u4ec5\u53d6\u51b3\u4e8e\u5206\u7c7b\u7f51\u7edc\u3002 SWTformer-V2 \u5c06\u751f\u6210\u7684\u79cd\u5b50 CAM \u7684\u7cbe\u5ea6\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86 5.32% mIoU\uff0c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86 Swin \u53d8\u538b\u5668\u63d0\u4f9b\u7684\u5c40\u90e8\u5230\u5168\u5c40\u89c6\u56fe\u7684\u6709\u6548\u6027\u3002|[2401.17828v1](http://arxiv.org/pdf/2401.17828v1)|null|\n", "2401.17821": "|**2024-01-31**|**Do Object Detection Localization Errors Affect Human Performance and Trust?**|\u5bf9\u8c61\u68c0\u6d4b\u5b9a\u4f4d\u9519\u8bef\u4f1a\u5f71\u54cd\u4eba\u7c7b\u8868\u73b0\u548c\u4fe1\u4efb\u5417\uff1f|Sven de Witte, Ombretta Strafforello, Jan van Gemert|Bounding boxes are often used to communicate automatic object detection results to humans, aiding humans in a multitude of tasks. We investigate the relationship between bounding box localization errors and human task performance. We use observer performance studies on a visual multi-object counting task to measure both human trust and performance with different levels of bounding box accuracy. The results show that localization errors have no significant impact on human accuracy or trust in the system. Recall and precision errors impact both human performance and trust, suggesting that optimizing algorithms based on the F1 score is more beneficial in human-computer tasks. Lastly, the paper offers an improvement on bounding boxes in multi-object counting tasks with center dots, showing improved performance and better resilience to localization inaccuracy.|\u8fb9\u754c\u6846\u901a\u5e38\u7528\u4e8e\u5411\u4eba\u7c7b\u4f20\u8fbe\u81ea\u52a8\u5bf9\u8c61\u68c0\u6d4b\u7ed3\u679c\uff0c\u5e2e\u52a9\u4eba\u7c7b\u5b8c\u6210\u591a\u79cd\u4efb\u52a1\u3002\u6211\u4eec\u7814\u7a76\u4e86\u8fb9\u754c\u6846\u5b9a\u4f4d\u9519\u8bef\u4e0e\u4eba\u5de5\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6211\u4eec\u4f7f\u7528\u89c6\u89c9\u591a\u5bf9\u8c61\u8ba1\u6570\u4efb\u52a1\u7684\u89c2\u5bdf\u8005\u6027\u80fd\u7814\u7a76\u6765\u8861\u91cf\u4e0d\u540c\u7ea7\u522b\u7684\u8fb9\u754c\u6846\u51c6\u786e\u5ea6\u4e0b\u7684\u4eba\u7c7b\u4fe1\u4efb\u548c\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5b9a\u4f4d\u9519\u8bef\u5bf9\u4eba\u7c7b\u51c6\u786e\u6027\u6216\u5bf9\u7cfb\u7edf\u7684\u4fe1\u4efb\u6ca1\u6709\u91cd\u5927\u5f71\u54cd\u3002\u53ec\u56de\u7387\u548c\u7cbe\u5ea6\u9519\u8bef\u90fd\u4f1a\u5f71\u54cd\u4eba\u7c7b\u7684\u8868\u73b0\u548c\u4fe1\u4efb\uff0c\u8fd9\u8868\u660e\u57fa\u4e8e F1 \u5206\u6570\u7684\u4f18\u5316\u7b97\u6cd5\u5728\u4eba\u673a\u4efb\u52a1\u4e2d\u66f4\u6709\u5229\u3002\u6700\u540e\uff0c\u672c\u6587\u5bf9\u5e26\u6709\u4e2d\u5fc3\u70b9\u7684\u591a\u5bf9\u8c61\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u8fb9\u754c\u6846\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u663e\u793a\u51fa\u6539\u8fdb\u7684\u6027\u80fd\u548c\u5bf9\u5b9a\u4f4d\u4e0d\u51c6\u786e\u7684\u66f4\u597d\u7684\u6062\u590d\u80fd\u529b\u3002|[2401.17821v1](http://arxiv.org/pdf/2401.17821v1)|null|\n", "2401.17803": "|**2024-01-31**|**SimAda: A Simple Unified Framework for Adapting Segment Anything Model in Underperformed Scenes**|SimAda\uff1a\u4e00\u4e2a\u7b80\u5355\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8868\u73b0\u4e0d\u4f73\u7684\u573a\u666f\u4e2d\u8c03\u6574\u5206\u6bb5\u4efb\u610f\u6a21\u578b|Yiran Song, Qianyu Zhou, Xuequan Lu, Zhiwen Shao, Lizhuang Ma|Segment anything model (SAM) has demonstrated excellent generalization capabilities in common vision scenarios, yet lacking an understanding of specialized data. Although numerous works have focused on optimizing SAM for downstream tasks, these task-specific approaches usually limit the generalizability to other downstream tasks. In this paper, we aim to investigate the impact of the general vision modules on finetuning SAM and enable them to generalize across all downstream tasks. We propose a simple unified framework called SimAda for adapting SAM in underperformed scenes. Specifically, our framework abstracts the general modules of different methods into basic design elements, and we design four variants based on a shared theoretical framework. SimAda is simple yet effective, which removes all dataset-specific designs and focuses solely on general optimization, ensuring that SimAda can be applied to all SAM-based and even Transformer-based models. We conduct extensive experiments on nine datasets of six downstream tasks. The results demonstrate that SimAda significantly improves the performance of SAM on multiple downstream tasks and achieves state-of-the-art performance on most of them, without requiring task-specific designs. Code is available at: https://github.com/zongzi13545329/SimAda|\u5206\u6bb5\u4efb\u610f\u6a21\u578b\uff08SAM\uff09\u5728\u5e38\u89c1\u89c6\u89c9\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e13\u4e1a\u6570\u636e\u7684\u7406\u89e3\u3002\u5c3d\u7ba1\u8bb8\u591a\u5de5\u4f5c\u90fd\u4e13\u6ce8\u4e8e\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u4f18\u5316 SAM\uff0c\u4f46\u8fd9\u4e9b\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u65b9\u6cd5\u901a\u5e38\u9650\u5236\u4e86\u5bf9\u5176\u4ed6\u4e0b\u6e38\u4efb\u52a1\u7684\u901a\u7528\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u65e8\u5728\u7814\u7a76\u901a\u7528\u89c6\u89c9\u6a21\u5757\u5bf9 SAM \u5fae\u8c03\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u5b83\u4eec\u80fd\u591f\u6cdb\u5316\u5230\u6240\u6709\u4e0b\u6e38\u4efb\u52a1\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a SimAda \u7684\u7b80\u5355\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8868\u73b0\u4e0d\u4f73\u7684\u573a\u666f\u4e2d\u8c03\u6574 SAM\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5c06\u4e0d\u540c\u65b9\u6cd5\u7684\u901a\u7528\u6a21\u5757\u62bd\u8c61\u4e3a\u57fa\u672c\u8bbe\u8ba1\u5143\u7d20\uff0c\u5e76\u57fa\u4e8e\u5171\u4eab\u7684\u7406\u8bba\u6846\u67b6\u8bbe\u8ba1\u4e86\u56db\u79cd\u53d8\u4f53\u3002 SimAda \u7b80\u5355\u800c\u6709\u6548\uff0c\u5b83\u6d88\u9664\u4e86\u6240\u6709\u7279\u5b9a\u4e8e\u6570\u636e\u96c6\u7684\u8bbe\u8ba1\uff0c\u53ea\u4e13\u6ce8\u4e8e\u4e00\u822c\u4f18\u5316\uff0c\u786e\u4fdd SimAda \u53ef\u4ee5\u5e94\u7528\u4e8e\u6240\u6709\u57fa\u4e8e SAM \u751a\u81f3\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u3002\u6211\u4eec\u5bf9\u516d\u4e2a\u4e0b\u6e38\u4efb\u52a1\u7684\u4e5d\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0cSimAda \u663e\u7740\u63d0\u9ad8\u4e86 SAM \u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u8bbe\u8ba1\u3002\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/zongzi13545329/SimAda|[2401.17803v1](http://arxiv.org/pdf/2401.17803v1)|null|\n", "2401.17759": "|**2024-01-31**|**Tiered approach for rapid damage characterisation of infrastructure enabled by remote sensing and deep learning technologies**|\u5229\u7528\u9065\u611f\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5feb\u901f\u8868\u5f81\u57fa\u7840\u8bbe\u65bd\u635f\u574f\u7684\u5206\u5c42\u65b9\u6cd5|Nadiia Kopiika, Andreas Karavias, Pavlos Krassakis, Zehao Ye, Jelena Ninic, Nataliya Shakhovska, Nikolaos Koukouzas, Sotirios Argyroudis, Stergios-Aristoteles Mitoulis|Critical infrastructure such as bridges are systematically targeted during wars and conflicts. This is because critical infrastructure is vital for enabling connectivity and transportation of people and goods, and hence, underpinning the national and international defence planning and economic growth. Mass destruction of bridges, along with minimal or no accessibility to these assets during natural and anthropogenic disasters, prevents us from delivering rapid recovery. As a result, systemic resilience is drastically reduced. A solution to this challenge is to use technology for stand-off observations. Yet, no method exists to characterise damage at different scales, i.e. regional, asset, and structural (component), and more so there is little or no systematic correlation between assessments at scale. We propose an integrated three-level tiered approach to fill this capability gap, and we demonstrate the methods for damage characterisation enabled by fit-for-purpose digital technologies. Next, this method is applied and validated to a case study in Ukraine that includes 17 bridges. From macro to micro, we deploy technology at scale, from Sentinel-1 SAR images, crowdsourced information, and high-resolution images to deep learning for damaged infrastructure. For the first time, the interferometric coherence difference and semantic segmentation of images were deployed to improve the reliability of damage characterisations from regional to infrastructure component level, when enhanced assessment accuracy is required. This integrated method improves the speed of decision-making, and thus, enhances resilience. Keywords: critical infrastructure, damage characterisation, targeted attacks, restoration|\u6865\u6881\u7b49\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u5728\u6218\u4e89\u548c\u51b2\u7a81\u671f\u95f4\u6210\u4e3a\u7cfb\u7edf\u6027\u653b\u51fb\u76ee\u6807\u3002\u8fd9\u662f\u56e0\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u5bf9\u4e8e\u5b9e\u73b0\u4eba\u5458\u548c\u8d27\u7269\u7684\u4e92\u8054\u4e92\u901a\u548c\u8fd0\u8f93\u81f3\u5173\u91cd\u8981\uff0c\u4ece\u800c\u652f\u6491\u56fd\u5bb6\u548c\u56fd\u9645\u56fd\u9632\u89c4\u5212\u548c\u7ecf\u6d4e\u589e\u957f\u3002\u6865\u6881\u7684\u5927\u89c4\u6a21\u7834\u574f\uff0c\u4ee5\u53ca\u5728\u81ea\u7136\u548c\u4eba\u4e3a\u707e\u5bb3\u671f\u95f4\u8fd9\u4e9b\u8d44\u4ea7\u7684\u8bbf\u95ee\u5f88\u5c11\u6216\u6839\u672c\u65e0\u6cd5\u8bbf\u95ee\uff0c\u4f7f\u6211\u4eec\u65e0\u6cd5\u5b9e\u73b0\u5feb\u901f\u6062\u590d\u3002\u7ed3\u679c\uff0c\u7cfb\u7edf\u5f39\u6027\u5927\u5927\u964d\u4f4e\u3002\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u7684\u4e00\u4e2a\u89e3\u51b3\u65b9\u6848\u662f\u4f7f\u7528\u8fdc\u8ddd\u79bb\u89c2\u6d4b\u6280\u672f\u3002\u7136\u800c\uff0c\u6ca1\u6709\u65b9\u6cd5\u53ef\u4ee5\u63cf\u8ff0\u4e0d\u540c\u89c4\u6a21\uff08\u5373\u533a\u57df\u3001\u8d44\u4ea7\u548c\u7ed3\u6784\uff08\u7ec4\u6210\u90e8\u5206\uff09\uff09\u7684\u635f\u5bb3\u7279\u5f81\uff0c\u66f4\u91cd\u8981\u7684\u662f\uff0c\u89c4\u6a21\u8bc4\u4f30\u4e4b\u95f4\u5f88\u5c11\u6216\u6ca1\u6709\u7cfb\u7edf\u76f8\u5173\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u7684\u4e09\u7ea7\u5206\u5c42\u65b9\u6cd5\u6765\u586b\u8865\u8fd9\u4e00\u80fd\u529b\u5dee\u8ddd\uff0c\u5e76\u6f14\u793a\u4e86\u7531\u9002\u5408\u7528\u9014\u7684\u6570\u5b57\u6280\u672f\u5b9e\u73b0\u7684\u635f\u4f24\u8868\u5f81\u65b9\u6cd5\u3002\u63a5\u4e0b\u6765\uff0c\u8be5\u65b9\u6cd5\u5728\u4e4c\u514b\u5170\u7684\u4e00\u4e2a\u5305\u62ec 17 \u5ea7\u6865\u6881\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u5f97\u5230\u5e94\u7528\u548c\u9a8c\u8bc1\u3002\u4ece\u5b8f\u89c2\u5230\u5fae\u89c2\uff0c\u6211\u4eec\u5927\u89c4\u6a21\u90e8\u7f72\u6280\u672f\uff0c\u4ece Sentinel-1 SAR \u56fe\u50cf\u3001\u4f17\u5305\u4fe1\u606f\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5230\u53d7\u635f\u57fa\u7840\u8bbe\u65bd\u7684\u6df1\u5ea6\u5b66\u4e60\u3002\u5f53\u9700\u8981\u63d0\u9ad8\u8bc4\u4f30\u7cbe\u5ea6\u65f6\uff0c\u9996\u6b21\u91c7\u7528\u5e72\u6d89\u76f8\u5e72\u5dee\u5f02\u548c\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u6765\u63d0\u9ad8\u4ece\u533a\u57df\u5230\u57fa\u7840\u8bbe\u65bd\u7ec4\u4ef6\u7ea7\u522b\u7684\u635f\u4f24\u8868\u5f81\u7684\u53ef\u9760\u6027\u3002\u8fd9\u79cd\u96c6\u6210\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51b3\u7b56\u901f\u5ea6\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5f39\u6027\u3002\u5173\u952e\u8bcd\uff1a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u3001\u635f\u5bb3\u7279\u5f81\u3001\u9488\u5bf9\u6027\u653b\u51fb\u3001\u6062\u590d|[2401.17759v1](http://arxiv.org/pdf/2401.17759v1)|null|\n", "2401.17736": "|**2024-01-31**|**Leveraging Human-Machine Interactions for Computer Vision Dataset Quality Enhancement**|\u5229\u7528\u4eba\u673a\u4ea4\u4e92\u63d0\u9ad8\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u96c6\u8d28\u91cf|Esla Timothy Anzaku, Hyesoo Hong, Jin-Woo Park, Wonjun Yang, Kangmin Kim, JongBum Won, Deshika Vinoshani Kumari Herath, Arnout Van Messem, Wesley De Neve|Large-scale datasets for single-label multi-class classification, such as \\emph{ImageNet-1k}, have been instrumental in advancing deep learning and computer vision. However, a critical and often understudied aspect is the comprehensive quality assessment of these datasets, especially regarding potential multi-label annotation errors. In this paper, we introduce a lightweight, user-friendly, and scalable framework that synergizes human and machine intelligence for efficient dataset validation and quality enhancement. We term this novel framework \\emph{Multilabelfy}. Central to Multilabelfy is an adaptable web-based platform that systematically guides annotators through the re-evaluation process, effectively leveraging human-machine interactions to enhance dataset quality. By using Multilabelfy on the ImageNetV2 dataset, we found that approximately $47.88\\%$ of the images contained at least two labels, underscoring the need for more rigorous assessments of such influential datasets. Furthermore, our analysis showed a negative correlation between the number of potential labels per image and model top-1 accuracy, illuminating a crucial factor in model evaluation and selection. Our open-source framework, Multilabelfy, offers a convenient, lightweight solution for dataset enhancement, emphasizing multi-label proportions. This study tackles major challenges in dataset integrity and provides key insights into model performance evaluation. Moreover, it underscores the advantages of integrating human expertise with machine capabilities to produce more robust models and trustworthy data development. The source code for Multilabelfy will be available at https://github.com/esla/Multilabelfy.   \\keywords{Computer Vision \\and Dataset Quality Enhancement \\and Dataset Validation \\and Human-Computer Interaction \\and Multi-label Annotation.}|\u7528\u4e8e\u5355\u6807\u7b7e\u591a\u7c7b\u5206\u7c7b\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4f8b\u5982 \\emph{ImageNet-1k}\uff0c\u5728\u63a8\u8fdb\u6df1\u5ea6\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u9762\u53d1\u6325\u4e86\u91cd\u8981\u4f5c\u7528\u3002\u7136\u800c\uff0c\u4e00\u4e2a\u5173\u952e\u4e14\u7ecf\u5e38\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u65b9\u9762\u662f\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u7efc\u5408\u8d28\u91cf\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5173\u4e8e\u6f5c\u5728\u7684\u591a\u6807\u7b7e\u6ce8\u91ca\u9519\u8bef\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u7528\u6237\u53cb\u597d\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u534f\u540c\u4eba\u7c7b\u548c\u673a\u5668\u667a\u80fd\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6570\u636e\u96c6\u9a8c\u8bc1\u548c\u8d28\u91cf\u589e\u5f3a\u3002\u6211\u4eec\u5c06\u8fd9\u4e2a\u65b0\u9896\u7684\u6846\u67b6\u79f0\u4e3a\\emph{Multilabelfy}\u3002 Multilabelfy \u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u9002\u5e94\u6027\u5f3a\u7684\u57fa\u4e8e\u7f51\u7edc\u7684\u5e73\u53f0\uff0c\u5b83\u7cfb\u7edf\u5730\u6307\u5bfc\u6ce8\u91ca\u8005\u5b8c\u6210\u91cd\u65b0\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u6709\u6548\u5730\u5229\u7528\u4eba\u673a\u4ea4\u4e92\u6765\u63d0\u9ad8\u6570\u636e\u96c6\u8d28\u91cf\u3002\u901a\u8fc7\u5728 ImageNetV2 \u6570\u636e\u96c6\u4e0a\u4f7f\u7528 Multilabelfy\uff0c\u6211\u4eec\u53d1\u73b0\u5927\u7ea6 $47.88\\%$ \u7684\u56fe\u50cf\u81f3\u5c11\u5305\u542b\u4e24\u4e2a\u6807\u7b7e\uff0c\u8fd9\u5f3a\u8c03\u4e86\u5bf9\u6b64\u7c7b\u6709\u5f71\u54cd\u529b\u7684\u6570\u636e\u96c6\u8fdb\u884c\u66f4\u4e25\u683c\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5206\u6790\u663e\u793a\u6bcf\u5f20\u56fe\u50cf\u7684\u6f5c\u5728\u6807\u7b7e\u6570\u91cf\u4e0e\u6a21\u578b top-1 \u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u8d1f\u76f8\u5173\uff0c\u8fd9\u9610\u660e\u4e86\u6a21\u578b\u8bc4\u4f30\u548c\u9009\u62e9\u7684\u5173\u952e\u56e0\u7d20\u3002\u6211\u4eec\u7684\u5f00\u6e90\u6846\u67b6 Multilabelfy \u4e3a\u6570\u636e\u96c6\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u4fbf\u3001\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u591a\u6807\u7b7e\u6bd4\u4f8b\u3002\u8fd9\u9879\u7814\u7a76\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u5b8c\u6574\u6027\u65b9\u9762\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5e76\u4e3a\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5f3a\u8c03\u4e86\u5c06\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u673a\u5668\u80fd\u529b\u76f8\u7ed3\u5408\u4ee5\u4ea7\u751f\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u6570\u636e\u5f00\u53d1\u7684\u4f18\u52bf\u3002 Multilabelfy \u7684\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/esla/Multilabelfy \u4e0a\u83b7\u53d6\u3002 \\keywords{\u8ba1\u7b97\u673a\u89c6\u89c9\\\u548c\u6570\u636e\u96c6\u8d28\u91cf\u589e\u5f3a\\\u548c\u6570\u636e\u96c6\u9a8c\u8bc1\\\u548c\u4eba\u673a\u4ea4\u4e92\\\u548c\u591a\u6807\u7b7e\u6ce8\u91ca\u3002}|[2401.17736v1](http://arxiv.org/pdf/2401.17736v1)|null|\n", "2401.17699": "|**2024-01-31**|**Unified Physical-Digital Face Attack Detection**|\u7edf\u4e00\u7684\u7269\u7406-\u6570\u5b57\u4eba\u8138\u653b\u51fb\u68c0\u6d4b|Hao Fang, Ajian Liu, Haocheng Yuan, Junze Zheng, Dingheng Zeng, Yanhong Liu, Jiankang Deng, Sergio Escalera, Xiaoming Liu, Jun Wan, et.al.|Face Recognition (FR) systems can suffer from physical (i.e., print photo) and digital (i.e., DeepFake) attacks. However, previous related work rarely considers both situations at the same time. This implies the deployment of multiple models and thus more computational burden. The main reasons for this lack of an integrated model are caused by two factors: (1) The lack of a dataset including both physical and digital attacks with ID consistency which means the same ID covers the real face and all attack types; (2) Given the large intra-class variance between these two attacks, it is difficult to learn a compact feature space to detect both attacks simultaneously. To address these issues, we collect a Unified physical-digital Attack dataset, called UniAttackData. The dataset consists of $1,800$ participations of 2 and 12 physical and digital attacks, respectively, resulting in a total of 29,706 videos. Then, we propose a Unified Attack Detection framework based on Vision-Language Models (VLMs), namely UniAttackDetection, which includes three main modules: the Teacher-Student Prompts (TSP) module, focused on acquiring unified and specific knowledge respectively; the Unified Knowledge Mining (UKM) module, designed to capture a comprehensive feature space; and the Sample-Level Prompt Interaction (SLPI) module, aimed at grasping sample-level semantics. These three modules seamlessly form a robust unified attack detection framework. Extensive experiments on UniAttackData and three other datasets demonstrate the superiority of our approach for unified face attack detection.|\u4eba\u8138\u8bc6\u522b (FR) \u7cfb\u7edf\u53ef\u80fd\u4f1a\u906d\u53d7\u7269\u7406\uff08\u5373\u6253\u5370\u7167\u7247\uff09\u548c\u6570\u5b57\uff08\u5373 DeepFake\uff09\u653b\u51fb\u3002\u7136\u800c\uff0c\u4ee5\u5f80\u7684\u76f8\u5173\u5de5\u4f5c\u5f88\u5c11\u540c\u65f6\u8003\u8651\u8fd9\u4e24\u79cd\u60c5\u51b5\u3002\u8fd9\u610f\u5473\u7740\u90e8\u7f72\u591a\u4e2a\u6a21\u578b\uff0c\u4ece\u800c\u5e26\u6765\u66f4\u591a\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002\u7f3a\u4e4f\u96c6\u6210\u6a21\u578b\u7684\u4e3b\u8981\u539f\u56e0\u6709\u4e24\u4e2a\u56e0\u7d20\uff1a\uff081\uff09\u7f3a\u4e4f\u5305\u542b\u7269\u7406\u653b\u51fb\u548c\u6570\u5b57\u653b\u51fb\u4e14\u5177\u6709ID\u4e00\u81f4\u6027\u7684\u6570\u636e\u96c6\uff0c\u5373\u76f8\u540c\u7684ID\u6db5\u76d6\u771f\u5b9e\u4eba\u8138\u548c\u6240\u6709\u653b\u51fb\u7c7b\u578b\uff1b \uff082\uff09\u8003\u8651\u5230\u8fd9\u4e24\u79cd\u653b\u51fb\u4e4b\u95f4\u5b58\u5728\u8f83\u5927\u7684\u7c7b\u5185\u65b9\u5dee\uff0c\u5f88\u96be\u5b66\u4e60\u7d27\u51d1\u7684\u7279\u5f81\u7a7a\u95f4\u6765\u540c\u65f6\u68c0\u6d4b\u8fd9\u4e24\u79cd\u653b\u51fb\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7269\u7406\u6570\u5b57\u653b\u51fb\u6570\u636e\u96c6\uff0c\u79f0\u4e3a UniAttackData\u3002\u8be5\u6570\u636e\u96c6\u5206\u522b\u5305\u542b 2 \u6b21\u548c 12 \u6b21\u7269\u7406\u653b\u51fb\u548c\u6570\u5b57\u653b\u51fb\uff0c\u4ef7\u503c 1,800 \u7f8e\u5143\uff0c\u603b\u5171 29,706 \u4e2a\u89c6\u9891\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u7edf\u4e00\u653b\u51fb\u68c0\u6d4b\u6846\u67b6\uff0c\u5373UniAttackDetection\uff0c\u5b83\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757\uff1a\u6559\u5e08-\u5b66\u751f\u63d0\u793a\uff08TSP\uff09\u6a21\u5757\uff0c\u5206\u522b\u4fa7\u91cd\u4e8e\u83b7\u53d6\u7edf\u4e00\u548c\u7279\u5b9a\u77e5\u8bc6\uff1b\u7edf\u4e00\u77e5\u8bc6\u6316\u6398\uff08UKM\uff09\u6a21\u5757\uff0c\u65e8\u5728\u6355\u83b7\u7efc\u5408\u7279\u5f81\u7a7a\u95f4\uff1b\u6837\u672c\u7ea7\u63d0\u793a\u4ea4\u4e92\uff08SLPI\uff09\u6a21\u5757\uff0c\u65e8\u5728\u638c\u63e1\u6837\u672c\u7ea7\u8bed\u4e49\u3002\u8fd9\u4e09\u4e2a\u6a21\u5757\u65e0\u7f1d\u5730\u6784\u6210\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u7edf\u4e00\u653b\u51fb\u68c0\u6d4b\u6846\u67b6\u3002\u5728 UniAttackData \u548c\u5176\u4ed6\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u7edf\u4e00\u4eba\u8138\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002|[2401.17699v1](http://arxiv.org/pdf/2401.17699v1)|null|\n", "2401.17695": "|**2024-01-31**|**Datacube segmentation via Deep Spectral Clustering**|\u901a\u8fc7\u6df1\u5ea6\u8c31\u805a\u7c7b\u8fdb\u884c\u6570\u636e\u7acb\u65b9\u5206\u5272|Alessandro Bombini, Fernando Garc\u00eda-Avello Bof\u00edas, Caterina Bracci, Michele Ginolfi, Chiara Ruberto|Extended Vision techniques are ubiquitous in physics. However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube.   Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering of data-cube's spectra, performed in a suitably defined low-dimensional embedding space.   To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels. A statistical dimensional reduction is performed by an ad hoc trained (Variational) AutoEncoder, in charge of mapping spectra into lower dimensional metric spaces, while the clustering process is performed by a (learnable) iterative K-Means clustering algorithm.   We apply this technique to two different use cases, of different physical origins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data on pictorial artworks, and a dataset of simulated astrophysical observations.|\u6269\u5c55\u89c6\u89c9\u6280\u672f\u5728\u7269\u7406\u5b66\u4e2d\u65e0\u5904\u4e0d\u5728\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4ece\u6784\u6210\u6570\u636e\u7acb\u65b9\u4f53\u7684\u5149\u8c31\u4e2d\u8fa8\u522b\u76f8\u5173\u4fe1\u606f\u7684\u5185\u5728\u56f0\u96be\uff0c\u4ece\u8fd9\u79cd\u5206\u6790\u4e2d\u4ea7\u751f\u7684\u6570\u636e\u7acb\u65b9\u4f53\u5e38\u5e38\u5bf9\u5b83\u4eec\u7684\u89e3\u91ca\u63d0\u51fa\u6311\u6218\u3002\u6b64\u5916\uff0c\u6570\u636e\u7acb\u65b9\u4f53\u5149\u8c31\u7684\u5de8\u5927\u7ef4\u6570\u5728\u5176\u7edf\u8ba1\u89e3\u91ca\u4e2d\u63d0\u51fa\u4e86\u590d\u6742\u7684\u4efb\u52a1\uff1b\u7136\u800c\uff0c\u8fd9\u79cd\u590d\u6742\u6027\u5305\u542b\u5927\u91cf\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u53ef\u4ee5\u4ee5\u65e0\u76d1\u7763\u7684\u65b9\u5f0f\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6765\u6982\u8ff0\u5f53\u524d\u6848\u4f8b\u7814\u7a76\u7684\u4e00\u4e9b\u57fa\u672c\u5c5e\u6027\uff0c\u4f8b\u5982\uff0c\u53ef\u4ee5\u901a\u8fc7\u6570\u636e\u7acb\u65b9\u4f53\u7684\uff08\u6df1\u5ea6\uff09\u805a\u7c7b\u6765\u83b7\u5f97\u56fe\u50cf\u5206\u5272\u5149\u8c31\uff0c\u5728\u9002\u5f53\u5b9a\u4e49\u7684\u4f4e\u7ef4\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6267\u884c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u4e3b\u9898\uff0c\u6211\u4eec\u63a2\u7d22\u5728\u7f16\u7801\u7a7a\u95f4\u4e2d\u5e94\u7528\u65e0\u76d1\u7763\u805a\u7c7b\u65b9\u6cd5\u7684\u53ef\u80fd\u6027\uff0c\u5373\u5bf9\u6570\u636e\u7acb\u65b9\u4f53\u50cf\u7d20\u7684\u5149\u8c31\u5c5e\u6027\u6267\u884c\u6df1\u5ea6\u805a\u7c7b\u3002\u7edf\u8ba1\u964d\u7ef4\u7531\u4e13\u95e8\u8bad\u7ec3\u7684\uff08\u53d8\u5206\uff09\u81ea\u52a8\u7f16\u7801\u5668\u6267\u884c\uff0c\u8d1f\u8d23\u5c06\u9891\u8c31\u6620\u5c04\u5230\u8f83\u4f4e\u7ef4\u7684\u5ea6\u91cf\u7a7a\u95f4\uff0c\u800c\u805a\u7c7b\u8fc7\u7a0b\u7531\uff08\u53ef\u5b66\u4e60\u7684\uff09\u8fed\u4ee3 K \u5747\u503c\u805a\u7c7b\u7b97\u6cd5\u6267\u884c\u3002\u6211\u4eec\u5c06\u8be5\u6280\u672f\u5e94\u7528\u4e8e\u5177\u6709\u4e0d\u540c\u7269\u7406\u8d77\u6e90\u7684\u4e24\u4e2a\u4e0d\u540c\u7528\u4f8b\uff1a\u4e00\u7ec4\u5173\u4e8e\u7ed8\u753b\u827a\u672f\u54c1\u7684\u5b8f\u89c2\u6620\u5c04 X \u5c04\u7ebf\u8367\u5149 (MA-XRF) \u5408\u6210\u6570\u636e\uff0c\u4ee5\u53ca\u4e00\u7ec4\u6a21\u62df\u5929\u4f53\u7269\u7406\u89c2\u6d4b\u6570\u636e\u3002|[2401.17695v1](http://arxiv.org/pdf/2401.17695v1)|null|\n", "2401.17654": "|**2024-01-31**|**All Beings Are Equal in Open Set Recognition**|\u5f00\u96c6\u8ba4\u8bc6\u4e2d\u4f17\u751f\u5e73\u7b49|Chaohua Li, Enhao Zhang, Chuanxing Geng, SongCan Chen|In open-set recognition (OSR), a promising strategy is exploiting pseudo-unknown data outside given $K$ known classes as an additional $K$+$1$-th class to explicitly model potential open space. However, treating unknown classes without distinction is unequal for them relative to known classes due to the category-agnostic and scale-agnostic of the unknowns. This inevitably not only disrupts the inherent distributions of unknown classes but also incurs both class-wise and instance-wise imbalances between known and unknown classes. Ideally, the OSR problem should model the whole class space as $K$+$\\infty$, but enumerating all unknowns is impractical. Since the core of OSR is to effectively model the boundaries of known classes, this means just focusing on the unknowns nearing the boundaries of targeted known classes seems sufficient. Thus, as a compromise, we convert the open classes from infinite to $K$, with a novel concept Target-Aware Universum (TAU) and propose a simple yet effective framework Dual Contrastive Learning with Target-Aware Universum (DCTAU). In details, guided by the targeted known classes, TAU automatically expands the unknown classes from the previous $1$ to $K$, effectively alleviating the distribution disruption and the imbalance issues mentioned above. Then, a novel Dual Contrastive (DC) loss is designed, where all instances irrespective of known or TAU are considered as positives to contrast with their respective negatives. Experimental results indicate DCTAU sets a new state-of-the-art.|\u5728\u5f00\u653e\u96c6\u8bc6\u522b (OSR) \u4e2d\uff0c\u4e00\u79cd\u6709\u524d\u9014\u7684\u7b56\u7565\u662f\u5229\u7528\u7ed9\u5b9a $K$ \u5df2\u77e5\u7c7b\u4e4b\u5916\u7684\u4f2a\u672a\u77e5\u6570\u636e\u4f5c\u4e3a\u9644\u52a0\u7684 $K$+$1$ \u7c7b\u6765\u663e\u5f0f\u5efa\u6a21\u6f5c\u5728\u7684\u5f00\u653e\u7a7a\u95f4\u3002\u7136\u800c\uff0c\u7531\u4e8e\u672a\u77e5\u7c7b\u4e0e\u7c7b\u522b\u65e0\u5173\u548c\u89c4\u6a21\u65e0\u5173\uff0c\u76f8\u5bf9\u4e8e\u5df2\u77e5\u7c7b\uff0c\u4e0d\u52a0\u533a\u522b\u5730\u5bf9\u5f85\u672a\u77e5\u7c7b\u662f\u4e0d\u5e73\u7b49\u7684\u3002\u8fd9\u4e0d\u53ef\u907f\u514d\u5730\u4e0d\u4ec5\u4f1a\u7834\u574f\u672a\u77e5\u7c7b\u7684\u56fa\u6709\u5206\u5e03\uff0c\u800c\u4e14\u8fd8\u4f1a\u5bfc\u81f4\u5df2\u77e5\u7c7b\u548c\u672a\u77e5\u7c7b\u4e4b\u95f4\u7684\u7c7b\u548c\u5b9e\u4f8b\u65b9\u9762\u7684\u4e0d\u5e73\u8861\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0cOSR \u95ee\u9898\u5e94\u8be5\u5c06\u6574\u4e2a\u7c7b\u7a7a\u95f4\u5efa\u6a21\u4e3a $K$+$\\infty$\uff0c\u4f46\u679a\u4e3e\u6240\u6709\u672a\u77e5\u6570\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002\u7531\u4e8e OSR \u7684\u6838\u5fc3\u662f\u6709\u6548\u5730\u5bf9\u5df2\u77e5\u7c7b\u7684\u8fb9\u754c\u8fdb\u884c\u5efa\u6a21\uff0c\u8fd9\u610f\u5473\u7740\u4ec5\u5173\u6ce8\u76ee\u6807\u5df2\u77e5\u7c7b\u8fb9\u754c\u9644\u8fd1\u7684\u672a\u77e5\u6570\u4f3c\u4e4e\u5c31\u8db3\u591f\u4e86\u3002\u56e0\u6b64\uff0c\u4f5c\u4e3a\u59a5\u534f\uff0c\u6211\u4eec\u5c06\u5f00\u653e\u7c7b\u4ece\u65e0\u9650\u8f6c\u6362\u4e3a$K$\uff0c\u91c7\u7528\u65b0\u9896\u7684\u6982\u5ff5\u76ee\u6807\u611f\u77e5\u5b87\u5b99\uff08TAU\uff09\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6846\u67b6\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u76ee\u6807\u611f\u77e5\u5b87\u5b99\uff08DCTAU\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u76ee\u6807\u5df2\u77e5\u7c7b\u522b\u7684\u5f15\u5bfc\u4e0b\uff0cTAU \u81ea\u52a8\u5c06\u672a\u77e5\u7c7b\u522b\u4ece\u4e4b\u524d\u7684 $1$ \u6269\u5c55\u5230 $K$\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4e0a\u8ff0\u5206\u914d\u4e2d\u65ad\u548c\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u7136\u540e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u91cd\u5bf9\u6bd4\uff08DC\uff09\u635f\u5931\uff0c\u5176\u4e2d\u6240\u6709\u5b9e\u4f8b\uff08\u65e0\u8bba\u5df2\u77e5\u8fd8\u662f TAU\uff09\u90fd\u88ab\u89c6\u4e3a\u6b63\u4f8b\uff0c\u4ee5\u4e0e\u5404\u81ea\u7684\u8d1f\u4f8b\u8fdb\u884c\u5bf9\u6bd4\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e DCTAU \u521b\u4e0b\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002|[2401.17654v1](http://arxiv.org/pdf/2401.17654v1)|null|\n", "2401.17604": "|**2024-01-31**|**Computation and Parameter Efficient Multi-Modal Fusion Transformer for Cued Speech Recognition**|\u7528\u4e8e\u63d0\u793a\u8bed\u97f3\u8bc6\u522b\u7684\u8ba1\u7b97\u548c\u53c2\u6570\u9ad8\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u53d8\u538b\u5668|Lei Liu, Li Liu, Haizhou Li|Cued Speech (CS) is a pure visual coding method used by hearing-impaired people that combines lip reading with several specific hand shapes to make the spoken language visible. Automatic CS recognition (ACSR) seeks to transcribe visual cues of speech into text, which can help hearing-impaired people to communicate effectively. The visual information of CS contains lip reading and hand cueing, thus the fusion of them plays an important role in ACSR. However, most previous fusion methods struggle to capture the global dependency present in long sequence inputs of multi-modal CS data. As a result, these methods generally fail to learn the effective cross-modal relationships that contribute to the fusion. Recently, attention-based transformers have been a prevalent idea for capturing the global dependency over the long sequence in multi-modal fusion, but existing multi-modal fusion transformers suffer from both poor recognition accuracy and inefficient computation for the ACSR task. To address these problems, we develop a novel computation and parameter efficient multi-modal fusion transformer by proposing a novel Token-Importance-Aware Attention mechanism (TIAA), where a token utilization rate (TUR) is formulated to select the important tokens from the multi-modal streams. More precisely, TIAA firstly models the modality-specific fine-grained temporal dependencies over all tokens of each modality, and then learns the efficient cross-modal interaction for the modality-shared coarse-grained temporal dependencies over the important tokens of different modalities. Besides, a light-weight gated hidden projection is designed to control the feature flows of TIAA. The resulting model, named Economical Cued Speech Fusion Transformer (EcoCued), achieves state-of-the-art performance on all existing CS datasets, compared with existing transformer-based fusion methods and ACSR fusion methods.|\u63d0\u793a\u8bed\u97f3\uff08CS\uff09\u662f\u542c\u969c\u4eba\u58eb\u4f7f\u7528\u7684\u4e00\u79cd\u7eaf\u89c6\u89c9\u7f16\u7801\u65b9\u6cd5\uff0c\u5b83\u5c06\u5507\u8bfb\u4e0e\u51e0\u79cd\u7279\u5b9a\u7684\u624b\u5f62\u76f8\u7ed3\u5408\uff0c\u4f7f\u53e3\u8bed\u53d8\u5f97\u53ef\u89c1\u3002\u81ea\u52a8 CS \u8bc6\u522b (ACSR) \u65e8\u5728\u5c06\u8bed\u97f3\u7684\u89c6\u89c9\u7ebf\u7d22\u8f6c\u5f55\u4e3a\u6587\u672c\uff0c\u8fd9\u53ef\u4ee5\u5e2e\u52a9\u542c\u529b\u969c\u788d\u4eba\u58eb\u6709\u6548\u5730\u8fdb\u884c\u4ea4\u6d41\u3002 CS\u7684\u89c6\u89c9\u4fe1\u606f\u5305\u542b\u5507\u8bfb\u548c\u624b\u52bf\uff0c\u4e24\u8005\u7684\u878d\u5408\u5728ACSR\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u5148\u524d\u7684\u878d\u5408\u65b9\u6cd5\u90fd\u96be\u4ee5\u6355\u83b7\u591a\u6a21\u6001 CS \u6570\u636e\u7684\u957f\u5e8f\u5217\u8f93\u5165\u4e2d\u5b58\u5728\u7684\u5168\u5c40\u4f9d\u8d56\u6027\u3002\u56e0\u6b64\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u5b66\u4e60\u6709\u52a9\u4e8e\u878d\u5408\u7684\u6709\u6548\u8de8\u6a21\u6001\u5173\u7cfb\u3002\u6700\u8fd1\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u53d8\u538b\u5668\u5df2\u6210\u4e3a\u6355\u83b7\u591a\u6a21\u6001\u878d\u5408\u4e2d\u957f\u5e8f\u5217\u7684\u5168\u5c40\u4f9d\u8d56\u6027\u7684\u6d41\u884c\u60f3\u6cd5\uff0c\u4f46\u73b0\u6709\u7684\u591a\u6a21\u6001\u878d\u5408\u53d8\u538b\u5668\u5728 ACSR \u4efb\u52a1\u4e2d\u5b58\u5728\u8bc6\u522b\u7cbe\u5ea6\u5dee\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u4ee4\u724c\u91cd\u8981\u6027\u611f\u77e5\u6ce8\u610f\u673a\u5236\uff08TIAA\uff09\u6765\u5f00\u53d1\u4e00\u79cd\u65b0\u9896\u7684\u8ba1\u7b97\u548c\u53c2\u6570\u9ad8\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u53d8\u538b\u5668\uff0c\u5176\u4e2d\u5236\u5b9a\u4e86\u4ee4\u724c\u5229\u7528\u7387\uff08TUR\uff09\u4ee5\u4ece\u591a\u6a21\u5f0f\u6d41\u3002\u66f4\u51c6\u786e\u5730\u8bf4\uff0cTIAA \u9996\u5148\u5bf9\u6bcf\u79cd\u6a21\u6001\u7684\u6240\u6709\u6807\u8bb0\u4e0a\u7279\u5b9a\u6a21\u6001\u7684\u7ec6\u7c92\u5ea6\u65f6\u95f4\u4f9d\u8d56\u6027\u8fdb\u884c\u5efa\u6a21\uff0c\u7136\u540e\u5b66\u4e60\u4e0d\u540c\u6a21\u6001\u7684\u91cd\u8981\u6807\u8bb0\u4e0a\u6a21\u6001\u5171\u4eab\u7684\u7c97\u7c92\u5ea6\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u6709\u6548\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u95e8\u63a7\u9690\u85cf\u6295\u5f71\u6765\u63a7\u5236 TIAA \u7684\u7279\u5f81\u6d41\u3002\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e Transformer \u7684\u878d\u5408\u65b9\u6cd5\u548c ACSR \u878d\u5408\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u5f97\u6a21\u578b\u540d\u4e3a\u7ecf\u6d4e Cued \u8bed\u97f3\u878d\u5408\u53d8\u538b\u5668 (EcoCued)\uff0c\u5728\u6240\u6709\u73b0\u6709 CS \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2401.17604v1](http://arxiv.org/pdf/2401.17604v1)|null|\n", "2401.17600": "|**2024-01-31**|**Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data**|\u64c5\u957f\u5b57\u5e55\uff0c\u4e0d\u64c5\u957f\u8ba1\u6570\uff1a\u5728\u5730\u7403\u89c2\u6d4b\u6570\u636e\u4e0a\u5bf9 GPT-4V \u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5|Chenhui Zhang, Sherrie Wang|Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting tasks. Our benchmark will be made publicly available at https://vleo.danielz.ch/ and on Hugging Face at https://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70 for easy model evaluation.|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u6d89\u53ca\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u89c6\u89c9\u8f93\u5165\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u81ea\u7136\u56fe\u50cf\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u80fd\u591f\u8f6c\u79fb\u5230\u5730\u7403\u89c2\u6d4b\uff08EO\uff09\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u4e3b\u8981\u662f\u536b\u661f\u548c\u822a\u7a7a\u56fe\u50cf\uff0c\u5728 VLM \u8bad\u7ec3\u6570\u636e\u4e2d\u4e0d\u592a\u5e38\u89c1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u8bc4\u4f30 VLM \u5728\u573a\u666f\u7406\u89e3\u3001\u5b9a\u4f4d\u548c\u8ba1\u6570\u4ee5\u53ca\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6765\u8861\u91cf VLM \u6210\u4e3a EO \u6570\u636e\u6709\u7528\u5de5\u5177\u7684\u8fdb\u5c55\u3002\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u63a8\u52a8\u4e0b\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u5305\u62ec\u57ce\u5e02\u76d1\u63a7\u3001\u6551\u707e\u3001\u571f\u5730\u5229\u7528\u548c\u4fdd\u62a4\u7b49\u573a\u666f\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5c3d\u7ba1\u50cf GPT-4V \u8fd9\u6837\u6700\u5148\u8fdb\u7684 VLM \u62e5\u6709\u5e7f\u6cdb\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u53ef\u4ee5\u5728\u4f4d\u7f6e\u7406\u89e3\u548c\u56fe\u50cf\u5b57\u5e55\u7b49\u5f00\u653e\u5f0f\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u7cdf\u7cd5\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u9650\u5236\u4e86\u5bf9\u8c61\u5b9a\u4f4d\u548c\u8ba1\u6570\u4efb\u52a1\u7684\u5b9e\u7528\u6027\u3002\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u5c06\u5728 https://vleo.danielz.ch/ \u548c Hugging Face \u4e0a\u516c\u5f00\u53d1\u5e03\uff1ahttps://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70\uff0c\u4ee5\u4fbf\u4e8e\u6a21\u578b\u8bc4\u4f30\u3002|[2401.17600v1](http://arxiv.org/pdf/2401.17600v1)|**[link](https://github.com/Earth-Intelligence-Lab/vleo-bench)**|\n", "2401.17593": "|**2024-01-31**|**Head and Neck Tumor Segmentation from [18F]F-FDG PET/CT Images Based on 3D Diffusion Model**|\u57fa\u4e8e 3D \u6269\u6563\u6a21\u578b\u7684 [18F]F-FDG PET/CT \u56fe\u50cf\u7684\u5934\u9888\u80bf\u7624\u5206\u5272|Yafei Dong, Kuang Gong|Head and neck (H&N) cancers are among the most prevalent types of cancer worldwide, and [18F]F-FDG PET/CT is widely used for H&N cancer management. Recently, the diffusion model has demonstrated remarkable performance in various image-generation tasks. In this work, we proposed a 3D diffusion model to accurately perform H&N tumor segmentation from 3D PET and CT volumes. The 3D diffusion model was developed considering the 3D nature of PET and CT images acquired. During the reverse process, the model utilized a 3D UNet structure and took the concatenation of PET, CT, and Gaussian noise volumes as the network input to generate the tumor mask. Experiments based on the HECKTOR challenge dataset were conducted to evaluate the effectiveness of the proposed diffusion model. Several state-of-the-art techniques based on U-Net and Transformer structures were adopted as the reference methods. Benefits of employing both PET and CT as the network input as well as further extending the diffusion model from 2D to 3D were investigated based on various quantitative metrics and the uncertainty maps generated. Results showed that the proposed 3D diffusion model could generate more accurate segmentation results compared with other methods. Compared to the diffusion model in 2D format, the proposed 3D model yielded superior results. Our experiments also highlighted the advantage of utilizing dual-modality PET and CT data over only single-modality data for H&N tumor segmentation.|\u5934\u9888 (H&N) \u764c\u75c7\u662f\u5168\u7403\u6700\u5e38\u89c1\u7684\u764c\u75c7\u7c7b\u578b\u4e4b\u4e00\uff0c[18F]F-FDG PET/CT \u5e7f\u6cdb\u7528\u4e8e H&N \u764c\u75c7\u7ba1\u7406\u3002\u6700\u8fd1\uff0c\u6269\u6563\u6a21\u578b\u5728\u5404\u79cd\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd 3D \u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ee5\u6839\u636e 3D PET \u548c CT \u4f53\u79ef\u51c6\u786e\u5730\u6267\u884c H&N \u80bf\u7624\u5206\u5272\u3002 3D \u6269\u6563\u6a21\u578b\u7684\u5f00\u53d1\u8003\u8651\u4e86\u6240\u83b7\u53d6\u7684 PET \u548c CT \u56fe\u50cf\u7684 3D \u6027\u8d28\u3002\u5728\u9006\u5411\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u5229\u75283D UNet\u7ed3\u6784\uff0c\u5e76\u5c06PET\u3001CT\u548c\u9ad8\u65af\u566a\u58f0\u91cf\u7684\u4e32\u8054\u4f5c\u4e3a\u7f51\u7edc\u8f93\u5165\u6765\u751f\u6210\u80bf\u7624\u63a9\u6a21\u3002\u57fa\u4e8e HECKTOR \u6311\u6218\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u91c7\u7528\u57fa\u4e8e U-Net \u548c Transformer \u7ed3\u6784\u7684\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u6280\u672f\u4f5c\u4e3a\u53c2\u200b\u200b\u8003\u65b9\u6cd5\u3002\u57fa\u4e8e\u5404\u79cd\u5b9a\u91cf\u6307\u6807\u548c\u751f\u6210\u7684\u4e0d\u786e\u5b9a\u6027\u56fe\uff0c\u7814\u7a76\u4e86\u91c7\u7528 PET \u548c CT \u4f5c\u4e3a\u7f51\u7edc\u8f93\u5165\u4ee5\u53ca\u8fdb\u4e00\u6b65\u5c06\u6269\u6563\u6a21\u578b\u4ece \u200b\u200b2D \u6269\u5c55\u5230 3D \u7684\u597d\u5904\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684 3D \u6269\u6563\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u5206\u5272\u7ed3\u679c\u3002\u4e0e 2D \u683c\u5f0f\u7684\u6269\u6563\u6a21\u578b\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684 3D \u6a21\u578b\u4ea7\u751f\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8fd8\u5f3a\u8c03\u4e86\u5229\u7528\u53cc\u6a21\u6001 PET \u548c CT \u6570\u636e\u76f8\u5bf9\u4e8e\u4ec5\u4f7f\u7528\u5355\u6a21\u6001\u6570\u636e\u8fdb\u884c H&N \u80bf\u7624\u5206\u5272\u7684\u4f18\u52bf\u3002|[2401.17593v1](http://arxiv.org/pdf/2401.17593v1)|null|\n", "2401.17592": "|**2024-01-31**|**Local Feature Matching Using Deep Learning: A Survey**|\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u5c40\u90e8\u7279\u5f81\u5339\u914d\uff1a\u8c03\u67e5|Shibiao Xu, Shunpeng Chen, Rongtao Xu, Changwei Wang, Peng Lu, Li Guo|Local feature matching enjoys wide-ranging applications in the realm of computer vision, encompassing domains such as image retrieval, 3D reconstruction, and object recognition. However, challenges persist in improving the accuracy and robustness of matching due to factors like viewpoint and lighting variations. In recent years, the introduction of deep learning models has sparked widespread exploration into local feature matching techniques. The objective of this endeavor is to furnish a comprehensive overview of local feature matching methods. These methods are categorized into two key segments based on the presence of detectors. The Detector-based category encompasses models inclusive of Detect-then-Describe, Joint Detection and Description, Describe-then-Detect, as well as Graph Based techniques. In contrast, the Detector-free category comprises CNN Based, Transformer Based, and Patch Based methods. Our study extends beyond methodological analysis, incorporating evaluations of prevalent datasets and metrics to facilitate a quantitative comparison of state-of-the-art techniques. The paper also explores the practical application of local feature matching in diverse domains such as Structure from Motion, Remote Sensing Image Registration, and Medical Image Registration, underscoring its versatility and significance across various fields. Ultimately, we endeavor to outline the current challenges faced in this domain and furnish future research directions, thereby serving as a reference for researchers involved in local feature matching and its interconnected domains.|\u5c40\u90e8\u7279\u5f81\u5339\u914d\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u6db5\u76d6\u56fe\u50cf\u68c0\u7d22\u30013D \u91cd\u5efa\u548c\u5bf9\u8c61\u8bc6\u522b\u7b49\u9886\u57df\u3002\u7136\u800c\uff0c\u7531\u4e8e\u89c6\u70b9\u548c\u7167\u660e\u53d8\u5316\u7b49\u56e0\u7d20\uff0c\u63d0\u9ad8\u5339\u914d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5f15\u5165\u5f15\u53d1\u4e86\u5bf9\u5c40\u90e8\u7279\u5f81\u5339\u914d\u6280\u672f\u7684\u5e7f\u6cdb\u63a2\u7d22\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u76ee\u6807\u662f\u63d0\u4f9b\u5c40\u90e8\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u7684\u5168\u9762\u6982\u8ff0\u3002\u6839\u636e\u68c0\u6d4b\u5668\u7684\u5b58\u5728\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5206\u4e3a\u4e24\u4e2a\u5173\u952e\u90e8\u5206\u3002\u57fa\u4e8e\u68c0\u6d4b\u5668\u7684\u7c7b\u522b\u6db5\u76d6\u7684\u6a21\u578b\u5305\u62ec\u68c0\u6d4b\u7136\u540e\u63cf\u8ff0\u3001\u8054\u5408\u68c0\u6d4b\u548c\u63cf\u8ff0\u3001\u63cf\u8ff0\u7136\u540e\u68c0\u6d4b\u4ee5\u53ca\u57fa\u4e8e\u56fe\u7684\u6280\u672f\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u65e0\u68c0\u6d4b\u5668\u7c7b\u522b\u5305\u62ec\u57fa\u4e8e CNN\u3001\u57fa\u4e8e\u53d8\u6362\u5668\u548c\u57fa\u4e8e\u8865\u4e01\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u8d85\u8d8a\u4e86\u65b9\u6cd5\u8bba\u5206\u6790\uff0c\u7ed3\u5408\u4e86\u5bf9\u6d41\u884c\u6570\u636e\u96c6\u548c\u6307\u6807\u7684\u8bc4\u4f30\uff0c\u4ee5\u4fc3\u8fdb\u6700\u5148\u8fdb\u6280\u672f\u7684\u5b9a\u91cf\u6bd4\u8f83\u3002\u672c\u6587\u8fd8\u63a2\u8ba8\u4e86\u5c40\u90e8\u7279\u5f81\u5339\u914d\u5728\u8fd0\u52a8\u7ed3\u6784\u3001\u9065\u611f\u56fe\u50cf\u914d\u51c6\u548c\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7b49\u4e0d\u540c\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u5404\u4e2a\u9886\u57df\u7684\u591a\u529f\u80fd\u6027\u548c\u91cd\u8981\u6027\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u52aa\u529b\u6982\u8ff0\u8be5\u9886\u57df\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\u5e76\u63d0\u4f9b\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ece\u800c\u4e3a\u6d89\u53ca\u5c40\u90e8\u7279\u5f81\u5339\u914d\u53ca\u5176\u4e92\u8fde\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u53c2\u8003\u3002|[2401.17592v1](http://arxiv.org/pdf/2401.17592v1)|null|\n", "2401.17515": "|**2024-01-31**|**Towards Image Semantics and Syntax Sequence Learning**|\u8fc8\u5411\u56fe\u50cf\u8bed\u4e49\u548c\u53e5\u6cd5\u5e8f\u5217\u5b66\u4e60|Chun Tao, Timur Ibrayev, Kaushik Roy|Convolutional neural networks and vision transformers have achieved outstanding performance in machine perception, particularly for image classification. Although these image classifiers excel at predicting image-level class labels, they may not discriminate missing or shifted parts within an object. As a result, they may fail to detect corrupted images that involve missing or disarrayed semantic information in the object composition. On the contrary, human perception easily distinguishes such corruptions. To mitigate this gap, we introduce the concept of \"image grammar\", consisting of \"image semantics\" and \"image syntax\", to denote the semantics of parts or patches of an image and the order in which these parts are arranged to create a meaningful object. To learn the image grammar relative to a class of visual objects/scenes, we propose a weakly supervised two-stage approach. In the first stage, we use a deep clustering framework that relies on iterative clustering and feature refinement to produce part-semantic segmentation. In the second stage, we incorporate a recurrent bi-LSTM module to process a sequence of semantic segmentation patches to capture the image syntax. Our framework is trained to reason over patch semantics and detect faulty syntax. We benchmark the performance of several grammar learning models in detecting patch corruptions. Finally, we verify the capabilities of our framework in Celeb and SUNRGBD datasets and demonstrate that it can achieve a grammar validation accuracy of 70 to 90% in a wide variety of semantic and syntactical corruption scenarios.|\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u89c6\u89c9\u8f6c\u6362\u5668\u5728\u673a\u5668\u611f\u77e5\u65b9\u9762\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u56fe\u50cf\u5206\u7c7b\u5668\u64c5\u957f\u9884\u6d4b\u56fe\u50cf\u7ea7\u7c7b\u522b\u6807\u7b7e\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u65e0\u6cd5\u533a\u5206\u5bf9\u8c61\u5185\u4e22\u5931\u6216\u79fb\u4f4d\u7684\u90e8\u5206\u3002\u56e0\u6b64\uff0c\u4ed6\u4eec\u53ef\u80fd\u65e0\u6cd5\u68c0\u6d4b\u5230\u5bf9\u8c61\u7ec4\u5408\u4e2d\u6d89\u53ca\u4e22\u5931\u6216\u6df7\u4e71\u7684\u8bed\u4e49\u4fe1\u606f\u7684\u635f\u574f\u56fe\u50cf\u3002\u76f8\u53cd\uff0c\u4eba\u7c7b\u7684\u611f\u77e5\u5f88\u5bb9\u6613\u533a\u5206\u8fd9\u79cd\u8150\u8d25\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u201c\u56fe\u50cf\u8bed\u6cd5\u201d\u7684\u6982\u5ff5\uff0c\u7531\u201c\u56fe\u50cf\u8bed\u4e49\u201d\u548c\u201c\u56fe\u50cf\u8bed\u6cd5\u201d\u7ec4\u6210\uff0c\u6765\u8868\u793a\u56fe\u50cf\u7684\u90e8\u5206\u6216\u5757\u7684\u8bed\u4e49\u4ee5\u53ca\u8fd9\u4e9b\u90e8\u5206\u6392\u5217\u4ee5\u521b\u5efa\u56fe\u50cf\u7684\u987a\u5e8f\u3002\u6709\u610f\u4e49\u7684\u5bf9\u8c61\u3002\u4e3a\u4e86\u5b66\u4e60\u4e0e\u4e00\u7c7b\u89c6\u89c9\u5bf9\u8c61/\u573a\u666f\u76f8\u5173\u7684\u56fe\u50cf\u8bed\u6cd5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u4f7f\u7528\u6df1\u5ea6\u805a\u7c7b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f9d\u8d56\u4e8e\u8fed\u4ee3\u805a\u7c7b\u548c\u7279\u5f81\u7ec6\u5316\u6765\u4ea7\u751f\u90e8\u5206\u8bed\u4e49\u5206\u5272\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u4e00\u4e2a\u5faa\u73af\u53cc LSTM \u6a21\u5757\u6765\u5904\u7406\u4e00\u7cfb\u5217\u8bed\u4e49\u5206\u5272\u8865\u4e01\u4ee5\u6355\u83b7\u56fe\u50cf\u8bed\u6cd5\u3002\u6211\u4eec\u7684\u6846\u67b6\u7ecf\u8fc7\u8bad\u7ec3\u53ef\u4ee5\u63a8\u7406\u8865\u4e01\u8bed\u4e49\u5e76\u68c0\u6d4b\u9519\u8bef\u7684\u8bed\u6cd5\u3002\u6211\u4eec\u5bf9\u51e0\u79cd\u8bed\u6cd5\u5b66\u4e60\u6a21\u578b\u5728\u68c0\u6d4b\u8865\u4e01\u635f\u574f\u65b9\u9762\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728 Celeb \u548c SUNRGBD \u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u6846\u67b6\u7684\u529f\u80fd\uff0c\u5e76\u8bc1\u660e\u5b83\u53ef\u4ee5\u5728\u5404\u79cd\u8bed\u4e49\u548c\u53e5\u6cd5\u635f\u574f\u573a\u666f\u4e2d\u5b9e\u73b0 70% \u5230 90% \u7684\u8bed\u6cd5\u9a8c\u8bc1\u51c6\u786e\u6027\u3002|[2401.17515v1](http://arxiv.org/pdf/2401.17515v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2401.17544": "|**2024-01-31**|**Trainable Fixed-Point Quantization for Deep Learning Acceleration on FPGAs**|\u7528\u4e8e FPGA \u4e0a\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u7684\u53ef\u8bad\u7ec3\u5b9a\u70b9\u91cf\u5316|Dingyi Dai, Yichi Zhang, Jiahao Zhang, Zhanqiu Hu, Yaohui Cai, Qi Sun, Zhiru Zhang|Quantization is a crucial technique for deploying deep learning models on resource-constrained devices, such as embedded FPGAs. Prior efforts mostly focus on quantizing matrix multiplications, leaving other layers like BatchNorm or shortcuts in floating-point form, even though fixed-point arithmetic is more efficient on FPGAs. A common practice is to fine-tune a pre-trained model to fixed-point for FPGA deployment, but potentially degrading accuracy.   This work presents QFX, a novel trainable fixed-point quantization approach that automatically learns the binary-point position during model training. Additionally, we introduce a multiplier-free quantization strategy within QFX to minimize DSP usage. QFX is implemented as a PyTorch-based library that efficiently emulates fixed-point arithmetic, supported by FPGA HLS, in a differentiable manner during backpropagation. With minimal effort, models trained with QFX can readily be deployed through HLS, producing the same numerical results as their software counterparts. Our evaluation shows that compared to post-training quantization, QFX can quantize models trained with element-wise layers quantized to fewer bits and achieve higher accuracy on both CIFAR-10 and ImageNet datasets. We further demonstrate the efficacy of multiplier-free quantization using a state-of-the-art binarized neural network accelerator designed for an embedded FPGA (AMD Xilinx Ultra96 v2). We plan to release QFX in open-source format.|\u91cf\u5316\u662f\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff08\u4f8b\u5982\u5d4c\u5165\u5f0f FPGA\uff09\u4e0a\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5173\u952e\u6280\u672f\u3002\u4e4b\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u91cf\u5316\u77e9\u9635\u4e58\u6cd5\uff0c\u800c\u5c06 BatchNorm \u6216\u5feb\u6377\u65b9\u5f0f\u7b49\u5176\u4ed6\u5c42\u4fdd\u7559\u4e3a\u6d6e\u70b9\u5f62\u5f0f\uff0c\u5c3d\u7ba1\u5b9a\u70b9\u7b97\u672f\u5728 FPGA \u4e0a\u6548\u7387\u66f4\u9ad8\u3002\u5e38\u89c1\u7684\u505a\u6cd5\u662f\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u5230\u5b9a\u70b9\u4ee5\u8fdb\u884c FPGA \u90e8\u7f72\uff0c\u4f46\u8fd9\u53ef\u80fd\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86 QFX\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u8bad\u7ec3\u5b9a\u70b9\u91cf\u5316\u65b9\u6cd5\uff0c\u53ef\u5728\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u81ea\u52a8\u5b66\u4e60\u4e8c\u8fdb\u5236\u70b9\u4f4d\u7f6e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728 QFX \u4e2d\u5f15\u5165\u4e86\u65e0\u4e58\u6cd5\u5668\u91cf\u5316\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11 DSP \u4f7f\u7528\u3002 QFX \u88ab\u5b9e\u73b0\u4e3a\u57fa\u4e8e PyTorch \u7684\u5e93\uff0c\u5728 FPGA HLS \u652f\u6301\u4e0b\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u4ee5\u53ef\u5fae\u5206\u7684\u65b9\u5f0f\u9ad8\u6548\u5730\u6a21\u62df\u5b9a\u70b9\u7b97\u6cd5\u3002\u53ea\u9700\u6700\u5c11\u7684\u52aa\u529b\uff0c\u4f7f\u7528 QFX \u8bad\u7ec3\u7684\u6a21\u578b\u5c31\u53ef\u4ee5\u901a\u8fc7 HLS \u8f7b\u677e\u90e8\u7f72\uff0c\u4ea7\u751f\u4e0e\u8f6f\u4ef6\u5bf9\u5e94\u7269\u76f8\u540c\u7684\u6570\u503c\u7ed3\u679c\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u8bad\u7ec3\u540e\u91cf\u5316\u76f8\u6bd4\uff0cQFX \u53ef\u4ee5\u91cf\u5316\u4f7f\u7528\u91cf\u5316\u4e3a\u66f4\u5c11\u4f4d\u6570\u7684\u9010\u5143\u7d20\u5c42\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5e76\u5728 CIFAR-10 \u548c ImageNet \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u4f7f\u7528\u4e13\u4e3a\u5d4c\u5165\u5f0f FPGA (AMD Xilinx Ultra96 v2) \u8bbe\u8ba1\u7684\u6700\u5148\u8fdb\u7684\u4e8c\u503c\u5316\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u65e0\u4e58\u6cd5\u5668\u91cf\u5316\u7684\u529f\u6548\u3002\u6211\u4eec\u8ba1\u5212\u4ee5\u5f00\u6e90\u683c\u5f0f\u53d1\u5e03 QFX\u3002|[2401.17544v1](http://arxiv.org/pdf/2401.17544v1)|null|\n"}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2401.18085": "|**2024-01-31**|**Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators**|\u8fd0\u52a8\u5f15\u5bfc\uff1a\u4f7f\u7528\u53ef\u5fae\u8fd0\u52a8\u4f30\u8ba1\u5668\u8fdb\u884c\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u7f16\u8f91|Daniel Geng, Andrew Owens|Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale. However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion models is still difficult. To this end, we propose motion guidance, a zero-shot technique that allows a user to specify dense, complex motion fields that indicate where each pixel in an image should move. Motion guidance works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network. Specifically, we design a guidance loss that encourages the sample to have the desired motion, as estimated by a flow network, while also being visually similar to the source image. By simultaneously sampling from a diffusion model and guiding the sample to have low guidance loss, we can obtain a motion-edited image. We demonstrate that our technique works on complex motions and produces high quality edits of real and generated images.|\u6269\u6563\u6a21\u578b\u80fd\u591f\u6839\u636e\u6587\u672c\u63cf\u8ff0\u751f\u6210\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u56fe\u50cf\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6a21\u578b\u7684\u6269\u5c55\u5141\u8bb8\u7528\u6237\u4ee5\u76f8\u5bf9\u7c97\u7cd9\u7684\u6bd4\u4f8b\u7f16\u8f91\u56fe\u50cf\u3002\u7136\u800c\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7cbe\u786e\u7f16\u8f91\u56fe\u50cf\u4e2d\u5bf9\u8c61\u7684\u5e03\u5c40\u3001\u4f4d\u7f6e\u3001\u59ff\u52bf\u548c\u5f62\u72b6\u7684\u80fd\u529b\u4ecd\u7136\u5f88\u56f0\u96be\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fd0\u52a8\u5f15\u5bfc\uff0c\u8fd9\u662f\u4e00\u79cd\u96f6\u6837\u672c\u6280\u672f\uff0c\u5141\u8bb8\u7528\u6237\u6307\u5b9a\u5bc6\u96c6\u3001\u590d\u6742\u7684\u8fd0\u52a8\u573a\uff0c\u6307\u793a\u56fe\u50cf\u4e2d\u6bcf\u4e2a\u50cf\u7d20\u5e94\u8be5\u79fb\u52a8\u7684\u4f4d\u7f6e\u3002\u8fd0\u52a8\u5f15\u5bfc\u7684\u5de5\u4f5c\u539f\u7406\u662f\u901a\u8fc7\u73b0\u6210\u7684\u5149\u6d41\u7f51\u7edc\u5229\u7528\u68af\u5ea6\u6765\u63a7\u5236\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5f15\u5bfc\u635f\u5931\uff0c\u9f13\u52b1\u6837\u672c\u8fdb\u884c\u7531\u6d41\u7f51\u7edc\u4f30\u8ba1\u7684\u6240\u9700\u8fd0\u52a8\uff0c\u540c\u65f6\u5728\u89c6\u89c9\u4e0a\u4e0e\u6e90\u56fe\u50cf\u76f8\u4f3c\u3002\u901a\u8fc7\u540c\u65f6\u4ece\u6269\u6563\u6a21\u578b\u4e2d\u91c7\u6837\u5e76\u5f15\u5bfc\u6837\u672c\u5177\u6709\u8f83\u4f4e\u7684\u5f15\u5bfc\u635f\u5931\uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97\u8fd0\u52a8\u7f16\u8f91\u7684\u56fe\u50cf\u3002\u6211\u4eec\u8bc1\u660e\u6211\u4eec\u7684\u6280\u672f\u9002\u7528\u4e8e\u590d\u6742\u7684\u8fd0\u52a8\uff0c\u5e76\u5bf9\u771f\u5b9e\u548c\u751f\u6210\u7684\u56fe\u50cf\u8fdb\u884c\u9ad8\u8d28\u91cf\u7684\u7f16\u8f91\u3002|[2401.18085v1](http://arxiv.org/pdf/2401.18085v1)|null|\n", "2401.18075": "|**2024-01-31**|**CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting**|CARFF\uff1a\u7528\u4e8e 3D \u573a\u666f\u9884\u6d4b\u7684\u6761\u4ef6\u81ea\u52a8\u7f16\u7801\u8f90\u5c04\u573a|Jiezhi Yang, Khushi Desai, Charles Packer, Harshil Bhatia, Nicholas Rhinehart, Rowan McAllister, Joseph Gonzalez|We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting, a method for predicting future 3D scenes given past observations, such as 2D ego-centric images. Our method maps an image to a distribution over plausible 3D latent scene configurations using a probabilistic encoder, and predicts the evolution of the hypothesized scenes through time. Our latent scene representation conditions a global Neural Radiance Field (NeRF) to represent a 3D scene model, which enables explainable predictions and straightforward downstream applications. This approach extends beyond previous neural rendering work by considering complex scenarios of uncertainty in environmental states and dynamics. We employ a two-stage training of Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we auto-regressively predict latent scene representations as a partially observable Markov decision process, utilizing a mixture density network. We demonstrate the utility of our method in realistic scenarios using the CARLA driving simulator, where CARFF can be used to enable efficient trajectory and contingency planning in complex multi-agent autonomous driving scenarios involving visual occlusions.|\u6211\u4eec\u63d0\u51fa\u4e86 CARFF\uff1a\u7528\u4e8e 3D \u573a\u666f\u9884\u6d4b\u7684\u6761\u4ef6\u81ea\u52a8\u7f16\u7801\u8f90\u5c04\u573a\uff0c\u8fd9\u662f\u4e00\u79cd\u6839\u636e\u8fc7\u53bb\u7684\u89c2\u5bdf\uff08\u4f8b\u5982\u4ee5 2D \u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\uff09\u9884\u6d4b\u672a\u6765 3D \u573a\u666f\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u6982\u7387\u7f16\u7801\u5668\u5c06\u56fe\u50cf\u6620\u5c04\u5230\u5408\u7406\u7684 3D \u6f5c\u5728\u573a\u666f\u914d\u7f6e\u4e0a\u7684\u5206\u5e03\uff0c\u5e76\u9884\u6d4b\u5047\u8bbe\u573a\u666f\u968f\u65f6\u95f4\u7684\u6f14\u53d8\u3002\u6211\u4eec\u7684\u6f5c\u5728\u573a\u666f\u8868\u793a\u4ee5\u5168\u5c40\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u4e3a\u6761\u4ef6\u6765\u8868\u793a 3D \u573a\u666f\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u548c\u76f4\u63a5\u7684\u4e0b\u6e38\u5e94\u7528\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u8003\u8651\u73af\u5883\u72b6\u6001\u548c\u52a8\u6001\u7684\u4e0d\u786e\u5b9a\u6027\u7684\u590d\u6742\u573a\u666f\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u795e\u7ecf\u6e32\u67d3\u5de5\u4f5c\u3002\u6211\u4eec\u91c7\u7528 Pose-Conditional-VAE \u548c NeRF \u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u6765\u5b66\u4e60 3D \u8868\u793a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\u81ea\u52a8\u56de\u5f52\u9884\u6d4b\u6f5c\u5728\u573a\u666f\u8868\u793a\u4f5c\u4e3a\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3002\u6211\u4eec\u4f7f\u7528 CARLA \u9a7e\u9a76\u6a21\u62df\u5668\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u5176\u4e2d CARFF \u53ef\u7528\u4e8e\u5728\u6d89\u53ca\u89c6\u89c9\u906e\u6321\u7684\u590d\u6742\u591a\u667a\u80fd\u4f53\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u8f68\u8ff9\u548c\u5e94\u6025\u8ba1\u5212\u3002|[2401.18075v1](http://arxiv.org/pdf/2401.18075v1)|null|\n", "2401.17807": "|**2024-01-31**|**Advances in 3D Generation: A Survey**|3D \u751f\u6210\u7684\u8fdb\u5c55\uff1a\u8c03\u67e5|Xiaoyu Li, Qi Zhang, Di Kang, Weihao Cheng, Yiming Gao, Jingbo Zhang, Zhihao Liang, Jing Liao, Yan-Pei Cao, Ying Shan|Generating 3D models lies at the core of computer graphics and has been the focus of decades of research. With the emergence of advanced neural representations and generative models, the field of 3D content generation is developing rapidly, enabling the creation of increasingly high-quality and diverse 3D models. The rapid growth of this field makes it difficult to stay abreast of all recent developments. In this survey, we aim to introduce the fundamental methodologies of 3D generation methods and establish a structured roadmap, encompassing 3D representation, generation methods, datasets, and corresponding applications. Specifically, we introduce the 3D representations that serve as the backbone for 3D generation. Furthermore, we provide a comprehensive overview of the rapidly growing literature on generation methods, categorized by the type of algorithmic paradigms, including feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis. Lastly, we discuss available datasets, applications, and open challenges. We hope this survey will help readers explore this exciting topic and foster further advancements in the field of 3D content generation.|\u751f\u6210 3D \u6a21\u578b\u662f\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u7684\u6838\u5fc3\uff0c\u4e5f\u662f\u6570\u5341\u5e74\u6765\u7814\u7a76\u7684\u7126\u70b9\u3002\u968f\u7740\u9ad8\u7ea7\u795e\u7ecf\u8868\u793a\u548c\u751f\u6210\u6a21\u578b\u7684\u51fa\u73b0\uff0c3D \u5185\u5bb9\u751f\u6210\u9886\u57df\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u4f7f\u5f97\u80fd\u591f\u521b\u5efa\u8d8a\u6765\u8d8a\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684 3D \u6a21\u578b\u3002\u8be5\u9886\u57df\u7684\u5feb\u901f\u589e\u957f\u4f7f\u5f97\u5f88\u96be\u8ddf\u4e0a\u6240\u6709\u6700\u65b0\u53d1\u5c55\u3002\u5728\u672c\u6b21\u8c03\u67e5\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u4ecb\u7ecd 3D \u751f\u6210\u65b9\u6cd5\u7684\u57fa\u672c\u65b9\u6cd5\uff0c\u5e76\u5efa\u7acb\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u8def\u7ebf\u56fe\uff0c\u5305\u62ec 3D \u8868\u793a\u3001\u751f\u6210\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u76f8\u5e94\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4f5c\u4e3a 3D \u751f\u6210\u652f\u67f1\u7684 3D \u8868\u793a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5168\u9762\u6982\u8ff0\u4e86\u6709\u5173\u751f\u6210\u65b9\u6cd5\u7684\u5feb\u901f\u589e\u957f\u7684\u6587\u732e\uff0c\u6309\u7b97\u6cd5\u8303\u5f0f\u7684\u7c7b\u578b\u8fdb\u884c\u5206\u7c7b\uff0c\u5305\u62ec\u524d\u9988\u751f\u6210\u3001\u57fa\u4e8e\u4f18\u5316\u7684\u751f\u6210\u3001\u7a0b\u5e8f\u751f\u6210\u548c\u751f\u6210\u65b0\u9896\u89c6\u56fe\u5408\u6210\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u53ef\u7528\u7684\u6570\u636e\u96c6\u3001\u5e94\u7528\u7a0b\u5e8f\u548c\u5f00\u653e\u6311\u6218\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u8c03\u67e5\u80fd\u591f\u5e2e\u52a9\u8bfb\u8005\u63a2\u7d22\u8fd9\u4e2a\u4ee4\u4eba\u5174\u594b\u7684\u4e3b\u9898\uff0c\u5e76\u4fc3\u8fdb 3D \u5185\u5bb9\u751f\u6210\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u8fdb\u6b65\u3002|[2401.17807v1](http://arxiv.org/pdf/2401.17807v1)|null|\n", "2401.17776": "|**2024-01-31**|**Double InfoGAN for Contrastive Analysis**|\u53ccInfoGAN\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790|Florence Carton, Robin Louiset, Pietro Gori|Contrastive Analysis (CA) deals with the discovery of what is common and what is distinctive of a target domain compared to a background one. This is of great interest in many applications, such as medical imaging. Current state-of-the-art (SOTA) methods are latent variable models based on VAE (CA-VAEs). However, they all either ignore important constraints or they don't enforce fundamental assumptions. This may lead to sub-optimal solutions where distinctive factors are mistaken for common ones (or viceversa). Furthermore, the generated images have a rather poor quality, typical of VAEs, decreasing their interpretability and usefulness. Here, we propose Double InfoGAN, the first GAN based method for CA that leverages the high-quality synthesis of GAN and the separation power of InfoGAN. Experimental results on four visual datasets, from simple synthetic examples to complex medical images, show that the proposed method outperforms SOTA CA-VAEs in terms of latent separation and image quality. Datasets and code are available online.|\u5bf9\u6bd4\u5206\u6790 (CA) \u65e8\u5728\u53d1\u73b0\u76ee\u6807\u57df\u4e0e\u80cc\u666f\u57df\u76f8\u6bd4\u7684\u5171\u540c\u70b9\u548c\u72ec\u7279\u6027\u3002\u8fd9\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u5f15\u8d77\u4e86\u6781\u5927\u7684\u5174\u8da3\uff0c\u4f8b\u5982\u533b\u5b66\u6210\u50cf\u3002\u5f53\u524d\u6700\u5148\u8fdb\u7684 (SOTA) \u65b9\u6cd5\u662f\u57fa\u4e8e VAE (CA-VAE) \u7684\u6f5c\u53d8\u91cf\u6a21\u578b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u8981\u4e48\u5ffd\u7565\u4e86\u91cd\u8981\u7684\u7ea6\u675f\uff0c\u8981\u4e48\u4e0d\u6267\u884c\u57fa\u672c\u5047\u8bbe\u3002\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6b21\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u4e2d\u72ec\u7279\u56e0\u7d20\u88ab\u8bef\u8ba4\u4e3a\u662f\u5e38\u89c1\u56e0\u7d20\uff08\u53cd\u4e4b\u4ea6\u7136\uff09\u3002\u6b64\u5916\uff0c\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u76f8\u5f53\u5dee\uff0c\u8fd9\u662f VAE \u7684\u5178\u578b\u7279\u5f81\uff0c\u964d\u4f4e\u4e86\u5b83\u4eec\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6709\u7528\u6027\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Double InfoGAN\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e GAN \u7684 CA \u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4e86 GAN \u7684\u9ad8\u8d28\u91cf\u5408\u6210\u548c InfoGAN \u7684\u5206\u79bb\u80fd\u529b\u3002\u5728\u56db\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\uff08\u4ece\u7b80\u5355\u7684\u5408\u6210\u793a\u4f8b\u5230\u590d\u6742\u7684\u533b\u5b66\u56fe\u50cf\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6f5c\u5728\u5206\u79bb\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e SOTA CA-VAE\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u53ef\u5728\u7ebf\u83b7\u53d6\u3002|[2401.17776v1](http://arxiv.org/pdf/2401.17776v1)|null|\n", "2401.17714": "|**2024-01-31**|**3D-Plotting Algorithm for Insects using YOLOv5**|\u4f7f\u7528 YOLOv5 \u7684\u6606\u866b 3D \u7ed8\u56fe\u7b97\u6cd5|Daisuke Mori, Hiroki Hayami, Yasufumi Fujimoto, Isao Goto|In ecological research, accurately collecting spatiotemporal position data is a fundamental task for understanding the behavior and ecology of insects and other organisms. In recent years, advancements in computer vision techniques have reached a stage of maturity where they can support, and in some cases, replace manual observation. In this study, a simple and inexpensive method for monitoring insects in three dimensions (3D) was developed so that their behavior could be observed automatically in experimental environments. The main achievements of this study have been to create a 3D monitoring algorithm using inexpensive cameras and other equipment to design an adjusting algorithm for depth error, and to validate how our plotting algorithm is quantitatively precise, all of which had not been realized in conventional studies. By offering detailed 3D visualizations of insects, the plotting algorithm aids researchers in more effectively comprehending how insects interact within their environments.|\u5728\u751f\u6001\u7814\u7a76\u4e2d\uff0c\u51c6\u786e\u6536\u96c6\u65f6\u7a7a\u4f4d\u7f6e\u6570\u636e\u662f\u4e86\u89e3\u6606\u866b\u548c\u5176\u4ed6\u751f\u7269\u7684\u884c\u4e3a\u548c\u751f\u6001\u7684\u57fa\u672c\u4efb\u52a1\u3002\u8fd1\u5e74\u6765\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u8fdb\u6b65\u5df2\u7ecf\u8fbe\u5230\u6210\u719f\u9636\u6bb5\uff0c\u53ef\u4ee5\u652f\u6301\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53d6\u4ee3\u624b\u52a8\u89c2\u5bdf\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u5ec9\u4ef7\u7684\u4e09\u7ef4\uff083D\uff09\u6606\u866b\u76d1\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u5728\u5b9e\u9a8c\u73af\u5883\u4e2d\u81ea\u52a8\u89c2\u5bdf\u5b83\u4eec\u7684\u884c\u4e3a\u3002\u8fd9\u9879\u7814\u7a76\u7684\u4e3b\u8981\u6210\u679c\u662f\u4f7f\u7528\u5ec9\u4ef7\u76f8\u673a\u548c\u5176\u4ed6\u8bbe\u5907\u521b\u5efa\u4e86 3D \u76d1\u6d4b\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u6df1\u5ea6\u8bef\u5dee\u7684\u8c03\u6574\u7b97\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u7ed8\u56fe\u7b97\u6cd5\u5728\u5b9a\u91cf\u4e0a\u7684\u7cbe\u786e\u6027\uff0c\u6240\u6709\u8fd9\u4e9b\u5728\u4f20\u7edf\u7814\u200b\u200b\u7a76\u4e2d\u90fd\u6ca1\u6709\u5b9e\u73b0\u3002\u901a\u8fc7\u63d0\u4f9b\u6606\u866b\u7684\u8be6\u7ec6 3D \u53ef\u89c6\u5316\uff0c\u7ed8\u56fe\u7b97\u6cd5\u53ef\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u66f4\u6709\u6548\u5730\u7406\u89e3\u6606\u866b\u5982\u4f55\u5728\u5176\u73af\u5883\u4e2d\u76f8\u4e92\u4f5c\u7528\u3002|[2401.17714v1](http://arxiv.org/pdf/2401.17714v1)|null|\n", "2401.17664": "|**2024-01-31**|**Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation**|Image Anything\uff1a\u8fc8\u5411\u63a8\u7406\u8fde\u8d2f\u4e14\u514d\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210|Yuanhuiyi Lyu, Xu Zheng, Lin Wang|The multifaceted nature of human perception and comprehension indicates that, when we think, our body can naturally take any combination of senses, a.k.a., modalities and form a beautiful picture in our brain. For example, when we see a cattery and simultaneously perceive the cat's purring sound, our brain can construct a picture of a cat in the cattery. Intuitively, generative AI models should hold the versatility of humans and be capable of generating images from any combination of modalities efficiently and collaboratively. This paper presents ImgAny, a novel end-to-end multi-modal generative model that can mimic human reasoning and generate high-quality images. Our method serves as the first attempt in its capacity of efficiently and flexibly taking any combination of seven modalities, ranging from language, audio to vision modalities, including image, point cloud, thermal, depth, and event data. Our key idea is inspired by human-level cognitive processes and involves the integration and harmonization of multiple input modalities at both the entity and attribute levels without specific tuning across modalities. Accordingly, our method brings two novel training-free technical branches: 1) Entity Fusion Branch ensures the coherence between inputs and outputs. It extracts entity features from the multi-modal representations powered by our specially constructed entity knowledge graph; 2) Attribute Fusion Branch adeptly preserves and processes the attributes. It efficiently amalgamates distinct attributes from diverse input modalities via our proposed attribute knowledge graph. Lastly, the entity and attribute features are adaptively fused as the conditional inputs to the pre-trained Stable Diffusion model for image generation. Extensive experiments under diverse modality combinations demonstrate its exceptional capability for visual content creation.|\u4eba\u7c7b\u611f\u77e5\u548c\u7406\u89e3\u7684\u591a\u9762\u6027\u8868\u660e\uff0c\u5f53\u6211\u4eec\u601d\u8003\u65f6\uff0c\u6211\u4eec\u7684\u8eab\u4f53\u53ef\u4ee5\u81ea\u7136\u5730\u91c7\u53d6\u4efb\u4f55\u611f\u5b98\u7ec4\u5408\uff0c\u4e5f\u5c31\u662f\u6a21\u6001\uff0c\u5e76\u5728\u6211\u4eec\u7684\u5927\u8111\u4e2d\u5f62\u6210\u4e00\u5e45\u7f8e\u4e3d\u7684\u56fe\u753b\u3002\u4f8b\u5982\uff0c\u5f53\u6211\u4eec\u770b\u5230\u732b\u820d\u5e76\u540c\u65f6\u611f\u77e5\u5230\u732b\u7684\u5495\u565c\u58f0\u65f6\uff0c\u6211\u4eec\u7684\u5927\u8111\u53ef\u4ee5\u6784\u5efa\u51fa\u732b\u820d\u91cc\u6709\u4e00\u53ea\u732b\u7684\u56fe\u7247\u3002\u76f4\u89c2\u5730\u8bf4\uff0c\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5e94\u8be5\u4fdd\u7559\u4eba\u7c7b\u7684\u591a\u529f\u80fd\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u9ad8\u6548\u3001\u534f\u4f5c\u5730\u4ece\u4efb\u610f\u6a21\u5f0f\u7ec4\u5408\u751f\u6210\u56fe\u50cf\u3002\u672c\u6587\u63d0\u51fa\u4e86 ImgAny\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\uff0c\u53ef\u4ee5\u6a21\u4eff\u4eba\u7c7b\u63a8\u7406\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u9996\u6b21\u5c1d\u8bd5\uff0c\u80fd\u591f\u9ad8\u6548\u3001\u7075\u6d3b\u5730\u4efb\u610f\u7ec4\u5408\u4e03\u79cd\u6a21\u6001\uff0c\u4ece\u8bed\u8a00\u3001\u97f3\u9891\u5230\u89c6\u89c9\u6a21\u6001\uff0c\u5305\u62ec\u56fe\u50cf\u3001\u70b9\u4e91\u3001\u70ed\u3001\u6df1\u5ea6\u548c\u4e8b\u4ef6\u6570\u636e\u3002\u6211\u4eec\u7684\u5173\u952e\u601d\u60f3\u53d7\u5230\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u542f\u53d1\uff0c\u6d89\u53ca\u5b9e\u4f53\u548c\u5c5e\u6027\u7ea7\u522b\u4e0a\u591a\u79cd\u8f93\u5165\u6a21\u5f0f\u7684\u96c6\u6210\u548c\u534f\u8c03\uff0c\u800c\u65e0\u9700\u8de8\u6a21\u5f0f\u8fdb\u884c\u7279\u5b9a\u8c03\u6574\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5e26\u6765\u4e86\u4e24\u4e2a\u65b0\u9896\u7684\u514d\u8bad\u7ec3\u6280\u672f\u5206\u652f\uff1a1\uff09\u5b9e\u4f53\u878d\u5408\u5206\u652f\u786e\u4fdd\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u5b83\u4ece\u7531\u6211\u4eec\u4e13\u95e8\u6784\u5efa\u7684\u5b9e\u4f53\u77e5\u8bc6\u56fe\u652f\u6301\u7684\u591a\u6a21\u6001\u8868\u793a\u4e2d\u63d0\u53d6\u5b9e\u4f53\u7279\u5f81\uff1b 2\uff09\u5c5e\u6027\u878d\u5408\u5206\u652f\u5de7\u5999\u5730\u4fdd\u5b58\u548c\u5904\u7406\u5c5e\u6027\u3002\u5b83\u901a\u8fc7\u6211\u4eec\u63d0\u51fa\u7684\u5c5e\u6027\u77e5\u8bc6\u56fe\u6709\u6548\u5730\u5408\u5e76\u4e86\u6765\u81ea\u4e0d\u540c\u8f93\u5165\u6a21\u5f0f\u7684\u4e0d\u540c\u5c5e\u6027\u3002\u6700\u540e\uff0c\u5b9e\u4f53\u548c\u5c5e\u6027\u7279\u5f81\u88ab\u81ea\u9002\u5e94\u5730\u878d\u5408\u4f5c\u4e3a\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u9884\u8bad\u7ec3\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u8f93\u5165\u3002\u4e0d\u540c\u6a21\u6001\u7ec4\u5408\u4e0b\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5353\u8d8a\u7684\u89c6\u89c9\u5185\u5bb9\u521b\u5efa\u80fd\u529b\u3002|[2401.17664v1](http://arxiv.org/pdf/2401.17664v1)|null|\n", "2401.17629": "|**2024-01-31**|**Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models**|\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7a7a\u95f4\u548c\u9891\u7387\u611f\u77e5\u6062\u590d\u65b9\u6cd5|Kyungsung Lee, Donggyu Lee, Myungjoo Kang|Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics.|\u6269\u6563\u6a21\u578b\u6700\u8fd1\u5df2\u6210\u4e3a\u56fe\u50cf\u6062\u590d\uff08IR\uff09\u7684\u4e00\u4e2a\u6709\u524d\u9014\u7684\u6846\u67b6\uff0c\u56e0\u4e3a\u5b83\u4eec\u80fd\u591f\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u5e76\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u517c\u5bb9\u3002\u89e3\u51b3\u7ea2\u5916\u566a\u58f0\u9006\u95ee\u9898\u7684\u73b0\u6709\u65b9\u6cd5\u8003\u8651\u4e86\u50cf\u7d20\u7ea7\u6570\u636e\u4fdd\u771f\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 SaFaRI\uff0c\u4e00\u79cd\u5177\u6709\u9ad8\u65af\u566a\u58f0\u7684 IR \u7a7a\u95f4\u548c\u9891\u7387\u611f\u77e5\u6269\u6563\u6a21\u578b\u3002\u6211\u4eec\u7684\u6a21\u578b\u9f13\u52b1\u56fe\u50cf\u5728\u7a7a\u95f4\u548c\u9891\u7387\u57df\u4e0a\u4fdd\u6301\u6570\u636e\u4fdd\u771f\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002\u6211\u4eec\u5168\u9762\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5728\u5404\u79cd\u566a\u58f0\u9006\u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u5305\u62ec\u4fee\u590d\u3001\u53bb\u566a\u548c\u8d85\u5206\u8fa8\u7387\u3002\u6211\u4eec\u7684\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0cSaFaRI \u5728 ImageNet \u6570\u636e\u96c6\u548c FFHQ \u6570\u636e\u96c6\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728 LPIPS \u548c FID \u6307\u6807\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u96f6\u6837\u672c IR \u65b9\u6cd5\u3002|[2401.17629v1](http://arxiv.org/pdf/2401.17629v1)|null|\n", "2401.17603": "|**2024-01-31**|**Topology-Aware Latent Diffusion for 3D Shape Generation**|\u7528\u4e8e\u751f\u6210 3D \u5f62\u72b6\u7684\u62d3\u6251\u611f\u77e5\u6f5c\u5728\u6269\u6563|Jiangbei Hu, Ben Fei, Baixin Xu, Fei Hou, Weidong Yang, Shengfa Wang, Na Lei, Chen Qian, Ying He|We introduce a new generative model that combines latent diffusion with persistent homology to create 3D shapes with high diversity, with a special emphasis on their topological characteristics. Our method involves representing 3D shapes as implicit fields, then employing persistent homology to extract topological features, including Betti numbers and persistence diagrams. The shape generation process consists of two steps. Initially, we employ a transformer-based autoencoding module to embed the implicit representation of each 3D shape into a set of latent vectors. Subsequently, we navigate through the learned latent space via a diffusion model. By strategically incorporating topological features into the diffusion process, our generative module is able to produce a richer variety of 3D shapes with different topological structures. Furthermore, our framework is flexible, supporting generation tasks constrained by a variety of inputs, including sparse and partial point clouds, as well as sketches. By modifying the persistence diagrams, we can alter the topology of the shapes generated from these input modalities.|\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u6f5c\u5728\u6269\u6563\u4e0e\u6301\u4e45\u540c\u6e90\u6027\u76f8\u7ed3\u5408\uff0c\u4ee5\u521b\u5efa\u5177\u6709\u9ad8\u5ea6\u591a\u6837\u6027\u7684 3D \u5f62\u72b6\uff0c\u5e76\u7279\u522b\u5f3a\u8c03\u5176\u62d3\u6251\u7279\u5f81\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d89\u53ca\u5c06 3D \u5f62\u72b6\u8868\u793a\u4e3a\u9690\u5f0f\u573a\uff0c\u7136\u540e\u91c7\u7528\u6301\u4e45\u540c\u6e90\u6027\u6765\u63d0\u53d6\u62d3\u6251\u7279\u5f81\uff0c\u5305\u62ec\u8d1d\u8482\u6570\u548c\u6301\u4e45\u6027\u56fe\u3002\u5f62\u72b6\u751f\u6210\u8fc7\u7a0b\u7531\u4e24\u4e2a\u6b65\u9aa4\u7ec4\u6210\u3002\u6700\u521d\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u4e8e Transformer \u7684\u81ea\u52a8\u7f16\u7801\u6a21\u5757\u5c06\u6bcf\u4e2a 3D \u5f62\u72b6\u7684\u9690\u5f0f\u8868\u793a\u5d4c\u5165\u5230\u4e00\u7ec4\u6f5c\u5728\u5411\u91cf\u4e2d\u3002\u968f\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u6269\u6563\u6a21\u578b\u6d4f\u89c8\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u901a\u8fc7\u7b56\u7565\u6027\u5730\u5c06\u62d3\u6251\u7279\u5f81\u878d\u5165\u5230\u6269\u6563\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u7684\u751f\u6210\u6a21\u5757\u80fd\u591f\u751f\u6210\u66f4\u4e30\u5bcc\u7684\u5177\u6709\u4e0d\u540c\u62d3\u6251\u7ed3\u6784\u7684 3D \u5f62\u72b6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u975e\u5e38\u7075\u6d3b\uff0c\u652f\u6301\u53d7\u5404\u79cd\u8f93\u5165\u7ea6\u675f\u7684\u751f\u6210\u4efb\u52a1\uff0c\u5305\u62ec\u7a00\u758f\u548c\u90e8\u5206\u70b9\u4e91\u4ee5\u53ca\u8349\u56fe\u3002\u901a\u8fc7\u4fee\u6539\u6301\u4e45\u6027\u56fe\uff0c\u6211\u4eec\u53ef\u4ee5\u6539\u53d8\u4ece\u8fd9\u4e9b\u8f93\u5165\u6a21\u5f0f\u751f\u6210\u7684\u5f62\u72b6\u7684\u62d3\u6251\u3002|[2401.17603v1](http://arxiv.org/pdf/2401.17603v1)|null|\n", "2401.17547": "|**2024-01-31**|**Task-Oriented Diffusion Model Compression**|\u9762\u5411\u4efb\u52a1\u7684\u6269\u6563\u6a21\u578b\u538b\u7f29|Geonung Kim, Beomsu Kim, Eunhyeok Park, Sunghyun Cho|As recent advancements in large-scale Text-to-Image (T2I) diffusion models have yielded remarkable high-quality image generation, diverse downstream Image-to-Image (I2I) applications have emerged. Despite the impressive results achieved by these I2I models, their practical utility is hampered by their large model size and the computational burden of the iterative denoising process. In this paper, we explore the compression potential of these I2I models in a task-oriented manner and introduce a novel method for reducing both model size and the number of timesteps. Through extensive experiments, we observe key insights and use our empirical knowledge to develop practical solutions that aim for near-optimal results with minimal exploration costs. We validate the effectiveness of our method by applying it to InstructPix2Pix for image editing and StableSR for image restoration. Our approach achieves satisfactory output quality with 39.2% and 56.4% reduction in model footprint and 81.4% and 68.7% decrease in latency to InstructPix2Pix and StableSR, respectively.|\u968f\u7740\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u4ea7\u751f\u4e86\u663e\u7740\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5404\u79cd\u4e0b\u6e38\u56fe\u50cf\u5230\u56fe\u50cf\uff08I2I\uff09\u5e94\u7528\u7a0b\u5e8f\u4e5f\u968f\u4e4b\u51fa\u73b0\u3002\u5c3d\u7ba1\u8fd9\u4e9b I2I \u6a21\u578b\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u5b83\u4eec\u7684\u5b9e\u7528\u6027\u5374\u53d7\u5230\u6a21\u578b\u5c3a\u5bf8\u5927\u548c\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u7684\u8ba1\u7b97\u8d1f\u62c5\u7684\u963b\u788d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ee5\u9762\u5411\u4efb\u52a1\u7684\u65b9\u5f0f\u63a2\u7d22\u8fd9\u4e9b I2I \u6a21\u578b\u7684\u538b\u7f29\u6f5c\u529b\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u65f6\u95f4\u6b65\u6570\u7684\u65b0\u65b9\u6cd5\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5173\u952e\u7684\u89c1\u89e3\uff0c\u5e76\u5229\u7528\u6211\u4eec\u7684\u7ecf\u9a8c\u77e5\u8bc6\u6765\u5f00\u53d1\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u4ee5\u6700\u5c0f\u7684\u52d8\u63a2\u6210\u672c\u83b7\u5f97\u63a5\u8fd1\u6700\u4f73\u7684\u7ed3\u679c\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u5176\u5e94\u7528\u4e8e InstructPix2Pix \u8fdb\u884c\u56fe\u50cf\u7f16\u8f91\u548c StableSR \u8fdb\u884c\u56fe\u50cf\u6062\u590d\u6765\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u8f93\u51fa\u8d28\u91cf\uff0c\u6a21\u578b\u5360\u7528\u7a7a\u95f4\u51cf\u5c11\u4e86 39.2% \u548c 56.4%\uff0cInstructPix2Pix \u548c StableSR \u7684\u5ef6\u8fdf\u5206\u522b\u51cf\u5c11\u4e86 81.4% \u548c 68.7%\u3002|[2401.17547v1](http://arxiv.org/pdf/2401.17547v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.18084": "|**2024-01-31**|**Binding Touch to Everything: Learning Unified Multimodal Tactile Representations**|\u5c06\u89e6\u6478\u4e0e\u4e00\u5207\u7ed3\u5408\u8d77\u6765\uff1a\u5b66\u4e60\u7edf\u4e00\u7684\u591a\u6a21\u6001\u89e6\u89c9\u8868\u5f81|Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, et.al.|The ability to associate touch with other modalities has huge implications for humans and computational systems. However, multimodal learning with touch remains challenging due to the expensive data collection process and non-standardized sensor outputs. We introduce UniTouch, a unified tactile model for vision-based touch sensors connected to multiple modalities, including vision, language, and sound. We achieve this by aligning our UniTouch embeddings to pretrained image embeddings already associated with a variety of other modalities. We further propose learnable sensor-specific tokens, allowing the model to learn from a set of heterogeneous tactile sensors, all at the same time. UniTouch is capable of conducting various touch sensing tasks in the zero-shot setting, from robot grasping prediction to touch image question answering. To the best of our knowledge, UniTouch is the first to demonstrate such capabilities. Project page: https://cfeng16.github.io/UniTouch/|\u5c06\u89e6\u6478\u4e0e\u5176\u4ed6\u65b9\u5f0f\u8054\u7cfb\u8d77\u6765\u7684\u80fd\u529b\u5bf9\u4eba\u7c7b\u548c\u8ba1\u7b97\u7cfb\u7edf\u5177\u6709\u5de8\u5927\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6602\u8d35\u7684\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u548c\u975e\u6807\u51c6\u5316\u7684\u4f20\u611f\u5668\u8f93\u51fa\uff0c\u89e6\u6478\u591a\u6a21\u6001\u5b66\u4e60\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u63a8\u51fa UniTouch\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u89e6\u6478\u4f20\u611f\u5668\u7684\u7edf\u4e00\u89e6\u89c9\u6a21\u578b\uff0c\u53ef\u8fde\u63a5\u5230\u591a\u79cd\u6a21\u5f0f\uff0c\u5305\u62ec\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u58f0\u97f3\u3002\u6211\u4eec\u901a\u8fc7\u5c06 UniTouch \u5d4c\u5165\u4e0e\u5df2\u7ecf\u4e0e\u5404\u79cd\u5176\u4ed6\u6a21\u5f0f\u76f8\u5173\u8054\u7684\u9884\u8bad\u7ec3\u56fe\u50cf\u5d4c\u5165\u5bf9\u9f50\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u53ef\u5b66\u4e60\u7684\u4f20\u611f\u5668\u7279\u5b9a\u6807\u8bb0\uff0c\u5141\u8bb8\u6a21\u578b\u540c\u65f6\u4ece\u4e00\u7ec4\u5f02\u6784\u89e6\u89c9\u4f20\u611f\u5668\u4e2d\u5b66\u4e60\u3002 UniTouch \u80fd\u591f\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u6267\u884c\u5404\u79cd\u89e6\u6478\u4f20\u611f\u4efb\u52a1\uff0c\u4ece\u673a\u5668\u4eba\u6293\u53d6\u9884\u6d4b\u5230\u89e6\u6478\u56fe\u50cf\u95ee\u7b54\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0cUniTouch \u662f\u7b2c\u4e00\u4e2a\u5c55\u793a\u6b64\u7c7b\u529f\u80fd\u7684\u516c\u53f8\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://cfeng16.github.io/UniTouch/|[2401.18084v1](http://arxiv.org/pdf/2401.18084v1)|null|\n", "2401.17910": "|**2024-01-31**|**Controllable Dense Captioner with Multimodal Embedding Bridging**|\u5177\u6709\u591a\u6a21\u5f0f\u5d4c\u5165\u6865\u63a5\u7684\u53ef\u63a7\u5bc6\u96c6\u5b57\u5e55\u5668|Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Qixiang Ye, Fang Wan|In this paper, we propose a controllable dense captioner (ControlCap), which accommodates user's intention to dense captioning by introducing linguistic guidance. ControlCap is defined as a multimodal embedding bridging architecture, which comprises multimodal embedding generation (MEG) module and bi-directional embedding bridging (BEB) module. While MEG module represents objects/regions by combining embeddings of detailed information with context-aware ones, it also endows ControlCap the adaptability to specialized controls by utilizing them as linguistic guidance. BEB module aligns the linguistic guidance with visual embeddings through borrowing/returning features from/to the visual domain and gathering such features to predict text descriptions. Experiments on Visual Genome and VG-COCO datasets show that ControlCap respectively outperforms the state-of-the-art methods by 1.5% and 3.7% (mAP). Last but not least, with the capability of converting region-category pairs to region-text pairs, ControlCap is able to act as a powerful data engine for dense captioning. Code is available at https://github.com/callsys/ControlCap.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63a7\u7684\u5bc6\u96c6\u5b57\u5e55\u5668\uff08ControlCap\uff09\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u8bed\u8a00\u6307\u5bfc\u6765\u9002\u5e94\u7528\u6237\u5bf9\u5bc6\u96c6\u5b57\u5e55\u7684\u610f\u56fe\u3002 ControlCap\u88ab\u5b9a\u4e49\u4e3a\u591a\u6a21\u6001\u5d4c\u5165\u6865\u63a5\u67b6\u6784\uff0c\u5305\u62ec\u591a\u6a21\u6001\u5d4c\u5165\u751f\u6210\uff08MEG\uff09\u6a21\u5757\u548c\u53cc\u5411\u5d4c\u5165\u6865\u63a5\uff08BEB\uff09\u6a21\u5757\u3002\u867d\u7136 MEG \u6a21\u5757\u901a\u8fc7\u5c06\u8be6\u7ec6\u4fe1\u606f\u7684\u5d4c\u5165\u4e0e\u4e0a\u4e0b\u6587\u611f\u77e5\u4fe1\u606f\u76f8\u7ed3\u5408\u6765\u8868\u793a\u5bf9\u8c61/\u533a\u57df\uff0c\u4f46\u5b83\u8fd8\u901a\u8fc7\u5229\u7528\u5b83\u4eec\u4f5c\u4e3a\u8bed\u8a00\u6307\u5bfc\uff0c\u8d4b\u4e88 ControlCap \u5bf9\u4e13\u95e8\u63a7\u4ef6\u7684\u9002\u5e94\u6027\u3002 BEB \u6a21\u5757\u901a\u8fc7\u4ece\u89c6\u89c9\u9886\u57df\u501f\u7528/\u8fd4\u56de\u7279\u5f81\u5e76\u6536\u96c6\u8fd9\u4e9b\u7279\u5f81\u6765\u9884\u6d4b\u6587\u672c\u63cf\u8ff0\uff0c\u4ece\u800c\u4f7f\u8bed\u8a00\u6307\u5bfc\u4e0e\u89c6\u89c9\u5d4c\u5165\u4fdd\u6301\u4e00\u81f4\u3002\u5728 Visual Genome \u548c VG-COCO \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cControlCap \u5206\u522b\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u9ad8\u51fa 1.5% \u548c 3.7% (mAP)\u3002\u6700\u540e\u4f46\u5e76\u975e\u6700\u4e0d\u91cd\u8981\u7684\u4e00\u70b9\u662f\uff0c\u51ed\u501f\u5c06\u533a\u57df\u7c7b\u522b\u5bf9\u8f6c\u6362\u4e3a\u533a\u57df\u6587\u672c\u5bf9\u7684\u80fd\u529b\uff0cControlCap \u80fd\u591f\u5145\u5f53\u5bc6\u96c6\u5b57\u5e55\u7684\u5f3a\u5927\u6570\u636e\u5f15\u64ce\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/callsys/ControlCap \u83b7\u53d6\u3002|[2401.17910v1](http://arxiv.org/pdf/2401.17910v1)|null|\n", "2401.17862": "|**2024-01-31**|**Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis**|\u90bb\u8fd1 QA\uff1a\u91ca\u653e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u529b\u91cf\u8fdb\u884c\u7a7a\u95f4\u90bb\u8fd1\u5206\u6790|Jianing Li, Xi Nan, Ming Lu, Li Du, Shanghang Zhang|Multi-modal large language models (MLLMs) have demonstrated remarkable vision-language capabilities, primarily due to the exceptional in-context understanding and multi-task learning strengths of large language models (LLMs). The advent of visual instruction tuning has further enhanced MLLMs' performance in vision-language understanding. However, while existing MLLMs adeptly recognize \\textit{what} objects are in an image, they still face challenges in effectively discerning \\textit{where} these objects are, particularly along the distance (scene depth) axis. To overcome this limitation in MLLMs, we introduce Proximity Question Answering (Proximity QA), a novel framework designed to enable MLLMs to infer the proximity relationship between objects in images. The framework operates in two phases: the first phase focuses on guiding the models to understand the relative depth of objects, and the second phase further encourages the models to infer the proximity relationships between objects based on their depth perceptions. We also propose a VQA dataset called Proximity-110K, containing additional instructions that incorporate depth information and the proximity relationships of objects. We have conducted extensive experiments to validate Proximity QA's superior ability in depth perception and proximity analysis, outperforming other state-of-the-art MLLMs. Code and dataset will be released at \\textcolor{magenta}{https://github.com/NorthSummer/ProximityQA.git}.|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5df2\u7ecf\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u89c6\u89c9\u8bed\u8a00\u80fd\u529b\uff0c\u8fd9\u4e3b\u8981\u5f52\u529f\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5353\u8d8a\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u4f18\u52bf\u3002\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u7684\u51fa\u73b0\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86 MLLM \u5728\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u867d\u7136\u73b0\u6709\u7684 MLLM \u80fd\u591f\u719f\u7ec3\u5730\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684 \\textit{what} \u5bf9\u8c61\uff0c\u4f46\u5b83\u4eec\u5728\u6709\u6548\u8bc6\u522b \\textit{where} \u8fd9\u4e9b\u5bf9\u8c61\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u6cbf\u7740\u8ddd\u79bb\uff08\u573a\u666f\u6df1\u5ea6\uff09\u8f74\u3002\u4e3a\u4e86\u514b\u670d MLLM \u4e2d\u7684\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u90bb\u8fd1\u95ee\u7b54\uff08Proximity QA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u65e8\u5728\u4f7f MLLM \u80fd\u591f\u63a8\u65ad\u56fe\u50cf\u4e2d\u5bf9\u8c61\u4e4b\u95f4\u7684\u90bb\u8fd1\u5173\u7cfb\u3002\u8be5\u6846\u67b6\u5206\u4e24\u4e2a\u9636\u6bb5\u8fd0\u884c\uff1a\u7b2c\u4e00\u9636\u6bb5\u4fa7\u91cd\u4e8e\u5f15\u5bfc\u6a21\u578b\u7406\u89e3\u5bf9\u8c61\u7684\u76f8\u5bf9\u6df1\u5ea6\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u4e00\u6b65\u9f13\u52b1\u6a21\u578b\u6839\u636e\u6df1\u5ea6\u611f\u77e5\u63a8\u65ad\u5bf9\u8c61\u4e4b\u95f4\u7684\u90bb\u8fd1\u5173\u7cfb\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a Proximity-110K \u7684 VQA \u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5305\u542b\u6df1\u5ea6\u4fe1\u606f\u548c\u5bf9\u8c61\u90bb\u8fd1\u5173\u7cfb\u7684\u9644\u52a0\u6307\u4ee4\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\u6765\u9a8c\u8bc1 Proximity QA \u5728\u6df1\u5ea6\u611f\u77e5\u548c\u90bb\u8fd1\u5206\u6790\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684 MLLM\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728 \\textcolor{magenta}{https://github.com/NorthSummer/ProximityQA.git} \u53d1\u5e03\u3002|[2401.17862v1](http://arxiv.org/pdf/2401.17862v1)|null|\n", "2401.17797": "|**2024-01-31**|**M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval**|M2-RAAP\uff1a\u4e00\u79cd\u591a\u6a21\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a8\u8fdb\u57fa\u4e8e\u9002\u5e94\u7684\u9884\u8bad\u7ec3\uff0c\u5b9e\u73b0\u6709\u6548\u4e14\u9ad8\u6548\u7684\u96f6\u6837\u672c\u89c6\u9891\u6587\u672c\u68c0\u7d22|Xingning Dong, Zipeng Feng, Chunluan Zhou, Xuzheng Yu, Ming Yang, Qingpei Guo|We present a Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards effective and efficient zero-shot video-text retrieval, dubbed M2-RAAP. Upon popular image-text models like CLIP, most current adaptation-based video-text pre-training methods are confronted by three major issues, i.e., noisy data corpus, time-consuming pre-training, and limited performance gain. Towards this end, we conduct a comprehensive study including four critical steps in video-text pre-training. Specifically, we investigate 1) data filtering and refinement, 2) video input type selection, 3) temporal modeling, and 4) video feature enhancement. We then summarize this empirical study into the M2-RAAP recipe, where our technical contributions lie in 1) the data filtering and text re-writing pipeline resulting in 1M high-quality bilingual video-text pairs, 2) the replacement of video inputs with key-frames to accelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy to enhance video features. We conduct extensive experiments by adapting three image-text foundation models on two refined video-text datasets from different languages, validating the robustness and reproducibility of M2-RAAP for adaptation-based pre-training. Results demonstrate that M2-RAAP yields superior performance with significantly reduced data (-90%) and time consumption (-95%), establishing a new SOTA on four English zero-shot retrieval datasets and two Chinese ones. We are preparing our refined bilingual data annotations and codebase, which will be available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a8\u8fdb\u57fa\u4e8e\u9002\u5e94\u7684\u9884\u8bad\u7ec3\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u4e14\u9ad8\u6548\u7684\u96f6\u955c\u5934\u89c6\u9891\u6587\u672c\u68c0\u7d22\uff0c\u79f0\u4e3a M2-RAAP\u3002\u5728\u6d41\u884c\u7684\u56fe\u50cf\u6587\u672c\u6a21\u578b\uff08\u5982CLIP\uff09\u4e0a\uff0c\u5f53\u524d\u5927\u591a\u6570\u57fa\u4e8e\u81ea\u9002\u5e94\u7684\u89c6\u9891\u6587\u672c\u9884\u8bad\u7ec3\u65b9\u6cd5\u90fd\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff0c\u5373\u566a\u58f0\u6570\u636e\u8bed\u6599\u3001\u8017\u65f6\u7684\u9884\u8bad\u7ec3\u548c\u6709\u9650\u7684\u6027\u80fd\u589e\u76ca\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u5168\u9762\u7684\u7814\u7a76\uff0c\u5305\u62ec\u89c6\u9891\u6587\u672c\u9884\u8bad\u7ec3\u7684\u56db\u4e2a\u5173\u952e\u6b65\u9aa4\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7814\u7a76 1) \u6570\u636e\u8fc7\u6ee4\u548c\u7ec6\u5316\uff0c2) \u89c6\u9891\u8f93\u5165\u7c7b\u578b\u9009\u62e9\uff0c3) \u65f6\u95f4\u5efa\u6a21\uff0c\u4ee5\u53ca 4) \u89c6\u9891\u7279\u5f81\u589e\u5f3a\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u8fd9\u9879\u5b9e\u8bc1\u7814\u7a76\u603b\u7ed3\u4e3a M2-RAAP \u914d\u65b9\uff0c\u5176\u4e2d\u6211\u4eec\u7684\u6280\u672f\u8d21\u732e\u5728\u4e8e 1) \u6570\u636e\u8fc7\u6ee4\u548c\u6587\u672c\u91cd\u5199\u7ba1\u9053\uff0c\u4ea7\u751f 1M \u9ad8\u8d28\u91cf\u53cc\u8bed\u89c6\u9891\u6587\u672c\u5bf9\uff0c2) \u5c06\u89c6\u9891\u8f93\u5165\u66ff\u6362\u4e3a\u5173\u952e\u5e27\u6765\u52a0\u901f\u9884\u8bad\u7ec3\uff0c3\uff09\u8f85\u52a9\u5b57\u5e55\u5f15\u5bfc\uff08ACG\uff09\u7b56\u7565\u6765\u589e\u5f3a\u89c6\u9891\u529f\u80fd\u3002\u6211\u4eec\u901a\u8fc7\u5728\u6765\u81ea\u4e0d\u540c\u8bed\u8a00\u7684\u4e24\u4e2a\u7cbe\u70bc\u89c6\u9891\u6587\u672c\u6570\u636e\u96c6\u4e0a\u91c7\u7528\u4e09\u4e2a\u56fe\u50cf\u6587\u672c\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86 M2-RAAP \u7528\u4e8e\u57fa\u4e8e\u9002\u5e94\u7684\u9884\u8bad\u7ec3\u7684\u9c81\u68d2\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cM2-RAAP \u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u663e\u7740\u51cf\u5c11\u4e86\u6570\u636e\u91cf (-90%) \u548c\u65f6\u95f4\u6d88\u8017 (-95%)\uff0c\u5728\u56db\u4e2a\u82f1\u6587\u96f6\u6837\u672c\u68c0\u7d22\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u4e2d\u6587\u96f6\u6837\u672c\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684 SOTA\u3002\u6211\u4eec\u6b63\u5728\u51c6\u5907\u5b8c\u5584\u7684\u53cc\u8bed\u6570\u636e\u6ce8\u91ca\u548c\u4ee3\u7801\u5e93\uff0c\u53ef\u5728 https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP \u83b7\u53d6\u3002|[2401.17797v1](http://arxiv.org/pdf/2401.17797v1)|null|\n", "2401.17773": "|**2024-01-31**|**SNP-S3: Shared Network Pre-training and Significant Semantic Strengthening for Various Video-Text Tasks**|SNP-S3\uff1a\u5404\u79cd\u89c6\u9891\u6587\u672c\u4efb\u52a1\u7684\u5171\u4eab\u7f51\u7edc\u9884\u8bad\u7ec3\u548c\u663e\u7740\u8bed\u4e49\u5f3a\u5316|Xingning Dong, Qingpei Guo, Tian Gan, Qing Wang, Jianlong Wu, Xiangyuan Ren, Yuan Cheng, Wei Chu|We present a framework for learning cross-modal video representations by directly pre-training on raw data to facilitate various downstream video-text tasks. Our main contributions lie in the pre-training framework and proxy tasks. First, based on the shortcomings of two mainstream pixel-level pre-training architectures (limited applications or less efficient), we propose Shared Network Pre-training (SNP). By employing one shared BERT-type network to refine textual and cross-modal features simultaneously, SNP is lightweight and could support various downstream applications. Second, based on the intuition that people always pay attention to several \"significant words\" when understanding a sentence, we propose the Significant Semantic Strengthening (S3) strategy, which includes a novel masking and matching proxy task to promote the pre-training performance. Experiments conducted on three downstream video-text tasks and six datasets demonstrate that, we establish a new state-of-the-art in pixel-level video-text pre-training; we also achieve a satisfactory balance between the pre-training efficiency and the fine-tuning performance. The codebase are available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/snps3_vtp.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7\u76f4\u63a5\u5bf9\u539f\u59cb\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u6765\u5b66\u4e60\u8de8\u6a21\u5f0f\u89c6\u9891\u8868\u793a\u7684\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdb\u5404\u79cd\u4e0b\u6e38\u89c6\u9891\u6587\u672c\u4efb\u52a1\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u9884\u8bad\u7ec3\u6846\u67b6\u548c\u4ee3\u7406\u4efb\u52a1\u3002\u9996\u5148\uff0c\u57fa\u4e8e\u4e24\u79cd\u4e3b\u6d41\u50cf\u7d20\u7ea7\u9884\u8bad\u7ec3\u67b6\u6784\u7684\u7f3a\u70b9\uff08\u5e94\u7528\u6709\u9650\u6216\u6548\u7387\u8f83\u4f4e\uff09\uff0c\u6211\u4eec\u63d0\u51fa\u5171\u4eab\u7f51\u7edc\u9884\u8bad\u7ec3\uff08SNP\uff09\u3002\u901a\u8fc7\u91c7\u7528\u4e00\u4e2a\u5171\u4eab\u7684 BERT \u578b\u7f51\u7edc\u540c\u65f6\u7ec6\u5316\u6587\u672c\u548c\u8de8\u6a21\u6001\u7279\u5f81\uff0cSNP \u662f\u8f7b\u91cf\u7ea7\u7684\uff0c\u53ef\u4ee5\u652f\u6301\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\u3002\u5176\u6b21\uff0c\u57fa\u4e8e\u4eba\u4eec\u5728\u7406\u89e3\u53e5\u5b50\u65f6\u603b\u662f\u5173\u6ce8\u51e0\u4e2a\u201c\u91cd\u8981\u5355\u8bcd\u201d\u7684\u76f4\u89c9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u91cd\u8981\u8bed\u4e49\u5f3a\u5316\uff08S3\uff09\u7b56\u7565\uff0c\u5176\u4e2d\u5305\u62ec\u4e00\u79cd\u65b0\u9896\u7684\u63a9\u853d\u548c\u5339\u914d\u4ee3\u7406\u4efb\u52a1\u6765\u63d0\u5347\u9884\u8bad\u7ec3\u6027\u80fd\u3002\u5bf9\u4e09\u4e2a\u4e0b\u6e38\u89c6\u9891\u6587\u672c\u4efb\u52a1\u548c\u516d\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u5728\u50cf\u7d20\u7ea7\u89c6\u9891\u6587\u672c\u9884\u8bad\u7ec3\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6280\u672f\uff1b\u6211\u4eec\u8fd8\u5728\u9884\u8bad\u7ec3\u6548\u7387\u548c\u5fae\u8c03\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u5e73\u8861\u3002\u4ee3\u7801\u5e93\u4f4d\u4e8e https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/snps3_vtp\u3002|[2401.17773v1](http://arxiv.org/pdf/2401.17773v1)|null|\n"}, "LLM": {}, "Transformer": {"2401.18032": "|**2024-01-31**|**DROP: Decouple Re-Identification and Human Parsing with Task-specific Features for Occluded Person Re-identification**|DROP\uff1a\u5c06\u91cd\u65b0\u8bc6\u522b\u548c\u4eba\u4f53\u89e3\u6790\u4e0e\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u7279\u5f81\u5206\u79bb\u4ee5\u8fdb\u884c\u88ab\u906e\u6321\u4eba\u5458\u91cd\u65b0\u8bc6\u522b|Shuguang Dou, Xiangyang Jiang, Yuanpeng Tu, Junyao Gao, Zefan Qu, Qingsong Zhao, Cairong Zhao|The paper introduces the Decouple Re-identificatiOn and human Parsing (DROP) method for occluded person re-identification (ReID). Unlike mainstream approaches using global features for simultaneous multi-task learning of ReID and human parsing, or relying on semantic information for attention guidance, DROP argues that the inferior performance of the former is due to distinct granularity requirements for ReID and human parsing features. ReID focuses on instance part-level differences between pedestrian parts, while human parsing centers on semantic spatial context, reflecting the internal structure of the human body. To address this, DROP decouples features for ReID and human parsing, proposing detail-preserving upsampling to combine varying resolution feature maps. Parsing-specific features for human parsing are decoupled, and human position information is exclusively added to the human parsing branch. In the ReID branch, a part-aware compactness loss is introduced to enhance instance-level part differences. Experimental results highlight the efficacy of DROP, especially achieving a Rank-1 accuracy of 76.8% on Occluded-Duke, surpassing two mainstream methods. The codebase is accessible at https://github.com/shuguang-52/DROP.|\u672c\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8e\u906e\u6321\u884c\u4eba\u91cd\u65b0\u8bc6\u522b\uff08ReID\uff09\u7684\u89e3\u8026\u91cd\u65b0\u8bc6\u522b\u548c\u4eba\u4f53\u89e3\u6790\uff08DROP\uff09\u65b9\u6cd5\u3002\u4e0e\u4f7f\u7528\u5168\u5c40\u7279\u5f81\u8fdb\u884c ReID \u548c\u4eba\u7c7b\u89e3\u6790\u540c\u65f6\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6216\u4f9d\u8d56\u8bed\u4e49\u4fe1\u606f\u8fdb\u884c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u4e3b\u6d41\u65b9\u6cd5\u4e0d\u540c\uff0cDROP \u8ba4\u4e3a\uff0c\u524d\u8005\u7684\u6027\u80fd\u8f83\u5dee\u662f\u7531\u4e8e ReID \u548c\u4eba\u7c7b\u89e3\u6790\u7279\u5f81\u7684\u7c92\u5ea6\u8981\u6c42\u4e0d\u540c\u3002 ReID\u4fa7\u91cd\u4e8e\u884c\u4eba\u90e8\u4f4d\u4e4b\u95f4\u7684\u5b9e\u4f8b\u90e8\u4f4d\u7ea7\u522b\u5dee\u5f02\uff0c\u800c\u4eba\u4f53\u89e3\u6790\u5219\u4fa7\u91cd\u4e8e\u8bed\u4e49\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u53cd\u6620\u4eba\u4f53\u7684\u5185\u90e8\u7ed3\u6784\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0cDROP \u5c06 ReID \u548c\u4eba\u4f53\u89e3\u6790\u7684\u7279\u5f81\u89e3\u8026\uff0c\u63d0\u51fa\u4fdd\u7559\u7ec6\u8282\u7684\u4e0a\u91c7\u6837\u4ee5\u7ec4\u5408\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u7279\u5f81\u56fe\u3002\u4eba\u7c7b\u89e3\u6790\u7684\u89e3\u6790\u7279\u5b9a\u7279\u5f81\u662f\u89e3\u8026\u7684\uff0c\u5e76\u4e14\u4eba\u7c7b\u4f4d\u7f6e\u4fe1\u606f\u4e13\u95e8\u6dfb\u52a0\u5230\u4eba\u7c7b\u89e3\u6790\u5206\u652f\u4e2d\u3002\u5728 ReID \u5206\u652f\u4e2d\uff0c\u5f15\u5165\u4e86\u96f6\u4ef6\u611f\u77e5\u7d27\u51d1\u6027\u635f\u5931\u4ee5\u589e\u5f3a\u5b9e\u4f8b\u7ea7\u96f6\u4ef6\u5dee\u5f02\u3002\u5b9e\u9a8c\u7ed3\u679c\u51f8\u663e\u4e86 DROP \u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728 Occlusion-Duke \u4e0a\u8fbe\u5230 76.8% \u7684 Rank-1 \u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u4e24\u79cd\u4e3b\u6d41\u65b9\u6cd5\u3002\u4ee3\u7801\u5e93\u53ef\u4ece https://github.com/shuguang-52/DROP \u8bbf\u95ee\u3002|[2401.18032v1](http://arxiv.org/pdf/2401.18032v1)|null|\n", "2401.17609": "|**2024-01-31**|**LaneGraph2Seq: Lane Topology Extraction with Language Model via Vertex-Edge Encoding and Connectivity Enhancement**|LaneGraph2Seq\uff1a\u901a\u8fc7\u70b9\u8fb9\u7f16\u7801\u548c\u8fde\u63a5\u589e\u5f3a\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u8f66\u9053\u62d3\u6251|Renyuan Peng, Xinyue Cai, Hang Xu, Jiachen Lu, Feng Wen, Wei Zhang, Li Zhang|Understanding road structures is crucial for autonomous driving. Intricate road structures are often depicted using lane graphs, which include centerline curves and connections forming a Directed Acyclic Graph (DAG). Accurate extraction of lane graphs relies on precisely estimating vertex and edge information within the DAG. Recent research highlights Transformer-based language models' impressive sequence prediction abilities, making them effective for learning graph representations when graph data are encoded as sequences. However, existing studies focus mainly on modeling vertices explicitly, leaving edge information simply embedded in the network. Consequently, these approaches fall short in the task of lane graph extraction. To address this, we introduce LaneGraph2Seq, a novel approach for lane graph extraction. It leverages a language model with vertex-edge encoding and connectivity enhancement. Our serialization strategy includes a vertex-centric depth-first traversal and a concise edge-based partition sequence. Additionally, we use classifier-free guidance combined with nucleus sampling to improve lane connectivity. We validate our method on prominent datasets, nuScenes and Argoverse 2, showcasing consistent and compelling results. Our LaneGraph2Seq approach demonstrates superior performance compared to state-of-the-art techniques in lane graph extraction.|\u4e86\u89e3\u9053\u8def\u7ed3\u6784\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\u3002\u590d\u6742\u7684\u9053\u8def\u7ed3\u6784\u901a\u5e38\u4f7f\u7528\u8f66\u9053\u56fe\u6765\u63cf\u8ff0\uff0c\u5176\u4e2d\u5305\u62ec\u5f62\u6210\u6709\u5411\u65e0\u73af\u56fe (DAG) \u7684\u4e2d\u5fc3\u7ebf\u66f2\u7ebf\u548c\u8fde\u63a5\u3002\u8f66\u9053\u56fe\u7684\u51c6\u786e\u63d0\u53d6\u4f9d\u8d56\u4e8e\u7cbe\u786e\u4f30\u8ba1 DAG \u5185\u7684\u9876\u70b9\u548c\u8fb9\u7f18\u4fe1\u606f\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u57fa\u4e8e Transformer \u7684\u8bed\u8a00\u6a21\u578b\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u5e8f\u5217\u9884\u6d4b\u80fd\u529b\uff0c\u4f7f\u5b83\u4eec\u5728\u56fe\u6570\u636e\u7f16\u7801\u4e3a\u5e8f\u5217\u65f6\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u56fe\u8868\u793a\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u663e\u5f0f\u5730\u5bf9\u9876\u70b9\u8fdb\u884c\u5efa\u6a21\uff0c\u800c\u5c06\u8fb9\u7f18\u4fe1\u606f\u7b80\u5355\u5730\u5d4c\u5165\u5230\u7f51\u7edc\u4e2d\u3002\u56e0\u6b64\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u8f66\u9053\u56fe\u63d0\u53d6\u4efb\u52a1\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 LaneGraph2Seq\uff0c\u4e00\u79cd\u7528\u4e8e\u8f66\u9053\u56fe\u63d0\u53d6\u7684\u65b0\u65b9\u6cd5\u3002\u5b83\u5229\u7528\u5177\u6709\u70b9\u8fb9\u7f16\u7801\u548c\u8fde\u63a5\u589e\u5f3a\u529f\u80fd\u7684\u8bed\u8a00\u6a21\u578b\u3002\u6211\u4eec\u7684\u5e8f\u5217\u5316\u7b56\u7565\u5305\u62ec\u4ee5\u9876\u70b9\u4e3a\u4e2d\u5fc3\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u548c\u7b80\u6d01\u7684\u57fa\u4e8e\u8fb9\u7f18\u7684\u5206\u533a\u5e8f\u5217\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\u4e0e\u6838\u91c7\u6837\u76f8\u7ed3\u5408\u6765\u6539\u5584\u8f66\u9053\u8fde\u63a5\u6027\u3002\u6211\u4eec\u5728\u8457\u540d\u6570\u636e\u96c6 nuScenes \u548c Argoverse 2 \u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4e00\u81f4\u4e14\u4ee4\u4eba\u4fe1\u670d\u7684\u7ed3\u679c\u3002\u4e0e\u6cf3\u9053\u56fe\u63d0\u53d6\u4e2d\u6700\u5148\u8fdb\u7684\u6280\u672f\u76f8\u6bd4\uff0c\u6211\u4eec\u7684 LaneGraph2Seq \u65b9\u6cd5\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002|[2401.17609v1](http://arxiv.org/pdf/2401.17609v1)|null|\n"}, "Nerf": {"2401.17895": "|**2024-01-31**|**ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural Radiance Fields**|ReplaceAnything3D\uff1a\u4f7f\u7528\u7ec4\u5408\u795e\u7ecf\u8f90\u5c04\u573a\u8fdb\u884c\u6587\u672c\u5f15\u5bfc\u7684 3D \u573a\u666f\u7f16\u8f91|Edward Bartrum, Thu Nguyen-Phuoc, Chris Xie, Zhengqin Li, Numair Khan, Armen Avetisyan, Douglas Lanman, Lei Xiao|We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene editing method that enables the replacement of specific objects within a scene. Given multi-view images of a scene, a text prompt describing the object to replace, and a text prompt describing the new object, our Erase-and-Replace approach can effectively swap objects in the scene with newly generated content while maintaining 3D consistency across multiple viewpoints. We demonstrate the versatility of ReplaceAnything3D by applying it to various realistic 3D scenes, showcasing results of modified foreground objects that are well-integrated with the rest of the scene without affecting its overall integrity.|\u6211\u4eec\u5f15\u5165 ReplaceAnything3D \u6a21\u578b (RAM3D)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6587\u672c\u5f15\u5bfc 3D \u573a\u666f\u7f16\u8f91\u65b9\u6cd5\uff0c\u53ef\u4ee5\u66ff\u6362\u573a\u666f\u4e2d\u7684\u7279\u5b9a\u5bf9\u8c61\u3002\u7ed9\u5b9a\u573a\u666f\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u3001\u63cf\u8ff0\u8981\u66ff\u6362\u7684\u5bf9\u8c61\u7684\u6587\u672c\u63d0\u793a\u4ee5\u53ca\u63cf\u8ff0\u65b0\u5bf9\u8c61\u7684\u6587\u672c\u63d0\u793a\uff0c\u6211\u4eec\u7684\u64e6\u9664\u548c\u66ff\u6362\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\u4e0e\u65b0\u751f\u6210\u7684\u5185\u5bb9\u4ea4\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u5bf9\u8c61\u7684 3D \u4e00\u81f4\u6027\u591a\u79cd\u89c2\u70b9\u3002\u6211\u4eec\u901a\u8fc7\u5c06 ReplaceAnything3D \u5e94\u7528\u4e8e\u5404\u79cd\u903c\u771f\u7684 3D \u573a\u666f\u6765\u5c55\u793a ReplaceAnything3D \u7684\u591a\u529f\u80fd\u6027\uff0c\u5c55\u793a\u4fee\u6539\u540e\u7684\u524d\u666f\u5bf9\u8c61\u7684\u7ed3\u679c\uff0c\u8fd9\u4e9b\u5bf9\u8c61\u4e0e\u573a\u666f\u7684\u5176\u4f59\u90e8\u5206\u5f88\u597d\u5730\u96c6\u6210\uff0c\u800c\u4e0d\u5f71\u54cd\u5176\u6574\u4f53\u5b8c\u6574\u6027\u3002|[2401.17895v1](http://arxiv.org/pdf/2401.17895v1)|null|\n"}, "3DGS": {}, "3D/CG": {}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2401.17766": "|**2024-01-31**|**Fine-Grained Zero-Shot Learning: Advances, Challenges, and Prospects**|\u7ec6\u7c92\u5ea6\u96f6\u6837\u672c\u5b66\u4e60\uff1a\u8fdb\u5c55\u3001\u6311\u6218\u548c\u524d\u666f|Jingcai Guo, Zhijie Rao, Song Guo, Jingren Zhou, Dacheng Tao|Recent zero-shot learning (ZSL) approaches have integrated fine-grained analysis, i.e., fine-grained ZSL, to mitigate the commonly known seen/unseen domain bias and misaligned visual-semantics mapping problems, and have made profound progress. Notably, this paradigm differs from existing close-set fine-grained methods and, therefore, can pose unique and nontrivial challenges. However, to the best of our knowledge, there remains a lack of systematic summaries of this topic. To enrich the literature of this domain and provide a sound basis for its future development, in this paper, we present a broad review of recent advances for fine-grained analysis in ZSL. Concretely, we first provide a taxonomy of existing methods and techniques with a thorough analysis of each category. Then, we summarize the benchmark, covering publicly available datasets, models, implementations, and some more details as a library. Last, we sketch out some related applications. In addition, we discuss vital challenges and suggest potential future directions.|\u6700\u8fd1\u7684\u96f6\u6837\u672c\u5b66\u4e60\uff08ZSL\uff09\u65b9\u6cd5\u96c6\u6210\u4e86\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u5373\u7ec6\u7c92\u5ea6 ZSL\uff0c\u4ee5\u51cf\u8f7b\u4f17\u6240\u5468\u77e5\u7684\u53ef\u89c1/\u4e0d\u53ef\u89c1\u9886\u57df\u504f\u5dee\u548c\u9519\u4f4d\u7684\u89c6\u89c9\u8bed\u4e49\u6620\u5c04\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u6df1\u8fdc\u7684\u8fdb\u5c55\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u79cd\u8303\u5f0f\u4e0d\u540c\u4e8e\u73b0\u6709\u7684\u7d27\u5bc6\u96c6\u7ec6\u7c92\u5ea6\u65b9\u6cd5\uff0c\u56e0\u6b64\u53ef\u80fd\u4f1a\u5e26\u6765\u72ec\u7279\u4e14\u91cd\u8981\u7684\u6311\u6218\u3002\u7136\u800c\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u4ecd\u7136\u7f3a\u4e4f\u5bf9\u8be5\u4e3b\u9898\u7684\u7cfb\u7edf\u603b\u7ed3\u3002\u4e3a\u4e86\u4e30\u5bcc\u8be5\u9886\u57df\u7684\u6587\u732e\u5e76\u4e3a\u5176\u672a\u6765\u7684\u53d1\u5c55\u63d0\u4f9b\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9 ZSL \u7ec6\u7c92\u5ea6\u5206\u6790\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u56de\u987e\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u63d0\u4f9b\u73b0\u6709\u65b9\u6cd5\u548c\u6280\u672f\u7684\u5206\u7c7b\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u7c7b\u522b\u8fdb\u884c\u5f7b\u5e95\u5206\u6790\u3002\u7136\u540e\uff0c\u6211\u4eec\u603b\u7ed3\u57fa\u51c6\uff0c\u6db5\u76d6\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u3001\u6a21\u578b\u3001\u5b9e\u73b0\u4ee5\u53ca\u4f5c\u4e3a\u5e93\u7684\u66f4\u591a\u7ec6\u8282\u3002\u6700\u540e\uff0c\u6211\u4eec\u52fe\u52d2\u51fa\u4e00\u4e9b\u76f8\u5173\u7684\u5e94\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86\u91cd\u5927\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u6f5c\u5728\u7684\u672a\u6765\u65b9\u5411\u3002|[2401.17766v1](http://arxiv.org/pdf/2401.17766v1)|null|\n"}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2401.17642": "|**2024-01-31**|**Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow**|\u63a2\u7d22\u591c\u95f4\u5149\u6d41\u7684\u5e38\u89c1\u5916\u89c2\u8fb9\u754c\u9002\u5e94|Hanyu Zhou, Yi Chang, Haoyue Liu, Wending Yan, Yuxing Duan, Zhiwei Shi, Luxin Yan|We investigate a challenging task of nighttime optical flow, which suffers from weakened texture and amplified noise. These degradations weaken discriminative visual features, thus causing invalid motion feature matching. Typically, existing methods employ domain adaptation to transfer knowledge from auxiliary domain to nighttime domain in either input visual space or output motion space. However, this direct adaptation is ineffective, since there exists a large domain gap due to the intrinsic heterogeneous nature of the feature representations between auxiliary and nighttime domains. To overcome this issue, we explore a common-latent space as the intermediate bridge to reinforce the feature alignment between auxiliary and nighttime domains. In this work, we exploit two auxiliary daytime and event domains, and propose a novel common appearance-boundary adaptation framework for nighttime optical flow. In appearance adaptation, we employ the intrinsic image decomposition to embed the auxiliary daytime image and the nighttime image into a reflectance-aligned common space. We discover that motion distributions of the two reflectance maps are very similar, benefiting us to consistently transfer motion appearance knowledge from daytime to nighttime domain. In boundary adaptation, we theoretically derive the motion correlation formula between nighttime image and accumulated events within a spatiotemporal gradient-aligned common space. We figure out that the correlation of the two spatiotemporal gradient maps shares significant discrepancy, benefitting us to contrastively transfer boundary knowledge from event to nighttime domain. Moreover, appearance adaptation and boundary adaptation are complementary to each other, since they could jointly transfer global motion and local boundary knowledge to the nighttime domain.|\u6211\u4eec\u7814\u7a76\u4e86\u591c\u95f4\u5149\u6d41\u7684\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u53d7\u5230\u7eb9\u7406\u51cf\u5f31\u548c\u566a\u58f0\u653e\u5927\u7684\u5f71\u54cd\u3002\u8fd9\u4e9b\u9000\u5316\u524a\u5f31\u4e86\u8fa8\u522b\u6027\u89c6\u89c9\u7279\u5f81\uff0c\u4ece\u800c\u5bfc\u81f4\u65e0\u6548\u7684\u8fd0\u52a8\u7279\u5f81\u5339\u914d\u3002\u901a\u5e38\uff0c\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u57df\u9002\u5e94\u5c06\u77e5\u8bc6\u4ece\u8f93\u5165\u89c6\u89c9\u7a7a\u95f4\u6216\u8f93\u51fa\u8fd0\u52a8\u7a7a\u95f4\u4e2d\u7684\u8f85\u52a9\u57df\u8f6c\u79fb\u5230\u591c\u95f4\u57df\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u76f4\u63a5\u9002\u5e94\u662f\u65e0\u6548\u7684\uff0c\u56e0\u4e3a\u7531\u4e8e\u8f85\u52a9\u57df\u548c\u591c\u95f4\u57df\u4e4b\u95f4\u7684\u7279\u5f81\u8868\u793a\u7684\u5185\u5728\u5f02\u6784\u6027\uff0c\u5b58\u5728\u5f88\u5927\u7684\u57df\u5dee\u8ddd\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u4e2a\u516c\u5171\u6f5c\u5728\u7a7a\u95f4\u4f5c\u4e3a\u4e2d\u95f4\u6865\u6881\uff0c\u4ee5\u52a0\u5f3a\u8f85\u52a9\u57df\u548c\u591c\u95f4\u57df\u4e4b\u95f4\u7684\u7279\u5f81\u5bf9\u9f50\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u4e24\u4e2a\u8f85\u52a9\u7684\u767d\u5929\u548c\u4e8b\u4ef6\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591c\u95f4\u5149\u6d41\u901a\u7528\u5916\u89c2\u8fb9\u754c\u9002\u5e94\u6846\u67b6\u3002\u5728\u5916\u89c2\u9002\u5e94\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u5c06\u8f85\u52a9\u767d\u5929\u56fe\u50cf\u548c\u591c\u95f4\u56fe\u50cf\u5d4c\u5165\u5230\u53cd\u5c04\u7387\u5bf9\u9f50\u7684\u516c\u5171\u7a7a\u95f4\u4e2d\u3002\u6211\u4eec\u53d1\u73b0\u4e24\u4e2a\u53cd\u5c04\u56fe\u7684\u8fd0\u52a8\u5206\u5e03\u975e\u5e38\u76f8\u4f3c\uff0c\u8fd9\u6709\u5229\u4e8e\u6211\u4eec\u4e00\u81f4\u5730\u5c06\u8fd0\u52a8\u5916\u89c2\u77e5\u8bc6\u4ece\u767d\u5929\u8f6c\u79fb\u5230\u591c\u95f4\u9886\u57df\u3002\u5728\u8fb9\u754c\u9002\u5e94\u4e2d\uff0c\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u63a8\u5bfc\u4e86\u591c\u95f4\u56fe\u50cf\u548c\u65f6\u7a7a\u68af\u5ea6\u5bf9\u9f50\u7684\u516c\u5171\u7a7a\u95f4\u5185\u7d2f\u79ef\u4e8b\u4ef6\u4e4b\u95f4\u7684\u8fd0\u52a8\u76f8\u5173\u516c\u5f0f\u3002\u6211\u4eec\u53d1\u73b0\u4e24\u4e2a\u65f6\u7a7a\u68af\u5ea6\u56fe\u7684\u76f8\u5173\u6027\u5b58\u5728\u663e\u7740\u5dee\u5f02\uff0c\u8fd9\u6709\u5229\u4e8e\u6211\u4eec\u5c06\u8fb9\u754c\u77e5\u8bc6\u4ece\u4e8b\u4ef6\u57df\u8f6c\u79fb\u5230\u591c\u95f4\u57df\u3002\u6b64\u5916\uff0c\u5916\u89c2\u9002\u5e94\u548c\u8fb9\u754c\u9002\u5e94\u662f\u76f8\u4e92\u8865\u5145\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u4ee5\u5171\u540c\u5c06\u5168\u5c40\u8fd0\u52a8\u548c\u5c40\u90e8\u8fb9\u754c\u77e5\u8bc6\u8f6c\u79fb\u5230\u591c\u95f4\u9886\u57df\u3002|[2401.17642v1](http://arxiv.org/pdf/2401.17642v1)|null|\n"}, "\u5176\u4ed6": {"2401.17883": "|**2024-01-31**|**Reimagining Reality: A Comprehensive Survey of Video Inpainting Techniques**|\u91cd\u65b0\u60f3\u8c61\u73b0\u5b9e\uff1a\u89c6\u9891\u4fee\u590d\u6280\u672f\u7684\u7efc\u5408\u8c03\u67e5|Shreyank N Gowda, Yash Thakre, Shashank Narayana Gowda, Xiaobo Jin|This paper offers a comprehensive analysis of recent advancements in video inpainting techniques, a critical subset of computer vision and artificial intelligence. As a process that restores or fills in missing or corrupted portions of video sequences with plausible content, video inpainting has evolved significantly with the advent of deep learning methodologies. Despite the plethora of existing methods and their swift development, the landscape remains complex, posing challenges to both novices and established researchers. Our study deconstructs major techniques, their underpinning theories, and their effective applications. Moreover, we conduct an exhaustive comparative study, centering on two often-overlooked dimensions: visual quality and computational efficiency. We adopt a human-centric approach to assess visual quality, enlisting a panel of annotators to evaluate the output of different video inpainting techniques. This provides a nuanced qualitative understanding that complements traditional quantitative metrics. Concurrently, we delve into the computational aspects, comparing inference times and memory demands across a standardized hardware setup. This analysis underscores the balance between quality and efficiency: a critical consideration for practical applications where resources may be constrained. By integrating human validation and computational resource comparison, this survey not only clarifies the present landscape of video inpainting techniques but also charts a course for future explorations in this vibrant and evolving field.|\u672c\u6587\u5bf9\u89c6\u9891\u4fee\u590d\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u89c6\u9891\u4fee\u590d\u6280\u672f\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u4eba\u5de5\u667a\u80fd\u7684\u4e00\u4e2a\u91cd\u8981\u5b50\u96c6\u3002\u4f5c\u4e3a\u4e00\u79cd\u7528\u53ef\u4fe1\u5185\u5bb9\u6062\u590d\u6216\u586b\u5145\u89c6\u9891\u5e8f\u5217\u4e2d\u4e22\u5931\u6216\u635f\u574f\u90e8\u5206\u7684\u8fc7\u7a0b\uff0c\u89c6\u9891\u4fee\u590d\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u51fa\u73b0\u800c\u53d1\u751f\u4e86\u663e\u7740\u53d1\u5c55\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u4f17\u591a\u4e14\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u60c5\u51b5\u4ecd\u7136\u590d\u6742\uff0c\u5bf9\u65b0\u624b\u548c\u6210\u719f\u7684\u7814\u7a76\u4eba\u5458\u90fd\u63d0\u51fa\u4e86\u6311\u6218\u3002\u6211\u4eec\u7684\u7814\u7a76\u89e3\u6784\u4e86\u4e3b\u8981\u6280\u672f\u3001\u5176\u57fa\u7840\u7406\u8bba\u53ca\u5176\u6709\u6548\u5e94\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u8be6\u5c3d\u7684\u6bd4\u8f83\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u4e24\u4e2a\u7ecf\u5e38\u88ab\u5ffd\u89c6\u7684\u7ef4\u5ea6\uff1a\u89c6\u89c9\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002\u6211\u4eec\u91c7\u7528\u4ee5\u4eba\u4e3a\u672c\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u89c6\u89c9\u8d28\u91cf\uff0c\u62db\u52df\u4e00\u7ec4\u6ce8\u91ca\u8005\u6765\u8bc4\u4f30\u4e0d\u540c\u89c6\u9891\u4fee\u590d\u6280\u672f\u7684\u8f93\u51fa\u3002\u8fd9\u63d0\u4f9b\u4e86\u7ec6\u81f4\u5165\u5fae\u7684\u5b9a\u6027\u7406\u89e3\uff0c\u8865\u5145\u4e86\u4f20\u7edf\u7684\u5b9a\u91cf\u6307\u6807\u3002\u540c\u65f6\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u8ba1\u7b97\u65b9\u9762\uff0c\u6bd4\u8f83\u6807\u51c6\u5316\u786c\u4ef6\u8bbe\u7f6e\u7684\u63a8\u7406\u65f6\u95f4\u548c\u5185\u5b58\u9700\u6c42\u3002\u8be5\u5206\u6790\u5f3a\u8c03\u4e86\u8d28\u91cf\u548c\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\uff1a\u5bf9\u4e8e\u8d44\u6e90\u53ef\u80fd\u53d7\u5230\u9650\u5236\u7684\u5b9e\u9645\u5e94\u7528\u6765\u8bf4\uff0c\u8fd9\u662f\u4e00\u4e2a\u5173\u952e\u7684\u8003\u8651\u56e0\u7d20\u3002\u901a\u8fc7\u6574\u5408\u4eba\u7c7b\u9a8c\u8bc1\u548c\u8ba1\u7b97\u8d44\u6e90\u6bd4\u8f83\uff0c\u8fd9\u9879\u8c03\u67e5\u4e0d\u4ec5\u9610\u660e\u4e86\u89c6\u9891\u4fee\u590d\u6280\u672f\u7684\u73b0\u72b6\uff0c\u8fd8\u4e3a\u8fd9\u4e2a\u5145\u6ee1\u6d3b\u529b\u548c\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\u7684\u672a\u6765\u63a2\u7d22\u5236\u5b9a\u4e86\u8def\u7ebf\u3002|[2401.17883v1](http://arxiv.org/pdf/2401.17883v1)|null|\n", "2401.17790": "|**2024-01-31**|**RADIN: Souping on a Budget**|RADIN\uff1a\u9884\u7b97\u4e2d\u7684\u6c64|Thibaut Menes, Olivier Risser-Maroix|Model Soups, extending Stochastic Weights Averaging (SWA), combine models fine-tuned with different hyperparameters. Yet, their adoption is hindered by computational challenges due to subset selection issues. In this paper, we propose to speed up model soups by approximating soups performance using averaged ensemble logits performances. Theoretical insights validate the congruence between ensemble logits and weight averaging soups across any mixing ratios. Our Resource ADjusted soups craftINg (RADIN) procedure stands out by allowing flexible evaluation budgets, enabling users to adjust his budget of exploration adapted to his resources while increasing performance at lower budget compared to previous greedy approach (up to 4% on ImageNet).|Model Soups \u6269\u5c55\u4e86\u968f\u673a\u6743\u91cd\u5e73\u5747 (SWA)\uff0c\u7ed3\u5408\u4e86\u4f7f\u7528\u4e0d\u540c\u8d85\u53c2\u6570\u8fdb\u884c\u5fae\u8c03\u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5b50\u96c6\u9009\u62e9\u95ee\u9898\uff0c\u5b83\u4eec\u7684\u91c7\u7528\u53d7\u5230\u8ba1\u7b97\u6311\u6218\u7684\u963b\u788d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u4f7f\u7528\u5e73\u5747\u96c6\u6210 Logits \u6027\u80fd\u6765\u8fd1\u4f3c soups \u6027\u80fd\u6765\u52a0\u901f\u6a21\u578b soups\u3002\u7406\u8bba\u89c1\u89e3\u9a8c\u8bc1\u4e86\u4efb\u4f55\u6df7\u5408\u6bd4\u4e0b\u7684\u6574\u4f53\u903b\u8f91\u4e0e\u91cd\u91cf\u5e73\u5747\u6c64\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u6211\u4eec\u7684\u8d44\u6e90\u8c03\u6574\u6c64\u5236\u4f5c (RADIN) \u7a0b\u5e8f\u56e0\u5141\u8bb8\u7075\u6d3b\u7684\u8bc4\u4f30\u9884\u7b97\u800c\u8131\u9896\u800c\u51fa\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u8c03\u6574\u9002\u5408\u5176\u8d44\u6e90\u7684\u63a2\u7d22\u9884\u7b97\uff0c\u540c\u65f6\u4e0e\u4e4b\u524d\u7684\u8d2a\u5a6a\u65b9\u6cd5\u76f8\u6bd4\uff08\u5728 ImageNet \u4e0a\u9ad8\u8fbe 4%\uff09\u4ee5\u8f83\u4f4e\u7684\u9884\u7b97\u63d0\u9ad8\u6027\u80fd\u3002|[2401.17790v1](http://arxiv.org/pdf/2401.17790v1)|null|\n", "2401.17789": "|**2024-01-31**|**Robustly overfitting latents for flexible neural image compression**|\u9c81\u68d2\u5730\u8fc7\u5ea6\u62df\u5408\u6f5c\u5728\u7684\u7075\u6d3b\u795e\u7ecf\u56fe\u50cf\u538b\u7f29|Yura Perugachi-Diaz, Arwin Gansekoele, Sandjai Bhulai|Neural image compression has made a great deal of progress. State-of-the-art models are based on variational autoencoders and are outperforming classical models. Neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image. While these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity. Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the latents of pre-trained neural image compression models. We extend this idea by introducing SGA+, which contains three different methods that build upon SGA. Further, we give a detailed analysis of our proposed methods, show how they improve performance, and show that they are less sensitive to hyperparameter choices. Besides, we show how each method can be extended to three- instead of two-class rounding. Finally, we show how refinement of the latents with our best-performing method improves the compression performance on the Tecnick dataset and how it can be deployed to partly move along the rate-distortion curve.|\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u5df2\u7ecf\u53d6\u5f97\u4e86\u5f88\u5927\u7684\u8fdb\u6b65\u3002\u6700\u5148\u8fdb\u7684\u6a21\u578b\u57fa\u4e8e\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\u3002\u795e\u7ecf\u538b\u7f29\u6a21\u578b\u5b66\u4e60\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u91cf\u5316\u7684\u6f5c\u5728\u8868\u793a\uff0c\u8be5\u8868\u793a\u53ef\u4ee5\u6709\u6548\u5730\u53d1\u9001\u5230\u89e3\u7801\u5668\uff0c\u89e3\u7801\u5668\u5c06\u91cf\u5316\u7684\u6f5c\u5728\u8868\u793a\u89e3\u7801\u4e3a\u91cd\u5efa\u56fe\u50cf\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u8df5\u4e2d\u88ab\u8bc1\u660e\u662f\u6210\u529f\u7684\uff0c\u4f46\u7531\u4e8e\u4e0d\u5b8c\u5584\u7684\u4f18\u5316\u4ee5\u53ca\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5bb9\u91cf\u7684\u9650\u5236\uff0c\u5b83\u4eec\u5bfc\u81f4\u4e86\u6b21\u4f18\u7ed3\u679c\u3002\u6700\u8fd1\u7684\u5de5\u4f5c\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u968f\u673a Gumbel \u9000\u706b (SGA) \u6765\u7ec6\u5316\u9884\u8bad\u7ec3\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u7684\u6f5c\u529b\u3002\u6211\u4eec\u901a\u8fc7\u5f15\u5165 SGA+ \u6765\u6269\u5c55\u8fd9\u4e2a\u60f3\u6cd5\uff0c\u5b83\u5305\u542b\u4e09\u79cd\u57fa\u4e8e SGA \u7684\u4e0d\u540c\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5bf9\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5982\u4f55\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u8868\u660e\u5b83\u4eec\u5bf9\u8d85\u53c2\u6570\u9009\u62e9\u4e0d\u592a\u654f\u611f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u6bcf\u79cd\u65b9\u6cd5\u6269\u5c55\u5230\u4e09\u7ea7\u820d\u5165\u800c\u4e0d\u662f\u4e24\u7ea7\u820d\u5165\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u6027\u80fd\u6700\u4f73\u7684\u65b9\u6cd5\u5bf9\u6f5c\u5728\u7279\u5f81\u8fdb\u884c\u7ec6\u5316\uff0c\u4ece\u800c\u63d0\u9ad8 Tecnick \u6570\u636e\u96c6\u7684\u538b\u7f29\u6027\u80fd\uff0c\u4ee5\u53ca\u5982\u4f55\u90e8\u7f72\u5b83\u4ee5\u90e8\u5206\u6cbf\u7740\u7387\u5931\u771f\u66f2\u7ebf\u79fb\u52a8\u3002|[2401.17789v1](http://arxiv.org/pdf/2401.17789v1)|null|\n", "2401.17728": "|**2024-01-31**|**COMET: Contrastive Mean Teacher for Online Source-Free Universal Domain Adaptation**|COMET\uff1a\u5728\u7ebf\u65e0\u6e90\u901a\u7528\u57df\u9002\u5e94\u7684\u5bf9\u6bd4\u5e73\u5747\u8001\u5e08|Pascal Schlachter, Bin Yang|In real-world applications, there is often a domain shift from training to test data. This observation resulted in the development of test-time adaptation (TTA). It aims to adapt a pre-trained source model to the test data without requiring access to the source data. Thereby, most existing works are limited to the closed-set assumption, i.e. there is no category shift between source and target domain. We argue that in a realistic open-world setting a category shift can appear in addition to a domain shift. This means, individual source classes may not appear in the target domain anymore, samples of new classes may be part of the target domain or even both at the same time. Moreover, in many real-world scenarios the test data is not accessible all at once but arrives sequentially as a stream of batches demanding an immediate prediction. Hence, TTA must be applied in an online manner. To the best of our knowledge, the combination of these aspects, i.e. online source-free universal domain adaptation (online SF-UniDA), has not been studied yet. In this paper, we introduce a Contrastive Mean Teacher (COMET) tailored to this novel scenario. It applies a contrastive loss to rebuild a feature space where the samples of known classes build distinct clusters and the samples of new classes separate well from them. It is complemented by an entropy loss which ensures that the classifier output has a small entropy for samples of known classes and a large entropy for samples of new classes to be easily detected and rejected as unknown. To provide the losses with reliable pseudo labels, they are embedded into a mean teacher (MT) framework. We evaluate our method across two datasets and all category shifts to set an initial benchmark for online SF-UniDA. Thereby, COMET yields state-of-the-art performance and proves to be consistent and robust across a variety of different scenarios.|\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u901a\u5e38\u4f1a\u51fa\u73b0\u4ece\u8bad\u7ec3\u6570\u636e\u5230\u6d4b\u8bd5\u6570\u636e\u7684\u9886\u57df\u8f6c\u53d8\u3002\u8fd9\u4e00\u89c2\u5bdf\u5bfc\u81f4\u4e86\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\uff08TTA\uff09\u7684\u53d1\u5c55\u3002\u5b83\u7684\u76ee\u7684\u662f\u4f7f\u9884\u5148\u8bad\u7ec3\u7684\u6e90\u6a21\u578b\u9002\u5e94\u6d4b\u8bd5\u6570\u636e\uff0c\u800c\u4e0d\u9700\u8981\u8bbf\u95ee\u6e90\u6570\u636e\u3002\u56e0\u6b64\uff0c\u5927\u591a\u6570\u73b0\u6709\u5de5\u4f5c\u4ec5\u9650\u4e8e\u95ed\u96c6\u5047\u8bbe\uff0c\u5373\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u6ca1\u6709\u7c7b\u522b\u8f6c\u6362\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u5728\u73b0\u5b9e\u7684\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\uff0c\u9664\u4e86\u9886\u57df\u8f6c\u79fb\u4e4b\u5916\uff0c\u8fd8\u53ef\u80fd\u51fa\u73b0\u7c7b\u522b\u8f6c\u79fb\u3002\u8fd9\u610f\u5473\u7740\uff0c\u5355\u4e2a\u6e90\u7c7b\u53ef\u80fd\u4e0d\u518d\u51fa\u73b0\u5728\u76ee\u6807\u57df\u4e2d\uff0c\u65b0\u7c7b\u7684\u6837\u672c\u53ef\u80fd\u662f\u76ee\u6807\u57df\u7684\u4e00\u90e8\u5206\uff0c\u751a\u81f3\u540c\u65f6\u662f\u4e24\u8005\u3002\u6b64\u5916\uff0c\u5728\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u6d4b\u8bd5\u6570\u636e\u65e0\u6cd5\u4e00\u6b21\u6027\u5168\u90e8\u8bbf\u95ee\uff0c\u800c\u662f\u4f5c\u4e3a\u9700\u8981\u7acb\u5373\u9884\u6d4b\u7684\u6279\u6b21\u6d41\u6309\u987a\u5e8f\u5230\u8fbe\u3002\u56e0\u6b64\uff0cTTA\u5fc5\u987b\u4ee5\u5728\u7ebf\u65b9\u5f0f\u7533\u8bf7\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u4e9b\u65b9\u9762\u7684\u7ec4\u5408\uff0c\u5373\u5728\u7ebf\u65e0\u6e90\u901a\u7528\u57df\u9002\u5e94\uff08online SF-UniDA\uff09\uff0c\u5c1a\u672a\u88ab\u7814\u7a76\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u9488\u5bf9\u8fd9\u79cd\u65b0\u9896\u573a\u666f\u91cf\u8eab\u5b9a\u5236\u7684\u5bf9\u6bd4\u5e73\u5747\u6559\u5e08\uff08COMET\uff09\u3002\u5b83\u5e94\u7528\u5bf9\u6bd4\u635f\u5931\u6765\u91cd\u5efa\u7279\u5f81\u7a7a\u95f4\uff0c\u5176\u4e2d\u5df2\u77e5\u7c7b\u7684\u6837\u672c\u6784\u5efa\u4e0d\u540c\u7684\u805a\u7c7b\uff0c\u800c\u65b0\u7c7b\u7684\u6837\u672c\u4e0e\u5b83\u4eec\u5f88\u597d\u5730\u5206\u79bb\u3002\u5b83\u7531\u71b5\u635f\u5931\u6765\u8865\u5145\uff0c\u71b5\u635f\u5931\u786e\u4fdd\u5206\u7c7b\u5668\u8f93\u51fa\u5bf9\u4e8e\u5df2\u77e5\u7c7b\u522b\u7684\u6837\u672c\u5177\u6709\u8f83\u5c0f\u7684\u71b5\uff0c\u5bf9\u4e8e\u65b0\u7c7b\u522b\u7684\u6837\u672c\u5177\u6709\u8f83\u5927\u7684\u71b5\uff0c\u4ee5\u4fbf\u6613\u4e8e\u68c0\u6d4b\u5e76\u62d2\u7edd\u4e3a\u672a\u77e5\u3002\u4e3a\u4e86\u7ed9\u635f\u5931\u63d0\u4f9b\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\uff0c\u5b83\u4eec\u88ab\u5d4c\u5165\u5230\u5e73\u5747\u6559\u5e08\uff08MT\uff09\u6846\u67b6\u4e2d\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u548c\u6240\u6709\u7c7b\u522b\u53d8\u5316\u4e2d\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u4e3a\u5728\u7ebf SF-UniDA \u8bbe\u7f6e\u521d\u59cb\u57fa\u51c6\u3002\u56e0\u6b64\uff0cCOMET \u4ea7\u751f\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u5728\u5404\u79cd\u4e0d\u540c\u7684\u573a\u666f\u4e2d\u90fd\u4fdd\u6301\u4e00\u81f4\u548c\u7a33\u5065\u3002|[2401.17728v1](http://arxiv.org/pdf/2401.17728v1)|null|\n", "2401.17617": "|**2024-01-31**|**Unveiling the Power of Self-supervision for Multi-view Multi-human Association and Tracking**|\u63ed\u793a\u591a\u89c6\u89d2\u591a\u4eba\u5173\u8054\u548c\u8ddf\u8e2a\u7684\u81ea\u6211\u76d1\u7763\u529b\u91cf|Wei Feng, Feifan Wang, Ruize Han, Zekun Qian, Song Wang|Multi-view multi-human association and tracking (MvMHAT), is a new but important problem for multi-person scene video surveillance, aiming to track a group of people over time in each view, as well as to identify the same person across different views at the same time, which is different from previous MOT and multi-camera MOT tasks only considering the over-time human tracking. This way, the videos for MvMHAT require more complex annotations while containing more information for self learning. In this work, we tackle this problem with a self-supervised learning aware end-to-end network. Specifically, we propose to take advantage of the spatial-temporal self-consistency rationale by considering three properties of reflexivity, symmetry and transitivity. Besides the reflexivity property that naturally holds, we design the self-supervised learning losses based on the properties of symmetry and transitivity, for both appearance feature learning and assignment matrix optimization, to associate the multiple humans over time and across views. Furthermore, to promote the research on MvMHAT, we build two new large-scale benchmarks for the network training and testing of different algorithms. Extensive experiments on the proposed benchmarks verify the effectiveness of our method. We have released the benchmark and code to the public.|\u591a\u89c6\u56fe\u591a\u4eba\u5173\u8054\u4e0e\u8ddf\u8e2a\uff08MvMHAT\uff09\u662f\u591a\u4eba\u573a\u666f\u89c6\u9891\u76d1\u63a7\u7684\u4e00\u4e2a\u65b0\u4f46\u91cd\u8981\u7684\u95ee\u9898\uff0c\u65e8\u5728\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u5728\u6bcf\u4e2a\u89c6\u56fe\u4e2d\u8ddf\u8e2a\u4e00\u7fa4\u4eba\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u7684\u89c6\u56fe\u4e2d\u8bc6\u522b\u540c\u4e00\u4e2a\u4eba\u4e0e\u4e4b\u524d\u7684 MOT \u548c\u591a\u6444\u50cf\u5934 MOT \u4efb\u52a1\u53ea\u8003\u8651\u8d85\u65f6\u7684\u4eba\u4f53\u8ddf\u8e2a\u4e0d\u540c\u3002\u8fd9\u6837\uff0cMvMHAT \u7684\u89c6\u9891\u9700\u8981\u66f4\u590d\u6742\u7684\u6ce8\u91ca\uff0c\u540c\u65f6\u5305\u542b\u66f4\u591a\u7528\u4e8e\u81ea\u5b66\u4e60\u7684\u4fe1\u606f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u611f\u77e5\u7aef\u5230\u7aef\u7f51\u7edc\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u8003\u8651\u81ea\u53cd\u6027\u3001\u5bf9\u79f0\u6027\u548c\u4f20\u9012\u6027\u4e09\u4e2a\u5c5e\u6027\u6765\u5229\u7528\u65f6\u7a7a\u81ea\u6d3d\u539f\u7406\u3002\u9664\u4e86\u81ea\u7136\u5b58\u5728\u7684\u81ea\u53cd\u6027\u5c5e\u6027\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u57fa\u4e8e\u5bf9\u79f0\u6027\u548c\u4f20\u9012\u6027\u7684\u5c5e\u6027\u8bbe\u8ba1\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u635f\u5931\uff0c\u7528\u4e8e\u5916\u89c2\u7279\u5f81\u5b66\u4e60\u548c\u5206\u914d\u77e9\u9635\u4f18\u5316\uff0c\u4ee5\u968f\u65f6\u95f4\u548c\u8de8\u89c6\u56fe\u5173\u8054\u591a\u4e2a\u4eba\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u4fc3\u8fdb MvMHAT \u7684\u7814\u7a76\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e24\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u7528\u4e8e\u4e0d\u540c\u7b97\u6cd5\u7684\u7f51\u7edc\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u5bf9\u6240\u63d0\u51fa\u7684\u57fa\u51c6\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5df2\u5411\u516c\u4f17\u53d1\u5e03\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ee3\u7801\u3002|[2401.17617v1](http://arxiv.org/pdf/2401.17617v1)|null|\n", "2401.17583": "|**2024-01-31**|**Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion**|\u654f\u6377\u4f46\u5b89\u5168\uff1a\u5b66\u4e60\u65e0\u78b0\u649e\u9ad8\u901f\u817f\u5f0f\u8fd0\u52a8|Tairan He, Chong Zhang, Wenli Xiao, Guanqi He, Changliu Liu, Guanya Shi|Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (< 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception representation network, all in simulation. These trained modules can be directly deployed in the real world with onboard sensing and computation, leading to high-speed and collision-free navigation in confined indoor and outdoor spaces with both static and dynamic obstacles.|\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u817f\u5f0f\u673a\u5668\u4eba\u5fc5\u987b\u540c\u65f6\u654f\u6377\uff0c\u4ee5\u9ad8\u6548\u6267\u884c\u4efb\u52a1\uff0c\u5e76\u5b89\u5168\u5730\u907f\u514d\u4e0e\u969c\u788d\u7269\u6216\u4eba\u7c7b\u53d1\u751f\u78b0\u649e\u3002\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5f00\u53d1\u4fdd\u5b88\u7684\u63a7\u5236\u5668\uff08< 1.0 m/s\uff09\u4ee5\u786e\u4fdd\u5b89\u5168\uff0c\u8981\u4e48\u4e13\u6ce8\u4e8e\u654f\u6377\u6027\u800c\u4e0d\u8003\u8651\u6f5c\u5728\u7684\u81f4\u547d\u78b0\u649e\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u654f\u6377\u4f46\u5b89\u5168\uff08ABS\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u6846\u67b6\uff0c\u53ef\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u654f\u6377\u4e14\u65e0\u78b0\u649e\u7684\u8fd0\u52a8\u3002 ABS \u6d89\u53ca\u5728\u969c\u788d\u7269\u4e2d\u6267\u884c\u654f\u6377\u8fd0\u52a8\u6280\u80fd\u7684\u654f\u6377\u7b56\u7565\u548c\u9632\u6b62\u6545\u969c\u7684\u6062\u590d\u7b56\u7565\uff0c\u534f\u540c\u5b9e\u73b0\u9ad8\u901f\u548c\u65e0\u78b0\u649e\u5bfc\u822a\u3002 ABS \u4e2d\u7684\u7b56\u7565\u5207\u6362\u7531\u5b66\u4e60\u63a7\u5236\u7406\u8bba\u7684\u5230\u8fbe\u907f\u514d\u4ef7\u503c\u7f51\u7edc\u63a7\u5236\uff0c\u8be5\u7f51\u7edc\u8fd8\u5c06\u6062\u590d\u7b56\u7565\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u6307\u5bfc\uff0c\u4ece\u800c\u5728\u95ed\u73af\u4e2d\u4fdd\u62a4\u673a\u5668\u4eba\u3002\u8bad\u7ec3\u8fc7\u7a0b\u6d89\u53ca\u5b66\u4e60\u654f\u6377\u7b56\u7565\u3001\u907f\u514d\u89e6\u53ca\u4ef7\u503c\u7f51\u7edc\u3001\u6062\u590d\u7b56\u7565\u548c\u5916\u90e8\u611f\u77e5\u8868\u793a\u7f51\u7edc\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u662f\u5728\u6a21\u62df\u4e2d\u8fdb\u884c\u7684\u3002\u8fd9\u4e9b\u8bad\u7ec3\u6709\u7d20\u7684\u6a21\u5757\u53ef\u4ee5\u901a\u8fc7\u673a\u8f7d\u4f20\u611f\u548c\u8ba1\u7b97\u76f4\u63a5\u90e8\u7f72\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u4ece\u800c\u5728\u5177\u6709\u9759\u6001\u548c\u52a8\u6001\u969c\u788d\u7269\u7684\u6709\u9650\u5ba4\u5185\u548c\u5ba4\u5916\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u9ad8\u901f\u3001\u65e0\u78b0\u649e\u5bfc\u822a\u3002|[2401.17583v1](http://arxiv.org/pdf/2401.17583v1)|null|\n", "2401.17571": "|**2024-01-31**|**Is Registering Raw Tagged-MR Enough for Strain Estimation in the Era of Deep Learning?**|\u6ce8\u518cRaw Tagged-MR\u8db3\u4ee5\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\u7684\u5e94\u53d8\u4f30\u8ba1\u5417\uff1f|Zhangxing Bian, Ahmed Alshareef, Shuwen Wei, Junyu Chen, Yuli Wang, Jonghye Woo, Dzung L. Pham, Jiachen Zhuo, Aaron Carass, Jerry L. Prince|Magnetic Resonance Imaging with tagging (tMRI) has long been utilized for quantifying tissue motion and strain during deformation. However, a phenomenon known as tag fading, a gradual decrease in tag visibility over time, often complicates post-processing. The first contribution of this study is to model tag fading by considering the interplay between $T_1$ relaxation and the repeated application of radio frequency (RF) pulses during serial imaging sequences. This is a factor that has been overlooked in prior research on tMRI post-processing. Further, we have observed an emerging trend of utilizing raw tagged MRI within a deep learning-based (DL) registration framework for motion estimation. In this work, we evaluate and analyze the impact of commonly used image similarity objectives in training DL registrations on raw tMRI. This is then compared with the Harmonic Phase-based approach, a traditional approach which is claimed to be robust to tag fading. Our findings, derived from both simulated images and an actual phantom scan, reveal the limitations of various similarity losses in raw tMRI and emphasize caution in registration tasks where image intensity changes over time.|\u5e26\u6807\u8bb0\u7684\u78c1\u5171\u632f\u6210\u50cf (tMRI) \u957f\u671f\u4ee5\u6765\u4e00\u76f4\u7528\u4e8e\u91cf\u5316\u53d8\u5f62\u8fc7\u7a0b\u4e2d\u7684\u7ec4\u7ec7\u8fd0\u52a8\u548c\u5e94\u53d8\u3002\u7136\u800c\uff0c\u4e00\u79cd\u79f0\u4e3a\u6807\u7b7e\u892a\u8272\u7684\u73b0\u8c61\uff0c\u5373\u6807\u7b7e\u53ef\u89c1\u6027\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u9010\u6e10\u964d\u4f4e\uff0c\u901a\u5e38\u4f1a\u4f7f\u540e\u5904\u7406\u53d8\u5f97\u590d\u6742\u3002\u8fd9\u9879\u7814\u7a76\u7684\u7b2c\u4e00\u4e2a\u8d21\u732e\u662f\u901a\u8fc7\u8003\u8651\u4e32\u884c\u6210\u50cf\u5e8f\u5217\u671f\u95f4 $T_1$ \u5f1b\u8c6b\u548c\u5c04\u9891 (RF) \u8109\u51b2\u91cd\u590d\u5e94\u7528\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u6765\u5bf9\u6807\u7b7e\u8870\u843d\u8fdb\u884c\u5efa\u6a21\u3002\u8fd9\u662f\u5148\u524d tMRI \u540e\u5904\u7406\u7814\u7a76\u4e2d\u88ab\u5ffd\u89c6\u7684\u4e00\u4e2a\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\u5728\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60 (DL) \u7684\u914d\u51c6\u6846\u67b6\u4e2d\u5229\u7528\u539f\u59cb\u6807\u8bb0 MRI \u8fdb\u884c\u8fd0\u52a8\u4f30\u8ba1\u7684\u65b0\u5174\u8d8b\u52bf\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8bc4\u4f30\u548c\u5206\u6790\u4e86\u5728\u539f\u59cb tMRI \u4e0a\u8bad\u7ec3 DL \u914d\u51c6\u65f6\u5e38\u7528\u7684\u56fe\u50cf\u76f8\u4f3c\u6027\u76ee\u6807\u7684\u5f71\u54cd\u3002\u7136\u540e\u5c06\u5176\u4e0e\u57fa\u4e8e\u8c10\u6ce2\u76f8\u4f4d\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u8fd9\u662f\u4e00\u79cd\u636e\u79f0\u5bf9\u6807\u7b7e\u8870\u843d\u5177\u6709\u9c81\u68d2\u6027\u7684\u4f20\u7edf\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u6765\u81ea\u6a21\u62df\u56fe\u50cf\u548c\u5b9e\u9645\u7684\u6a21\u578b\u626b\u63cf\uff0c\u63ed\u793a\u4e86\u539f\u59cb tMRI \u4e2d\u5404\u79cd\u76f8\u4f3c\u6027\u635f\u5931\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u5728\u56fe\u50cf\u5f3a\u5ea6\u968f\u65f6\u95f4\u53d8\u5316\u7684\u914d\u51c6\u4efb\u52a1\u4e2d\u8981\u5c0f\u5fc3\u3002|[2401.17571v1](http://arxiv.org/pdf/2401.17571v1)|null|\n", "2401.17542": "|**2024-01-31**|**Data-Effective Learning: A Comprehensive Medical Benchmark**|\u6570\u636e\u6709\u6548\u5b66\u4e60\uff1a\u7efc\u5408\u533b\u5b66\u57fa\u51c6|Wenxuan Yang, Weimin Tan, Yuqi Sun, Bo Yan|Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive experimental results show the baseline MedDEL can achieve performance comparable to the original large dataset with only 5% of the data. Establishing such an open data-effective learning benchmark is crucial for the medical AI research community because it facilitates efficient data use, promotes collaborative breakthroughs, and fosters the development of cost-effective, scalable, and impactful healthcare solutions. The project can be accessed at https://github.com/shadow2469/Data-Effective-Learning-A-Comprehensive-Medical-Benchmark.git.|\u6570\u636e\u6709\u6548\u5b66\u4e60\u65e8\u5728\u4ee5\u6700\u6709\u5f71\u54cd\u529b\u7684\u65b9\u5f0f\u4f7f\u7528\u6570\u636e\u6765\u8bad\u7ec3\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff0c\u5176\u4e2d\u6d89\u53ca\u6ce8\u91cd\u6570\u636e\u8d28\u91cf\u800c\u4e0d\u662f\u6570\u91cf\u7684\u7b56\u7565\uff0c\u786e\u4fdd\u7528\u4e8e\u8bad\u7ec3\u7684\u6570\u636e\u5177\u6709\u8f83\u9ad8\u7684\u4fe1\u606f\u4ef7\u503c\u3002\u6570\u636e\u6709\u6548\u5b66\u4e60\u5728\u52a0\u901f\u4eba\u5de5\u667a\u80fd\u8bad\u7ec3\u3001\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u8282\u7701\u6570\u636e\u5b58\u50a8\u65b9\u9762\u53d1\u6325\u7740\u6df1\u8fdc\u7684\u4f5c\u7528\uff0c\u8fd9\u5728\u8fd1\u5e74\u6765\u533b\u7597\u6570\u636e\u91cf\u7684\u589e\u957f\u8d85\u51fa\u4e86\u8bb8\u591a\u4eba\u7684\u9884\u671f\u7684\u60c5\u51b5\u4e0b\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u548c\u7efc\u5408\u57fa\u51c6\uff0c\u533b\u5b66\u6570\u636e\u6709\u6548\u5b66\u4e60\u7684\u7814\u7a76\u8fd8\u5f88\u5c11\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u7684\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u533b\u5b66\u9886\u57df\u6570\u636e\u6709\u6548\u5b66\u4e60\u7684\u7efc\u5408\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5305\u62ec\u6765\u81ea 31 \u4e2a\u533b\u7597\u4e2d\u5fc3\u7684\u6570\u767e\u4e07\u6570\u636e\u6837\u672c\u7684\u6570\u636e\u96c6 (DataDEL)\u3001\u6bd4\u8f83\u57fa\u7ebf\u65b9\u6cd5 (MedDEL) \u548c\u65b0\u7684\u8bc4\u4f30\u6307\u6807 (NormDEL)\uff0c\u4ee5\u5ba2\u89c2\u5730\u8861\u91cf\u6570\u636e\u6709\u6548\u7684\u5b66\u4e60\u7ee9\u6548\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u7ebf MedDEL \u4ec5\u9700 5% \u7684\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u4e0e\u539f\u59cb\u5927\u578b\u6570\u636e\u96c6\u76f8\u5f53\u7684\u6027\u80fd\u3002\u5efa\u7acb\u8fd9\u6837\u4e00\u4e2a\u5f00\u653e\u7684\u6570\u636e\u6709\u6548\u7684\u5b66\u4e60\u57fa\u51c6\u5bf9\u4e8e\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u793e\u533a\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u6709\u52a9\u4e8e\u6709\u6548\u7684\u6570\u636e\u4f7f\u7528\uff0c\u4fc3\u8fdb\u534f\u4f5c\u7a81\u7834\uff0c\u5e76\u4fc3\u8fdb\u5f00\u53d1\u5177\u6709\u6210\u672c\u6548\u76ca\u3001\u53ef\u6269\u5c55\u548c\u6709\u5f71\u54cd\u529b\u7684\u533b\u7597\u4fdd\u5065\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u9879\u76ee\u53ef\u4ee5\u901a\u8fc7 https://github.com/shadow2469/Data-Effective-Learning-A-Compressive-Medical-Benchmark.git \u8bbf\u95ee\u3002|[2401.17542v1](http://arxiv.org/pdf/2401.17542v1)|null|\n"}}